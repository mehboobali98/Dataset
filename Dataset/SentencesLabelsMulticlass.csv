Sentence,label
"To address the need for a large and high - quality dataset for a new complex and cross-domain semantic parsing task , we introduce Spider , which consists of 200 databases with multiple tables , 10,181 questions , and 5,693 corresponding complex SQL queries , all written by 11 college students spending a total of 1,000 man-hours .",dataset
"As illustrates , given a database with multiple tables including foreign keys , our corpus creates and annotates complex questions and SQL queries including different SQL clauses such as joining and nested query .",dataset
"The performances of the Seq2Seq - based basic models including Seq2Seq , Seq2Seq + Attention , and Seq2Seq + Copying are very low .",result
"In contrast , SQLNet and TypeSQL that utilize SQL structure information to guide the SQL generation process significantly outperform other Seq2Seq models .",result
"As Component Matching results in shows , all models struggle with WHERE clause prediction the most .",result
"In general , the over all performances of all models are low , indicating that our task is challenging and there is still a large room for improvement .",result
Spider : A Large - Scale Human - Labeled Dataset for Complex and Cross - Domain Semantic Parsing and Text - to - SQL Task,research-problem
Semantic parsing ( SP ) is one of the most important tasks in natural language processing ( NLP ) .,research-problem
Existing datasets for SP have two shortcomings .,research-problem
Our dataset and task are publicly available at https://yale-lily. github.io/ spider .,code
"Overall , we observe that COARSE2FINE outperforms ONESTAGE , which suggests that disentangling high - level from low - level information dur - 62.3 SNM + COPY 71 and .",result
Again we observe that the sketch encoder is beneficial and that there is an 8.9 point difference in accuracy between COARSE2FINE and the oracle .,result
"Compared with previous neural models that utilize syntax or grammatical information ( SEQ2 TREE , ASN ; the second block in ) , our method performs competitively despite the use of relatively simple decoders .",result
Our model is superior to ONESTAGE as well as to previous best performing systems .,result
"COARSE2FINE 's accuracies on aggregation agg op and agg col are 90.2 % and 92.0 % , respectively , which is comparable to SQLNET .",result
So the most gain is obtained by the improved decoder of the WHERE clause .,result
Sketches produced by COARSE2FINE are more accurate across the board .,result
"As can be seen , predicting the sketch correctly boosts performance .",result
"We also find that a tableaware input encoder is critical for doing well on this task , since the same question might lead to different SQL queries depending on the table schemas .",result
"On WIKISQL , the sketches predicted by COARSE2FINE are marginally better compared with ONESTAGE .",result
"Dimensions of hidden vectors and word embeddings were selected from { 250 , 300 } and { 150 , 200 , 250 , 300 } , respectively .",hyperparameter
"The dropout rate was selected from { 0.3 , 0.5 } .",hyperparameter
Label smoothing was employed for GEO and ATIS .,hyperparameter
The smoothing parameter was set to 0.1 .,hyperparameter
"Word embeddings were initialized by GloVe , and were shared by table encoder and input encoder in Section 4.3 .",hyperparameter
The part - of - speech tags were obtained by the spaCy toolkit .,hyperparameter
"The learning rate was selected from { 0.002 , 0.005 } .",hyperparameter
"The batch size was 200 for WIKISQL , and was 64 for other datasets .",hyperparameter
Early stopping was used to determine the number of epochs .,hyperparameter
We appended 10 - dimensional part - of - speech tag vectors to embeddings of the question words in WIKISQL .,hyperparameter
We used the RMSProp optimizer to train the models .,hyperparameter
"In this work , we propose to decompose the decoding process into two stages .",model
"The first decoder focuses on predicting a rough sketch of the meaning representation , which omits low - level details , such as arguments and variable names .",model
"Then , a second decoder fills in missing details by conditioning on the natural language input and the sketch itself .",model
"Specifically , the sketch constrains the generation process and is encoded into vectors to guide decoding .",model
"Firstly , the decomposition disentangles high - level from low - level semantic information , which enables the decoders to model meaning at different levels of granularity .",model
"Secondly , the model can explicitly share knowledge of coarse structures for the examples that have the same sketch ( i.e. , basic meaning ) , even though their actual meaning representations are different ( e.g. , due to different details ) .",model
"Thirdly , after generating the sketch , the decoder knows what the basic meaning of the utterance looks like , and the model can use it as global context to improve the prediction of the final details .",model
Coarse - to - Fine Decoding for Neural Semantic Parsing,research-problem
Semantic parsing aims at mapping natural language utterances into structured meaning representations .,research-problem
Our implementation and pretrained models are available at https :// github.com/donglixp/coarse2fine.,code
Semantic Parsing Tab Our system outperforms existing neural network - based approaches .,result
"Interestingly , we found the model without parent feeding achieves slightly better accuracy on GEO , probably because that its relative simple grammar does not require extra handling of parent information .",result
Code Generation Tab 2 lists the results on DJANGO TRANX achieves state - of - the - art results on DJANGO .,result
"We also find parent feeding yields + 1 point gain in accuracy , suggesting the importance of modeling parental connections in ASTs with complex domain grammars ( e.g. , Python ) .",result
"3 . We find TRANX , although just with simple extensions to adapt to this dataset , achieves impressive results and outperforms many task - specific methods .",result
TRANX : A Transition - based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation,research-problem
"In this paper we introduce a new NLP dataset and benchmark for predicting prosodic prominence from text which is based on the recently published Libri TTS corpus , containing automatically generated prosodic prominence labels for over 260 hours or 2.8 million words of English audio books , read by 1230 different speakers .",dataset
To our knowledge this will be the largest publicly available dataset with prosodic annotations .,dataset
Predicting Prosodic Prominence from Text with Pre-trained Contextualized Word Representations,research-problem
In this paper we introduce a new natural language processing dataset and benchmark for predicting prosodic prominence from written text .,research-problem
Prosody prediction can be turned into a sequence labeling task by giving each word in a text a discrete prominence value based on the amount of emphasis the speaker gives to the word when reading the text .,research-problem
All systems except the Minitagger and CRF are our implementations using PyTorch and are made available on GitHub : https://github.com/Helsinki - NLP / prosody .,code
Our NQG framework outperforms the PCFG - Trans and s 2s + att baselines by a large margin .,result
"The extended version , NQG ++ , has 1.11 BLEU score gain over NQG + , which shows that initializing with pre-trained word vectors and sharing them between encoder and decoder help learn better word representation .",result
"With the help of copy mechanism , NQG + has a 2.05 BLEU improvement since it solves the rare words problem .",result
"In this work we conduct a preliminary study on question generation from text with neural networks , which is denoted as the Neural Question Generation ( NQG ) framework , to generate natural language questions from text without pre-defined rules .",model
The Neural Question Generation framework extends the sequence - to - sequence models by enriching the encoder with answer and lexical features to generate answer focused questions .,model
"Concretely , the encoder reads not only the input sentence , but also the answer position indicator and lexical features .",model
"The answer position feature denotes the answer span in the input sentence , which is essential to generate answer relevant questions .",model
The lexical features include part - of - speech ( POS ) and named entity ( NER ) tags to help produce better sentence encoding .,model
"Lastly , the decoder with attention mechanism generates an answer specific question of the sentence .",model
Neural Question Generation from Text : A Preliminary Study,research-problem
Automatic question generation aims to generate questions from a text passage where the generated questions can be answered by certain sub- spans of the given passage .,research-problem
"Automatic question generation from natural language text aims to generate questions taking text as input , which has the potential value of education purpose ) .",research-problem
"As the reverse task of question answering , question generation also has the potential for providing a large scale corpus of question - answer pairs .",research-problem
"The answer position indicator , as expected , plays a crucial role in answer focused question generation as shown in the NQG ?",ablation-analysi
"NER , show that word case , POS and NER tag features contributes to question generation .",ablation-analysi
Multimodal Differential Network for Visual Question Generation,research-problem
Generating natural questions from an image is a semantic task that requires using visual and language modality to learn multimodal representations .,research-problem
Here the au-thors have proposed the challenging task of generating natural questions for an image .,research-problem
Deep Learning For Smile Recognition,research-problem
"Inspired by recent successes of deep learning in computer vision , we propose a novel application of deep convolutional neural networks to facial expression recognition , in particular smile recognition .",research-problem
"Due to training time constraints , some parameters have been fixed to reasonable and empirical values , such as the size of convolutions ( 5 5 pixels , 32 feature maps ) and the size of subsamplings ( 2 2 pixels using max pooling ) .",experimental-setup
"All layers use ReLU units , except of softmax being used in the output layer .",experimental-setup
The learning rate is fixed to ? = 0.01 and not subject to model selection as it would significantly prolong the model selection .,experimental-setup
The entire database has been randomly split into a 60% / 20 % / 20 % training / validation / test ratio .,experimental-setup
Stochastic gradient descent with a batch size of 500 is used .,experimental-setup
Each model was trained for 50 epochs in the model selection .,experimental-setup
"The same considerations apply to the momentum , which is fixed to = 0.9 .",experimental-setup
The model is implemented using Lasagne 4 and the generated CUDA code is executed on a Tesla K40c 9 as training on a GPU allows to perform a comprehensive model selection in a feasible amount of time .,experimental-setup
"contains the four parameters to be optimized : the number of convolutions , the number of hidden layers , the number of units per hidden layer and the dropout factor .",experimental-setup
"FLOWQA yields substantial improvement over existing models on both datasets ( + 7.2 % F 1 on CoQA , + 4.0 % F 1 on QuAC ) .",result
We find that FLOW is a critical component .,result
"Removing QHier - RNN has a minor impact ( 0.1 % on both datasets ) , while removing FLOW results in a substantial performance drop , with or without using QHierRNN ( 2 - 3 % on QuAC , 4.1 % on CoQA ) .",result
"By comparing 0 - Ans and 1 - Ans on two datasets , we can see that providing gold answers is more crucial for QuAC .",result
"Based on the training time each epoch takes ( i.e. , time needed for passing through the data once ) , the speedup is 8.1x on CoQA and 4.2 x on QuAC .",result
"applied BiDAF ++ , a strong extractive QA model to QuAC dataset .",baseline
"Here we briefly describe the ablated systems : "" - FLOW "" removes the flow component from IF layer ( Eq. 2 in Section 3.2 ) , "" - QHIER - RNN "" removes the hierarchical LSTM layers on final question vectors ( Eq. 7 in Section 3.3 ) .",baseline
"We present FLOWQA , a model designed for conversational machine comprehension .",model
FLOWQA consists of two main components : a base neural model for single - turn MC and a FLOW mechanism that encodes the conversation history .,model
"This FLOW mechanism is also remarkably effective at tracking the world states for sequential instruction understanding ( Long et al. , 2016 ) : after mapping world states as context and instructions as questions , FLOWQA can interpret a sequence of inter-connected instructions and generate corresponding world state changes as answers .",model
"The FLOW mechanism can be viewed as stacking single - turn QA models along the dialog progression ( i.e. , the question turns ) and building information flow along the dialog .",model
"This information transfer happens for each context word , allowing rich information in the reasoning process to flow .",model
"Instead of using the shallow history , i.e. , previous questions and answers , we feed the model with the entire hidden representations generated during the process of answering previous questions .",model
"To handle this issue , we propose an alternating parallel processing structure , which alternates between sequentially processing one dimension in parallel of the other dimension , and thus speeds up training significantly .",model
FLOWQA : GRASPING FLOW IN HISTORY FOR CONVERSATIONAL MACHINE COMPREHENSION,research-problem
Recently proposed conversational machine comprehension ( MC ) datasets aim to enable models to assist in such information seeking dialogs .,research-problem
Our code can be found in https://github.com/momohuang/FlowQA.,code
"This paper introduces a neural variational framework for generative models of text , inspired by the variational autoencoder .",model
"The principle idea is to build an inference network , implemented by a deep neural network conditioned on text , to approximate the intractable distributions over the latent variables .",model
"Instead of providing an analytic approximation , as in traditional variational Bayes , neural variational inference learns to model the posterior probability , thus endowing the model with strong generalis ation abilities .",model
A primary feature of NVDM is that each word is generated directly from a dense continuous document representation instead of the more common binary semantic vector .,model
The NASM ( is a supervised conditional model which imbues LSTMs with a latent stochastic attention mechanism to model the semantics of question - answer pairs and predict their relatedness .,model
The attention model is designed to focus on the phrases of an answer that are strongly connected to the question semantics and is modelled by a latent distribution .,model
"Bayesian inference provides a natural safeguard against overfitting , especially as the training sets available for this task are small .",model
"By using the reparameteris ation method , the inference network is trained through back - propagating unbiased and low variance gradients w.r.t. the latent variables .",model
"Within this framework , we propose a Neural Variational Document Model ( NVDM ) for document modelling and a Neural Answer Selection Model ( NASM ) for question answering , a task that selects the sentences that correctly answer a factoid question from a set of candidate sentences .",model
Neural Variational Inference for Text Processing Phil Blunsom 12,research-problem
"We initialized word embedding with 300d Glo Ve vectors pre-trained from the 840B Common Crawl corpus ( Pennington , Socher , and Manning 2014 ) , while the word embeddings for the out - of - vocabulary words were initialized randomly .",hyperparameter
The dropout was applied after the word and character embedding layers with a keep rate of 0.5 .,hyperparameter
It was also applied before the fully - connected layers with a keep rate of 0.8 .,hyperparameter
"The batch normalization was applied on the fully - connected layers , only for the one - way type datasets .",hyperparameter
The RMSProp optimizer with an initial learning rate of 0.001 was applied .,hyperparameter
The learning rate was decreased by a factor of 0.85 when the dev accuracy does not improve .,hyperparameter
All weights except embedding matrices are constrained by L2 regularization with a regularization constant ? = 10 ?6 .,hyperparameter
"The sequence lengths of the sentence are all different for each dataset : 35 for SNLI , 55 for MultiNLI , 25 for Quora question pair and 50 for TrecQA .",hyperparameter
We also randomly initialized character embedding with a 16d vector and extracted 32d character representation with a convolutional network .,hyperparameter
"For the densely - connected recurrent layers , we stacked 5 layers each of which have 100 hidden units .",hyperparameter
"For the bottleneck component , we set 200 hidden units as encoded features of the autoencoder with a dropout rate of 0.2 .",hyperparameter
We set 1000 hidden units with respect to the fullyconnected layers .,hyperparameter
"Inspired by Densenet ) , we propose a densely - connected recurrent network where the recurrent hidden features are retained to the uppermost layer .",model
"In addition , instead of the conventional summation operation , the concatenation operation is used in combination with the attention mechanism to preserve co-attentive information better .",model
The proposed architecture shown in is called DRCN which is an abbreviation for Densely - connected Recurrent and Co -attentive neural Network .,model
The proposed DRCN can utilize the increased representational power of deeper recurrent networks and attentive information .,model
"Furthermore , to alleviate the problem of an ever- increasing feature vector size due to concatenation operations , we adopted an autoencoder and forwarded a fixed length vector to the higher layer recurrent module as shown in the figure .",model
Semantic Sentence Matching with Densely - connected Recurrent and Co - attentive Information,research-problem
"Sentence matching is widely used in various natural language tasks such as natural language inference , paraphrase identification , and question answering .",research-problem
"Although the number of parameters in the DRCN significantly decreased as shown in , we could see that the performance was rather higher because of the regularization effect .",ablation-analysi
The result shows that the dense connections over attentive features are more effective .,ablation-analysi
"The models ( 5 - 9 ) which have connections between layers , are more robust to the increased depth of network , however , the performances of ( 10 - 11 ) tend to degrade as layers get deeper .",ablation-analysi
"In addition , the models with dense connections rather than residual connections , have higher performance in general .",ablation-analysi
"In , we removed dense connections over both co-attentive and recurrent features , and the performance degraded to 88.5 % .",ablation-analysi
"The results of ( 8 - 9 ) demonstrate that the dense connection using concatenation operation over deeper layers , has more powerful capability retaining collective knowledge to learn textual semantics .",ablation-analysi
The result of ( 10 ) shows that the connections among the layers are important to help gradient flow .,ablation-analysi
"And , the result of ( 11 ) shows that the attentive information functioning as a soft - alignment is significantly effective in semantic sentence matching .",ablation-analysi
"shows that the connection between layers is essential , especially in deep models , endowing more representational power , and the dense connection is more effective than the residual connection .",ablation-analysi
Comparison to baseline DCN with CoVe. DCN + outperforms the baseline by 3.2 % exact match accuracy and 3.2 % F1 on the SQuAD development set .,result
"shows the consistent performance gain of DCN + over the baseline across question types , question lengths , and answer lengths .",result
"In particular , DCN + provides a significant advantage for long questions .",result
"To address this problem , we propose a mixed objective that combines traditional cross entropy loss over positions with a measure of word overlap trained with reinforcement learning .",model
We obtain the latter objective using self - critical policy learning in which the reward is based on word overlap between the proposed answer and the ground truth answer .,model
"In addition to our mixed training objective , we extend the Dynamic Coattention Network ( DCN ) by with a deep residual coattention encoder .",model
DCN + : MIXED OBJECTIVE AND DEEP RESIDUAL COATTENTION FOR QUESTION ANSWERING,research-problem
"To preprocess the corpus , we use the reversible tokenizer from Stanford CoreNLP .",experimental-setup
"For word embeddings , we use GloVe embeddings pretrained on the 840B Common Crawl corpus as well as character ngram embeddings by .",experimental-setup
"In addition , we concatenate these embeddings with context vectors ( CoVe ) trained on .",experimental-setup
"For out of vocabulary words , we set the embeddings and context vectors to zero .",experimental-setup
We perform word dropout on the document which zeros a word embedding with probability 0.075 .,experimental-setup
"In addition , we swap the first maxout layer of the highway maxout network in the DCN decoder with a sparse mixture of experts layer .",experimental-setup
"We note that the deep residual coattention yielded the highest contribution to model performance , followed by the mixed objective .",ablation-analysi
The sparse mixture of experts layer in the decoder added minor improvements to the model performance . :,ablation-analysi
"Our final model achieves the accuracy of 88.6 % , the best result observed on SNLI , while our enhanced sequential encoding model attains an accuracy of 88.0 % , which also outperform the previous models .",result
"The table shows that our ESIM model achieves an accuracy of 88.0 % , which has already outperformed all the previous models , including those using much more complicated network architectures .",result
"In general , adding intra-sentence attention yields further improvement , which is not very surprising as it could help align the relevant text spans between premise and hypothesis .",result
"We ensemble our ESIM model with syntactic tree - LSTMs based on syntactic parse trees and achieve significant improvement over our best sequential encoding model ESIM , attaining an accuracy of 88.6 % .",result
"We use the Adam method ( Kingma and Ba , 2014 ) for optimization .",hyperparameter
"We use dropout with a rate of 0.5 , which is applied to all feedforward connections .",hyperparameter
We use pre-trained 300 - D Glove 840B vectors to initialize our word embeddings .,hyperparameter
The first momentum is set to be 0.9 and the second 0.999 .,hyperparameter
The initial learning rate is 0.0004 and the batch size is 32 .,hyperparameter
"All hidden states of LSTMs , tree - LSTMs , and word embeddings have 300 dimensions .",hyperparameter
Out - of - vocabulary ( OOV ) words are initialized randomly with Gaussian samples .,hyperparameter
"While some previous top - performing models use rather complicated network architectures to achieve the state - of - the - art results , we demonstrate in this paper that enhancing sequential inference models based on chain models can outperform all previous results , suggesting that the potentials of such sequential inference approaches have not been fully exploited yet .",model
"We show that by explicitly encoding parsing information with recursive networks in both local inference modeling and inference composition and by incorporating it into our framework , we achieve additional improvement , increasing the performance to a new state of the art with an 88.6 % accuracy .",model
Enhanced LSTM for Natural Language Inference,research-problem
Reasoning and inference are central to human and artificial intelligence .,research-problem
"Specifically , natural language inference ( NLI ) is concerned with determining whether a naturallanguage hypothesis h can be inferred from a premise p , as depicted in the following example from MacCartney ( 2009 ) , where the hypothesis is regarded to be entailed from the premise .",research-problem
Exploring syntax for NLI is very attractive to us .,research-problem
"As we can see , our system obtains state - of the - art results by achieving an EM score of 71.7 and a F 1 score of 74.2 on the test set .",result
Notice that SLQA + has reached a comparable result compared to our approach .,result
"To address the above issue , we propose a read - then - verify system that aims to be robust to unanswerable questions in this paper .",model
"As shown in , our system consists of two components : ( 1 ) a no-answer reader for extracting candidate answers and detecting unanswerable questions , and ( 2 ) an answer verifier for deciding whether or not the extracted candidate is legitimate .",model
"First , we augment existing readers with two auxiliary losses , to better handle answer extraction and no - answer detection respectively .",model
We solve this problem by introducing an independent span loss that aims to concentrate on the answer extraction task regardless of the answerability of the question .,model
"In order to not conflict with no - answer detection , we leverage a multi-head pointer network to generate two pairs of span scores , where one pair is normalized with the no -answer score and the other is used for our auxiliary loss .",model
"Besides , we present another independent noanswer loss to further alleviate the confliction , by focusing on the no-answer detection task without considering the shared normalization of answer extraction .",model
"Second , in addition to the standard reading phase , we introduce an additional answer verifying phase , which aims at finding local entailment that supports the answer by comparing the answer sentence with the question .",model
"Inspired by recent advances in natural language inference ( NLI ) , we investigate three different architectures for the answer verifying task .",model
"The first one is a sequential model that takes two sentences as along sequence , while the second one attempts to capture interactions between two sentences .",model
The last one is a hybrid model that combines the above two models to test if the performance can be further improved .,model
Read + Verify : Machine Reading Comprehension with Unanswerable Questions,research-problem
"We run a grid search on ? and ? among [ 0.1 , 0.3 , 0.5 , 0.7 , 1 , 2 ] .",experimental-setup
"As for answer verifiers , we use the original configuration from for Model - I. For Model - II , the Adam optimizer ( Kingma and Ba 2014 ) with a learning rate of 0.0008 is used , the hidden size is set as 300 , and a dropout ) of 0.3 is applied for preventing overfitting .",experimental-setup
"The batch size is 48 for the reader , 64 for Model - II , and 32 for Model - I as well as Model - III .",experimental-setup
"We use the Glo Ve 100D embeddings for the reader , and 300D embeddings for Model - II and Model - III .",experimental-setup
"We utilize the nltk tokenizer 3 to preprocess passages and questions , as well as split sentences .",experimental-setup
"Removing the independent span loss ( indep - I ) results in a performance drop for all answerable questions ( HasAns ) , indicating that this loss helps the model in better identifying the answer boundary .",ablation-analysi
"Ablating independent no - answer loss ( indep - II ) , on the other hand , causes little influence on HasAns , but leads to a severe decline on no - answer accuracy ( NoAns ACC ) .",ablation-analysi
"Ablating two auxiliary losses , however , leads to an over all degradation on the curve , but it still outperforms the baseline by a large margin .",ablation-analysi
"Finally , deleting both of two losses causes a degradation of more than 1.5 points on the over all performance in terms of F1 , with or without ELMo embeddings .",ablation-analysi
"Adding ELMo embeddings , however , does not boost the performance .",ablation-analysi
We find that the improvement on noanswer accuracy is significant .,ablation-analysi
We observe that RMR + ELMo + Verifier achieves the best precision when the recall is less than 80 .,ablation-analysi
"Within this domain , we propose an approach that learns to both select and explain answers , when the only supervision available is for which answer is correct ( but not how to explain it ) .",approach
"Intuitively , our approach chooses the justifications that provide the most help towards ranking the correct answers higher than incorrect ones .",approach
"More formally , our neural network approach alternates between using the current model with max - pooling to choose the highest scoring justifications for correct answers , and optimizing the answer ranking model given these justifications .",approach
Tell Me Why : Using Question Answering as Distant Supervision for Answer Justification,research-problem
"Developing interpretable machine learning ( ML ) models , that is , models where a human user can understand what the model is learning , is considered by many to be crucial for ensuring usability and accelerating progress .",research-problem
Iterative Alternating Neural Attention for Machine Reading,research-problem
"We propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering Cloze - style queries with respect to a document .",research-problem
"To train our model , we used stochastic gradient descent with the ADAM optimizer ( Kingma and Ba , 2014 ) , with an initial learning rate of 0.001 .",experimental-setup
"We set the batch size to 32 and we decay the learning rate by 0.8 if the accuracy on the validation set does not increase after a half - epoch , i.e. 2000 batches ( for CBT ) and 5000 batches for ( CNN ) .",experimental-setup
"We initialize all weights of our model by sampling from the normal distribution N ( 0 , 0.05 ) .",experimental-setup
"Following , the GRU recurrent weights are initialized to be orthogonal and biases are initialized to zero .",experimental-setup
"Our model is implemented in Theano , using the Keras library .",experimental-setup
"In order to stabilize the learning , we clip the gradients if their norm is greater than 5 and those marked with 2 are from .",experimental-setup
"We found that setting embedding regularization to 0.0001 , T = 8 , d = 384 , h = 128 , s = 512 worked robustly across the datasets .",experimental-setup
"The key contribution of this work is that we propose Attentive Pooling ( AP ) , a two - way attention mechanism , that significantly improves such discriminative models ' performance on pair - wise ranking or classification , by enabling a joint learning of the representations of both inputs as well as their similarity measurement .",model
"Specifically , AP enables the pooling layer to be aware of the current input pair , in a way that information from the two input items can directly influence the computation of each other 's representations .",model
"The main idea in AP consists of learning a similarity measure over projected segments ( e.g. trigrams ) of the two items in the input pair , and using the similarity scores between the segments to compute attention vectors in both directions .",model
"Next , the attention vectors are used to perform pooling .",model
"Neural networks ( NN ) with attention mechanisms have recently proven to be successful at different computer vision ( CV ) and natural language processing ( NLP ) tasks such as image captioning , machine translation and factoid question answering .",research-problem
"However , most recent work on neural attention models have focused on one - way attention mechanisms based on recurrent neural networks designed . for generation tasks .",research-problem
"We use a context window of size 3 for Insurance QA , while we set this parameter to 4 for TREC - QA and Wiki QA .",experimental-setup
"For AP - CNN , AP - biLSTM and QA - LSTM , we also use a learning rate schedule that decreases the learning rate ?",experimental-setup
"Using the selected hyperparameters , the best results are normally achieved using between 15 and 25 training epochs .",experimental-setup
"In our experiments , the four NN architectures QA - CNN , AP - CNN , QA - biLSTM and AP - biLSTM are implemented using Theano .",experimental-setup
"shows that our single model , trained with two styles and controlled with the NQA style , pushed forward the state - of - the - art by a significant margin .",result
The evaluation scores of the model controlled with the NLG style were low because the two styles are different .,result
"Also , our model without multi-style learning ( trained with only the NQA style ) outperformed the baselines in terms of ROUGE - L .",result
"In this study , we propose Masque , a generative model for multi-passage RC .",model
"We introduce the pointer - generator mechanism for generating an abstractive answer from the question and multiple passages , which covers various answer styles .",model
We introduce multi-style learning that enables our model to control answer styles and improves RC for all styles involved .,model
We extend the mechanism to a Transformer based one that allows words to be generated from a vocabulary and to be copied from the question and passages .,model
"We also extend the pointer - generator to a conditional decoder by introducing an artificial token corresponding to each style , as in .",model
"For each decoding step , it controls the mixture weights over three distributions with the given style ( ) .",model
Multi - Style Generative Reading Comprehension,research-problem
"This study tackles generative reading comprehension ( RC ) , which consists of answering questions based on textual evidence and natural language generation ( NLG ) .",research-problem
"Recently , reading comprehension ( RC ) , a challenge to answer a question given textual evidence provided in a document set , has received much attention .",research-problem
"Current mainstream studies have treated RC as a process of extracting an answer span from one passage or multiple passages , which is usually done by predicting the start and end positions of the answer .",research-problem
"In this study , we propose Masque , a generative model for multi-passage RC .",research-problem
"We train the model with the Adadelta optimizer ( Zeiler , 2012 ) with a batch size 60 for Triv - ia QA and 45 for SQuAD .",hyperparameter
The Glo Ve 300 dimensional word vectors released by are used for word embeddings .,hyperparameter
"On SQuAD , we use a dimensionality of size 100 for the GRUs and of size 200 for the linear layers employed after each attention mechanism .",hyperparameter
"We find for TriviaQA , likely because there is more data , using a larger dimensionality of 140 for each GRU and 280 for the linear layers is beneficial .",hyperparameter
"During training , we maintain an exponential moving average of the weights with a decay rate of 0.999 .",hyperparameter
In this paper we start by proposing an improved pipelined method which achieves state - of - the - art results .,model
"Then we introduce a method for training models to produce accurate per-paragraph confidence scores , and we show how combining this method with multiple paragraph selection further increases performance .",model
We propose a TF - IDF heuristic to select which paragraphs to train and test on .,model
"To handle the noise this creates , we use a summed objective function that marginalizes the model 's output over all locations the answer text occurs .",model
"We then use a shared - normalization objective where paragraphs are processed independently , but the probability of an answer candidate is marginalized over all paragraphs sampled from the same document .",model
"We resolve these problems by sampling paragraphs from the context documents , including paragraphs that do not contain an answer , to train on .",model
Simple and Effective Multi - Paragraph Reading Comprehension,research-problem
We consider the problem of adapting neural paragraph - level question answering models to the case where entire documents are given as input .,research-problem
The recent success of neural models at answering questions given a related paragraph suggests neural models have the potential to be a key part of a solution to this problem .,research-problem
Recurrent Relational Networks,research-problem
"We introduce the recurrent relational network , a general purpose module that operates on a graph representation of objects .",research-problem
Code to reproduce all experiments can be found at github.com/rasmusbergpalm/recurrent-relationalnetworks. designed as a set of prerequisite tasks for reasoning .,code
"As can be seen , the bigram model performs better than the unigram model and the addition of the IDF - weighted word count features significantly improve performance for both models by 10 % - 15 % .",result
"As can be seen in , our best models ( bigram + count ) outperform all baselines and prior work on MAP and are very close to the best model proposed by Yih et al. on MRR .",result
"In this paper , we show that a neural network - based sentence model can be applied to the task of answer sentence selection .",model
"We construct two distributional sentence models ; first a bag - of - words model , and second , a bigram model based on a convolutional neural network .",model
"Assuming a set of pre-trained semantic word embeddings , we train a supervised model to learn a semantic matching between question and answer pairs .",model
"We also present an enhanced version of this model , which combines the signal of the distributed matching algorithm with two simple word matching features .",model
Deep Learning for Answer Sentence Selection,research-problem
We used word embeddings ( d = 50 ) that were computed using Collobert and Weston 's neural language model and provided by Turian et al ..,experimental-setup
"The other model weights were randomly intitialised using a Gaussian distribution ( = 0 , ? = 0.01 ) .",experimental-setup
All hyperparameters were optimised via grid search on the MAP score on the development data .,experimental-setup
"L - BFGS was used to train the logistic regression classifier , with L2 regulariser of 0.01 .",experimental-setup
We use the AdaGrad algorithm for training .,experimental-setup
"The first , WIKIHOP , uses sets of WIKIPEDIA articles where answers to queries about specific properties of an entity can not be located in the entity 's article .",dataset
"In the second dataset , MEDHOP , the goal is to establish drug - drug interactions based on scientific findings about drugs and proteins and their interactions , found across multiple MEDLINE abstracts .",dataset
"For both datasets we draw upon existing Knowledge Bases ( KBs ) , WIKIDATA and DRUG - BANK , as ground truth , utilizing distant supervision ) to induce the data - similar to and .",dataset
"The Document - cue baseline can predict more than a third of the samples correctly , for both datasets , even after sub - sampling frequent document - answer pairs for WIKIHOP .",result
Both neural RC models are able to largely retain or even improve their strong performance when answers are masked : they are able to leverage the textual context of the candidate expressions .,result
"In the masked setup all baseline models reliant on lexical cues fail in the face of the randomized answer expressions , since the same answer option has different placeholders in different samples .",result
"In contrast , for the open - domain setting of WIKIHOP , a reduction of the answer vocabulary to 100 random single - token mask expressions clearly helps the model in selecting a candidate span , compared to the multi-token candidate expressions in the unmasked setting .",result
Constructing Datasets for Multi-hop Reading Comprehension Across Documents,research-problem
"Most Reading Comprehension methods limit themselves to queries which can be answered using a single sentence , paragraph , or document .",research-problem
Contemporary end - to - end Reading Comprehension ( RC ) methods can learn to extract the correct answer span within a given text and approach human - level performance .,research-problem
"Furthermore , our boundary model has outperformed the sequence model , achieving an exact match score of 61.1 % and an F1 score of 71.2 % .",result
"In particular , in terms of the exact match score , the boundary model has a clear advantage over the sequence model .",result
"While by adding Bi - Ans - Ptr with bi-directional pre-processing LSTM , we can get 1.2 % improvement in F1 .",result
"Specifically , observing that in the SQuAD dataset many questions are paraphrases of sentences from the original text , we adopt a match - LSTM model that we developed earlier for textual entailment .",model
"We further adopt the Pointer Net ( Ptr - Net ) model developed by , which enables the predictions of tokens from the input sequence only rather than from a larger fixed vocabulary and thus allows us to generate answers that consist of multiple tokens from the original text .",model
We propose two ways to apply the Ptr - Net model for our task : a sequence model and a boundary model .,model
We also further extend the boundary model with a search mechanism .,model
MACHINE COMPREHENSION USING MATCH - LSTM AND ANSWER POINTER,research-problem
Machine comprehension of text is an important problem in natural language processing .,research-problem
"We first tokenize all the passages , questions and answers .",experimental-setup
We use word embeddings from GloVe to initialize the model .,experimental-setup
We use ADAMAX with the coefficients ? 1 = 0.9 and ? 2 = 0.999 to optimize the model .,experimental-setup
Words not found in Glo Ve are initialized as zero vectors .,experimental-setup
The dimensionality l of the hidden layers is set to be 150 or 300 .,experimental-setup
"As shown in , Max - pooling described in Section 2.2 drastically improves performance , showing the effect of accumulating information on entities .",result
"The 99 % confidence intervals of the results of full DER Network and the one initialized by word2vec on the test set were [ 0.700 , 0.740 ] and [ 0.708 , 0.749 ] , respectively ( measured by bootstrap tests ) .",result
"Further , we note that initializing our model with pre-trained word vectors 10 is helpful , though world knowledge of entities has been prevented by the anonymization process .",result
"Finally , we note that our model , full DER Network , shows the best results compared to several previous reader models , endorsing our approach as promising .",result
"For preprocessing , we segment sentences at punctuation marks "" . "" , "" ! "" , and "" ? "" .",hyperparameter
"We train our model 8 with hyper - parameters lightly tuned on the validation set 9 , and we conduct ablation test on several techniques that improve our basic model .",hyperparameter
"We , however , take it as a strong motivation to implement a reader that dynamically builds meaning representations for each entity , by gathering and accumulating information on that entity as it reads a document ( Section 2 ) .",model
Dynamic Entity Representation with Max - pooling Improves Machine Reading,research-problem
Our code for the model is available at https://github.com/soskek/der-network,code
"In we compare these two variants over the development set and observe superior performance by the contextual one , illustrating the benefit of contextualization and specifically per-sequence contextualization which is done separately for the question and for the passage .",result
"On average , the less frequent a word - type is , the smaller are its gate activations , i.e. , the reembedded representation of a rare word places less weight on its fixed word - embedding and more on its contextual representation , compared to a common word .",result
"Besides a crosscutting boost in results , we note that the performance due to utilizing the LM hidden states of the first LSTM layer significantly surpasses the other two variants .",result
Supplementing the calculation of token reembeddings with the hidden states of a strong language model proves to be highly effective .,result
"Overall , we observe a significant improvement with all three configurations , effectively showing the benefit of training a QA model in a semisupervised fashion with a large language model .",result
We use pre-trained GloVe embeddings of dimension d w = 300 and produce character - based word representations via dc = 100 convolutional filters over character embeddings as in .,hyperparameter
"To illustrate this idea , we take a model that carries out only basic question - document interaction and prepend to it a module that produces token embeddings by explicitly gating between contextual and non-contextual representations ( for both the document and question ) .",model
"Motivated by these findings , we turn to a semisupervised setting in which we leverage a language model , pre-trained on large amounts of data , as a sequence encoder which forcibly facilitates context utilization .",model
Contextualized Word Representations for Reading Comprehension,research-problem
Reading comprehension ( RC ) is a high - level task in natural language understanding that requires reading a document and answering questions about its content .,research-problem
"RC has attracted substantial attention over the last few years with the advent of large annotated datasets , computing resources , and neural network models and optimization procedures .",research-problem
Our vanilla approach achieves state - of - theart results with almost an order of magnitude fewer parameters than the LSTMN of .,result
Adding intra-sentence attention gives a considerable improvement of 0.5 percentage points over the existing state of the art .,result
"In contrast to existing approaches , our approach only relies on alignment and is fully computationally decomposable with respect to the input text .",model
"Finally , the results of these subproblems are merged to produce the final classification .",model
"Given two sentences , where each word is repre-sented by an embedding vector , we first create a soft alignment matrix using neural attention .",model
We then use the ( soft ) alignment to decompose the task into subproblems that are solved separately .,model
"In addition , we optionally apply intra-sentence attention to endow the model with a richer encoding of substructures prior to the alignment step .",model
A Decomposable Attention Model for Natural Language Inference,research-problem
Natural language inference ( NLI ) refers to the problem of determining entailment and contradiction relationships between a premise and a hypothesis .,research-problem
NLI is a central problem in language understanding ) and recently the large SNLI corpus of 570K sentence pairs was created for this task .,research-problem
The method was implemented in TensorFlow .,experimental-setup
We use 300 dimensional GloVe embeddings to represent words .,experimental-setup
"Each embedding vector was normalized to have 2 norm of 1 and projected down to 200 dimensions , a number determined via hyperparameter tuning .",experimental-setup
Out - of - vocabulary ( OOV ) words are hashed to one of 100 random embeddings each initialized to mean 0 and standard deviation 1 .,experimental-setup
All other parameter weights ( hidden layers etc. ) were initialized from random Gaussians with mean 0 and standard deviation 0.01 .,experimental-setup
"Each hyperparameter setting was run on a single machine with 10 asynchronous gradient - update threads , using Adagrad for optimization with the default initial accumulator value of 0.1 .",experimental-setup
"Dropout regularization was used for all ReLU layers , but not for the final linear layer .",experimental-setup
"We additionally tuned the following hyperparameters and present their chosen values in , 1 dropout ratio ( 0.2 ) and learning rate ( 0.05 - vanilla , 0.025 - intra-attention ) .",experimental-setup
We can see a NMM trained with TRAIN - ALL set beats all the previous state - of - the art systems including both methods using feature engineering and deep learning models .,result
"Furthermore , even without combining additional features , a NMM still performs well for answer ranking , showing significant improvements over previous deep learning model with no additional features and linguistic feature engineering methods .",result
"For the setting of hyper - parameters , we set the number of bins as 600 , word embedding dimension as 700 for a NNM - 1 , the number of bins as 200 , word embedding dimension as 700 for a NNM - 2 after we tune hyper - parameters on the provided DEV set of TREC QA data .",hyperparameter
"As an alternative to question answering methods based on feature engineering , deep learning approaches such as convolutional neural networks ( CNNs ) and Long Short - Term Memory Models ( LSTMs ) have recently been proposed for semantic matching of questions and answers .",research-problem
"Question answering ( QA ) , which returns exact answers as either short facts or long passages to natural language questions issued by users , is a challenging task and plays a central role in the next generation of advanced web search .",research-problem
"Many of current QA systems use a learning to rank approach that encodes question / answer pairs with complex linguistic features including lexical , syntactic and semantic features .",research-problem
Multi - Passage Machine Reading Comprehension with Cross - Passage Answer Verification,research-problem
Machine reading comprehension ( MRC ) on real web data usually requires the machine to answer a question by analyzing multiple passages retrieved by search engine .,research-problem
"Compared with MRC on a single passage , multi-passage MRC is more challenging , since we are likely to get multiple confusing answer candidates from different passages .",research-problem
"For MS - MARCO , we preprocess the corpus with the reversible tokenizer from Stanford CoreNLP and we choose the span that achieves the highest ROUGE - L score with the reference answers as the gold span for training .",experimental-setup
We employ the 300 - D pre-trained Glove embeddings and keep it fixed during training .,experimental-setup
The character embeddings are randomly initialized with its dimension as 30 .,experimental-setup
"From , we can see that the answer verification makes a great contribution to the over all improvement , which confirms our hypothesis that cross - passage answer verification is useful for the multi-passage MRC .",ablation-analysi
"For the ablation of the content model , we analyze that it will not only affect the content score itself , but also violate the verification model since the content probabilities are necessary for the answer representation , which will be further analyzed in Section 4.3 .",ablation-analysi
"Another discovery is that jointly training the three models can provide great benefits , which shows that the three tasks are actually closely related and can boost each other with shared representations at bottom layers .",ablation-analysi
"At last , comparing our method with the baseline , we achieve an improvement of nearly 3 points without the yes / no classification .",ablation-analysi
"In this work , we push the multi-head attention to a extreme by building a word - by - word dimension - wise alignment tensor which we call interaction tensor .",model
The interaction tensor encodes the high - order alignment relationship between sentences pair .,model
We dub the general framework as Interactive Inference Network ( IIN ) .,model
Published as a conference paper at ICLR 2018 NATURAL LANGUAGE INFERENCE OVER INTERACTION SPACE,research-problem
"Natural Language Inference ( NLI also known as recognizing textual entiailment , or RTE ) task requires one to determine whether the logical relationship between two sentences is among entailment ( if the premise is true , then the hypothesis must be true ) , contradiction ( if the premise is true , then the hypothesis must be false ) and neutral ( neither entailment nor contradiction ) .",research-problem
There are several new novel components in our work .,model
"Each scalar valued feature is used to augment the base word representation , allowing the subsequent RNN encoder layers to benefit from not only global but also cross sentence information .",model
"Firstly , we propose a compare , compress and propagate ( Com Prop ) architecture where compressed alignment features are propagated to upper layers ( such as a RNN - based encoder ) for enhancing representation learning .",model
"Secondly , in order to achieve an efficient propagation of alignment features , we propose alignment factorization layers to reduce each alignment vector to a single scalar valued feature .",model
"Compare , Compress and Propagate : Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference",research-problem
This paper presents a new deep learning architecture for Natural Language Inference ( NLI ) .,research-problem
"More concretely , given a premise and hypothesis , NLI aims to detect whether the latter entails or contradicts the former .",research-problem
We implement our model in TensorFlow and train them on Nvidia P100 GPUs .,experimental-setup
"We use the Adam optimizer ( Kingma and Ba , 2014 ) with an initial learning rate of 0.0003 .",experimental-setup
L2 regularization is set to 10 ?6 .,experimental-setup
"Dropout with a keep probability of 0.8 is applied after each fullyconnected , recurrent or highway layer .",experimental-setup
"The batch size is tuned amongst { 128 , 256 , 512 } .",experimental-setup
"The number of latent factors k for the factorization layer is tuned amongst { 5 , 10 , 50 , 100 , 150 } .",experimental-setup
The size of the hidden layers of the highway network layers are set to 300 .,experimental-setup
All parameters are initialized with xavier initialization .,experimental-setup
Word embeddings are preloaded with 300d Glo Ve embeddings and fixed during training .,experimental-setup
Sequence lengths are padded to batch - wise maximum .,experimental-setup
The batch order is ( randomly ) sorted within buckets following .,experimental-setup
The 1 - layer linear setting performs the best and is therefore reported in .,ablation-analysi
Using ReLU seems to be worse than nonlinear FC layers .,ablation-analysi
"In , we explore the utility of using character and syntactic embeddings , which we found to have helped CAFE marginally .",ablation-analysi
"In ( 4 ) , we remove the inter-attention alignment features , which naturally impact the model performance significantly .",ablation-analysi
We observe that both highway layers have marginally helped the over all performance .,ablation-analysi
We observe that the Sub and Concat compositions were more important than the Mul composition .,ablation-analysi
"Finally , in ( 10 ) , we replace the LSTM encoder with a BiLSTM , observing that adding bi-directionality did not improve performance for our model .",ablation-analysi
"In this paper , we propose the novel framework named Smarnet with the hope that it can become as smart as humans .",model
"Specifically , we first introduce the Smarnet framework that exploits fine - grained word understanding with various attribution discriminations , like humans recite words with corresponding properties .",model
We then develop the interactive attention with memory network to mimic human reading procedure .,model
We also add a checking layer on the answer refining in order to ensure the accuracy .,model
"Machine Comprehension ( MC ) is a challenging task in Natural Language Processing field , which aims to guide the machine to comprehend a passage and answer the given question .",research-problem
"Many existing approaches on MC task are suffering the inefficiency in some bottlenecks , such as insufficient lexical understanding , complex question - passage interaction , incorrect answer extraction and soon .",research-problem
Recently machine comprehension task accumulates much concern among NLP researchers .,research-problem
"We preprocess each passage and question using the library of nltk and exploit the popular pretrained word embedding GloVe with 100 - dimensional vectors ( Pennington , Socher , and Manning 2014 ) for both questions and passages .",experimental-setup
The size of char - level embedding is also set as 100 - dimensional and is obtained by CNN filters under the instruction of ( Kim 2014 ) .,experimental-setup
The batch size is set to be 48 for both the SQuAD and TriviaQA datasets .,experimental-setup
We adopt the AdaDelta ( Zeiler 2012 ) optimizer for training with an initial learning rate of 0.0005 .,experimental-setup
We also apply dropout ( Srivastava et al. 2014 ) between layers with a dropout rate of 0.2 .,experimental-setup
"For the multi-hop reasoning , we set the number of hops as 2 which is imitating human reading procedure on skimming and scanning .",experimental-setup
"During training , we set the moving averages of all weights as the exponential decay rate of 0.999 .",experimental-setup
"We see the full features integration obtain the best performance , which demonstrates the necessity of combining all the features into consideration .",ablation-analysi
"Among all the feature ablations , the Part - Of - Speech , Exact Match , Qtype features drop much more than the other features , which shows the importance of these three features .",ablation-analysi
"As for the final ablation of POS and NER , we can see the performance decays over 3 % point , which clearly proves the usefulness of the comprehensive lexical information .",ablation-analysi
"We first replace our input gate mechanism into simplified feature concatenation strategy , the performance drops nearly 2.3 % on the EM score , which proves the effectiveness of our proposed dynamic input gating mechanism .",ablation-analysi
The result proves that our modification of employing question influence on the passage encoding can boost the result up to 1.3 % on the EM score .,ablation-analysi
"Inspired by the positive results of Vaswani et al. in machine translation , we have applied a similar architecture to the domain of question - answering , a model that we have named Fully Attention - Based Information Retriever ( FABIR ) .",model
"Our goal then was to verify how much performance we can get exclusively from the attention mechanism , without combining it with several other techniques .",model
"Convolutional attention : a novel attention mechanism that encodes many - to - many relationships between words , enabling richer contextual representations .",model
Reduction layer : a new layer design that fits the pipeline proposed by Vaswani et al .,model
Column - wise cross - attention : we modify the crossattention operation by and propose a new technique that is better suited to question - answering .,model
A Fully Attention - Based Information Retriever,research-problem
Question - answering ( QA ) systems that can answer queries expressed in natural language have been a perennial goal of the artificial intelligence community .,research-problem
"That is , in fact , the proposed focus of recent open - domain QA datasets , such as SQuAD .",research-problem
"In SQuAD , each problem instance consists of a passage P and a question Q. A QA system must then provide an answer A by selecting a snippet from P .",research-problem
We have trained our FABIR model during 54 epochs with a batch size of 75 in a GPU NVidia Titan X with 12 GB of RAM .,experimental-setup
We developed our model in Tensorflow and made it available at https://worksheets.codalab.org/worksheets/ 0xee647ea284674396831ecb5aae9ca297 / for replicability .,experimental-setup
We pre-processed the texts with the NLTK Tokenizer .,experimental-setup
"For regularization , we applied residual and attention dropout of 0.9 in processing layers and of 0.8 in the reduction layer .",experimental-setup
"In the character - level embedding process , a dropout of 0.75 was added before the convolution .",experimental-setup
"Additionally , a dropout of 0.8 was added before each convolutional layer in the answer selector .",experimental-setup
"We set processing layers dimension d model to 100 , the number of heads n heads in each attention sublayer to 4 , the feed - forward hidden size to 200 in processing layers and 400 in the reduction layer .",experimental-setup
"This analysis confirms the effectiveness of char- embeddings , as its addition increased the F1 and EM scores , by 2.7 % and 3.1 % , respectively .",ablation-analysi
"Most importantly , when the convolutional attention was replaced by the standard attention mechanism proposed in , the performance dropped by 2.4 % in F1 and 2.5 % in EM .",ablation-analysi
"Moreover , the tests also indicate that the reduction layer is capable of producing useful word representations when compressing the embeddings .",ablation-analysi
"Indeed , when we replaced that layer by a standard feedforward layer with the same reduction ratio , there was a drop of 2.1 % and 2.5 % in the F1 and EM scores , respectively .",ablation-analysi
"In this paper , we propose Gumbel Tree - LSTM , which is a novel RvNN architecture that does not require structured data and learns to compose task - specific tree structures without explicit guidance .",model
"Our Gumbel Tree - LSTM model is based on tree - structured long short - term memory ( Tree - LSTM ) architecture , which is one of the most renowned variants of RvNN .",model
"To learn how to compose task - specific tree structures without depending on structured input , our model introduces composition query vector that measures validity of a composition .",model
"Using validity scores computed by the composition query vector , our model recursively selects compositions until only a single representation remains .",model
We use Straight - Through ( ST ) Gumbel - Softmax estimator to sample compositions in the training phase .,model
"ST Gumbel - Softmax estimator relaxes the discrete sampling operation to be continuous in the backward pass , thus our model can be trained via the standard backpropagation .",model
Learning to Compose Task - Specific Tree Structures,research-problem
"In this paper , we propose Gumbel Tree - LSTM , a novel tree - structured long short - term memory architecture that learns how to compose task - specific tree structures only from plain text data efficiently .",research-problem
"In this paper , aiming to make the machine comprehension fast , we propose to remove the recurrent nature of these models .",model
We instead exclusively use convolutions and self - attentions as the building blocks of encoders that separately encodes the query and context .,model
Then we learn the interactions between context and question by standard attentions .,model
The resulting representation is encoded again with our recurrency - free encoder before finally decoding to the probability of each position being the start or end of the answer span .,model
"We call this architecture QANet , which is shown in .",model
COMBINING LOCAL CONVOLUTION WITH GLOBAL SELF - ATTENTION FOR READING COMPRE - HENSION,research-problem
Current end - to - end machine reading and question answering ( Q&A ) models are primarily based on recurrent neural networks ( RNNs ) with attention .,research-problem
There is growing interest in the tasks of machine reading comprehension and automated question answering .,research-problem
"In this paper , aiming to make the machine comprehension fast , we propose to remove the recurrent nature of these models .",research-problem
"As can be seen from the table , the use of convolutions in the encoders is crucial : both F1 and EM drop drastically by almost 3 percent if it is removed .",ablation-analysi
Self- attention in the encoders is also a necessary component that contributes 1.4/1.3 gain of EM / F1 to the ultimate performance .,ablation-analysi
"As the last block of rows in the table shows , data augmentation proves to be helpful in further boosting performance .",ablation-analysi
"Empirically , the ratio ( 3:1:1 ) yields the best performance , with 1.5/1.1 gain over the base model on EM / F1 .",ablation-analysi
"Making the training data twice as large by adding the En - Fr - En data only ( ratio 1:1 between original training data and augmented data , as indicated by row "" data augmentation 2 ( 1:1:0 ) "" ) yields an increase in the F1 by 0.5 percent .",ablation-analysi
"Although injecting more data beyond 3 does not benefit the model , we observe that a good sampling ratio between the original and augmented data during training can further boost the model performance .",ablation-analysi
"In particular , when we increase the sampling weight of augmented data from ( 1:1:1 ) to ( 1:2:1 ) , the EM / F1 performance drops by 0.5/0.3 .",ablation-analysi
"Obviously , the performance of most of the integrated methods are better than the sentence encoding based models above .",result
"The performance of our model achieves 89.6 % on SNLI , 80.3 % on matched MultiNLI and 79.4 % on mismatched MultiNLI , which are all state - of - the - art results .",result
"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , where we transfer the knowledge from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .",model
We first propose a sentence encoder model that learns the representations of the sentences from the DMP task and then inject the encoder to the NLI network .,model
"In consideration of that different confidence level of the final labels should be discriminated , we employ reinforcement learning with a reward defined by the uniformity extent of the original labels to train the model .",model
Discourse Marker Augmented Network with Reinforcement Learning for Natural Language Inference,research-problem
"Natural Language Inference ( NLI ) , also known as Recognizing Textual Entailment ( RTE ) , is one of the most important problems in natural language processing .",research-problem
"While current approaches mostly focus on the interaction architectures of the sentences , in this paper , we propose to transfer knowledge from some important discourse markers to augment the quality of the NLI model .",research-problem
We use the Stanford CoreNLP toolkit to tokenize the words and generate POS and NER tags .,experimental-setup
We use the AdaDelta for optimization as described in with ? as 0.95 and as 1 e - 8 .,experimental-setup
"The word embeddings are initialized by 300d Glove , the dimensions of POS and NER embeddings are 30 and 10 .",experimental-setup
We set our batch size as 36 and the initial learning rate as 0.6 .,experimental-setup
"We set the hidden size as 300 for all the LSTM layers and apply dropout between layers with an initial ratio of 0.9 , the decay rate as 0.97 for every 5000 step .",experimental-setup
We apply Tensorflow r 1.3 as our neural network framework .,experimental-setup
"The number of epochs is set to be 10 , and the feedforward dropout rate is 0.2 .",experimental-setup
"For DMP task , we use stochastic gradient descent with initial learning rate as 0.1 , and we anneal by half each time the validation accuracy is lower than the previous epoch .",experimental-setup
"On MCTest - 500 , the Parallel Hierarchical model significantly outperforms these methods on single questions ( > 2 % ) and slightly outperforms the latter two on multi questions ( ? 0.3 % ) and over all ( ?",result
The method of achieves the best over all result on MCTest - 160 .,result
Here we see our model outperforming the alternatives by a large margin across the board ( > 15 % ) .,result
We present a parallel - hierarchical approach to machine comprehension designed to work well in a data - limited setting .,model
Our model learns to comprehend at a high level even when data is sparse .,model
The key to our model is that it compares the question and answer candidates to the text using several distinct perspectives .,model
"The semantic perspective compares the hypothesis to sentences in the text viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .",model
"The word - by - word perspective focuses on similarity matches between individual words from hypothesis and text , at various scales .",model
"As in the semantic perspective , we consider matches over complete sentences .",model
"We also use a sliding window acting on a subsentential scale ( inspired by the work of ) , which implicitly considers the linear distance between matched words .",model
A Parallel - Hierarchical Model for Machine Comprehension on Sparse Data,research-problem
Understanding unstructured text is a major goal within natural language processing .,research-problem
Machine comprehension ( MC ) is evaluated by posing a set of questions based on a text passage ( akin to the reading tests we all took in school ) .,research-problem
"For word vectors we use Google 's publicly available embeddings , trained with word2vec on the 100 - billion - word News corpus .",experimental-setup
"These vectors are kept fixed throughout training , since we found that training them was not helpful ( likely because of MCTest 's size ) .",experimental-setup
The vectors are 300 - dimensional ( d = 300 ) .,experimental-setup
"We found dropout to be particularly effective at improving generalization from the training to the test set , and used 0.5 as the dropout probability .",experimental-setup
"We used the Adam optimizer with the standard settings ( Kingma and Ba , 2014 ) and a learning rate of 0.003 .",experimental-setup
"Dropout occurs after all neural - network transformations , if those transformations are allowed to change with training .",experimental-setup
"Not surprisingly , the n-gram functionality is important , contributing almost 5 % accuracy improvement .",ablation-analysi
"The top N function contributes very little to the over all performance , suggesting that most multi questions have their evidence distributed across contiguous sentences .",ablation-analysi
Simple word - by - word matching is obviously useful on MCTest .,ablation-analysi
"The sequential sliding window makes a 3 % contribution , highlighting the importance of word - distance measures .",ablation-analysi
"On the other hand , the dependency - based sliding window makes only a minor contribution .",ablation-analysi
"Finally , the exogenous word weights make a significant contribution of almost 5 % .",ablation-analysi
"Ablating the sentential component made the most significant difference , reducing performance by more than 5 % .",ablation-analysi
"In this paper , we first propose a novel hard attention mechanism called "" reinforced sequence sampling ( RSS ) "" , which selects tokens from an input sequence in parallel , and differs from existing ones in that it is highly parallelizable without any recurrent structure .",model
"We then develop a model , "" reinforced self - attention ( ReSA ) "" , which naturally combines the RSS with a soft self - attention .",model
"In ReSA , two parameter - untied RSS are respectively applied to two copies of the input sequence , where the tokens from one and another are called dependent and head tokens , respectively .",model
Re SA only models the sparse dependencies between the head and dependent tokens selected by the two RSS modules .,model
"Finally , we build an sentence - encoding model , "" reinforced self - attention network ( ReSAN ) "" , based on ReSA without any CNN / RNN structure .",model
Equipping deep neural networks ( DNN ) with attention mechanisms provides an effective and parallelizable approach for context fusion and sequence compression .,research-problem
All experiments are conducted in Python with Tensorflow and run on a Nvidia GTX 1080 Ti .,experimental-setup
"We use Adadelta as optimizer , which performs more stable than Adam on ReSAN .",experimental-setup
We use 300D Glo Ve 6B pre-trained vectors,experimental-setup
All the experiments codes are released at https://github.com/ taoshen58/DiSAN /tree/master/ReSAN .,code
"In terms of prediction quality , the results show that 1 ) the unselected head tokens do contribute to the prediction , bringing 0.2 % improvement ; 2 ) using separate RSS modules to select the head and dependent tokens improves accuracy by 0.5 % ; and 3 ) hard attention and soft self - attention modules improve the accuracy by 0.3 % and 2.9 % respectively .",ablation-analysi
"We observe that SAN achieves 76.235 EM and 84.056 F1 , outperforming all other models .",result
Standard 1 - step model only achieves 75.139 EM and dynamic steps ( via ReasoNet ) achieves only 75.355 EM .,result
"SAN also outperforms a 5 - step memory net with averaging , which implies averaging predictions is not the only thing that led to SAN 's superior results ; indeed , stochastic prediction dropout is an effective technique .",result
SAN also outperforms the other models in terms of K- best oracle scores .,result
We see that SAN is very competitive in both single and ensemble settings ( ranked in second ) despite its simplicity .,result
"In this work , we derive an alternative multi-step reasoning neural network for MRC .",model
"During training , we fix the number of reasoning steps , but perform stochastic dropout on the answer module ( final layer predictions ) .",model
"During decoding , we generate answers based on the average of predictions in all steps , rather than the final step .",model
"We call this a stochastic answer network ( SAN ) because the stochastic dropout is applied to the answer module ; albeit simple , this technique significantly improves the robustness and over all accuracy of the model .",model
Stochastic Answer Networks for Machine Reading Comprehension,research-problem
Machine reading comprehension ( MRC ) is a challenging task : the goal is to have machines read a text passage and then answer any question about the passage .,research-problem
It has been hypothesized that difficult MRC problems require some form of multi-step synthesis and reasoning .,research-problem
"The spaCy tool 2 is used to tokenize the both passages and questions , and generate lemma , part - of - speech and named entity tags .",experimental-setup
The mini-batch size is set to 32 and Adamax is used as our optimizer .,experimental-setup
The learning rate is set to 0.002 at first and decreased by half after every 10 epochs .,experimental-setup
We use 2 - layer BiLSTM with d = 128 hidden units for both passage and question encoding .,experimental-setup
"We set the dropout rate for all the hidden units of LSTM , and the answer module output layer to 0.4 .",experimental-setup
"To prevent degenerate output , we ensure that at least one step in the answer module is active during training .",experimental-setup
"Here , we propose a novel RNN cell that resolves simultaneously those weaknesses of basic RNN .",model
The Rotational Unit of Memory is a modified gated model whose rotational operation acts as associative memory and is strictly an orthogonal matrix .,model
The concepts of unitary evolution matrices and associative memory have boosted the field of Recurrent Neural Networks ( RNN ) to state - of - the - art performance in a variety of sequential tasks .,research-problem
"However , RNN still have a limited capacity to manipulate long - term memory .",research-problem
"We propose a new architecture , the Gated Orthogonal Recurrent Unit ( GORU ) , which combines the advantages of the above two frameworks , namely ( i ) the ability to capture long term dependencies by using orthogonal matrices and ( ii ) the ability to "" forget "" by using a GRU structure .",model
"We demonstrate that GORU is able to learn long term dependencies effectively , even in complicated datasets which require a forgetting ability .",model
"In this work , we focus on implementation of orthogonal transition matrices which is just a subset of the unitary matrices .",model
We present a novel recurrent neural network ( RNN ) based model that combines the remembering ability of unitary RNNs with the ability of gated RNNs to effectively forget redundant / irrelevant information in its memory .,research-problem
Recurrent Neural Networks with gating units - such as Long Short Term Memory ( LSTMs ) and Gated Recurrent Units ( GRUs ) ),research-problem
These works have proven the importance of gating units for Recurrent Neural Networks .,research-problem
The main advantage of using these gated units in RNNs is primarily due to the ease of optimization of the models using them and to reduce the learning degeneracies such as vanishing gradients that can cripple conventional RNNs .,research-problem
"GORU is implemented in Tensorflow , available from https://github.com/jingli9111/GORU-tensorflow",code
"At the time of submission , our model is tied in accuracy on the hidden test set with the bestperforming published single model .",result
We achieve an F 1 score of 79.5 and EM score of 70.6 .,result
"We propose an extension of BIDAF , called Ruminating Reader , which uses a second pass of reading and reasoning to allow it to learn to avoid mistakes and to ensure that it is able to effectively use the full context when selecting an answer .",model
"In addition to adding a second pass , we also introduce two novel layer types , the ruminate layers , which use gating mechanisms to fuse the obtained from the first and second passes .",model
"In addition , we introduce an answer-question similarity loss to penalize overlap between question and predicted answer , a common feature in the errors of our base model .",model
"To answer the question in machine comprehension ( MC ) task , the models need to establish the interaction between the question and the context .",research-problem
Machine comprehension ( MC ) - especially in the form of question answering ( QA ) - is therefore attracting a significant amount of attention from the machine learning community .,research-problem
"In the character encoding layer , we use 100 filters of width 5 .",experimental-setup
"In the remainder of the model , we set the hidden layer dimension ( d ) to 100 .",experimental-setup
We use pretrained 100D Glo Ve vectors ( 6B - token version ) as word embeddings .,experimental-setup
"We use the AdaDelta optimizer ( Zeiler , 2012 ) for optimization .",experimental-setup
"Out - of - vocobulary tokens are represented by an UNK symbol in the word embedding layer , but treated normally by the character embedding layer .",experimental-setup
Batch size is 30 .,experimental-setup
"Learning rate starts at 0.5 , and decreases to 0.2 once the model stops improving .",experimental-setup
"The L2-regularization weight is 1 e - 4 , AQSL weight is 1 and dropout with a drop rate of 0.2 is A typical model run converges in about 40 k steps .",experimental-setup
We selected hyperparameter values through random search .,experimental-setup
This takes two days using Tensorflow and a single NVIDIA K80 GPU . provide an official evaluation script that allows us to measure F 1 score and EM score by comparing the prediction and ground truth answers .,experimental-setup
1 The latest results are listed at https://rajpurkar.github.io/SQuAD -explorer/,code
Experiments 3 and 4 show that the two ruminate layers are both important and helpful in contributing performance .,ablation-analysi
It is worth noting that the BiLSTM in the context ruminate layer contributes substantially to model performance .,ablation-analysi
"To tackle this limitation , we propose Distancebased Self - Attention Network which introduces a distance mask which models the relative distance between words .",model
"In conjunction with a directional mask , the distance mask allows us to incorporate complete positional information of words in our model .",model
Distance - based Self - Attention Network for Natural Language Inference,research-problem
"Our model shows good performance with NLI data , and it records the new state - of - the - art result with SNLI data .",research-problem
"More recently , models incorporating attention mechanisms have shown good performance in machine translation , Natural Language Inference ( NLI ) , and Question Answering ( QA ) etc .",research-problem
"According to the results in , a ESIM model achieved 88.1 % on SNLI corpus , elevating 0.8 percent higher than ESIM model .",result
It promoted almost 0.5 percent accuracy and outperformed the baselines on MultiNLI .,result
We use the Adam method for optimization .,hyperparameter
We use pre-trained 300 - D Glove 840B vectors to initialize word embeddings .,hyperparameter
"e initial learning rate is set to 0.0005 , and the batch size is 128 .",hyperparameter
e dimensions of all hidden states of Bi - aLSTM and word embedding are 300 .,hyperparameter
Dropout rate is set to 0.2 during training .,hyperparameter
Out - of - vocabulary ( OOV ) words are initialized randomly with Gaussian samples .,hyperparameter
We employ non-linearity function f = selu replacing recti ed linear unit ReLU on account of its faster convergence rate .,hyperparameter
"erefore , in this study , using ESIM model as the baseline , we add an a ention layer behind each Bi - LSTM layer , then use an adaptive orientation embedding layer to jointly represent the forward and backward vectors .",model
"We name this a ention boosted Bi - LSTM as Bi - a LSTM , and denote the modi ed ESIM as aESIM .",model
"is paper proposes an a ention boosted natural language inference model named a ESIM by adding word a ention and adaptive direction - oriented a ention mechanisms to the traditional Bi - LSTM layer of natural language inference models , e.g. ESIM . is makes the inference model a ESIM has the ability toe ectively learn the representation of words and model the local subsentential inference between pairs of premise and hypothesis .",research-problem
Natural language inference ( NLI ) is an important and signicant task in natural language processing ( NLP ) .,research-problem
"In the literature , the task of NLI is usually viewed as a relation classi cation .",research-problem
"We see empirically that our model outperforms all generative models on NarrativeQA , and is competitive with the top span prediction models .",result
"Furthermore , with the NOIC commonsense integration , we were able to further improve performance ( p < 0.001 on all metrics 5 ) , establishing a new state - of - the - art for the task .",result
"We also see that our model performs reasonably well on WikiHop , and further achieves promising initial improvements via the addition of commonsense , hinting at the generalizability of our approaches .",result
"We speculate that the improvement is smaller on Wikihop because only approximately 11 % of WikiHop data points require commonsense and because WikiHop data requires more fact - based commonsense ( e.g. , from Freebase ) as opposed to semantics - based commonsense ( e.g. , from Con-ceptNet ( Speer and Havasi , 2012 ) ) .",result
"In this paper , we first propose the Multi - Hop Pointer - Generator Model ( MHPGM ) , a strong baseline model that uses multiple hops of bidirectional attention , self - attention , and a pointer - generator decoder to effectively read and reason within along passage and synthesize a coherent response .",model
"Next , to address the issue that understanding human - generated text and performing longdistance reasoning on it often involves intermittent access to missing hops of external commonsense ( background ) knowledge , we present an algorithm for selecting useful , grounded multi-hop relational knowledge paths from ConceptNet ) via a pointwise mutual information ( PMI ) and term - frequency - based scoring function .",model
"We then present a novel method of inserting these selected commonsense paths between the hops of document - context reasoning within our model , via the Necessary and Optional Information Cell ( NOIC ) , which employs a selectivelygated attention mechanism that utilizes commonsense information to effectively fill in gaps of inference .",model
Commonsense for Generative Multi - Hop Question Answering Tasks,research-problem
"Reading comprehension QA tasks have seen a recent surge in popularity , yet most works have focused on fact - finding extractive QA .",research-problem
"In this paper , we explore the task of machine reading comprehension ( MRC ) based QA .",research-problem
"Much progress has been made in reasoning - based MRC - QA on the bAbI dataset , which contains questions that require the combination of multiple disjoint pieces of evidence in the context .",research-problem
https://github.com/yicheng-w/CommonSenseMultiHopQA task tests a model 's natural language understanding capabilities by asking it to answer a question based on a passage of relevant content .,code
Experiment 1 and 5 are our models presented in were also important for the model 's performance and that self - attention is able to contribute significantly to performance on top of other components of the model .,ablation-analysi
"Finally , we see that effectively introducing external knowledge via our commonsense selection algorithm and NOIC can improve performance even further on top of our strong baseline .",ablation-analysi
"The results of these are shown in , where we see that neither NumberBatch nor random - relationships nor single - hop common - sense offer statistically significant improvements 7 , whereas our commonsense selection and incorporation mechanism improves performance significantly across all metrics .",ablation-analysi
"We present a new task and dataset , which we call NarrativeQA , which will test and reward artificial agents approaching this level of competence ( Section 3 ) .",dataset
"The dataset consists of stories , which are books and movie scripts , with human written questions and answers based solely on human - generated abstractive summaries .",dataset
"For the RC tasks , questions maybe answered using just the summaries or the full story text .",dataset
The NarrativeQA Reading Comprehension Challenge,research-problem
"Reading comprehension ( RC ) - in contrast to information retrieval - requires integrating information and reasoning about events , entities , and their relations across a full document .",research-problem
"Question answering is conventionally used to assess RC ability , in both artificial agents and children learning to read .",research-problem
"We develop a simple log - linear model , in the spirit of traditional web - based QA systems , that answers questions by querying the web and extracting the answer from returned web snippets .",model
"Thus , our evaluation scheme is suitable for semantic parsing benchmarks in which the knowledge required for answering questions is covered by the web ( in contrast with virtual assitants for which the knowledge is specific to an application ) .",model
Evaluating Semantic Parsing against a Simple Web - based Question Answering Model,research-problem
"We compare our model , WEBQA , to STAGG and COMPQ , which are to the best of our knowledge the highest performing semantic parsing models on both COMPLEXQUESTIONS and WEBQUES - TIONS .",experiment
"WEBQA obtained 32.6 F 1 ( 33.5 p@1 , 42.4 MRR ) compared to 40.9 F 1 of COMPQ .",experiment
Our candidate extraction step finds the correct answer in the top - K candidates in 65.9 % of development examples and 62.7 % of test examples .,experiment
"Thus , our test F 1 on examples for which candidate extraction succeeded ( WEBQA - SUBSET ) is 51.9 ( 53.4 p@1 , 67.5 MRR ) .",experiment
"In this setup , COMPQ obtained 42.2 F 1 on the test set ( compared to 40.9 F 1 , when training on COM - PLEXQUESTIONS only , as we do ) .",experiment
"Restricting the predictions to the subset for which candidate extraction succeeded , the F 1 of COMPQ - SUBSET is 48.5 , which is 3.4 F 1 points lower than WEBQA - SUBSET , which was trained on less data .",experiment
"Code , data , annotations , and experiments for this paper are available on the CodaLab platform at https://worksheets. codalab.org/worksheets/ 0x91d77db37e0a4bbbaeb37b8972f4784f/.",code
"Note that TF - IDF is by far the most impactful feature , leading to a large drop of 12 points in performance .",ablation-analysi
We initialize word embeddings in the word representation layer with the 300 - dimensional GloVe word vectors pretrained from the 840B Common Crawl corpus .,hyperparameter
"For the out - of - vocabulary ( OOV ) words , we initialize the word embeddings randomly .",hyperparameter
"For the charactercomposed embeddings , we initialize each character as a 20 - dimensional vector , and compose each word into a 50 dimensional vector with a LSTM layer .",hyperparameter
We set the hidden size as 100 for all BiLSTM layers .,hyperparameter
We set the learning rate as 0.001 .,hyperparameter
"We apply dropout to every layers in , and set the dropout ratio as 0.1 .",hyperparameter
"To train the model , we minimize the cross entropy of the training set , and use the ADAM optimizer [ Kingma and Ba , 2014 ] to update parameters .",hyperparameter
"During training , we do not update the pre-trained word embeddings .",hyperparameter
"In this paper , to tackle these limitations , we propose a bilateral multi-perspective matching ( BiMPM ) model for NLSM tasks .",model
"Our model essentially belongs to the "" matching aggregation "" framework .",model
"Then , another BiLSTM layer is utilized to aggregate the matching results into a fixed - length matching vector .",model
"Finally , based on the matching vector , a decision is made through a fully connected layer .",model
Natural language sentence matching is a fundamental technology for a variety of tasks .,research-problem
Natural language sentence matching ( NLSM ) is the task of comparing two sentences and identifying the relationship between them .,research-problem
"For example , in a paraphrase identification task , NLSM is used to determine whether two sentences are paraphrase or not .",research-problem
We call embeddings learned in this way order- embeddings .,model
"In contrast , we propose to exploit the partial order structure of the visual - semantic hierarchy by learning a mapping which is not distance - preserving but order - preserving between the visualsemantic hierarchy and a partial order over the embedding space .",model
ORDER - EMBEDDINGS OF IMAGES AND LANGUAGE,research-problem
"Hypernymy , textual entailment , and image captioning can be seen as special cases of a single visual - semantic hierarchy over words , sentences , and images .",research-problem
"In fact , all three relations can be seen as special cases of a partial order over images and language , illustrated in , which we refer to as the visualsemantic hierarchy .",research-problem
"With this motivation , we propose a novel neural network architecture called Reasoning Network ( ReasoNet ) .",model
"With a question in mind , ReasoNets read a document repeatedly , each time focusing on di erent parts of the document until a satisfying answer is found or formed .",model
"Moreover , unlike previous approaches using xed number of hops or iterations , ReasoNets introduce a termination state in the inference .",model
"Motivated by , we tackle this challenge by proposing a reinforcement learning approach , which utilizes an instance - dependent reward baseline , to successfully train ReasoNets .",model
ReasoNet : Learning to Stop Reading in Machine Comprehension,research-problem
"In this work , we build a QA model that can understand long documents by utilizing Memory Augmented Neural Networks ( MANNs ) .",model
This type of neural networks decouples the memory capacity from the number of model parameters .,model
A Multi - Stage Memory Augmented Neural Network for Machine Reading Comprehension,research-problem
Reading Comprehension ( RC ) of text is one of the fundamental tasks in natural language processing .,research-problem
"In recent years , several end - to - end neural network models have been proposed to solve RC tasks .",research-problem
"One possible way of measuring RC is by formulating it as answer span prediction style Question Answering ( QA ) task , which is finding an answer to the question based on the given document ( s ) .",research-problem
"Recently , influential deep learning approaches have been proposed to solve this QA task . ; propose the attention mechanism between question and context for question - aware contextual representation .",research-problem
We develop MAMCN using Tensorflow 1 deep learning framework and Sonnet 2 library .,experimental-setup
"For the word - level embedding , we tokenize the documents using NLTK toolkit and substitute words with GloVe 6B 43.16 46.90 49.28 55.83 BiDAF 40.32 45.91 44.86 50.71 hidden size is set to 200 for QUASAR - T and Triv - iaQA , and 100 for SQuAD .",experimental-setup
"In the memory controller , we use 100 x 36 size memory initialized with zeros , 4 read heads and 1 write head .",experimental-setup
"The optimizer is AdaDelta ( Zeiler , 2012 ) with an initial learning rate of 0.5 .",experimental-setup
"We train our model for 12 epochs , and batch size is set to 30 .",experimental-setup
"During the training , we keep the exponential moving average of weights with 0.001 decay and use these averages at test time .",experimental-setup
"The word embeddings for all of the models are initialized with the 100d GloVe vectors ( 840B token version , ) and fine - tuned during training to improve the performance .",hyperparameter
"The other parameters are initialized by randomly sampling from uniform distribution in [ ? 0.1 , 0.1 ] .",hyperparameter
"For each task , we take the hyperparameters which achieve the best performance on the development set via an small grid search over combinations of the initial learning rate [ 0.05 , 0.0005 , 0.0001 ] , l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ] and the threshold value",hyperparameter
"In this paper , we propose a new deep neural network architecture to model the strong interactions of two sentences .",model
"Specifically , we propose two interdependent ways for the coupled - LSTMs : loosely coupled model ( LC - LSTMs ) and tightly coupled model ( TC - LSTMs ) .",model
"Different with modelling two sentences with separated LSTMs , we utilize two interdependent LSTMs , called coupled - LSTMs , to fully affect each other at different time steps .",model
"To utilize all the information of four directions of coupled - LSTMs , we aggregate them and adopt a dynamic pooling strategy to automatically select the most informative interaction signals .",model
"Finally , we feed them into a fully connected layer , followed by an output layer to compute the matching score .",model
The output of coupled - LSTMs at each step depends on both sentences .,model
Modelling Interaction of Sentence Pair with Coupled- LSTMs,research-problem
"Recently , there is rising interest in modelling the interactions of two sentences with deep neural networks .",research-problem
"Among these tasks , a common problem is modelling the relevance / similarity of the sentence pair , which is also called text semantic matching .",research-problem
"In contrast , we focus on capturing fine - grained word - level information directly .",approach
"First , instead of using sentence modeling , we propose pairwise word interaction modeling that encourages explicit word context interactions across sentences .",approach
"Second , based on the pairwise word interactions , we describe a novel similarity focus layer which helps the model selectively identify important word interactions depending on their importance for similarity measurement .",approach
Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement,research-problem
"Textual similarity measurement is a challenging problem , as it requires understanding the semantics of input sentences .",research-problem
"Given two pieces of text , measuring their semantic textual similarity ( STS ) remains a fundamental problem in language research and lies at the core of many language processing tasks , including question answering , query ranking , and paraphrase generation .",research-problem
"For the SICK and MSRVID experiments , we used 300 - dimension Glo Ve word embeddings .",experimental-setup
"For the STS2014 , WikiQA , and TrecQA experiments , we used 300 dimension PARAGRAM - SL999 embeddings from and the PARAGRAM - PHRASE embeddings from , trained on word pairs from the Paraphrase Database ( PPDB ) .",experimental-setup
Our timing experiments were conducted on an Intel Xeon E5 - 2680 CPU .,experimental-setup
"Due to sentence length variations , for the SICK and MSRVID data we padded the sentences to 32 words ; for the STS2014 , WikiQA , and TrecQA data , we padded the sentences to 48 words ..",experimental-setup
"We found large drops when removing the context modeling component , indicating that the context information provided by the Bi - LSTMs is crucial for the following components ( e.g. , interaction modeling ) .",ablation-analysi
"The use of our similarity focus layer is also beneficial , especially on the WikiQA data .",ablation-analysi
"When we replaced the entire similarity focus layer with a random dropout layer ( p = 0.3 ) , the dropout layer hurts accuracy ; this shows the importance of directing the model to focus on important pairwise word interactions , to better capture similarity .",ablation-analysi
Our neural BoW baseline achieves good results on both datasets ( Tables 3 and 1 ) 5 .,result
"For instance , it outperforms a feature rich logistic - regression baseline on the SQuAD development set and nearly reaches the BiLSTM baseline system ( i.e. , FastQA without character embeddings and features ) .",result
It is very competitive to previously established stateof - the - art results on the two datasets and even improves those for News QA .,result
"In particular , we develop a simple neural , bag - of - words ( BoW ) - and a recurrent neural network ( RNN ) baseline , namely FastQA .",model
"Crucially , both models do not make use of a complex interaction layer but model interaction between question and context only through computable features on the word level .",model
Making Neural QA as Simple as Possible but not Simpler,research-problem
Recent development of large - scale question answering ( QA ) datasets triggered a substantial amount of research into end - toend neural architectures for QA .,research-problem
Question answering is an important end - user task at the intersection of natural language processing ( NLP ) and information retrieval ( IR ) .,research-problem
Swag : A Large - Scale Adversarial Dataset for Grounded Commonsense Inference,research-problem
Gated Self - Matching Networks for Reading Comprehension and Question Answering,research-problem
"In this paper , we present the gated selfmatching networks for reading comprehension style question answering , which aims to answer questions from a given passage .",research-problem
We use the tokenizer from Stanford CoreNLP to preprocess each passage and question .,experimental-setup
The hidden vector length is set to 75 for all layers .,experimental-setup
The hidden size used to compute attention scores is also 75 .,experimental-setup
The Gated Recurrent Unit variant of LSTM is used throughout our model .,experimental-setup
"The model is optimized with AdaDelta ( Zeiler , 2012 ) with an initial learning rate of 1 .",experimental-setup
"For word embedding , we use pretrained case - sensitive GloVe embeddings 2 ( Pennington et al. , 2014 ) for both questions and passages , and it is fixed during training ; We use zero vectors to represent all out - of - vocab words .",experimental-setup
"We utilize 1 layer of bi-directional GRU to compute character - level embeddings and 3 layers of bi-directional GRU to encode questions and passages , the gated attention - based recurrent network for question and passage matching is also encoded bidirectionally in our experiment .",experimental-setup
We also apply dropout between layers with a dropout rate of 0.2 .,experimental-setup
The ? and used in AdaDelta are 0.95 and 1e ? 6 respectively .,experimental-setup
attention - based recurrent network ( GARNN ) and self - matching attention mechanism positively contribute to the final results of gated self - matching networks .,ablation-analysi
Characterlevel embeddings contribute towards the model 's performance since it can better handle out - ofvocab or rare words .,ablation-analysi
Character - level embeddings are not utilized .,ablation-analysi
"As shown in , the gate introduced in question and passage matching layer is helpful for both GRU and LSTM on the SQuAD dataset .",ablation-analysi
"Removing self - matching results in 3.5 point EM drop , which reveals that information in the passage plays an important role .",ablation-analysi
All models are trained end - to - end jointly with the refinement module using a dimensionality of n = 300 for all but the TriviaQA experiments for which we had to reduce n to 150 due to memory constraints .,hyperparameter
All baselines operate on the unrefined word embeddings E 0 described in 3.1 .,hyperparameter
For the DQA baseline system we add the lemma in - question feature ( liq ) suggested in .,hyperparameter
"In this paper , we develop a new architecture for dynamically incorporating external background knowledge in NLU models .",model
"Rather than relying only on static knowledge implicitly present in the training data , supplementary knowledge is retrieved from external knowledge sources ( in this paper , ConceptNet and Wikipedia ) to assist with understanding text inputs .",model
These refined embeddings are then used as input to a task - specific NLU architecture ( any architecture that reads text as a sequence of word embeddings can be used here ) .,model
The retrieved supplementary texts are read together with the task inputs by an initial reading module whose outputs are contextually refined word embeddings ( 3 ) .,model
"The initial reading module and the task module are learnt jointly , end - to - end .",model
Dynamic Integration of Background Knowledge in Neural NLU Systems,research-problem
"Common- sense and background knowledge is required to understand natural language , but in most neural natural language understanding ( NLU ) systems , this knowledge must be acquired from training corpora during learning , and then it is static at test time .",research-problem
The idea is to use multiple memory slots outside the recurrence to piece - wise store representations of the input ; read and write operations for each slot can be modeled as an attention mechanism with a recurrent controller .,model
We also leverage memory and attention to empower a recurrent network with stronger memorization capability and more importantly the ability to discover relations among tokens .,model
This is realized by inserting a memory network module in the update of a recurrent network together with attention for memory addressing .,model
"The resulting model , which we term Long Short - Term Memory - Network ( LSTMN ) , is a reading simulator that can be used for sequence processing tasks .",model
The model processes text incrementally while learning which past tokens in the memory and to what extent they relate to the current token being processed .,model
"As a result , the model induces undirected relations among tokens as an intermediate step of learning representations .",model
Long Short - Term Memory - Networks for Machine Reading,research-problem
Our code is available at https://github.com/cheng6076/,code
"Learning representations for rare words is a well - known challenge of natural language understanding , since the standard end - to - end supervised learning methods require many occurrences of each word to generalize well .",research-problem
LEARNING TO COMPUTE WORD EMBEDDINGS ON THE FLY,research-problem
It also outperforms all the published works and achieves the 2nd place on the leaderboard when submitting SG - NET .,result
"We also find that adding an extra answer verifier module could yield better result , which is pre-trained only to determine whether question is answerable or not with the same training data as SG - Net .",result
"In this paper , we extend the self - attention mechanism with syntax - guided constraint , to capture syntax related parts with each concerned word .",model
"Specifically , we adopt pre-trained dependency syntactic parse tree structure to produce the related nodes for each word in a sentence , namely syntactic dependency of interest ( SDOI ) , by regarding each word as a child node and the SDOI consists all its ancestor nodes and itself in the dependency parsing tree .",model
"To effectively accommodate such SDOI information , we propose a novel syntax - guided network ( SG - Net ) , which fuses the original SAN and SDOI - SAN , to provide more linguistically inspired representation for challenging reading comprehension tasks 1 .",model
SG - Net : Syntax - Guided Machine Reading Comprehension,research-problem
"Understanding the meaning of a sentence is a prerequisite to solve many natural language understanding ( NLU ) problems , such as machine reading comprehension ( MRC ) based question answering .",research-problem
We observe that the accuracy of MRC models decreases when answering long questions ( shown in Section 5.1 ) .,research-problem
"We propose a novel attention mechanism that differs from previous ones in that it is 1 ) multi-dimensional : the attention w.r.t. each pair of elements from the source ( s ) is a vector , where each entry is the attention computed on each feature ; and 2 ) directional : it uses one or multiple positional masks to model the asymmetric attention between two elements .",model
"We compute feature - wise attention since each element in a sequence is usually represented by a vector , e.g. , word / character embedding , and attention on different features can contain different information about dependency , thus to handle the variation of contexts around the same word .",model
We apply positional masks to attention distribution since they can easily encode prior structure knowledge such as temporal order and dependency parsing .,model
"We then build a light - weight and RNN / CNN - free neural network , "" Directional Self - Attention Network ( DiSAN ) "" , for sentence encoding .",model
"In DiSAN , the input sequence is processed by directional ( forward and backward ) self - attentions to model context dependency and produce context - aware representations for all tokens .",model
"Then , a multi-dimensional attention computes a vector representation of the entire sequence , which can be passed into a classification / regression module to compute the final prediction for a particular task .",model
DiSAN : Directional Self - Attention Network for RNN / CNN - Free Language Understanding,research-problem
"In this paper , we study the task of learning universal representations of sentences , i.e. , a sentence encoder model that is trained on a large corpus and subsequently transferred to other tasks .",approach
"Here , we investigate whether supervised learning can be leveraged instead , taking inspiration from previous results in computer vision , where many models are pretrained on the ImageNet ) before being transferred .",approach
"Hence , we investigate the impact of the sentence encoding architecture on representational transferability , and compare convolutional , recurrent and even simpler word composition schemes .",approach
"For all our models trained on SNLI , we use SGD with a learning rate of 0.1 and a weight decay of 0.99 .",hyperparameter
"For the classifier , we use a multi - layer perceptron with 1 hidden - layer of 512 hidden units .",hyperparameter
"At each epoch , we divide the learning rate by 5 if the dev accuracy decreases .",hyperparameter
We use minibatches of size 64 and training is stopped when the learning rate goes under the threshold of 10 ?5 .,hyperparameter
We use opensource GloVe vectors trained on Common Crawl 840B with 300 dimensions as fixed word embeddings .,hyperparameter
Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,research-problem
"In this work , we present a fine - grained gating mechanism to combine the word - level and characterlevel representations .",model
We compute a vector gate as a linear projection of the token features followed 1 Code is available at https://github.com/kimiyoung/fg-gating 1 ar Xiv: 1611.01724v2 [ cs.CL ] 11 Sep 2017,model
We then multiplicatively apply the gate to the character - level and wordlevel representations .,model
Each dimension of the gate controls how much information is flowed from the word - level and character - level representations respectively .,model
"More generally , our fine - grained gating mechanism can be used to model multiple levels of structure in language , including words , characters , phrases , sentences and paragraphs .",model
"We use named entity tags , part - ofspeech tags , document frequencies , and word - level representations as the features for token properties which determine the gate .",model
Published as a conference paper at ICLR 2017 WORDS OR CHARACTERS ? FINE - GRAINED GATING FOR READING COMPREHENSION,research-problem
We compute a vector gate as a linear projection of the token features followed 1 Code is available at https://github.com/kimiyoung/fg-gating 1 ar Xiv: 1611.01724v2 [ cs.CL ] 11 Sep 2017,code
"In this paper , we propose Structural Embedding of Syntactic Trees ( SEST ) that encode syntactic information structured by constituency tree and dependency tree into neural attention models for the question answering task .",model
Structural Embedding of Syntactic Trees for Machine Comprehension,research-problem
"Reading comprehension such as SQuAD or News QA requires identifying a span from a given context , which is an extension to the traditional question answering task , aiming at responding questions posed by human with natural language .",research-problem
We run our experiments on a machine that contains a single GTX 1080 GPU with 8 GB VRAM .,experimental-setup
"As introduced in Section 2 , we use a variable character embedding with a fixed pre-trained word embedding to serve as part of the input into the model .",experimental-setup
The character embedding is implemented using CNN with a one -dimensional layer consists of 100 units with a channel size of 5 .,experimental-setup
It has an input depth of 8 .,experimental-setup
The max length of SQuAD is 16 which means there are a maximum 16 words in a sentence .,experimental-setup
"The fixed word embedding has a dimension of 100 , which is provided by the GloVe data set .",experimental-setup
The POS model contains syntactic information with 39 different POS tags that serve as both input and output .,experimental-setup
For SECT and SEDT the input of the model has a size of 8 with 30 units to be output .,experimental-setup
"Both of them has a maximum length size that is set to be 10 and 20 respectively , which values will be further discussed in Section 4.5 .",experimental-setup
"We use the Adam optimizer [ Kingma and Ba , 2014 ] for both ML and DCRL training .",hyperparameter
"The initial learning rates are 0.0008 and 0.0001 respectively , and are halved whenever meeting a bad iteration .",hyperparameter
Word embeddings remain fixed during training .,hyperparameter
"The size of character embedding and corresponding LSTMs is 50 , the main hidden size is 100 , and the hyperparameter ? is 3 .",hyperparameter
The batch size is 48 and a dropout rate of 0.3 is used to prevent overfitting .,hyperparameter
"For out of vocabulary words , we set the embeddings from Gaussian distributions and keep them trainable .",hyperparameter
Reinforced Mnemonic Reader for Machine Reading Comprehension,research-problem
"We notice that reattention has more influences on EM score while DCRL contributes more to F1 metric , and removing both of them results in huge drops on both metrics .",ablation-analysi
Replacing DCRL with SCST also causes a marginal decline of performance on both metrics .,ablation-analysi
"Next , we relace the default attention function with the dot product : f ( u , v ) = u v ( 5 ) , and both metrics suffer from degradations .",ablation-analysi
"Removing any of the two heuristics leads to some performance declines , and heuristic subtraction is more effective than multiplication .",ablation-analysi
In both cases the highway - like function has outperformed its simpler variants .,ablation-analysi
"Interestingly , a very deep alignment with 5 blocks results in a significant performance decline .",ablation-analysi
"We notice that using 2 blocks causes a slight performance drop , while increasing to 4 blocks barely affects the SoTA result .",ablation-analysi
"We have the following observations : ( 1 ) First of all , we can see that when we set d to 300 , our model achieves an accuracy of 86.1 % on the test data , which to the best of our knowledge is the highest on and |?| M is the number of parameters excluding the word embeddings .",result
"( 2 ) If we compare our m LSTM model with our implementation of the word - by - word attention model by under the same setting with d = 150 , we can see that our performance on the test data ( 85.7 % ) is higher than that of their model ( 82.6 % ) .",result
"( 3 ) The performance of mLSTM with bi - LSTM sentence modeling compared with the model with standard LSTM sentence modeling when d is set to 150 shows that using bi - LSTM to process the original sentences helps ( 86.0 % vs. 85.7 % on the test data ) , but the difference is small and the complexity of bi - LSTM is much higher than LSTM .",result
"( 4 ) Interestingly , when we experimented with the m LSTM model using the pre-trained word embeddings instead of LSTMgenerated hidden states as initial representations of the premise and the hypothesis , we were able to achieve an accuracy of 85.3 % on the test data , which is still better than previously reported state of the art .",result
"In this paper , we propose a new LSTM - based architecture for learning natural language inference .",model
"Instead , we use an LSTM to perform word - by - word matching of the hypothesis with the premise .",model
"Our LSTM sequentially processes the hypothesis , and at each position , it tries to match the current word in the hypothesis with an attention - weighted representation of the premise .",model
"We refer to this architecture a match - LSTM , or m LSTM for short .",model
Learning Natural Language Inference with LSTM,research-problem
Natural language inference ( NLI ) is a fundamentally important task in natural language processing that has many applications .,research-problem
"In this paper , we propose a special long short - term memory ( LSTM ) architecture for NLI .",research-problem
1 https://github.com/shuohangwang/,code
"In this work , we explore the supervised learning of task - specific , dynamic meta-embeddings , and apply the technique to sentence representations .",approach
"First , it is embedding - agnostic , meaning that one of the main ( and perhaps most important ) hyperparameters in NLP pipelines is made obsolete .",approach
Dynamic Meta - Embeddings for Improved Sentence Representations,research-problem
"In this work , we explore the supervised learning of task - specific , dynamic meta-embeddings , and apply the technique to sentence representations .",research-problem
Linguistic Knowledge as Memory for Recurrent Neural Networks,research-problem
Training recurrent neural networks to model long term dependencies is difficult .,research-problem
"In this work , we introduce the COmmonsense Dataset Adversarially - authored by Humans ( CODAH ) for commonsense question answering in the style of SWAG multiple choice sentence completion .",dataset
"As a baseline , we evaluate both models on the full SWAG training and validation sets , providing an accuracy of 84.2 % on BERT and 80.2 % on GPT .",result
"To adjust for the difference in size between our dataset and SWAG , we also train the models on a sample of 2,241 SWAG questions ( the size of the training set in each of CODAH 's crossvalidation folds ) and evaluate them on the full SWAG validation set .",result
This produces an accuracy of 75.2 % for BERT ( using the cross-validation grid search ) and 63.6 % for GPT . :,result
"We propose a novel method for question generation , in which human annotators are educated on the workings of a state - of - the - art question answering model , and are asked to submit questions that adversarially target the weaknesses .",model
"Annotators are rewarded for submissions in which the model fails to identify the correct sentence completion both before and after fine - tuning on a sample of the submitted questions , encouraging the creation of questions that are not easily learnable .",model
CODAH : An Adversarially - Authored Question Answering Dataset for Common Sense,research-problem
"Commonsense reasoning is a critical AI capability , but it is difficult to construct challenging datasets that test commonsense .",research-problem
The rise of datadriven methods has led to interest in developing large datasets for commonsense reasoning over text .,research-problem
"In this work , we introduce the COmmonsense Dataset Adversarially - authored by Humans ( CODAH ) for commonsense question answering in the style of SWAG multiple choice sentence completion .",research-problem
The full dataset is available at https://github.com/Websail-NU /CODAH .,code
It clearly outperforms the similar but non-hierarchical BiLSTM models reported in the literature and fares well in comparison to other state of the art architectures in the sentence encoding category .,result
"In particular , our results are close to the current state of the art on SNLI in this category and strong on both , the matched and mismatched test sets of MultiNLI .",result
"Finally , on SciTail , we achieve the new state of the art with an accuracy of 86.0 % .",result
"For the SNLI dataset , our model provides the test accuracy of 86.6 % after 4 epochs of training .",result
"For the MultiNLI matched test set ( MultiNLI - m ) our model achieves a test accuracy of 73.7 % after 3 epochs of training , which is 0.8 % points lower than the state of the art 74.5 % by .",result
"For the mismatched test set ( MultiNLI - mm ) our model achieves a test accuracy of 73.0 % after 3 epochs of training , which is 0.6 % points lower than the state of the art 73.6 % by Chen , Zhu , Ling , Wei , Jiang , and Inkpen ( 2017 b ) .",result
"On the SciTail dataset we compared our model also against non-sentence embedding - based models , as no results have been previously published which are based on independent sentence embeddings .",result
"We obtain a score of 86.0 % after 4 epochs of training , which is + 2.7 % points absolute improvement on the previous published state of the art by .",result
Our model also outperforms In - fer Sent which achieves an accuracy of 85.1 % in our experiments .,result
The results achieved by our proposed model are significantly higher than the previously published results .,result
"With the goal of obtaining general - purpose sentence representations in mind , we opt for the sentence encoding approach .",model
Motivated by the success of the InferSent architecture we extend their architecture with a hierarchylike structure of bidirectional LSTM ( BiLSTM ) layers with max pooling .,model
Sentence Embeddings in NLI with Iterative Refinement Encoders,research-problem
Sentence - level representations are necessary for various NLP tasks .,research-problem
The architecture was implemented using PyTorch .,experimental-setup
"The sentence embeddings have hidden size of 600 for both direction ( except for SentEval test , where we test models with 600D and 1200D per direction ) and the 3 - layer multilayer perceptron ( MLP ) have the size of 600 dimensions .",experimental-setup
"For all of our models we used a gradient descent optimization algorithm based on the Adam update rule , which is pre-implemented in PyTorch .",experimental-setup
We used a learning rate of 5e - 4 for all our models .,experimental-setup
The learning rate was decreased by the factor of 0.2 after each epoch if the model did not improve .,experimental-setup
We used a batch size of 64 .,experimental-setup
"We use pre-trained Glo Ve word embeddings of size 300 dimensions ( Glo Ve 840B 300D ; , which were fine - tuned during training .",experimental-setup
We use a dropout of 0.1 between the MLP layers ( except just before the final layer ) .,experimental-setup
Our models were trained using one NVIDIA Tesla P100 GPU .,experimental-setup
We have published our code in GitHub : https://github.com/Helsinki-NLP/HBMP.,code
Replacing C 2 by C 1 induces a large drop in performance because many questions do not have answers that are directly connected to their inluded entity ( not in C 1 ) .,result
"However , using all 2 - hops connections as a candidate set is also detrimental , because the larger number of candidates confuses ( and slows a lot ) our ranking based inference .",result
"Our results also verify our hypothesis of Section 3.1 , that a richer representation for answers ( using the local subgraph ) can store more pertinent information .",result
"Finally , we demonstrate that we greatly improve upon the model of , which actually corresponds to a setting with the Path representation and C 1 as candidate set .",result
"The ensemble improves the state - of - the - art , and indicates that our models are significantly different in their design .",result
"In this paper , we improve the model of by providing the ability to answer more complicated questions .",model
s The main contributions of the paper are : ( 1 ) a more sophisticated inference procedure that is both efficient and can consider longer paths ( considered only answers directly connected to the question in the graph ) ; and ( 2 ) a richer representation of the answers which encodes the question - answer path and surrounding subgraph of the KB .,model
Question Answering with Subgraph Embeddings,research-problem
This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few handcrafted features .,research-problem
Teaching machines how to automatically answer questions asked in natural language on any topic or in any domain has always been along standing goal in Artificial Intelligence .,research-problem
"With the rise of large scale structured knowledge bases ( KBs ) , this problem , known as open - domain question answering ( or open QA ) , boils down to being able to query efficiently such databases with natural language .",research-problem
"These KBs , such as Freebase encompass huge ever growing amounts of information and ease open QA by organizing a great variety of answers in a structured format .",research-problem
We use cross - entropy loss as the training objective with Adam - based opti-Model Accuracy SNLI Multi - NLI Matched Multi - NLI Mismatched CBOW 80.6 65.2 64.6 biLSTM Encoder 81.5 67.5 67.1 300D Tree - CNN Encoder 82.1 --300D SPINN - PI Encoder 83.2 --300D NSE Encoder 84.6 --biLSTM -Max Encoder 84 . mization with 32 batch size .,hyperparameter
The starting learning rate is 0.0002 with half decay every two epochs .,hyperparameter
The number of hidden units for MLP in classifier is 1600 .,hyperparameter
"Dropout layer is also applied on the output of each layer of MLP , with dropout rate set to 0.1 .",hyperparameter
We used pre-trained 300D Glove 840B vectors to initialize the word embeddings .,hyperparameter
"In this paper , we follow the former approach of encoding - based models , and propose a novel yet simple sequential sentence encoder for the Multi - NLI problem .",model
It is basically a stacked ( multi-layered ) bidirectional LSTM - RNN with shortcut connections ( feeding all previous layers ' outputs and word embeddings to each layer ) and word embedding fine - tuning .,model
"The over all supervised model uses these shortcutstacked encoders to encode two input sentences into two vectors , and then we use a classifier over the vector combination to label the relationship between these two sentences as that of entailment , contradiction , or neural ( similar to the classifier setup of and ) .",model
Shortcut - Stacked Sentence Encoders for Multi- Domain Inference,research-problem
We present a simple sequential sentence encoder for multi-domain natural language inference .,research-problem
Natural language inference ( NLI ) or recognizing textual entailment ( RTE ) is a fundamental semantic task in the field of natural language processing .,research-problem
Github Code Link : https://github.com/ easonnie/multiNLI_encoder,code
"These ablation results are shown in and 4 , all based on the Multi - NLI development sets .",ablation-analysi
"As shown , each added layer model improves the accuracy and we achieve a substantial improvement in accuracy ( around 2 % ) on both matched and mismatched settings , compared to the single - layer biLSTM in .",ablation-analysi
The last ablation in shows that a classifier with two layers of relu is preferable than other options .,ablation-analysi
"Next , in , we show that the shortcut connections among the biLSTM layers is also an important contributor to accuracy improvement ( around 1.5 % on top of the full 3 - layered stacked - RNN model ) .",ablation-analysi
"Next , in , we show that fine - tuning the word embeddings also improves results , again for both the in - domain task and cross - domain tasks ( the ablation results are based on a smaller model with a 128 +256 2 - layer biLSTM ) .",ablation-analysi
"We find that the bare SPINN - PI - NT model performs little better than the RNN baseline , but that SPINN - PI with the added tracking LSTM performs well .",result
"The full SPINN model with its relatively weak internal parser performs slightly less well , but nonetheless robustly exceeds the performance of the RNN baseline .",result
Both SPINN - PI and the full SPINN significantly outperform all previous sentence - encoding models .,result
"Most notably , these models outperform the tree - based CNN of , which also uses tree - structured composition for local feature extraction , but uses simpler pooling techniques to build sentence features in the interest of efficiency .",result
"The full SPINN performed moderately well at reproducing the Stanford Parser 's parses of the SNLI data at a transition - by - transition level , with 92.4 % accuracy at test time .",result
"Our results show that a model that uses tree - structured composition fully ( SPINN ) outper - forms one which uses it only partially ( tree - based CNN ) , which in turn outperforms one which does not use it at all ( RNN ) .",result
A Fast Unified Model for Parsing and Sentence Understanding,research-problem
Our optimized C ++/ CUDA models and the Theano source code for the full SPINN are available at https://github.com / stanfordnlp/spinn. 30 tokens or fewer .,experimental-setup
We fix the model dimension D and the word embedding dimension at 300 .,experimental-setup
We run the CPU performance test on a 2.20 GHz 16 core Intel Xeon E5-2660 processor with hyperthreading enabled .,experimental-setup
We test our thin - stack implementation and the RNN model on an NVIDIA Titan X GPU .,experimental-setup
Our optimized C ++/ CUDA models and the Theano source code for the full SPINN are available at https://github.com / stanfordnlp/spinn. 30 tokens or fewer .,code
"In this work we propose the Key - Value Memory Network ( KV - MemNN ) , a new neural network architecture that generalizes the original Memory Network and can work with either knowledge source .",model
The KV - MemNN performs QA by first storing facts in a key - value structured memory before reasoning on them in order to predict an answer .,model
"The memory is designed so that the model learns to use keys to address relevant memories with respect to the question , whose corresponding values are subsequently returned .",model
"This structure allows the model to encode prior knowledge for the considered task and to leverage possibly complex transforms between keys and values , while still being trained using standard backpropagation via stochastic gradient descent .",model
Directly reading documents and being able to answer questions from them is an unsolved challenge .,research-problem
"To avoid its inherent difficulty , question answering ( QA ) has been directed towards using Knowledge Bases ( KBs ) instead , which has proven effective .",research-problem
"To compare using KBs , information extraction or Wikipedia documents directly in a single framework we construct an analysis tool , WIKIMOVIES , a QA dataset that contains raw text alongside a preprocessed KB , in the domain of movies .",research-problem
All the above subtasks have been modeled as binary classification problems : kernel - based classifiers are trained and the classification score is used to sort the instances and produce the final ranking .,model
"All classifiers and kernels have been implemented within the Kernel - based Learning Platform 2 ( KeLP ) , thus determining the team 's name .",model
"The proposed solution provides three main contributions : ( i ) we employ the approach proposed in , which applies tree kernels directly to question and answer texts modeled as pairs of linked syntactic trees .",model
( iii ) we propose a stacking schema so that classifiers for Subtask B and C exploit the inferences obtained in the previous subtasks .,model
This paper describes the KeLP system participating in the SemEval - 2016 Community Question Answering ( c QA ) task .,research-problem
"In this task , participants are asked to automatically provide good answers in a c QA setting .",research-problem
"In this paper , we aim to develop a QA system that is scalable to large documents as well as robust to adversarial inputs .",model
"First , we study the context required to answer the question by sampling examples in the dataset and carefully analyzing them .",model
"Second , inspired by this observation , we propose a sentence selector to select the minimal set of sentences to give to the QA model in order to answer the question .",model
"Since the minimum number of sentences depends on the question , our sentence selector chooses a different number of sentences for each question , in contrast with previous models that select a fixed number of sentences .",model
"Our sentence selector leverages three simple techniques - weight transfer , data modification and score normalization , which we show to be highly effective on the task of sentence selection .",model
Efficient and Robust Question Answering from Minimal Context over Documents,research-problem
Neural models for question answering ( QA ) over documents have achieved significant performance improvements .,research-problem
"Inspired by this observation , we propose a simple sentence selector to select the minimal set of sentences to feed into the QA model .",research-problem
"In addition , we also use our implementation of ESIM , which achieves an accuracy of 76.8 % in the in - domain test set , and 75.8 % in the cross - domain test set , which presents the state - of - the - art results .",result
"After removing the cross - sentence attention and adding our gated - attention model , we achieve accuracies of 73.5 % and 73.6 % , which ranks first in the cross - domain test set and ranks second in the in - domain test set among the single models .",result
"When ensembling our models , we obtain accuracies 74.9 % and 74.9 % , which ranks first in both test sets .",result
Recurrent Neural Network - Based Sentence Encoder with Gated Attention for Natural Language Inference,research-problem
"Task aims to evaluate language understanding models for sentence representation with natural language inference ( NLI ) tasks , where a sentence is represented as a fixedlength vector .",research-problem
"Specifically , NLI is concerned with determining whether a hypothesis sentence h can be inferred from a premise sentence p.",research-problem
"We use the Adam ( Kingma and Ba , 2014 ) for optimization .",experimental-setup
We use pretrained GloVe - 840B - 300D vectors as our word - level embeddings and fix these embeddings during the training process .,experimental-setup
"Stacked BiLSTM has 3 layers , and all hidden states of BiLSTMs and MLP have 600 dimensions .",experimental-setup
"The character embedding has 15 dimensions , and CNN filters length is [ 1 , 3 , 5 ] , each of those is 100 dimensions .",experimental-setup
Out - of - vocabulary ( OOV ) words are initialized randomly with Gaussian samples .,experimental-setup
"To help replicate our results , we publish our code at https : //github.com/lukecq1231/enc_nli",code
"If we remove the gated - attention , the accuracies drop to 72.8 % and 73.6 % .",ablation-analysi
"If we remove charactercomposition vector , the accuracies drop to 72.9 % and 73.5 % .",ablation-analysi
"If we remove word - level embedding , the accuracies drop to 65.6 % and 66.0 % .",ablation-analysi
"As can be seen from the for long documents , bag - of - words models perform quite well and it is difficult to improve upon them using word vectors .",result
The combination of two models yields an improvement approximately 1.5 % in terms of error rates .,result
The method described in this paper is the only approach that goes significantly beyond the barrier of 10 % error rate .,result
It achieves 7.42 % which is another 1.3 % absolute improvement ( or 15 % relative improvement ) over the best previous result of ..,result
"In this paper , we propose Paragraph Vector , an unsupervised framework that learns continuous distributed vector representations for pieces of texts .",model
"The name Paragraph Vector is to emphasize the fact that the method can be applied to variable - length pieces of texts , anything from a phrase or sentence to a large document .",model
"In our model , the vector representation is trained to be useful for predicting words in a paragraph .",model
Both word vectors and paragraph vectors are trained by the stochastic gradient descent and backpropagation .,model
"While paragraph vectors are unique among paragraphs , the word vectors are shared .",model
"More precisely , we concatenate the paragraph vector with several word vectors from a paragraph and predict the following word in the given context .",model
"At prediction time , the paragraph vectors are inferred by fixing the word vectors and training the new paragraph vector until convergence .",model
Distributed Representations of Sentences and Documents,research-problem
"In this work , we focus on the SQuAD dataset and propose an end - to - end deep neural network model for machine comprehension .",model
"Based on this assumption , we design a Multi - Perspective Context Matching ( MPCM ) model to identify the answer span by matching the context of each point in the passage with the question from multiple perspectives .",model
"Instead of enumerating all the possible spans explicitly and ranking them , our model identifies the answer span by predicting the beginning and ending points individually with globally normalized probability distributions across the whole passage .",model
Multi - Perspective Context Matching for Machine Comprehension,research-problem
"Previous machine comprehension ( MC ) datasets are either too small to train endto - end deep learning models , or not difficult enough to evaluate the ability of current MC techniques .",research-problem
We process the corpus with the tokenizer from Stanford CorNLP .,experimental-setup
"To initialize the word embeddings in the word representation layer , we use the 300 - dimensional GloVe word vectors pre-trained from the 840B Common Crawl corpus .",experimental-setup
"For the out - of - vocabulary ( OOV ) words , we initialize the word embeddings randomly .",experimental-setup
"We set the hidden size as 100 for all the LSTM layers , and set the number of perspectives l of our multiperspective matching function ( Equation ( 5 ) ) as 50 .",experimental-setup
We set the learning rate as 0.0001 .,experimental-setup
"We apply dropout to every layers in , and set the dropout ratio as 0.2 .",experimental-setup
"To train the model , we minimize the cross entropy of the be - ginning and end points , and use the ADAM optimizer to update parameters .",experimental-setup
We can see that removing any components from the MPCM model decreases the performance significantly .,ablation-analysi
"Among all the layers , the Aggregation Layer is the most crucial layer .",ablation-analysi
"Among all the matching strategies , Maxpooling - Matching has the biggest effect .",ablation-analysi
"First , we explore the effect of additional information by adopting a pretrained language model ( LM ) to compute the vector representation of the input text .",approach
"Last , we explore the effect of different objective functions ( listwise and pointwise learning ) .",approach
"Following this study , we select an ELMo language model for this study .",approach
"We investigate the applicability of transfer learning ( TL ) using a large - scale corpus that is created for a relevant - sentence - selection task ( i.e. , question - answering NLI ( QNLI ) dataset ) .",approach
"Second , we further enhance one of the baseline models , Comp - Clip ( refer to the discussion in 3.1 ) , for the target QA task by proposing a novel latent clustering ( LC ) method .",approach
The LC method computes latent cluster information for target samples by creating a latent memory space and calculating the similarity between the sample and the memory .,approach
"By an endto - end learning process with the answer-selection task , the LC method assigns true - label question - answer pairs to similar clusters .",approach
"Wiki QA : For the WikiQA dataset , the pointwise learning approach shows a better performance than the listwise learning approach .",result
We combine LM with the base model ( Comp - Clip + LM ) and observe a significant improvement in performance in terms of MAP ( 0.714 to 0.746 absolute ) .,result
"When we add the LC method ( Comp - Clip + LM + LC ) , the best previous results are surpassed in terms of MAP ( 0.718 to 0.764 absolute ) .",result
The pointwise learning approach also shows excellent performance with the TREC - QA dataset .,result
"In particular , our model outperforms the best previous result when we add LC method , ( Comp - Clip + LM + LC ) in terms of MAP ( 0.865 to 0.868 ) .",result
"As in the WikiQA case , we achieve additional performance gains in terms of the MAP as we apply LM , LC , and TL ( 0.850 , 0.868 and 0.875 , respectively ) .",result
A Compare - Aggregate Model with Latent Clustering for Answer Selection,research-problem
"In this paper , we propose a novel method for a sentence - level answer- selection task that is a fundamental problem in natural language processing .",research-problem
Automatic question answering ( QA ) is a primary objective of artificial intelligence .,research-problem
"Except for the Copy task , which is too simple , other tasks observe convergence speed improvement of NUTM over that of NTM , thereby validating the benefit of using two programs across timesteps even for the single task setting .",result
NUTM requires fewer training samples to converge and it generalizes better to unseen sequences that are longer than training sequences .,result
Our goal is to advance a step further towards UTM by coupling a MANN with an external program memory .,model
"The program memory co-exists with the data memory in the MANN , providing more flexibility , reuseability and modularity in learning complicated tasks .",model
"The program memory stores the weights of the MANN 's controller network , which are retrieved quickly via a key - value attention mechanism across timesteps yet updated slowly via backpropagation .",model
"By introducing a meta network to moderate the operations of the program memory , our model , henceforth referred to as Neural Stored - program Memory ( NSM ) , can learn to switch the programs / weights in the controller network appropriately , adapting to different functionalities aligning with different parts of a sequential task , or different tasks in continual and few - shot learning .",model
Neural Stored - program Memory,research-problem
"In this paper , we introduce a new memory to store weights for the controller , analogous to the stored - program memory in modern computer architectures .",research-problem
"Interestingly , we observe that feature engineering leads to significant improvements for WDW and CBT datasets , but not for CNN and Daily Mail datasets .",result
"Similarly , fixing the word embeddings provides an improvement for the WDW and CBT , but not for CNN and Daily Mail .",result
"Comparing with prior work , on the WDW dataset the basic version of the GA Reader outperforms all previously published models when trained on the Strict setting .",result
"Lastly , on CBT - CN the GA Reader with the qe-comm feature outperforms all previously published single models except the NSE , and AS Reader trained on a larger corpus .",result
By adding the qecomm feature the performance increases by 3.2 % and 3.5 % on the Strict and Relaxed settings respectively to set a new state of the art on this dataset .,result
On the CNN and Daily Mail datasets the GA Reader leads to an improvement of 3.2 % and 4.3 % respectively over the best previous single models .,result
"For CBT - NE , GA Reader with the qecomm feature outperforms all previous single and ensemble models except the AS Reader trained on the much larger BookTest Corpus .",result
"More specifically , unlike existing models where the query attention is applied either token - wise or sentence - wise to allow weighted aggregation , the Gated - Attention ( GA ) module proposed in this work allows the query to directly interact with each dimension of the token embeddings at the semantic - level , and is applied layer - wise as information filters during the multi-hop representation learning process .",model
"Such a fine - grained attention enables our model to learn conditional token representations w.r.t. the given question , leading to accurate answer selections .",model
Gated - Attention Readers for Text Comprehension,research-problem
A recent trend to measure progress towards machine reading is to test a system 's ability to answer questions about a document it has to comprehend .,research-problem
Source code is available on github : https:// github.com/bdhingra/ga-reader,code
"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only by parts of the query relevant to that token rather than the over all query representation .",ablation-analysi
"Finally , removing the character embeddings , which were only used for WDW and CBT , leads to a reduction of about 1 % in the performance .",ablation-analysi
"In this paper , we instead take the approach of converting questions to ( uninterpretable ) vectorial representations which require no pre-defined grammars or lexicons and can query any KB independent of its schema .",approach
"Following , we focus on answering simple factual questions on a broad range of topics , more specifically , those for which single KB triples stand for both the question and an answer ( of which there maybe many ) .",approach
Our approach is based on learning low - dimensional vector embeddings of words and of KB triples so that representations of questions and corresponding answers end up being similar in the embedding space .,approach
"In order to avoid transferring the cost of manual intervention to the one of labeling large amounts of data , we make use of weak supervision .",approach
We show empirically that our model is able to take advantage of noisy and indirect supervision by ( i ) automatically generating questions from KB triples and treating this as training data ; and ( ii ) supplementing this with a data set of questions collaboratively marked as paraphrases but with no associated answers .,approach
We end up learning meaningful vectorial representations for questions involving up to 800 k words and for triples of an mostly automatically created KB with 2.4 M entities and 600 k relationships .,approach
"First , we can see that multitasking with paraphrase data is essential since it improves F1 from 0.60 to 0.68 .",result
Fine - tuning the embedding model is very beneficial to optimize the top of the list and grants a bump of 5 points of F1 : carefully tuning the similarity makes a clear difference .,result
"All versions of our system greatly outperform paralex : the fine - tuned model improves the F1 - score by almost 20 points and , according to , is better in precision for all levels of recall .",result
"As expected , string matching greatly improves results , both in precision and recall , and also significantly reduces evaluation time .",result
"The final F1 obtained by our fine - tuned model is even better then the result of paralex in reranking , which is pretty remarkable , because this time , this setting advantages it quite a lot .",result
Open Question Answering with Weakly Supervised Embedding Models,research-problem
"This paper addresses the challenging problem of open - domain question answering , which consists of building systems able to answer questions from any domain .",research-problem
Question answering is then defined as the task of retrieving the correct entity or set of entities from a KB given a query expressed as a question in natural language .,research-problem
"In this work , we propose an end - to - end question - focused multi-factor attention network for document - based question answering ( AMANDA ) , which learns to aggregate evidence distributed across multiple sentences and identifies the important question words to help extract the answer .",model
"Intuitively , AMANDA extracts the answer not only by synthesizing relevant facts from the passage but also by implicitly determining the suitable answer type during prediction .",model
A Question - Focused Multi- Factor Attention Network for Question Answering,research-problem
Neural network models recently proposed for question answering ( QA ) primarily focus on capturing the passagequestion relation .,research-problem
They also do not explicitly focus on the question and answer type which often plays a critical role in QA .,research-problem
"In machine comprehension - based ( MC ) question answering ( QA ) , a machine is expected to provide an answer for a given question by understanding texts .",research-problem
We tokenize the corpora with NLTK 2 .,experimental-setup
"We use the 300 dimension pre-trained word vectors from GloVe ( Pennington , Socher , and Manning 2014 ) and we do not update them during training .",experimental-setup
We use 50 - dimension character - level embedding vectors .,experimental-setup
We use dropout ) with probability 0.3 for every learnable layer .,experimental-setup
We use the Adam optimizer with learning rate of 0.001 and clipnorm of 5 .,experimental-setup
The out - of - vocabulary words are initialized with zero vectors .,experimental-setup
The number of hidden units in all the LSTMs is 150 .,experimental-setup
"For multi-factor attentive encoding , we choose 4 factors ( m ) based on our experimental findings ( refer to ) .",experimental-setup
"During training , the minibatch size is fixed at 60 .",experimental-setup
"In this paper we explore three semantic aspects of story understanding : ( i ) the sequence of events described in the story , ( ii ) the evolution of sentiment and emotional trajectories , and ( iii ) topical consistency .",model
"The first aspect is motivated from approaches in semantic script induction , and evaluates if events described in an ending - alternative are likely to occur within the sequence of events described in the preceding context .",model
Our model captures this by evaluating if the sentiment described in an ending option makes sense considering the context of the story .,model
Our model accounts for that by analyzing if the topic of an ending option is consistent with the preceding context .,model
We present a log - linear model that is used to weigh the various aspects of the story using a hidden variable .,model
Story Comprehension for Predicting What Happens Next,research-problem
"Automatic story comprehension is a fundamental challenge in Natural Language Understanding , and can enable computers to learn about social norms , human behavior and commonsense .",research-problem
"For these reasons , automatically understanding stories is an interesting but challenging task for Computational Linguists .",research-problem
"Recently , introduced the story - cloze task for testing this ability , albeit without the aspect of language generation .",research-problem
shows the performance of a logistic regression model trained using all the features ( All ) and then using individual feature - groups .,ablation-analysi
"We can see that the features extracted from the aspect analyzing the event - sequence have the strongest predictive power , followed by those characterizing Sentiment - trajectory .",ablation-analysi
The features measuring top - ical consistency result in lowest accuracy but they still perform better than random on the task .,ablation-analysi
"In this work , to alleviate such an obvious shortcoming about semantics , we make attempt to explore integrative models for finer - grained text comprehension and inference .",model
"In this work , we propose a semantics enhancement framework for TC tasks , which boosts the strong baselines effectively .",model
We implement an easy and feasible scheme to integrate semantic signals in downstream neural models in end - to - end manner to boost strong baselines effectively .,model
Explicit Contextual Semantics for Text Comprehension,research-problem
"This paper focuses on two core text comprehension ( TC ) tasks , machine reading comprehension ( MRC ) and textual entailment ( TE ) .",research-problem
Multi-range Reasoning for Machine Comprehension,research-problem
"We propose MRU ( Multi - Range Reasoning Units ) , a new fast compositional encoder for machine comprehension ( MC ) .",research-problem
"While the usage of recurrent encoder is often regarded as indispensable in highly complex MC tasks , there are still several challenges and problems pertaining to it 's usage in modern MC tasks .",research-problem
We implement all models in TensorFlow .,experimental-setup
Word embeddings are initialized with 300d Glo Ve vectors and are not fine - tuned during training .,experimental-setup
"Dropout rate is tuned amongst { 0.1 , 0.2 , 0.3 } on all layers including the embedding layer .",experimental-setup
The batch size is set to 64/256/32 accordingly .,experimental-setup
The maximum sequence lengths are 500/200/1100 respectively .,experimental-setup
We adopt the Adam optimizer with a learning rate of 0.0003/ 0.001/0.001 for RACE / SearchQA / NarrativeQA respectively .,experimental-setup
All models are trained and all runtime benchmarks are based on a TitanXP GPU .,experimental-setup
"Our work exploits this in the context of a simple one - to - many multi -task learning ( MTL ) framework , wherein a single recurrent sentence encoder is shared across multiple tasks .",approach
"While our work aims at learning fixed - length distributed sentence representations , it is not always practical to assume that the entire "" meaning "" of a sentence can be encoded into a fixed - length vector .",approach
The primary contribution of our work is to combine the benefits of diverse sentence - representation learning objectives into a single multi-task framework .,approach
It is evident from that adding more tasks improves the transfer performance of our model .,result
Increasing the capacity our sentence encoder with more hidden units ( + L ) as well as an additional layer ( + 2L ) also lead to improved transfer performance .,result
"We observe gains of 1.1 - 2.0 % on the sentiment classification tasks ( MR , CR , SUBJ & MPQA ) over Infersent .",result
"For example , we observe a 0.2-0.5 % improvement over the decomposable attention model .",result
"We demonstrate substantial gains on TREC ( 6 % over Infersent and roughly 2 % over the CNN - LSTM ) , outperforming even a competitive supervised baseline .",result
"We see similar gains ( 2.3 % ) on paraphrase identification ( MPRC ) , closing the gap on supervised approaches trained from scratch .",result
The addition of constituency parsing improves performance on sentence relatedness ( SICK - R ) and entailment ( SICK - E ) consistent with observations made by .,result
"In , we show that simply training an MLP on top of our fixed sentence representations outperforms several strong & complex supervised approaches that use attention mechanisms , even on this fairly large dataset .",result
"When using only a small fraction of the training data , indicated by the columns 1 k - 25 k , we are able to outperform the Siamese and Multi - Perspective CNN using roughly 6 % of the available training set .",result
We also outperform the Deconv LVM model proposed by in this low - resource setting .,result
Representations learned solely from NLI do appear to encode syntax but incorporation into our multi-task framework does not amplify this signal .,result
"Somewhat surprisingly , in we observe that the learned word embeddings are competitive with popular methods such as GloVe , word2vec , and fasttext on the benchmarks presented by and .",result
"Similarly , we observe that sentence characteristics such as length and word order are better encoded with the addition of parsing .",result
Published as a conference paper at ICLR 2018 LEARNING GENERAL PURPOSE DISTRIBUTED SEN - TENCE REPRESENTATIONS VIA LARGE SCALE MULTI - TASK LEARNING,research-problem
A lot of the recent success in natural language processing ( NLP ) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner .,research-problem
"However , extending this success to learning representations of sequences of words , such as sentences , remains an open problem .",research-problem
Some recent work has addressed this by learning general - purpose sentence representations .,research-problem
"In paragraph selection stage , we use the uncased version of BERT Tokenizer to tokenize all passages and questions .",hyperparameter
"In graph construction stage , we use a pretrained NER model from Stanford CoreNLP Toolkits 1 to extract named entities .",hyperparameter
"In the encoding stage , we also use a pre-trained BERT model as the encoder , thus d 1 is 768 .",hyperparameter
The encoding vectors of sentence pairs are generated from a pre-trained BERT model .,hyperparameter
The maximum number of entities in a graph is set to be 40 .,hyperparameter
Each entity node in the entity graphs has an average degree of 3.52 .,hyperparameter
All the hidden state dimensions d 2 are set to 300 .,hyperparameter
We set a relatively low threshold during selection to keep a high recall ( 97 % ) and a reasonable precision ( 69 % ) on supporting facts .,hyperparameter
We set the dropout rate for all hidden units of LSTM and dynamic graph attention to 0.3 and 0.5 respectively .,hyperparameter
"For optimization , we use Adam Optimizer with an initial learning rate of 1 e ?4 .",hyperparameter
Dynamically Fused Graph Network for Multi-hop Reasoning,research-problem
Text - based question answering ( TBQA ) has been studied extensively in recent years .,research-problem
Question answering ( QA ) has been a popular topic in natural language processing .,research-problem
QA provides a quantifiable way to evaluate an NLP system 's capability on language understanding and reasoning .,research-problem
"Our proposed model , called dynamic chunk reader ( DCR ) , not only significantly differs from both the above systems in the way that answer candidates are generated and ranked , but also shares merits with both works .",model
"First , our model uses deep networks to learn better representations for candidate answer chunks , instead of using fixed feature representations as in .",model
"Second , it represents answer candidates as chunks , as in ( Rajpurkar et al. ) , instead of word - level representations , to make the model aware of the subtle differences among candidates ( importantly , overlapping candidates ) .",model
End - to - End Answer Chunk Extraction and Ranking for Reading Comprehension,research-problem
"This paper proposes dynamic chunk reader ( DCR ) , an end - toend neural reading comprehension ( RC ) model that is able to extract and rank a set of answer candidates from a given document to answer questions .",research-problem
Reading comprehension - based question answering ( RCQA ) is the task of answering a question with a chunk of text taken from related document ( s ) .,research-problem
"Different from the above two assumptions for RCQA , in the real - world QA scenario , people may ask questions about both entities ( factoid ) and non-entities such as explanations and reasons ( non -factoid ) ( see for examples ) .",research-problem
We pre-processed the SQuAD dataset using Stanford CoreNLP tool 5 with its default setting to tokenize the text and obtain the POS and NE annotations .,experimental-setup
"To train our model , we used stochastic gradient descent with the ADAM optimizer , with an initial learning rate of 0.001 .",experimental-setup
"All GRU weights were initialized from a uniform distribution between ( - 0.01 , 0.01 ) .",experimental-setup
"The hidden state size , d , was set to 300 for all GRUs .",experimental-setup
We trained in mini-batch style ( mini - batch size is 180 ) and applied zero - padding to the passage and question inputs in each batch .,experimental-setup
"We also applied dropout of rate 0.2 to the embedding layer of input bi - GRU encoder , and gradient clipping when the norm of gradients exceeded 10 .",experimental-setup
"We also set the maximum passage length to be 300 tokens , and pruned all the tokens after the 300 - th token in the training set to save memory and speedup the training process .",experimental-setup
"We trained the model for at most 30 epochs , and in case the accuracy did not improve for 10 epochs , we stopped training .",experimental-setup
"For the feature ranking - based system , we used jforest ranker ( Ganjis affar , Caruana , and Lopes 2011 ) with Lambda MART - Regression Tree algorithm and the ranking metric was NDCG @ 10 .",experimental-setup
"First , replacing the word - by - word attention with Attentive Reader style attention decreases the EM score by about 4.5 % , showing the strength of our proposed attention mechanism .",ablation-analysi
The result shows that POS feature ( 1 ) and question - word feature ( 3 ) are the two most important features .,ablation-analysi
"Finally , combining the DCR model with the proposed POS - trie constraints yields a score similar to the one obtained using the DCR model with all possible n-gram chunks .",ablation-analysi
"The sum of words model performed slightly worse than the fundamentally similar lexicalized classifier while the sum of words model can use pretrained word embeddings to better handle rare words , it lacks even the rudimentary sensitivity to word order that the lexicalized model 's bigram features provide .",result
"While the lexicalized model fits the training set almost perfectly , the gap between train and test set accuracy is relatively small for all three neural network models , suggesting that research into significantly higher capacity versions of these models would be productive .",result
"In addition , though the LSTM and the lexicalized model show similar performance when trained on the current full corpus , the somewhat steeper slope for the LSTM hints that its ability to learn arbitrarily structured representations of sentence meaning may give it an advantage over the more constrained lexicalized model on still larger datasets .",result
"Of the two RNN models , the LSTM 's more robust ability to learn long - term dependencies serves it well , giving it a substantial advantage over the plain RNN , and resulting in performance that is essentially equivalent to the lexicalized classifier on the test set ( LSTM performance near the stopping iteration varies by up to 0.5 % between evaluation steps ) .",result
A large annotated corpus for learning natural language inference,research-problem
"Thus , natural language inference ( NLI ) - characterizing and using these relations in computational systems ) - is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning .",research-problem
"NLI has been addressed using a variety of techniques , including those based on symbolic logic , knowledge bases , and neural networks .",research-problem
"In these experimental results , we see that PAAG achieves a 111 % , 8 % and 62.73 % increment over the stateof - the - art baseline SNet in terms of BLEU , embedding greedy and consistency score , respectively .",result
"In , we see that our PAAG outperforms all the baseline significantly in semantic distance with respect to the ground truth .",result
"In , we can see that PAAG outperforms other baseline models in both sentence fluency and consistency with the facts .",result
"Although there is a small increment of S2 SAR with respect to S2SA in all metrics , we still find a noticeable gap between S2SAR and PAAG .",result
( 1 ) S2SA : Sequence - to - sequence framework has been proposed for language generation task .,baseline
( 2 ) S2SAR : We implement a simple method which can incorporate the review information when generating the answer .,baseline
( 3 ) SNet : S- Net is a two - stage state - of - the - art model which extracts some text spans from multiple documents context and synthesis the answer from those spans .,baseline
( 4 ) QS : We implement the query - based summarization model proposed by Hasselqvist et al ..,baseline
( 5 ) BM25 : BM25 is a bag - of - words retrieval function that ranks a set of reviews based on the question terms appearing in each review .,baseline
( 6 ) TF - IDF : Term Frequency - Inverse Document Frequency is a numerical statistic that is intended to reflect how important a question word is to a review .,baseline
"In this paper , we propose the product - aware answer generator ( PAAG ) , a product related question answering model which incorporates customer reviews with product attributes .",model
"Eventually , we propose a recurrent neural network ( RNN ) based decoder , which combines product - aware review representation and attributes to generate the answer .",model
"Specifically , at the beginning we employ an attention mechanism to model interactions between a question and reviews .",model
"Simultaneously , we employ a key - value memory network to store the product attributes and extract the relevance values according to the question .",model
"More importantly , to tackle the problem of meaningless answers , we propose an adversarial learning mechanism in the loss calculation for optimizing parameters .",model
Product - Aware Answer Generation in E - Commerce Question - Answering,research-problem
"In recent years , the explosive popularity of question - answering ( QA ) is revitalizing the task of reading comprehension with promising results .",research-problem
"We examine a simple model family , the decomposable attention model of , that has shown promise in modeling natural language inference and has inspired recent work on similar tasks .",approach
"First , to mitigate data sparsity , we modify the input representation of the decomposable attention model to use sums of character n-gram embeddings instead of word embeddings .",approach
"Second , to significantly improve our model performance , we pretrain all our model parameters on the noisy , automatically collected question - paraphrase corpus Paralex , followed by fine - tuning the parameters on the Quora dataset .",approach
"We observe that the simple FFNN baselines work better than more complex Siamese and Multi - Perspective CNN or LSTM models , more so if character n-gram based embeddings are used .",result
"Our basic decomposable attention model DECATT word without pre-trained embeddings is better than most of the models , all of which used GloVe embeddings .",result
An interesting observation is that DECATT char model without any pretrained embeddings outperforms DE - CATT glove that uses task - agnostic GloVe embeddings .,result
"Furthermore , when character n-gram embeddings are pre-trained in a task - specific manner in DECATT paralex ? char model , we observe a significant boost in performance .",result
"Finally , we note that our best performing model is pt - DECATT char , which leverages the full power of character embeddings and pretraining the model on Paralex .",result
"We tuned the following hyperparameters by grid search on the development set ( settings for our best model are in parenthesis ) : embedding dimension ( 300 ) , shape of all feedforward networks ( two layers with 400 and 200 width ) , character n -gram sizes ( 5 ) , context size ( 1 ) , learning rate ( 0.1 for both pretraining and tuning ) , batch size ( 256 for pretraining and 64 for tuning ) , dropout ratio ( 0.1 for tuning ) and prediction threshold ( positive paraphrase for a score ? 0.3 ) .",hyperparameter
Neural Paraphrase Identification of Questions with Noisy Pretraining,research-problem
We present a solution to the problem of paraphrase identification of questions .,research-problem
Question paraphrase identification is a widely useful NLP application .,research-problem
"Our multi-evidence QA model , the Coarse - grain Fine - grain Coattention Network ( CFC ) , selects among a set of candidate answers given a set of support documents and a query .",model
The CFC is inspired by coarse - grain reasoning and fine - grain reasoning .,model
Each module employs a novel hierarchical attention - a hierarchy of coattention and self - attention - to combine information from the support documents conditioned on the query and candidates .,model
"In coarse - grain reasoning , the model builds a coarse summary of support documents conditioned on the query without knowing what candidates are available , then scores each candidate .",model
"In fine - grain reasoning , the model matches specific finegrain contexts in which the candidate is mentioned with the query in order to gauge the relevance of the candidate .",model
COARSE - GRAIN FINE - GRAIN COATTENTION NET - WORK FOR MULTI - EVIDENCE QUESTION ANSWERING,research-problem
"End - to - end neural models have made significant progress in question answering , however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document .",research-problem
A requirement of scalable and practical question answering ( QA ) systems is the ability to reason over multiple documents and combine their information to answer questions .,research-problem
"Although existing datasets enabled the development of effective end - to - end neural question answering systems , they tend to focus on reasoning over localized sections of a single document .",research-problem
Both the coarse - grain module and the fine - grain module significantly contribute to model performance .,ablation-analysi
The fine - grain - only model under-performs the coarse - grain - only model consistently across almost all length measures .,ablation-analysi
"However , the fine - grain - only model matches or outperforms the coarse - grain - only model on examples with a large number of support documents or with long support documents .",ablation-analysi
Replacing selfattention layers with mean - pooling and the bidirectional GRUs with unidirectional GRUs result in less performance degradation .,ablation-analysi
"Replacing the encoder with a projection over word embeddings result in significant performance drop , which suggests that contextual encodings that capture positional information is crucial to this task .",ablation-analysi
"Despite not having access to any external representation of linguistic structure , RASOR achieves an error reduction of more than 50 % over this baseline , both in terms of exact match and F1 , relative to the human performance upper bound .",result
"In contrast , RASOR can efficiently and explicitly model the quadratic number of possible answers , which leads to a 14 % error reduction over the best performing Match - LSTM model .",result
"To overcome this , we present a novel neural architecture called RASOR that builds fixed - length span representations , reusing recurrent computations for shared substructures .",model
"We demonstrate that directly classifying each of the competing spans , and training with global normalization over all possible spans , leads to a significant increase in performance .",model
LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,research-problem
"In this paper , we focus on this answer extraction task , presenting a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network .",research-problem
"First , as an effort to study the coverage of existing systems and the possibility to train jointly on different data sources via multitasking , we collected the first large - scale dataset of questions and answers based on a KB , called SimpleQuestions .",dataset
"This dataset , which is presented in Section 2 , contains more than 100 k questions written by human anno-What American cartoonist is the creator of Andy Lippincott ?",dataset
"The embedding dimension and the learning rate were chosen among { 64 , 128 , 256 } and { 1 , 0.1 , ... , 1.0e ? 4 } respectively , and the margin ?was set to 0.1 .",hyperparameter
"Second , in sections 3 and 4 , we present an embedding - based QA system developed under the framework of Memory Networks ( Mem NNs ) .",model
"Memory Networks are learning systems centered around a memory component that can be read and written to , with a particular focus on cases where the relationship between the input and response languages ( here natural language ) and the storage language ( here , the facts from KBs ) is performed by embedding all of them in the same vector space .",model
The setting of the simple QA corresponds to the elementary operation of performing a single lookup in the memory .,model
Large - scale Simple Question Answering with Memory Networks,research-problem
Training large - scale question answering systems is complicated because training sources usually cover a small portion of the range of possible questions .,research-problem
"This paper studies the impact of multitask and transfer learning for simple question answering ; a setting for which the reasoning required to answer is quite easy , as long as one can retrieve the correct evidence given a question , which can be difficult in large - scale conditions .",research-problem
"However , while most recent efforts have focused on designing systems with higher reasoning capabilities , that could jointly retrieve and use multiple facts to answer , the simpler problem of answering questions that refer to a single fact of the KB , which we call Simple Question Answering in this paper , is still far from solved .",research-problem
"From the results , we can see that our models not only perform well on the original SQuAD dataset , but also outperform all previous models by more than 5 % in EM score on the adversarial datasets .",result
This shows that FusionNet is better at language understanding of both the context and question .,result
"To alleviate this challenge , we identify an attention scoring function utilizing all layers of representation with less training burden .",model
This leads to an attention that thoroughly captures the complete information between the question and the context .,model
"With this fully - aware attention , we put forward a multi -level attention mechanism to understand the information in the question , and exploit it layer by layer on the context side .",model
"All of these innovations are integrated into a new end - to - end structure called FusionNet in , with details described in Section 3 .",model
FUSIONNET : FUSING VIA FULLY - AWARE ATTENTION WITH APPLICATION TO MACHINE COMPREHENSION,research-problem
"Teaching machines to read , process and comprehend text and then answer questions is one of key problems in artificial intelligence .",research-problem
Many neural network models have been proposed for this challenge and they generally frame this problem as a machine reading comprehension ( MRC ) task .,research-problem
We argue that this hypothesis also holds in language understanding and MRC .,research-problem
DR - BiLSTM ( Single ) achieves 88.5 % accuracy on the test set which is noticeably the best reported result among the existing single models for this task .,result
"DR - BiLSTM ( Ensemble ) achieves the accuracy of 89.3 % , the best result observed on SNLI , while DR - BiLSTM ( Single ) obtains the accuracy of 88.5 % , which considerably outperforms the previous non-ensemble models .",result
Our ensemble model considerably outperforms the current state - of - the - art by obtaining 89.3 % accuracy .,result
We can see that our preprocessing mechanism leads to further improvements of 0.4 % and 0.3 % on the SNLI test set for our single and ensemble models respectively .,result
"In fact , our single model ( "" DR - BiLSTM ( Single ) + Process "" ) obtains the state - of - the - art performance over both reported single and ensemble models by performing a simple preprocessing step .",result
"Furthermore , "" DR - BiLSTM ( Ensem . ) + Process "" outperforms the existing state - of - the - art remarkably ( 0.7 % improvement ) .",result
"Also , utilizing a trivial preprocessing step yields to further improvements of 0.4 % and 0.3 % for single and ensemble DR - BiLSTM models respectively .",result
We use pre-trained 300 - D Glove 840B vectors to initialize our word embedding vectors .,hyperparameter
We use a fairly small batch size of 32 to provide more exploration power to the model .,hyperparameter
All hidden states of BiLSTMs during input encoding and inference have 450 dimensions ( r = 300 and d = 450 ) .,hyperparameter
"The weights are learned by minimizing the log - loss on the training data via the Adam optimizer ( Kingma and Ba , 2014 ) .",hyperparameter
The initial learning rate is 0.0004 .,hyperparameter
"To avoid overfitting , we use dropout with the rate of 0.4 for regularization , which is applied to all feedforward connections .",hyperparameter
"During training , the word embeddings are updated to learn effective representations for the NLI task .",hyperparameter
We propose a dependent reading bidirectional LSTM ( DR - BiLSTM ) model to address these limitations .,model
"Given a premise u and a hypothesis v , our model first encodes them considering dependency on each other .",model
"Next , the model employs a soft attention mechanism to extract relevant information from these encodings .",model
"The augmented sentence representations are then passed to the inference stage , which uses a similar dependent reading strategy in both directions , i.e. u ? v and v ? u .",model
"Finally , a decision is made through a multi - layer perceptron ( MLP ) based on the aggregated information .",model
DR- BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference,research-problem
We present a novel deep learning architecture to address the natural language inference ( NLI ) task .,research-problem
"The goal of NLI is to identify the logical relationship ( entailment , neutral , or contradiction ) between a premise and a corresponding hypothesis .",research-problem
We can see that all modifications lead to a new model and their differences are statistically significant with a p-value of < 0.001 over Chi square test .,ablation-analysi
"Among all components , three of them have noticeable influences : max pooling , difference in the attention stage , and dependent reading .",ablation-analysi
"They illustrate the importance of our proposed dependent reading strategy which leads to significant improvement , specifically in the encoding stage .",ablation-analysi
demonstrates that we achieve the best performance with 450 - dimensional BiLSTMs .,ablation-analysi
"The latest work on language representations carefully integrates contextualized features into language model training , which enables a series of success especially in various machine reading comprehension and natural language inference tasks .",research-problem
"Recently , deep contextual language model ( LM ) has been shown effective for learning universal language representations , achieving state - of - the - art results in a series of flagship natural language understanding ( NLU ) tasks .",research-problem
Our implementation is based on the PyTorch implementation of BERT 6 .,experimental-setup
"The batch size is selected in { 16 , 24 , 32 } .",experimental-setup
"The maximum number of epochs is set in [ 2 , 5 ] depending on tasks .",experimental-setup
"Texts are tokenized using wordpieces , with maximum length of 384 for SQuAD and 128 or 200 for other tasks .",experimental-setup
The dimension of SRL embedding is set to 10 .,experimental-setup
The default maximum number of predicateargument structures m is set to 3 .,experimental-setup
"We use the pre-trained weights of BERT and follow the same fine - tuning procedure as BERT without any modification , and all the layers are tuned with moderate model size increasing , as the extra SRL embedding volume is less than 15 % of the original encoder size .",experimental-setup
"We set the initial learning rate in { 8e -6 , 1 e - 5 , 2 e - 5 , 3 e - 5 } with warm - up rate of 0.1 and L2 weight decay of 0.01 .",experimental-setup
"From the results , we observe that the concatenation would yield an improvement , verifying that integrating contextual semantics would be quite useful for language understanding .",ablation-analysi
"However , SemBERT still outperforms the simple BERT + SRL model just like the latter outperforms the original BERT by a large performance margin , which shows that SemBERT works more effectively for integrating both plain contextual representation and contextual semantics at the same time .",ablation-analysi
"On the one hand , we propose a data enrichment method , which uses WordNet to extract inter-word semantic connections as general knowledge from each given passage - question pair .",approach
"On the other hand , we propose an end - to - end MRC model named as Knowledge Aided Reader ( KAR ) , which explicitly uses the above extracted general knowledge to assist its attention mechanisms .",approach
"Finally we find that after only one epoch of training , KAR already achieves an EM of 71.9 and an F 1 score of 80.8 on the development set , which is even better than the final performance of several strong baselines , such as DCN ( EM / F1 : 65.4 / 75.6 ) and BiDAF ( EM / F1 : 67.7 / 77.3 ) .",result
Explicit Utilization of General Knowledge in Machine Reading Comprehension,research-problem
"To bridge the gap between Machine Reading Comprehension ( MRC ) models and human beings , which is mainly reflected in the hunger for data and the robustness to noise , in this paper , we explore how to integrate the neural networks of MRC models with the general knowledge of human beings .",research-problem
"We tokenize the MRC dataset with spa Cy 2.0.13 , manipulate WordNet 3.0 with NLTK 3.3 , and implement KAR with TensorFlow 1.11.0 .",experimental-setup
"For the dense layers and the BiLSTMs , we set the dimensionality unit d to 600 .",experimental-setup
"For model optimization , we apply the Adam ( Kingma and Ba , 2014 ) optimizer with a learning rate of 0.0005 and a minibatch size of 32 .",experimental-setup
"To avoid overfitting , we apply dropout to the dense layers and the BiLSTMs with a dropout rate of 0.3 .",experimental-setup
"To boost the performance , we apply exponential moving average with a decay rate of 0.999 .",experimental-setup
"Then we conduct an ablation study by replacing the knowledge aided attention mechanisms with the mutual attention proposed by and the self attention proposed by separately , and find that the F 1 score of KAR drops by 4.2 on the development set , 7.8 on AddSent , and 9.1 on AddOneSent .",ablation-analysi
We found that processing the hypothesis conditioned on the premise instead of encoding each sentence independently gives an improvement of 3.3 percentage points in accuracy over Bowman et al. 's LSTM .,result
Our LSTM outperforms a simple lexicalized classifier by 2.7 percentage points .,result
"By incorporating an attention mechanism we found a 0.9 percentage point improvement over a single LSTM with a hidden size of 159 , and a 1.4 percentage point increase over a benchmark model that uses two LSTMs for conditional encoding ( one for the premise and one for the hypothesis conditioned on the representation of the premise ) .",result
Enabling the model to attend over output vectors of the premise for every word in the hypothesis yields another 1.2 percentage point improvement compared to attending based only on the last output vector of the premise .,result
Allowing the model to also attend over the hypothesis based on the premise does not seem to improve performance for RTE .,result
"In contrast , we are proposing an attentive neural network that is capable of reasoning over entailments of pairs of words and phrases by processing the hypothesis conditioned on the premise .",model
"Our contributions are threefold : ( i ) We present a neural model based on LSTMs that reads two sentences in one go to determine entailment , as opposed to mapping each sentence independently into a semantic space ( 2.2 ) , ( ii ) We extend this model with a neural word - by - word attention mechanism to encourage reasoning over entailments of pairs of words and phrases ( 2.4 ) , and ( iii ) We provide a detailed qualitative analysis of neural attention for RTE ( 4.1 ) .",model
"Recognizing textual entailment ( RTE ) is the task of determining whether two natural language sentences are ( i ) contradicting each other , ( ii ) not related , or whether ( iii ) the first sentence ( called premise ) entails the second sentence ( called hypothesis ) .",research-problem
"This task is important since many natural language processing ( NLP ) problems , such as information extraction , relation extraction , text summarization or machine translation , rely on it explicitly or implicitly and could benefit from more accurate RTE systems .",research-problem
"In 1 k data , QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 ) outperforms all other models by a large margin ( 2.8 + % ) .",result
"In 10 k dataset , the average accuracy of QRN 's ' 6r200 ' ( 6 layers + reset gate + d = 200 ) model outperforms all previous models by a large margin ( 2.5 + % ) , achieving a nearly perfect score of 99.7 % .",result
QRN outperforms previous work by a large margin ( 2.0 + % ) in every comparison .,result
"These include LSTM , End - to - end Memory Networks ( N2N ) , Dynamic Memory Networks ( DMN + ) , Gated End - to - end Memory Networks ( GMe m N2N ) , and Differentiable Neural Computer ( DNC ) .",baseline
We withhold 10 % of the training for development .,hyperparameter
We use the hidden state size of 50 by deafult .,hyperparameter
"Batch sizes of 32 for bAbI story - based QA 1k , bAb I dialog and DSTC2 dialog , and 128 for bAbI QA 10 k are used .",hyperparameter
The weights in the input and output modules are initialized with zero mean and the standard deviation of 1 / ? d.,hyperparameter
Forget bias of 2.5 is used for update gates ( no bias for reset gates ) .,hyperparameter
L2 weight decay of 0.001 ( 0.0005 for QA 10 k ) is used for all weights .,hyperparameter
The loss function is the cross entropy between v and the one - hot vector of the true answer .,hyperparameter
"The loss is minimized by stochastic gradient descent for maximally 500 epochs , but training is early stopped if the loss on the development data does not decrease for 50 epochs .",hyperparameter
The learning rate is controlled by AdaGrad with the initial learning rate of 0.5 ( 0.1 for QA 10 k ) .,hyperparameter
"Since the model is sensitive to the weight initialization , we repeat each training procedure 10 times ( 50 times for 10 k ) with the new random initialization of the weights and report the result on the test data with the lowest loss on the development data .",hyperparameter
"Our proposed model , Query - Reduction Network 1 ( QRN ) , is a single recurrent unit that addresses the long - term dependency problem of most RNN - based models by simplifying the recurrent update , while taking the advantage of RNN 's capability to model sequential data ) .",model
"QRN considers the context sentences as a sequence of state - changing triggers , and transforms ( reduces ) the original query to a more informed query as it observes each trigger through time .",model
"Compared to memory - based approaches , QRN can better encodes locality information because it does not use a global memory access controller ( circle nodes in ) , and the query updates are performed locally .",model
Published as a conference paper at ICLR 2017 QUERY - REDUCTION NETWORKS FOR QUESTION ANSWERING,research-problem
"In this paper , we study the problem of question answering when reasoning over multiple facts is required .",research-problem
"According to the ablation results , we infer that : ( a ) When the number of layers is only one , the model lacks reasoning capability .",ablation-analysi
"In the case of 1 k dataset , when there are too many layers ( 6 ) , it seems correctly training the model becomes increasingly difficult .",ablation-analysi
"In the case of 10 k dataset , many layers ( 6 ) and hidden dimensions ( 200 ) helps reasoning , most notably in difficult task such as task 16 .",ablation-analysi
( b ) Adding the reset gate helps .,ablation-analysi
"( c ) Including vector gates hurts in 1 k datasets , as the model either overfits to the training data or converges to local minima .",ablation-analysi
"On the other hand , vector gates in bAbI story - based QA 10 k dataset sometimes help .",ablation-analysi
It can be hypothesized that a larger hidden state is required for real data . Parallelization .,ablation-analysi
"( d ) Increasing the dimension of the hidden state to 100 in the dialog 's Task 6 ( DSTC2 ) helps , while there is not much improvement in the dialog 's Task 1 - 5 .",ablation-analysi
"We built ASNQ , a dataset for AS2 , by transforming the recently released Natural Questions ( NQ ) corpus ) from MR to AS2 task .",dataset
"TANDA provides a large improvement over the state of the art , which has been regularly contributed to by hundreds of researchers .",result
"TANDA improves all the models : BERT - Base , RoBERTa- Base , BERT - Large and RoBERTa - Large , outperforming the previous state of the art with all of them .",result
RoBERTa- Large TANDA using ASNQ ?,result
"Wiki QA establish an impressive new state of the art for AS2 on WikiQA of 0.920 and 0.933 in MAP and MRR , respectively .",result
RoBERTa - Large TANDA with ASNQ ?,result
"TREC - QA again establishes an impressive performance of 0.943 in MAP and 0.974 in MRR , outperforming the previous state of the art by .",result
We adopt Adam optimizer ( Kingma and Ba 2014 ) with a learning rate of 2e - 5 for the transfer step on the ASNQ dataset and a learning rate of 1e - 6 for the adapt step on the target dataset .,hyperparameter
We apply early stopping on the dev. set of the target corpus for both steps based on the highest MAP score .,hyperparameter
"We set the max number of epochs equal to 3 and 9 for adapt and transfer steps , respectively .",hyperparameter
We set the maximum sequence length for BERT / RoBERTa to 128 tokens .,hyperparameter
"In this paper , we study the use of Transformer - based models for AS2 and provide effective solutions to tackle the data scarceness problem for AS2 and the instability of the finetuning step .",model
"We improve stability of Transformer models by adding an intermediate fine - tuning step , which aims at specializing them to the target task ( AS2 ) , i.e. , this step transfers a pretrained language model to a model for the target task .",model
"We show that the transferred model can be effectively adapted to the target domain with a subsequent finetuning step , even when using target data of small size .",model
TANDA : Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection,research-problem
"We demonstrate the benefits of our approach for answer sentence selection , which is a well - known inference task in Question Answering .",research-problem
"This has renewed the research interest in Question Answering ( QA ) and , in particular , in two main tasks :",research-problem
"( i ) answer sentence selection ( AS2 ) , which , given a question and a set of answer sentence candidates , consists in selecting sentences ( e.g. , retrieved by a search engine ) correctly answering the question ; and ( ii ) machine reading ( MR ) or reading comprehension , which , given a question and a reference text , consists in finding a text span answering it .",research-problem
"Even though the latter is gaining more and more popularity , AS2 is more relevant to a production scenario since , a combination of a search engine and an AS2 model already implements an initial QA system .",research-problem
"In this paper , we present BART , which pre-trains a model combining Bidirectional and Auto - Regressive Transformers .",model
BART is a denoising autoencoder built with a sequence - to - sequence model that is applicable to a very wide range of end tasks .,model
"BART uses a standard Tranformer - based neural machine translation architecture which , despite its simplicity , can be seen as generalizing BERT ( due to the bidirectional encoder ) , GPT ( with the left - to - right decoder ) , and many other more recent pretraining schemes ( see .",model
"Pretraining has two stages ( 1 ) text is corrupted with an arbitrary noising function , and ( 2 ) a sequence - to - sequence model is learned to reconstruct the original text .",model
"BART : Denoising Sequence - to - Sequence Pre-training for Natural Language Generation , Translation , and Comprehension",research-problem
"We present BART , a denoising autoencoder for pretraining sequence - to - sequence models .",research-problem
"This paper presents RE2 , a fast and strong neural architecture with multiple alignment processes for general purpose text matching .",model
"These components , which the name RE2 stands for , are previous aligned features ( Residual vectors ) , original point - wise features ( Embedding vectors ) , and contextual features ( Encoded vectors ) .",model
An embedding layer first embeds discrete tokens .,model
"Several same - structured blocks consisting of encoding , alignment and fusion layers then process the sequences consecutively .",model
These blocks are connected by an augmented version of residual connections ( see section 2.1 ) .,model
A pooling layer aggregates sequential representations into vectors which are finally processed by a prediction layer to give the final prediction .,model
"The implementation of each layer is kept as simple as possible , and the whole model , as a well - organized combination , is quite powerful and lightweight at the same time .",model
Simple and Effective Text Matching with Richer Alignment Features,research-problem
We implement our model with TensorFlow and train on Nvidia P100 GPUs .,experimental-setup
"We tokenize sentences with the NLTK toolkit , convert them to lower cases and remove all punctuations .",experimental-setup
Word embeddings are initialized with 840B - 300d,experimental-setup
Glo Ve word vectors and fixed during training .,experimental-setup
Embeddings of out - ofvocabulary words are initialized to zeros and fixed as well .,experimental-setup
All other parameters are initialized with He initialization and normalized by weight normalization .,experimental-setup
Dropout with a keep probability of 0.8 is applied before every fully - connected or convolutional layer .,experimental-setup
The kernel size of the convolutional encoder is set to 3 .,experimental-setup
The prediction layer is a two - layer feed - forward network .,experimental-setup
The hidden size is set to 150 in all experiments .,experimental-setup
"Activations in all feed - forward networks are GeLU activations , and we use ?",experimental-setup
The number of blocks is tuned in a range from 1 to 3 .,experimental-setup
The number of layers of the convolutional encoder is tuned from 1 to 3 .,experimental-setup
The initial learning rate is tuned from 0.0001 to 0.003 .,experimental-setup
The batch size is tuned from 64 to 512 .,experimental-setup
The threshold for gradient clipping is set to 5 .,experimental-setup
We scale the summation in augmented residual connections by 1 / ? 2 when n ? 3 to preserve the variance under the assumption that the two addends have the same variance .,experimental-setup
"We use the Adam optimizer ( Kingma and Ba , 2015 ) and an exponentially decaying learning rate with a linear warmup .",experimental-setup
"The first ablation baseline shows that without richer features as the alignment input , the performance on all datasets degrades significantly .",ablation-analysi
The results of the second baseline show that vanilla residual connections without direct access to the original pointwise features are not enough to model the relations in many text matching tasks .,ablation-analysi
"The simpler implementation of the fusion layer leads to evidently worse performance , indicating that the fu- sion layer can not be further simplified .",ablation-analysi
"In the last ablation study , we can see that parallel blocks perform worse than stacked blocks , which supports the preference for deeper models over wider ones .",ablation-analysi
Hyper QA is implemented in Tensor - Flow .,hyperparameter
"The batch size is tuned amongst { 50 , 100 , 200 } .",hyperparameter
Models are trained for 25 epochs and the model parameters are saved each time the performance on the validation set is topped .,hyperparameter
"The dimension of the projection layer is tuned amongst { 100 , 200 , 300 , 400 } .",hyperparameter
"L2 regularization is tuned amongst { 0.001 , 0.0001 , 0.00001 }.",hyperparameter
The negative sampling rate is tuned from 2 to 8 .,hyperparameter
"We adopt the AdaGrad optimizer with initial learning rate tuned amongst { 0.2 , 0.1 , 0.05 , 0.01 } .",hyperparameter
"In this paper , we propose an extremely simple neural ranking model for question answering that achieves highly competitive results on several benchmarks with only a fraction of the runtime and only 40K - 90 K parameters ( as opposed to millions ) .",model
Our neural ranking models the relationships between QA pairs in Hyperbolic space instead of Euclidean space .,model
Hyperbolic space is an embedding space with a constant negative curvature in which the distance towards the border is increasing exponentially .,model
Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering,research-problem
"In this paper , we show it is possible to formulate a wide range of weakly supervised QA tasks as discrete latent - variable learning problems .",model
"The learning challenge is then to determine which solution in the set is the correct one , while estimating a complete QA model .",model
"Intuitively , these hard updates more strongly enforce our prior beliefs that there is a single correct solution .",model
"We model the set of possible solutions as a discrete latent variable , and develop a learning strategy that uses hard - EM - style parameter updates .",model
"This algorithm repeatedly ( i ) predicts the most likely solution according to the current model from the precomputed set , and ( ii ) updates the model parameters to further encourage its own prediction .",model
A Discrete Hard EM Approach for Weakly Supervised Question Answering,research-problem
Many question answering ( QA ) tasks only provide weak supervision for how the answer should be computed .,research-problem
"Despite its simplicity , we show that this approach significantly outperforms previous methods on six QA tasks , including absolute gains of 2 - 10 % , and achieves the stateof - the - art on five of them .",research-problem
"For our dataset , we construct queries , answers and supporting passages from BMJ Case Reports , the largest online repository of such documents .",dataset
"A case report is a detailed description of a clinical case that focuses on rare diseases , unusual presentation of common conditions and novel treatment methods .",dataset
"Each report contains a Learning points section , summarizing the key pieces of information from that report .",dataset
We use these learning points to create queries by blanking out a medical entity .,dataset
"Our dataset contains around 100,000 queries on 12,000 case reports , has long support passages ( around 1,500 tokens on average ) and includes answers which are single - or multiword medical entities .",dataset
We also include a distance - based method that uses word embeddings ( sim-entity ) .,baseline
We trained a 4 - gram Kneser - Ney model on CliCR training data ( with multi-word entities represented as a single token ) using SRILM .,baseline
CliCR : A Dataset of Clinical Case Reports for Machine Reading Comprehension *,research-problem
We present a new dataset for machine comprehension in the medical domain .,research-problem
MEMEN : Multi-layer Embedding with Memory Networks for Machine Comprehension,research-problem
Machine comprehension ( MC ) style question answering is a representative problem in natural language processing .,research-problem
Machine comprehension ( MC ) has gained significant popularity over the past few years and it is a coveted goal in the field of natural language processing and artificial intelligence .,research-problem
The tokenizers we use in the step of preprocessing data are from Stanford CoreNLP .,experimental-setup
We also use part - of - speech tagger and named - entity recognition tagger in Stanford CoreNLP utilities to transform the passage and question .,experimental-setup
"For the skip - gram model , our model refers to the word2 vec module in open source software library , Tensorflow , the skip window is set as 2 .",experimental-setup
"For the memory networks , we set the number of layer as 3 .",experimental-setup
"To improve the reliability and stabllity , we screen out the sentences whose length are shorter than 9 .",experimental-setup
"We use 100 one dimensional filters for CNN in the character level embedding , with width of 5 for each one .",experimental-setup
"We use the AdaDelta ( Zeiler , 2012 ) optimizer with a initial learning rate as 0.001 .",experimental-setup
We set the hidden size as 100 for all the LSTM and GRU layers and apply dropout between layers with a dropout ratio as 0.2 .,experimental-setup
"In this work , we are interested in universal language agnostic sentence embeddings , that is , vector representations of sentences that are general with respect to two dimensions : the input language and the NLP task .",model
"To that end , we train a single encoder to handle multiple languages , so that semantically similar sentences in different languages are close in the embedding space .",model
Massively Multilingual Sentence Embeddings for Zero - Shot Cross - Lingual Transfer and Beyond,research-problem
"We introduce an architecture to learn joint multilingual sentence representations for 93 languages , belonging to more than 30 different families and written in 28 different scripts .",research-problem
"Our implementation , the pretrained encoder and the multilingual test set are available at https://github.com / facebookresearch/LASER . . 2018 .",code
"In Gigaword dataset where the texts are short , our best model achieves a comparable performance with the current state - of - the - art .",result
"In CNN dataset where the texts are longer , our best model outperforms all the previous models .",result
"Overall , E2T achieves a significant improvement over the baseline model BASE , with at least 2 ROUGE - 1 points increase in the Gigaword dataset and 6 ROUGE - 1 points increase in the CNN dataset .",result
"Among the model variants , the CNN - based encoder with selective disambiguation and firm attention performs the best .",result
Entity Commonsense Representation for Neural Abstractive Summarization,research-problem
Text summarization is a task to generate a shorter and concise version of a text while preserving the meaning of the original text .,research-problem
"For both datasets , we further reduce the size of the input , output , and entity vocabularies to at most 50 K as suggested in and replace less frequent words to "" < unk > "" .",experimental-setup
We use 300D Glove 6 and 1000D wiki2vec 7 pre-trained vectors to initialize our word and entity vectors .,experimental-setup
"For GRUs , we set the state size to 500 .",experimental-setup
"For CNN , we set h = 3 , 4 , 5 with 400 , 300 , 300 feature maps , respectively .",experimental-setup
"For firm attention , k is tuned by calculating the perplexity of the model starting with smaller values ( i.e. k = 1 , 2 , 5 , 10 , 20 , ... ) and stopping when the perplexity of the model becomes worse than the previous model .",experimental-setup
We use dropout on all non-linear connections with a dropout rate of 0.5 .,experimental-setup
We use beam search of size 10 to generate the summary .,experimental-setup
"We set the batch sizes of Gigaword and CNN datasets to 80 and 10 , respectively .",experimental-setup
"Training is done via stochastic gradient descent over shuffled mini-batches with the Adadelta update rule , with l 2 constraint ( Hinton et al. , 2012 ) of 3 .",experimental-setup
We perform early stopping using a subset of the given development dataset .,experimental-setup
"Motivated by this approach , we consider bottom - up attention for neural abstractive summarization .",approach
Our approach first selects a selection mask for the source document and then constrains a standard neural model by this mask .,approach
Our full model incorporates a separate content selection system to decide on relevant aspects of the source document .,approach
"We frame this selection task as a sequence - tagging problem , with the objective of identifying tokens from a document that are part of its summary .",approach
"To incorporate bottom - up attention into abstractive summarization models , we employ masking to constrain copying words to the selected parts of the text , which produces grammatical outputs .",approach
Bottom - Up Abstractive Summarization,research-problem
Text summarization systems aim to generate natural language summaries that compress the information in a longer text .,research-problem
Current state - of - the - art neural abstractive summarization models combine extractive and abstractive techniques by using pointergenerator style models which can copy words from the source document .,research-problem
All inference parameters are tuned on a 200 example subset of the validation set .,experimental-setup
"Length penalty parameter ? and copy mask differ across models , with ? ranging from 0.6 to 1.4 , and ranging from 0.1 to 0.2 .",experimental-setup
"The coverage penalty parameter ? is set to 10 , and the copy attention normalization parameter ? to 2 for both approaches .",experimental-setup
The minimum length of the generated summary is set to 35 for CNN - DM and 6 for NYT .,experimental-setup
"We use AllenNLP for the content selector , and Open NMT - py for the abstractive models .",experimental-setup
"Further , we also present novel multi-task learning architectures based on multi-layered encoder and decoder models , where we empirically show that it is substantially better to share the higherlevel semantic layers between the three aforementioned tasks , while keeping the lower - level ( lexico- syntactic ) layers unshared .",model
Soft Layer - Specific Multi - Task Summarization with Entailment and Question Generation,research-problem
"We improve these important aspects of abstractive summarization via multi-task learning with the auxiliary tasks of question generation and entailment generation , where the former teaches the summarization model how to look for salient questioning - worthy details , and the latter teaches the model how to rewrite a summary which is a directed - logical subset of the input document .",research-problem
"In this work , we improve abstractive text summarization via soft , high - level ( semantic ) layerspecific multi-task learning with two relevant auxiliary tasks .",research-problem
We observe that our model outperformed all the strong state of - the - art models on both datasets in all metrics except for RG - 2 on Gigaword .,result
"In terms of the pointer generator performance , the improvements made by our concept pointer are statistically significant ( p < 0.01 ) across all metrics .",result
ABS + is a tuned ABS model with additional features .,baseline
RAS - Elman ) is a convolution encoder and an Elman RNN decoder with attention .,baseline
Seq2seq + att is two - layer BiLSTM encoder and one - layer LSTM decoder equipped with attention .,baseline
lvt5 k - lsent uses temporal attention to keep track of the past attentive weights of the decoder and restrains the repetition in later sequences .,baseline
SEASS includes an additional selective gate to control information flow from the encoder to the decoder .,baseline
Pointer - generator is an integrated pointer network and seq2seq model .,baseline
CGU ) sets a convolutional gated unit and self - attention for global encoding .,baseline
"Hence , in this paper , we propose a novel model based on a concept pointer generator that encourages the generation of conceptual and abstract words .",model
"As a hidden benefit , the model also alleviates the OOV problems .",model
"Our model uses pointer network to capture the salient information from a source text , and then employs another pointer to generalize the detailed words according to their upper level of expressions .",model
The optimization function is adaptive so as to cater for different datasets with distantly - supervised training .,model
"The network is then optimized end - to - end using reinforcement learning , with the distant - supervision strategy as a complement to further improve the summary .",model
Concept Pointer Network for Abstractive Summarization,research-problem
Abstractive summarization ( ABS ) has gained overwhelming success owing to a tremendous development of sequence - to - sequence ( seq2seq ) model and its variants .,research-problem
We initialize word embeddings with 128 - d vectors and fine - tune them during training .,experimental-setup
The vocabulary size was set to 150 k for both the source and target text .,experimental-setup
The hidden state size was set to 256 .,experimental-setup
We trained our models on a single GTX TI - TAN GPU machine .,experimental-setup
We used the Adagrad optimizer with a batch size of 64 to minimize the loss .,experimental-setup
We used gradient clipping with a maximum gradient norm of 2 .,experimental-setup
"The initial learning rate and the accumulator value were set to 0.15 and 0.1 , respectively .",experimental-setup
"We trained our concept pointer generator for 450 k iterations yielded the best performance , then took the optimization using RL rewards for RG - L at 95 K iterations on DUC - 2004 and at 50 K iterations on Gigaword .",experimental-setup
We took the distancesupervised training at 5 K iterations on DUC - 2004 and at 6.5 K iterations on Gigaword .,experimental-setup
"Our code is available on https :// github.com/wprojectsn/codes , and the vocabularies and candidate concepts are also included .",code
"In the experiments on the two datasets , our model achieves advantages of ROUGE score over the baselines , and the advantages of ROUGE score on the LCSTS are significant .",result
"Compared with the conventional seq2seq model , our model owns an advantage of ROUGE - 2 score 3.7 and 1.5 on the LCSTS and Gigaword respectively .",result
"To tackle this problem , we propose a model of global encoding for abstractive summarization .",model
We set a convolutional gated unit to perform global encoding on the source context .,model
"The gate based on convolutional neural network ( CNN ) filters each encoder output based on the global context due to the parameter sharing , so that the representations at each time step are refined with consideration of the global context .",model
Global Encoding for Abstractive Summarization,research-problem
"Therefore , sequence - to - sequence learning can be applied to neural abstractive summarization , whose model consists of an encoder and a decoder .",research-problem
We implement our experiments in PyTorch on an NVIDIA 1080 Ti GPU .,experimental-setup
The word embedding dimension and the number of hidden units are both 512 .,experimental-setup
"In both experiments , the batch size is set to 64 .",experimental-setup
The learning rate is halved every epoch .,experimental-setup
"Gradient clipping is applied with range [ - 10 , 10 ] .",experimental-setup
"We use Adam optimizer ( Kingma and Ba , 2014 ) with the default setting ? = 0.001 , ? 1 = 0.9 , ? 2 = 0.999 and = 1 10 ?8 .",experimental-setup
"To incorporate entailment knowledge into abstractive summarization models , we propose in this work an entailment - aware encoder and an entailment - aware decoder .",model
"Furthermore , we propose an entailment Reward Augmented Maximum Likelihood ( RAML ) training that encourages the decoder of the summarization system to produce summary entailed by the source .",model
"We share the encoder of the summarization generation system with the entailment recognition system , so that the encoder can grasp both the gist of the source sentence and be aware of entailment relationships .",model
Ensure the Correctness of the Summary : Incorporate Entailment Knowledge into Abstractive Sentence Summarization,research-problem
"In this paper , we investigate the sentence summarization task that produces a summary from a source sentence .",research-problem
In this paper we propose Selective Encoding for Abstractive Sentence Summarization ( SEASS ) .,model
"We treat the sentence summarization as a threephase task : encoding , selection , and decoding .",model
"It consists of a sentence encoder , a selective gate network , and a summary decoder .",model
"First , the sentence encoder reads the input words through an RNN unit to construct the first level sentence representation .",model
Then the selective gate network selects the encoded information to construct the second level sentence representation .,model
"The selective mechanism controls the information flow from encoder to decoder by applying a gate network according to the sentence information , which helps improve encoding effectiveness and release the burden of the decoder .",model
"Finally , the attention - equipped decoder generates the summary using the second level sentence representation .",model
Selective Encoding for Abstractive Sentence Summarization,research-problem
"The second level representation is tailored for sentence summarization task , which leads to better performance .",research-problem
"Retrieve , Rerank and Rewrite : Soft Template Based Neural Summarization",research-problem
"In this paper , we focus on an increasingly intriguing task , i.e. , abstractive sentence summarization , which generates a shorter version of a given sentence while attempting to preserve its original meaning .",research-problem
We use the popular seq2seq framework Open - NMT 5 as the starting point .,experimental-setup
"To make our model more general , we retain the default settings of Open NMT to build the network architecture .",experimental-setup
"Specifically , the dimensions of word embeddings and RNN are both 500 , and the encoder and decoder structures are two - layer bidirectional Long Short Term Memory Networks ( LSTMs ) .",experimental-setup
"On our computer ( GPU : GTX 1080 , Memory : 16G , CPU : i7-7700 K ) , the training spends about 2 days .",experimental-setup
"During test , we use beam search of size 5 to generate summaries .",experimental-setup
"We add the argument "" replace unk "" to replace the generated unknown words with the source word that holds the highest attention weight .",experimental-setup
"Since the generated summaries are often shorter than the actual ones , we introduce an additional length penalty argument "" alpha 1 "" to encourage longer generation , like .",experimental-setup
Code and results can be found at http://www4.comp.polyu.edu.hk/cszqcao/,code
"We see that RASG achieves a 11.0 % , 9.1 % and 6.6 % increment over the state - of - the - art method CGU in terms of ROUGE - 1 , ROUGE - 2 and ROUGE - L respectively .",result
It is worth noticing that the baseline model S2SR achieves better performance than S2S which demonstrates the effectiveness of incorporating reader focused aspect in summary generation .,result
( 1 ) S2S : Sequence - to - sequence framework has been proposed for language generation task .,baseline
"( 2 ) S2SR : We simply add the reader attention on attention distribution ? t , in each decoding step .",baseline
"( 3 ) CGU : propose to use the convolutional gated unit to refine the source representation , which achieves the state - of - the - art performance on social media text summarization dataset .",baseline
"( 4 ) LEAD1 : LEAD1 is a commonly used baseline , which selects the first sentence of document as the summary .",baseline
"( 5 ) TextRank : propose to build a graph , then add each sentence as a vertex and use link to represent semantic similarity .",baseline
"In this paper , we propose a summarization framework named reader - aware summary generator ( RASG ) that incorporates reader comments to improve the summarization performance .",model
"Specifically , a seq2seq architecture with attention mechanism is employed as the basic summary generator .",model
"We first calculate alignment between the reader comments words and document words , and this alignment information is regarded as reader attention representing the "" reader focused aspect "" .",model
"Then , we treat the decoder attention weights as the focused aspect of the generated summary , a.k.a. , "" decoder focused aspect "" .",model
"After each decoding step , a supervisor is designed to measure the distance between the reader focused aspect and the decoder focused aspect .",model
The training of our framework RASG is conducted in an adversarial way .,model
Abstractive Text Summarization by Incorporating Reader Comments,research-problem
"In neural abstractive summarization field , conventional sequence - to - sequence based models often suffer from summarizing the wrong aspect of the document with respect to the main aspect .",research-problem
"To tackle this problem , we propose the task of reader - aware abstractive summary generation , which utilizes the reader comments to help the model produce better summary about the main aspect .",research-problem
"Unlike traditional abstractive summarization task , reader - aware summarization confronts two main challenges :",research-problem
We implement our experiments in TensorFlow ) on an NVIDIA P40 GPU .,experimental-setup
The word embedding dimension is set to 256 and the number of hidden units is 512 .,experimental-setup
We use Adagrad optimizer as our optimizing algorithm .,experimental-setup
We employ beam search with beam size 5 to generate more fluency summary sentence .,experimental-setup
In this paper we seek to address this problem by incorporating source syntactic structure in neural sentence summarization to help the system identify summary - worthy content and compose summaries that preserve the important meaning of the source texts .,model
We present structure - infused copy mechanisms to facilitate copying source words and relations to the summary based on their semantic and structural importance in the source sentences .,model
Structure - Infused Copy Mechanisms for Abstractive Summarization,research-problem
Seq2seq learning has produced promising results on summarization .,research-problem
"Therefore , in order to scale attention models for this problem , we aim to prune down the length of the source sequence in an intelligent way .",approach
"Instead of naively attending to all the words of the source at once , our solution is to use a two - layer hierarchical attention .",approach
"For document summarization , this means dividing the document into chunks of text , sparsely attending to one or a few chunks at a time using hard attention , then applying the usual full attention over those chunks - we call this method coarse - to - fine attention .",approach
The ILP model ROUGE scores are surprisingly low .,result
C2 F results are significantly worse than soft attention results .,result
Coarse-to-Fine Attention Models for Document Summarization,research-problem
"We train with minibatch stochastic gradient descent ( SGD ) with batch size 20 for 20 epochs , renormalizing gradients below norm 5 .",experimental-setup
"We initialize the learning rate to 0.1 for the top - level encoder and 1 for the rest of the model , and begin decaying it by a factor of 0.5 each epoch after the validation perplexity stops decreasing .",experimental-setup
"We initialize all other parameters as uniform in the interval [ ? 0.1 , 0.1 ] .",experimental-setup
"We use 2 layer LSTMs with 500 hidden units , and we initialize word embeddings with 300 dimensional word2vec embeddings .",experimental-setup
We use dropout between stacked LSTM hidden states and before the final word generator layer to regularize ( with dropout probability 0.3 ) .,experimental-setup
"For convolutional layers , we use a kernel width of 6 and 600 filters .",experimental-setup
Positional embeddings have dimension 25 .,experimental-setup
"At test time , we run beam search to produce the summary with a beam size of 5 .",experimental-setup
Our models are implemented using Torch based on a past version of the Open NMT system,experimental-setup
4 . We ran our experiments on a 12GB Geforce GTX Titan X GPU .,experimental-setup
The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a summary during the encoding process and exploit the estimation to control the output words in each decoding step .,model
We refer to our additional component as a wordfrequency estimation ( WFE ) sub-model .,model
The WFE sub-model explicitly manages how many times each word has been generated so far and might be generated in the future during the decoding process .,model
Cutting - off Redundant Repeating Generations for Neural Abstractive Summarization,research-problem
"The RNN - based encoder - decoder ( EncDec ) approach has recently been providing significant progress in various natural language generation ( NLG ) tasks , i.e. , machine translation ( MT ) and abstractive summarization ( ABS ) .",research-problem
Mixture Content Selection for Diverse Sequence Generation,research-problem
Generating diverse sequences is important in many NLP applications such as question generation or summarization that exhibit semantically one - to - many relationships between source and the target sequences .,research-problem
Generating target sequences given a source sequence has applications in a wide range of problems in NLP with different types of relationships between the source and target sequences .,research-problem
"Encoder - decoder models are widely used for sequence generation , most notably in machine translation where neural models are now often almost as good as human translators in some language pairs .",research-problem
"For all experiments , we tie the weights of the encoder embedding , the decoder embedding , and the decoder output layers .",experimental-setup
We train up to 20 epochs and select the checkpoint with the best oracle metric .,experimental-setup
"We use Adam ( Kingma and Ba , 2015 ) optimizer with learning rate 0.001 and momentum parmeters ? 1 = 0.9 and ? 2 = 0.999 .",experimental-setup
Minibatch size is 64 and 32 for question generation and abstractive summarization .,experimental-setup
"All models are implemented in PyTorch and trained on single Tesla P40 GPU , based on NAVER Smart Machine Learning ( NSML ) platform .",experimental-setup
"Inspired by the recently proposed architectures for machine translation , our model consists of a conditional recurrent neural network , which acts as a decoder to generate the summary of an input sentence , much like a standard recurrent language model .",model
"In addition , at every time step the decoder also takes a conditioning input which is the output of an encoder module .",model
"Depending on the current state of the RNN , the encoder computes scores over the words in the input sentence .",model
"Lastly , our encoder uses a convolutional network to encode input words .",model
Both the decoder and encoder are jointly trained on a data set consisting of sentence - summary pairs .,model
Abstractive Sentence Summarization with Attentive Recurrent Neural Networks,research-problem
Generating a condensed version of a passage while preserving its meaning is known as text summarization .,research-problem
We implemented our models in the Torch library ( http://torch.ch/),experimental-setup
2 . To optimize our loss ( Equation 5 ) we used stochastic gradient descent with mini-batches of size 32 .,experimental-setup
"During training we measure the perplexity of the summaries in the validation set and adjust our hyper - parameters , such as the learning rate , based on this number .",experimental-setup
For the decoder we experimented with both the Elman RNN and the Long - Short Term Memory ( LSTM ) architecture ( as discussed in 3.1 ) .,experimental-setup
We chose hyper - parameters based on a grid search and picked the one which gave the best perplexity on the validation set .,experimental-setup
"Our final Elman architecture ( RAS - Elman ) uses a single layer with H = 512 , ? = 0.5 , ? = 2 , and ? = 10 .",experimental-setup
"The LSTM model ( RAS - LSTM ) also has a single layer with H = 512 , ? = 0.1 , ? = 2 , and ? = 10 .",experimental-setup
"To tackle the above mentioned problems , we design a new framework based on sequence to - sequence oriented encoder - decoder model equipped with a latent structure modeling component .",model
We employ Variational Auto - Encoders ( VAEs ) as the base model for our generative framework which can handle the inference problem associated with complex generative modeling .,model
"Inspired by , we add historical dependencies on the latent variables of VAEs and propose a deep recurrent generative decoder ( DRGD ) for latent structure modeling .",model
Then the standard discriminative deterministic decoder and the recurrent generative decoder are integrated into a unified decoding framework .,model
The target summaries will be decoded based on both the discriminative deterministic variables and the generative latent structural information .,model
Deep Recurrent Generative Decoder for Abstractive Text Summarization,research-problem
Automatic summarization is the process of automatically generating a summary that retains the most important content of the original text document .,research-problem
"The embedding dimension was chosen from K = { 100 , . . . , 800 } , and regularization weight from ? = { 0.0001 , . . . , 10.0 }.",hyperparameter
"In this paper , we present Bayesian subspace multinomial model ( Bayesian SMM ) as a generative model for bag - ofwords representation of documents .",model
"We show that our model can learn to represent each document in the form of a Gaussian distribution , thereby encoding the uncertainty in its covariance .",model
"Further , we propose a generative Gaussian classifier that exploits this uncertainty for topic identification ( ID ) .",model
"The proposed VB framework can be extended in a straightforward way for subspace n-gram model , that can model n-gram distribution of words in sentences .",model
Learning document embeddings along with their uncertainties,research-problem
We also present a generative Gaussian linear classifier for topic identification that exploits the uncertainty in document embeddings .,research-problem
"L EARNING word and document embeddings have proven to be useful in wide range of information retrieval , speech and natural language processing applications -.",research-problem
"We use a batch size of B = 32 , where for a batch of image - caption pairs each image ( caption ) is only related to one caption ( image ) .",hyperparameter
We use D = 1024 for common space mapping dimension and ? = 0.25 for Leaky ReLU in the non-linear mappings .,hyperparameter
Image - caption pairs are sampled randomly with a uniform distribution .,hyperparameter
Both visual and textual networks weights are fixed during training and only common space mapping weights are trainable .,hyperparameter
We train the network for 20 epochs with the Adam optimizer with lr = 0.001 where the learning rate is divided by 2 once at the 10 - th epoch and again at the 15 - th epoch .,hyperparameter
We regularize weights of the mappings with l 2 regularization with reg value = 0.0005 .,hyperparameter
"For VGG , we take outputs from { conv 4 1 , conv 4 3 , conv5 1 , conv5 3 } and map to semantic feature maps with dimension 18181024 , and for PNAS - Net we take outputs from { Cell 5 , Cell 7 , Cell 9 , Cell 11 } pointing game accuracy attention correctness Ours Ours Ours Ours Class",hyperparameter
"In this work , we propose to explicitly learn a non-linear mapping of the visual and textual modalities into a common space , and do so at different granularity for each domain .",model
"This common space mapping is trained with weak supervision and exploited at test - time with a multi - level multimodal attention mechanism , where a natural formalism for computing attention heatmaps at each level , attended features and pertinence scoring , enables us to solve the phrase grounding task elegantly and effectively .",model
Multi - level Multimodal Common Semantic Space for Image - Phrase Grounding,research-problem
We address the problem of phrase grounding by learning a multi - level common semantic space shared by the textual and visual modalities .,research-problem
"The results in rows 1 , 2 show that using level - attention mechanism based on multi-level feature maps significantly improves the performance over single visual - textual feature comparison .",ablation-analysi
"By comparing rows 2 , 4 , 5 , 7 , we see that non-linear mapping in our model is really important , and replacing any mapping with a linear one significantly degrades the performance .",ablation-analysi
"We can also see that non-linear mapping seems more important on the visual side , but best results are obtained with both text and visual non-linear mappings .",ablation-analysi
"The results in rows 1 , 3 and 2 , 6 show the importance of using a strong contextualized text embedding as the performance drops significantly .",ablation-analysi
"We also study the use of softmax on the heatmaps , comparing rows 2 , 8 , we see that applying softmax leads to a very negative effect on the performance .",ablation-analysi
"In this work , we make available the first corpus 1 for sarcasm detection that has both unbalanced and self - annotated labels and does not consist of short text snippets from Twitter 2 .",dataset
"With more than a million examples of sarcastic statements , each provided with author , topic , and contex information , the dataset exceeds all previous sarcasm corpora by an order of magnitude in size .",dataset
"The baselines in perform reasonably well and much better than the random baseline , but none of them match human performance on either dataset .",result
"There is clear scope for improvement for machine learning methods , starting with the use of context provided to make better decisions about sarcasm .",result
"We introduce the Self - Annotated Reddit Corpus ( SARC ) , a large corpus for sarcasm research and for training and evaluating systems for sarcasm detection .",research-problem
Code to reproduce our results is provided at https://github.com/NLPrinceton/,code
CASCADE manages to achieve major improvement across all datasets with statistical significance .,result
CASCADE comfortably beats the state - of - the - art neural models CNN - SVM and CUE - CNN .,result
The lowest performance is obtained by the Bag - of - words approach whereas all neural architectures outperform it .,result
Its improved performance on the Main imbalanced dataset also reflects its robustness towards class imbalance and establishes it as a real - world deployable network .,result
"Since CUE - CNN generates its user embeddings using a method similar to the ParagraphVector , we test the importance of personality features being included in our user profiling .",result
"Amongst the neural networks , the CNN baseline receives the least performance .",result
We holdout 10 % of the training data for validation .,hyperparameter
"To optimize the parameters , Adam optimizer ( Kingma and Ba , 2014 ) is used , starting with an initial learning rate of 1e ? 4 .",hyperparameter
Training termination is decided using early stopping technique with a patience of 12 .,hyperparameter
"For the batched - modeling of comments in CNNs , each comment is either restricted or padded to 100 words for uniformity .",hyperparameter
"Particularly , we propose a hybrid network , named CASCADE , that utilizes both content and contextual - information required for sarcasm detection .",model
"First , it performs user profiling to create user embeddings that capture indicative behavioral traits for sarcasm .",model
It performs content - modeling using a Convolutional Neural Network ( CNN ) to extract its syntactic features .,model
"It makes use of users ' historical posts to model their writing style ( stylometry ) and personality indicators , which are then fused into comprehensive user embeddings using a multi-view fusion approach , Canonical Correlation Analysis ( CCA ) .",model
"Second , it extracts contextual information from the discourse of comments in the discussion forums .",model
This is done by document modeling of these consolidated comments belonging to the same forum .,model
"After the contextual modeling phase , CASCADE is provided with a comment for sarcasm detection .",model
This CNN representation is then concatenated with the relevant user embedding and discourse features to get the final representation which is used for classification .,model
CASCADE : Contextual Sarcasm Detection in Online Discussion Forums,research-problem
"The literature in automated sarcasm detection has mainly focused on lexical , syntactic and semantic - level analysis of text .",research-problem
"In this paper , we propose CASCADE ( a ContextuAl SarCasm DEtector ) that adopts a hybrid approach of both content and context - driven modeling for sarcasm detection in online social media discussions .",research-problem
"In this paper , we explore an alternative approach based on enriching the document representation ( prior to indexing ) .",approach
"Focusing on question answering , we train a sequence - to - sequence model , that given a document , generates possible questions that the document might answer .",approach
Document Expansion by Query Prediction,research-problem
"In this paper , we describe in detail how we have re-purposed BERT as a passage re-ranker and achieved state - of - the - art results on the MS MARCO passage re-ranking task .",approach
"Despite training on a fraction of the data available , the proposed BERT - based models surpass the previous state - of - the - art models by a large margin on both of the tasks .",result
PASSAGE RE - RANKING WITH BERT,research-problem
"In this paper , we describe a simple re-implementation of BERT for query - based passage re-ranking .",research-problem
"The evaluation shows that CATENA is the best performing system in both tasks , even if in Task C best precision and best recall are yielded by and , respectively .",result
"If we consider the different entity pairs , CATENA performs best on timex - timex and event - timex relations , while CAEVO still achieves the best results on event - DCT and event - event pairs .",result
"The CATENA system includes two main classification modules , one for temporal and the other for causal relations between events .",model
"As shown in , they both take as input a document annotated with the so - called temporal entities according to TimeML guidelines , including the document creation time ( DCT ) , events and time expressions ( timexes ) .",model
"The output is the same document with temporal links ( TLINKs ) set between pairs of temporal entities , each assigned to one of the TimeML temporal relation types , such as or SIMULTANEOUS , which denotes the temporal ordering .",model
The document is also annotated with causal relations ( CLINKs ) between event pairs .,model
"The modules for temporal and causal relation classification rely both on a sieve - based architecture , in which the remaining unlabelled pairs - after running a rule - based component and / or a transitive reasoner are fed into a supervised classifier .",model
"We present CATENA , a sieve - based system to perform temporal and causal relation extraction and classification from English texts , exploiting the interaction between the temporal and the causal model .",research-problem
"As expected , running a transitive closure module after the temporal rule - based sieve ( RB + TR ) results in improving recall , but the over all performance is still lacking ( less than .30 F1-score ) .",ablation-analysi
Combining rule - based and machine - learned sieves ( RB + ML ) yields a slight improvement compared with enabling only the machine - learned sieve in the system ( ML ) .,ablation-analysi
Introducing the temporal reasoner module between the two sieves ( RB + TR + ML ) proves to be even more beneficial .,ablation-analysi
"In this paper , we propose a structured learning approach to temporal relation extraction , where local models are updated based on feedback from global inferences .",approach
"The structured approach also gives rise to a semisupervised method , making it possible to take advantage of the readily available unlabeled data .",approach
The first is the regularized averaged perceptron ( AP ) implemented in the LBJava package and is a local method .,baseline
"On top of the first baseline , we performed global inference in Eq .",baseline
"Both of them used the same feature set ( i.e. , as designed in ) as in the proposed structured perceptron ( SP ) and CoDL for fair comparisons .",baseline
A Structured Learning Approach to Temporal Relation Extraction,research-problem
Identifying temporal relations between events is an essential step towards natural language understanding .,research-problem
"The fundamental tasks in temporal processing , as identified in the TE workshops , are 1 ) time expression ( the so - called "" timex "" ) extraction and normalization and 2 ) temporal relation ( also known as TLINKs ) extraction .",research-problem
"Secondly , we markedly improve the availability of supervised training data by using Mechanical Turk crowd annotation to produce a large supervised training dataset , suitable for the common relations between people , organizations and locations which are used in the TAC KBP evaluations .",dataset
"We name this dataset the TAC Relation Extraction Dataset ( TACRED ) , and will make it available through the Linguistic Data Consortium ( LDC ) in order to respect copyrights on the underlying text .",dataset
"We observe that all neural models achieve higher F 1 scores than the logistic regression and patterns systems , which demonstrates the effectiveness of neural models for relation extraction .",result
"Although positional embeddings help increase the F 1 by around 2 % over the plain CNN model , a simple ( 2 - layer ) LSTM model performs surprisingly better than CNN and dependency - based models .",result
"Lastly , our proposed position - aware mechanism is very effective and achieves an F 1 score of 65.4 % , with an absolute increase of 3.9 % over the best baseline neural model ( LSTM ) and 7.9 % over the baseline logistic regression system .",result
CNN - based models tend to have higher precision ; RNN - based models have better recall .,result
"Evaluating relation extraction systems on slot filling is particularly challenging in that : ( 1 ) Endto - end cold start slot filling scores conflate the performance of all modules in the system ( i.e. , entity recognizer , entity linker and relation extractor ) .",result
( 2 ) Errors in hop - 0 predictions can easily propagate to hop - 1 predictions .,result
We also run an ensemble of our position - aware attention model which takes majority votes from 5 runs with random initializations and it further pushes the F 1 score up by 1.6 % .,result
"We find that : ( 1 ) by only training our logistic regression model on TACRED ( in contrast to on the 2 million bootstrapped examples used in the 2015 Stanford system ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",result
We map words that occur less than 2 times in the training set to a special < UNK > token .,hyperparameter
We use the pre-trained GloVe vectors to initialize word embeddings .,hyperparameter
"For all the LSTM layers , we find that 2 - layer stacked LSTMs generally work better than one - layer LSTMs .",hyperparameter
We minimize cross - entropy loss over all 42 relations using AdaGrad .,hyperparameter
We apply Dropout with p = 0.5 to CNNs and LSTMs .,hyperparameter
During training we also find a word dropout strategy to be very effective : we randomly set a token to be < UNK > with a probability p.,hyperparameter
We set p to be 0.06 for the SDP - LSTM model and 0.04 for all other models .,hyperparameter
"We propose a new , effective neural network sequence model for relation classification .",model
Its architecture is better customized for the slot filling task : the word representations are augmented by extra distributed representations of word position relative to the subject and object of the putative relation .,model
This means that the neural attention model can effectively exploit the combination of semantic similarity - based attention and positionbased attention .,model
"However , the ability to populate knowledge bases with facts automatically extracted from documents has improved frustratingly slowly .",research-problem
A basic but highly important challenge in natural language understanding is being able to populate a knowledge base with relational facts contained in a piece of text .,research-problem
"Existing work on relation extraction ( e.g. , has been unable to achieve sufficient recall or precision for the results to be usable versus hand - constructed knowledge bases .",research-problem
"shows the precision - recall curves for each method , where PCNNs + MIL denotes our method , and demonstrates that PCNNs + MIL achieves higher precision over the entire range of recall .",result
PCNNs + MIL enhances the recall to ap - proximately 34 % without any loss of precision .,result
"In terms of both precision and recall , PCNNs + MIL outperforms all other evaluated approaches .",result
Automatically learning features via PCNNs can alleviate the error propagation that occurs in traditional feature extraction .,result
Incorporating multi-instance learning into a convolutional neural network is an effective means of addressing the wrong label problem .,result
Mintz represents a traditional distantsupervision - based model that was proposed by .,baseline
MultiR is a multi-instance learning method that was proposed by .,baseline
MIML is a multi-instance multilabel model that was proposed by .,baseline
"In this paper , we propose a novel model dubbed Piecewise Convolutional Neural Networks ( PC - NNs ) with multi-instance learning to address the two problems described above .",model
"To address the first problem , distant supervised relation extraction is treated as a multi-instance problem similar to previous studies .",model
The piecewise max pooling procedure returns the maximum value in each segment instead of a single maximum value over the entire sentence .,model
We design an objective function at the bag level .,model
"In the learning process , the uncertainty of instance labels can be taken into account ; this alleviates the wrong label problem .",model
"To address the second problem , we adopt convolutional architecture to automatically learn relevant features without complicated NLP preprocessing inspired by .",model
"To capture structural and other latent information , we divide the convolution results into three segments based on the positions of the two given entities and devise a piecewise max pooling layer instead of the single max pooling layer .",model
Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks,research-problem
"To address the first problem , distant supervised relation extraction is treated as a multi-instance problem similar to previous studies .",research-problem
"In relation extraction , one challenge that is faced when building a machine learning system is the generation of training examples .",research-problem
"In this paper , we use the Skip - gram model ( word2 vec ) 5 to train the word embeddings on the NYT corpus .",experimental-setup
"We use a grid search to determine the optimal parameters and manually specify subsets of the parameter spaces : w ? { 1 , 2 , 3 , , 7 } and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",experimental-setup
"We use Adadelta in the update procedure ; it relies on two main parameters , ? and ? , which do not significantly affect the performance .",experimental-setup
"Following , we tune all of the models using three - fold validation on the training set .",experimental-setup
"Because the position dimension has little effect on the result , we heuristically choose d p = 5 .",experimental-setup
The batch size is fixed to 50 .,experimental-setup
"In the dropout operation , we randomly set the hidden unit activities to zero with a probability of 0.5 during training .",experimental-setup
"Our proposed model achieves a new SOTA on RE with a F 1 of 62. 83 , more than 2.3 F 1 above the previous SOTA .",result
Our proposed model also beats a multitask model which uses signals from additional tasks by more than 1.5 F 1 points .,result
The Recall gains for RE ( 4.3 absolute points ) are much higher than for EMD ( 0.6 absolute points ) .,result
"Thus , our large gains in RE Recall ( and F 1 ) showcase the effectiveness of our simple modeling of ordered span pairs for relation extraction ( Section 3.3 ) .",result
"For both tasks , our model 's Precision is close to and Recall is significantly higher than previous works .",result
"The learned character embeddings are of size 8 . 1 - dimensional convolutions of window size 3 , 4,5 are applied per-token with 50 filters of each window size .",hyperparameter
Our stacked bi - LSTMs ( Section 3.1 ) has 3 layers with 200 - dimensional hidden states and highway connections .,hyperparameter
"All Multi Layer Perceptrons ( MLP ) has two hidden layers with 500 dimensions , each followed by ReLU activation .",hyperparameter
"Regularization Dropout is applied with dropout rate 0.2 to all hidden layers of all MLPs and feature encodings , with dropout rate 0.5 to all word and character embeddings and with dropout rate 0.4 to all LSTM layer outputs .",hyperparameter
"Learning Learning is done with Adam ( Kingma and Ba , 2015 ) with default parameters .",hyperparameter
The learning rate is annealed by 1 % every 100 iterations .,hyperparameter
Minibatch Size is 1 .,hyperparameter
Early Stopping of 20 evaluations on the dev set is used .,hyperparameter
We only consider spans that are entirely within a sentence and limit spans to a max length of L = 10 .,hyperparameter
We propose a simple bi - LSTM based model which generates span representations for each possible span .,model
The span representations are used to perform entity mention detection on all spans in parallel .,model
The same span representations are then used to perform relation extraction on all pairs of detected entity mentions .,model
Span - Level Model for Relation Extraction,research-problem
"This paper focuses on Relation Extraction ( RE ) , which is the task of entity mention detection and classifying the relations between each pair of those mentions .",research-problem
"Since , work on RE has revolved around end - to - end systems : single models which first perform entity mention detection and then relation extraction .",research-problem
"This work presents a solution that can resolve the inefficient multiple - passes issue of existing solutions for MRE by encoding the input only once , which significantly increases the efficiency and scalability .",model
"Specifically , the proposed solution is built on top of the existing transformer - based , pretrained general - purposed language encoders .",model
"In this paper we use Bidirectional Encoder Representations from Transformers ( BERT ) as the transformer - based encoder , but this solution is not limited to using BERT alone .",model
The two novel modifications to the original BERT architecture are : ( 1 ) we introduce a structured prediction layer for predicting multiple relations for different entity pairs ; and ( 2 ) we make the selfattention layers aware of the positions of all en-tities in the input paragraph .,model
Extracting Multiple - Relations in One - Pass with Pre-Trained Transformers,research-problem
The state - of - the - art solutions for extracting multiple entity - relations from an input paragraph always require a multiple - pass encoding on the input .,research-problem
"This paper proposes a new solution that can complete the multiple entityrelations extraction task with only one - pass encoding on the input corpus , and achieve a new state - of - the - art accuracy performance , as demonstrated in the ACE 2005 benchmark .",research-problem
Relation extraction ( RE ) aims to find the semantic relation between a pair of entity mentions from an input paragraph .,research-problem
One particular type of the RE task is multiplerelations extraction ( MRE ) that aims to recognize relations of multiple pairs of entity mentions from an input paragraph .,research-problem
"Because in real - world applications , whose input paragraphs dominantly contain multiple pairs of entities , an efficient and effective solution for MRE has more important and more practical implications .",research-problem
"Overall , we find that RESIDE achieves higher precision over the entire recall range on both the datasets .",result
All the non-neural baselines could not perform well as the features used by them are mostly derived from NLP tools which can be erroneous .,result
RESIDE outperforms PCNN + ATT and BGWA which indicates that incorporating side information helps in improving the performance of the model .,result
The higher performance of BGWA and PCNN + ATT over PCNN shows that attention helps in distant supervised RE .,result
"In this paper , we propose RESIDE , a novel distant supervised relation extraction method which utilizes additional supervision from KB through its neural network based architecture .",model
"RESIDE makes principled use of entity type and relation alias information from KBs , to impose soft constraints while predicting the relation .",model
"It uses encoded syntactic information obtained from Graph Convolution Networks ( GCN ) , along with embedded side information , to improve neural relation extraction .",model
RESIDE : Improving Distantly - Supervised Neural Relation Extraction using Side Information,research-problem
Distantly - supervised Relation Extraction ( RE ) methods train an extractor by automatically aligning relation instances in a Knowledge Base ( KB ) with unstructured text .,research-problem
Relation Extraction ( RE ) attempts to fill this gap by extracting semantic relationships between entity pairs from plain text .,research-problem
RE models usually ignore such readily available side information .,research-problem
RESIDE 's source code and datasets used in the paper are available at http://github.com / malllabiisc / RESIDE .,code
The results validate that GCNs are effective at encoding syntactic information .,ablation-analysi
"Further , the improvement from side information shows that it is complementary to the features extracted from text , thus validating the central thesis of this paper , that inducing side information leads to improved relation extraction .",ablation-analysi
We find that the model performs best when aliases are provided by the KB itself .,ablation-analysi
"Overall , we find that RESIDE gives competitive performance even when very limited amount of relation alias information is available .",ablation-analysi
We observe that performance improves further with the availability of more alias information .,ablation-analysi
"We choose the number of heads N for attention guided layer from { 1 , 2 , 3 , 4 } , the block number M from { 1 , 2 , 3 } , the number of sub - layers L in each densely connected layer from { 2 , 3 , 4 }.",hyperparameter
Glo Ve vectors are used as the initialization for word embeddings .,hyperparameter
"In this paper , we propose the novel Attention Guided Graph Convolutional Networks ( AGGCNs ) , which operate directly on the full tree .",model
"Intuitively , we develop a "" soft pruning "" strategy that transforms the original dependency tree into a fully connected edgeweighted graph .",model
"These weights can be viewed as the strength of relatedness between nodes , which can be learned in an end - to - end fashion by using self - attention mechanism .",model
we next introduce dense connections ) to the GCN model following .,model
"For GCNs , L layers will be needed in order to capture neighborhood information that is L hops away .",model
"With the help of dense connections , we are able to train the AGGCN model with a large depth , allowing rich local and non-local dependency information to be captured .",model
Attention Guided Graph Convolutional Networks for Relation Extraction,research-problem
Our code is available at https://github.com/Cartus / AGGCN_TACRED,code
We can observe that adding either attention guided layers or densely connected layers improves the performance of the model .,ablation-analysi
We can observe that all the C - AGGCN models with varied values of K are able to outperform the state - of - the - art C - GCN model ( reported in ) .,ablation-analysi
We also notice that the feed - forward layer is effective in our model .,ablation-analysi
"In addition , we notice that the performance of C - AGGCN with full trees outperforms all C - AGGCNs with pruned trees .",ablation-analysi
"Without the feed - forward layer , the result drops to an F1 score of 67.8 .",ablation-analysi
"In general , C - AGGCN with full trees outperforms C - AGGCN with pruned trees and C - GCN against various sentence lengths .",ablation-analysi
"Moreover , the improvement achieved by C - AGGCN with pruned trees decays when the sentence length increases .",ablation-analysi
C - AGGCN consistently outperforms C - GCN under the same amount of training data .,ablation-analysi
This suggests that C - AGGCN can benefit more from larger graphs ( full tree ) .,ablation-analysi
"When the size of training data increases , we can observe that the performance gap becomes more obvious .",ablation-analysi
"Particularly , using 80 % of the training data , the C - AGGCN model is able to achieve a F 1 score of 66.5 , higher than C - GCN trained on the whole dataset .",ablation-analysi
"In this work , we propose a novel extension of the graph convolutional network ) that is tailored for relation extraction .",model
"Our model encodes the dependency structure over the input sentence with efficient graph convolution operations , then extracts entity - centric representations to make robust relation predictions .",model
"We also apply a novel path - centric pruning technique to remove irrelevant information from the tree while maximally keeping relevant content , which further improves the performance of several dependencybased models including ours .",model
Graph Convolution over Pruned Dependency Trees Improves Relation Extraction,research-problem
We find that : The entity representations and feedforward layers contribute 1.0 F 1 .,ablation-analysi
"( 3 ) F 1 drops by 10.3 when we remove the feedforward layers , the LSTM component and the dependency structure altogether .",ablation-analysi
"( 2 ) When we remove the dependency structure ( i.e. , setting to I ) , the score drops by 3.2 F 1 .",ablation-analysi
"( 4 ) Removing the pruning ( i.e. , using full trees as input ) further hurts the result by another 9.7 F 1 .",ablation-analysi
We train our model using Adadelta with gradient clipping .,hyperparameter
We regularize our network using dropout with the drop - out rate tuned using development set .,hyperparameter
We have 3 hidden layers in our network and the dimensionality of the hidden units is 100 .,hyperparameter
All the weights in the network are initialized from small random uniform noise .,hyperparameter
We tune our hyperparameters based on ACE05 development set and use them for training on ACE04 dataset .,hyperparameter
"In this paper , we propose a novel RNN - based model for the joint extraction of entity mentions and relations .",model
"Unlike other models , our model does not depend on any dependency tree information .",model
Our RNN - based model is a multi - layer bidirectional LSTM over a sequence .,model
We encode the output sequence from left - to - right .,model
"At each time step , we use an attention - like model on the previously decoded time steps , to identify the tokens in a specified relation with the current token .",model
We also add an additional layer to our network to encode the output sequence from right - to - left and find significant improvement on the performance of relation identification using bi-directional encoding .,model
Going out on a limb : Joint Extraction of Entity Mentions and Relations without Dependency Trees,research-problem
Extraction of entities and their relations from text belongs to a very well - studied family of structured prediction tasks in NLP .,research-problem
Several methods have been proposed for entity mention and relation extraction at the sentencelevel .,research-problem
We can see that R - BERT significantly beats all the baseline methods .,result
"The MACRO F1 value of R - BERT is 89. 25 , which is much better than the previous best solution on this dataset .",result
"We compare our method , R - BERT , against results by multiple methods recently published for the SemEval - 2010 Task 8 dataset , including SVM , RNN , MVRNN , CNN + Softmax , FCM , CR - CNN , Attention - CNN , Entity Attention Bi-LSTM .",baseline
We add dropout before each add - on layer .,hyperparameter
"For the pre-trained BERT model , we use the uncased basic model .",hyperparameter
"In this paper , we apply the pretrained BERT model for relation classification .",model
"We insert special tokens before and after the target entities before feeding the text to BERT for fine - tuning , in order to identify the locations of the two target entities and transfer the information into the BERT model .",model
We then locate the positions of the two target entities in the output embedding from BERT model .,model
We use their embeddings as well as the sentence encoding ( embedding of the special first token in the setting of BERT ) as the input to a multi - layer neural network for classification .,model
Enriching Pre-trained Language Model with Entity Information for Relation Classification,research-problem
We observe that the three methods all perform worse than R - BERT .,ablation-analysi
"Of the methods , BERT - NO - SEP - NO - ENT performs worst , with its F1 8.16 absolute points worse than R - BERT .",ablation-analysi
BERT without special separate tokens can not locate the target entities and lose this key information .,ablation-analysi
This ablation study demonstrates that both the special separate tokens and the hidden entity vectors make important contributions to our approach .,ablation-analysi
"On the other hand , incorporating the output of the target entity vectors further enriches the information and helps to make more accurate prediction .",ablation-analysi
shows that the task agnostic BERT EM and BERT EM + MTB models outperform the previous published state of the art on FewRel task even when they have not seen any FewRel training data .,result
The additional MTB based training further increases F 1 scores for all tasks .,result
"For BERT EM + MTB , the increase over 's supervised approach is very significant - 8.8 % on the 5 - way - 1 - shot task and 12.7 % on the 10 - way - 1 - shot task .",result
"BERT EM + MTB also significantly outperforms BERT EM in this unsupervised setting , which is to be expected since there is no relation - specific loss during BERT EM 's training .",result
"When given access to all of the training data , BERT EM approaches BERT EM + MTB 's performance .",result
The results in show that MTB training could be used to significantly reduce effort in implementing an exemplar based relation extraction system .,result
"For all tasks , we see that MTB based training is even more effective for low - resource cases , where there is a larger gap in performance between our BERT EM and BERT EM + MTB based classifiers .",result
"This further supports our argument that training by matching the blanks can significantly reduce the amount of human input required to create relation extractors , and populate a knowledge base .",result
"First , we study the ability of the Transformer neural network architecture to encode relations between entity pairs , and we identify a method of representation that outperforms previous work in supervised relation extraction .",model
"Then , we present a method of training this relation representation without any supervision from a knowledge graph or human annotators by matching the blanks .",model
Matching the Blanks : Distributional Similarity for Relation Learning,research-problem
Reading text to identify and extract relations between entities has been along standing goal in natural language processing .,research-problem
Typically efforts in relation extraction fall into one of three groups .,research-problem
"In this paper , we propose a novel word - level approach for distant supervised relation extraction by reducing inner-sentence noise and improving robustness against noisy words .",approach
"To reduce innersentence noise , we utilize a novel Sub - Tree Parse ( STP ) method to remove irrelevant words by intercepting a subtree under the parent of entities ' lowest common ancestor .",approach
"Furthermore , the entity - wise attention is adopted to alleviate the influence of noisy words in the subtree and emphasize the task - relevant features .",approach
"To tackle the second challenge , we initialize our model parameters with a priori knowledge learned from the entity type classification task by transfer learning .",approach
"From , we can observe that the model with the STP performs best , and the SDP model obtains an even worse result than the pure one .",result
"The PR curve areas of BGRU + SDP and BGRU are about 0.332 and 0.337 respectively , while BGRU + STP increases it to 0.366 .",result
The result indicates : ( 1 ) Our STP can get rid of irrelevant words in each instance and obtain more precise sentence representation for relation extraction .,result
( 2 ) The SDP method is not appropriate to handle low - quality sentences where key relation words are not in the SDP .,result
"From and , we can obtain : ( 1 ) Regardless of the dataset that we employ , BGRU - WLA ( + STP ) + EWA outperforms BGRU (+ STP ) .",result
"To be more specific , the PR curve area has a relative improvement of over 2.3 % , which demonstrates that entity - wise hidden states in the BGRU present more precise relational features than other word states .",result
"EWA achieves further improvements and outperforms the baseline by over 4.6 % , because it considers more information than entity or relational words alone .",result
"( 1 ) Regardless of the dataset that we use , models with TL achieve better performance , which improve the PR curve area by over 4.7 % .",result
"( 2 ) BGRU + STP + TL achieves the best performance and increases the area to 0.383 , while areas of BGRU , BGRU + STP and BGRU + TL are 0.337 , 0.366 and 0.372 respectively .",result
Mintz proposes the humandesigned feature model .,baseline
MultiR puts forward a graphical model .,baseline
MIML proposes a multi -instance multi-label model .,baseline
PCNN puts forward a piecewise CNN for relation extraction .,baseline
PCNN + ATT proposes the selective attention mechanism with PCNN .,baseline
BGRU proposes a BGRU with the word - level attention mechanism .,baseline
Neural Relation Extraction via Inner - Sentence Noise Reduction and Transfer Learning,research-problem
Relation extraction aims to extract relations between pairs of marked entities in raw texts .,research-problem
"In this paper , we propose a novel word - level approach for distant supervised relation extraction by reducing inner-sentence noise and improving robustness against noisy words .",research-problem
We see that the BERT - LSTM - large model achieves the state - of - the - art F 1 score among single models and outperforms the ensemble model on the CoNLL 2005 in - domain and out - of - domain tests .,result
"However , it falls short on the CoNLL 2012 benchmark because the model of obtains very high precision .",result
We conduct experiments on two SRL tasks : and the predicate indicator embedding size is 10 .,hyperparameter
The learning rate is 5 10 ?5 . BERT base - cased and large - cased models are used in our experiments .,hyperparameter
The position embeddings are randomly initialized and fine - tuned during the training process .,hyperparameter
We show that simple neural architectures built on top of BERT yields state - of - the - art performance on a variety of benchmark datasets for these two tasks .,model
Simple BERT Models for Relation Extraction and Semantic Role Labeling,research-problem
Relation extraction and semantic role labeling ( SRL ) are two fundamental tasks in natural language understanding .,research-problem
"For SRL , the task is to extract the predicate - argument structure of a sentence , determining "" who did what to whom "" , "" when "" , "" where "" , etc .",research-problem
We observe that our novel attentionbased architecture achieves new state - of - the - art results on this relation classification dataset .,result
"Att - Input - CNN relies only on the primal attention at the input level , performing standard max - pooling after the convolution layer to generate the network output w O , in which the new objective function is utilized .",result
Our full dual attention model Att - Pooling - CNN achieves an even more favorable F1- score of 88 % .,result
"With Att - Input - CNN , we achieve an F1-score of 87.5 % , thus already outperforming not only the original winner of the SemEval task , an SVM - based approach ( 82.2 % ) , but also the wellknown CR - CNN model ( 84.1 % ) with a relative improvement of 4.04 % , and the newly released DRNNs ( 85.8 % ) with a relative improvement of 2.0 % , although the latter approach depends on the Stanford parser to obtain dependency parse information .",result
We propose a novel CNN architecture that addresses some of the shortcomings of previous approaches .,model
"Our CNN architecture relies on a novel multi-level attention mechanism to capture both entity - specific attention ( primary attention at the input level , with respect to the target entities ) and relation - specific pooling attention ( secondary attention with respect to the target relations ) .",model
2 . We introduce a novel pair - wise margin - based objective function that proves superior to standard loss functions .,model
Relation Classification via Multi - Level Attention CNNs,research-problem
We use the word2 vec skip - gram model to learn initial word representations on Wikipedia .,experimental-setup
Other matrices are initialized with random values following a Gaussian distribution .,experimental-setup
We apply a cross-validation procedure on the training data to select suitable hyperparameters .,experimental-setup
"The models that take the context into account perform similar to the baselines at the smallest recall numbers , but start to positively deviate from them at higher recall rates .",result
"In particular , the ContextAtt model performs better than any other system in our study over the entire recall range .",result
"Compared to the competitive LSTM - baseline that uses the same relation encoder , the ContextAtt model achieves a 24 % reduction of the average error : from 0.2096 0.002 to 0.1590 0.002 .",result
shows that the ContextAtt model performs best over all relation types .,result
One can also see that the ContextSum does n't universally outperforms the LSTM - baseline .,result
It demonstrates again that using attention is crucial to extract relevant information from the context relations .,result
"On the relation - specific results we observe that the context - enabled model demonstrates the most improvement on precision and seems to be especially useful for taxonomy relations ( see SUBCLASS OF , PART OF ) .",result
All models were trained using the Adam optimizer with categorical crossentropy as the loss function .,hyperparameter
We use an early stopping criterion on the validation data to determine the number of training epochs .,hyperparameter
"The learning rate is fixed to 0.01 and the rest of the optimization parameters are set as recommended in : ? 1 = 0.9 , ? 2 = 0.999 , ? = 1e ? 08 . The training is performed in batches of 128 instances .",hyperparameter
We apply Dropout on the penultimate layer as well as on the embeddings layer with a probability of 0.5 .,hyperparameter
We choose the size of the layers ( RNN layer size o = 256 ) and entity marker embeddings ( d = 3 ) with a random search on the validation set .,hyperparameter
We present a novel architecture that considers other relations in the sentence as a context for predicting the label of the target relation .,model
Our architecture uses an LSTM - based encoder to jointly learn representations for all relations in a single sentence .,model
The representation of the target relation and representations of the context relations are combined to make the final prediction .,model
Context - Aware Representations for Knowledge Base Relation Extraction,research-problem
We demonstrate that for sentence - level relation extraction it is beneficial to consider other relations in the sentential context while predicting the target relation .,research-problem
The main goal of relation extraction is to determine a type of relation between two target entities that appear together in a text .,research-problem
"In this paper , we consider the sentential relation extraction task : to each occurrence of the target entity pair e 1 , e 2 in some sentence s one has to assign a relation type r from a given set R. A triple e 1 , r , e 2 is called a relation instance and we refer to the relation of the target entity pair as target relation .",research-problem
We start with baseline model which we take as a simple encoder and decoder network with only the local loss ( ED - Local ) .,baseline
Further we have experimented with encoder - decoder and a discriminator network with only global loss ( EDD - Global ) to distinguish the ground truth paraphrase with the predicted one .,baseline
Another variation of our model is used both the global and local loss ( EDD - LG ) .,baseline
"Finally , we make the discriminator share weights with the encoder and train this network with both the losses ( EDD - LG ( shared ) ) .",baseline
Our model consists of a sequential encoder - decoder that is further trained using a pairwise discriminator .,model
The encoder - decoder architecture has been widely used for machine translation and machine comprehension tasks .,model
"In general , the model ensures a ' local ' loss that is incurred for each recurrent unit cell .",model
"To ensure that the whole sentence is correctly encoded , we make further use of a pair - wise discriminator that encodes the whole sentence and obtains an embedding for it .",model
We further ensure that this is close to the desired ground - truth embeddings while being far from other ( sentences in the corpus ) embeddings .,model
This model thus provides a ' global ' loss that ensures the sentence embedding as a whole is close to other semantically related sentence embeddings .,model
Learning Semantic Sentence Embeddings using Pair- wise Discriminator,research-problem
"In this paper , we propose a method for obtaining sentence - level embeddings .",research-problem
"Among the ablations , the proposed EDD - LG ( shared ) method works way better than the other variants in terms of BLEU and METEOR metrics by achieving an improvement of 8 % and 6 % in the scores respectively over the baseline method for 50 K dataset and an improvement of 10 % and 7 % in the scores respectively for 100 K dataset .",ablation-analysi
Residual LSTM is also the current state - of - the - art on the MSCOCO dataset .,baseline
"For the Quora dataset , there were no known baseline results , so we compare our model with ( 1 ) standard VAE model i.e. , the unsupervised version , and ( 2 ) a "" supervised "" variant VAE - S of the unsupervised model .",baseline
"In this paper , we present a deep generative framework for automatically generating paraphrases , given a sentence .",model
"To address this limitation , we present a mechanism to condition our VAE model on the original sentence for which we wish to generate the paraphrases .",model
"Our framework combines the power of sequenceto - sequence models , specifically the long short - term memory ( LSTM ) , and deep generative models , specifically the variational autoencoder ( VAE ) , to develop a novel , end - to - end deep learning architecture for the task of paraphrase generation .",model
"Unlike these methods where number of classes are finite , and do not require any intermediate representation , our method conditions both the sides ( i.e. encoder and decoder ) of VAE on the intermediate representation of the input question obtained through LSTM .",model
"In contrast , our deep generative model enjoys a simple , modular architecture , and can generate not just a single but multiple , semantically sensible , paraphrases for any given sentence .",model
"This is in contrast to the proposed method where all variations will be of relatively better quality since they are the top beam - search result , generated based on different z sampled from a latent space .",model
A Deep Generative Framework for Paraphrase Generation,research-problem
"In this paper , we address the problem of generating paraphrases automatically .",research-problem
"Firstly , our network is densely connected , connecting every layer of P with every layer of Q .",model
"The propagated features are collectively passed into prediction layers , which effectively connect shallow layers to deeper layers .",model
"To this end , we propose efficient Bidirectional Attention Connectors ( BAC ) as a base building block to connect two sequences at arbitrary layers .",model
"Overall , we propose DECAPROP ( Densely Connected Attention Propagation ) , a novel architecture for reading comprehension .",model
"The key idea is to compress the attention outputs so that they can be small enough to propagate , yet enabling a connection between two sequences .",model
Densely Connected Attention Propagation for Reading Comprehension,research-problem
"We propose DECAPROP ( Densely Connected Attention Propagation ) , a new densely connected neural architecture for reading comprehension ( RC ) .",research-problem
We conduct extensive experiments on four challenging RC benchmarks .,research-problem
Our model is implemented in Tensorflow .,experimental-setup
"The sequence lengths are capped at 800/700/1500/1100 for News QA , Search QA , Quasar - T and Narrative QA respectively .",experimental-setup
"Batch size is tuned amongst { 16 , 32 , 64 } .",experimental-setup
"Dropout rate is tuned amongst { 0.1 , 0.2 , 0.3 } and applied to all RNN and fully - connected layers .",experimental-setup
The size of the character embeddings is set to 8 and the character RNN is set to the same as the word - level RNN encoders .,experimental-setup
The maximum characters per word is set to 16 .,experimental-setup
The number of layers in DECAENC is set to 3 and the number of factors in the factorization kernel is set to 64 .,experimental-setup
"We use Adadelta with ? = 0.5 for News QA , Adam with ? = 0.001 for Search QA , Quasar - T and Narrative QA .",experimental-setup
We use the CUDNN implementation of the RNN encoder .,experimental-setup
We use a learning rate decay factor of 2 and patience of 3 epochs whenever the EM ( or ROUGE - L ) score on the development set does not increase .,experimental-setup
"The choice of the RNN encoder is tuned between GRU and LSTM cells and the hidden size is tuned amongst { 32 , 50 , 64 , 75 } .",experimental-setup
We apply variational dropout in - between RNN layers .,experimental-setup
We initialize the word embeddings with 300D Glo Ve embeddings and are fixed during training .,experimental-setup
We conduct an ablation study on the New s QA development set .,ablation-analysi
"Finally , in ( 8 - 9 ) , we varied the FM with linear and nonlinear feed - forward layers . From ( 1 ) , we observe a significant gap in performance between DECAPROP and R - NET .",ablation-analysi
We observe that the superiority of DECAPROP over R - NET is consistent and relatively stable .,ablation-analysi
"Overall , the key insight is that all model components are crucial to DECAPROP .",ablation-analysi
"Notably , the DECAENC seems to contribute the most to the over all performance .",ablation-analysi
"To this end , we propose a new compositional encoder that can either be used in place of standard RNN encoders or serve as a new module that is complementary to existing neural architectures .",model
Our proposed encoder leverages dilated compositions to model relationships across multiple granularities .,model
"The output of the dilated composition mechanism acts as gating functions , which are then used to learn compositional representations of the input sequence .",model
"That is , for a given word in the target sequence , our encoder exploits both long - term ( far ) and short - term ( near ) information to decide how much information to retain for it .",model
Multi - Granular Sequence Encoding via Dilated Compositional Units for Reading Comprehension,research-problem
This paper presents a new compositional encoder for reading comprehension ( RC ) .,research-problem
"We conduct experiments on three RC datasets , showing that our proposed encoder demonstrates very promising results both as a standalone encoder as well as a complementary building block .",research-problem
We implement all models in TensorFlow .,experimental-setup
Word embeddings are initialized with 300d Glo Ve vectors and are not fine - tuned during training .,experimental-setup
"Dropout rate is tuned amongst { 0.1 , 0.2 , 0.3 } on all layers including the embedding layer .",experimental-setup
The batch size is set to 64/256/32 accordingly .,experimental-setup
The maximum sequence lengths are 500/200/1100 respectively .,experimental-setup
All models are trained and all runtime benchmarks are based on a TitanXP GPU .,experimental-setup
"We adopt the Adam optimizer ( Kingma and Ba , 2014 ) with a learning rate of 0.0003/ 0.001/0.001 for RACE / SearchQA / Narrative QA respectively .",experimental-setup
"For Narrative QA , we use the Rouge - L score to find the best approximate answer relative to the human written answer for training the span model .",experimental-setup
"The results showed that R 3 achieved F1 56.0 , EM 50.9 on Wiki domain and F1 68.5 , EM 63.0 on Web domain , which is competitive to the state - of - the - arts .",result
"From the results , we can clearly see that the full re-ranker , the combination of different re-rankers , significantly outperforms the previous best performance by a large margin , especially on Quasar - T and Search QA .",result
"In addition , we see that our coverage - based re-ranker achieves consistently good performance on the three datasets , even though its performance is marginally lower than the strength - based re-ranker on the Search QA dataset .",result
"Moreover , our model is much better than the human performance on the Search QA dataset .",result
"Our baseline models 9 include the following : GA , a reading comprehension model with gated - attention ; BiDAF ) , a RC model with bidirectional attention flow ; AQA ) , a reinforced system learning to aggregate the answers generated by the re-written questions ; R 3 ) , a reinforced model making use of a ranker for selecting passages to train the RC model .",baseline
"We first use a pre-trained R 3 model , which gets the state - of - the - art performance on the three public datasets we consider , to generate the top 50 candidate spans for the training , development and test datasets , and we use them for further ranking .",hyperparameter
"For the coverage - based re-ranker , we use Adam to optimize the model .",hyperparameter
We set all the words beyond Glove as zero vectors .,hyperparameter
"We set l to 300 , batch size to 30 , learning rate to 0.002 .",hyperparameter
"We tune the dropout probability from 0 to 0.5 and the number of candidate answers for re-ranking ( K ) in [ 3 , 5 , 10 ] 11 .",hyperparameter
EVIDENCE AGGREGATION FOR ANSWER RE - RANKING IN OPEN - DOMAIN QUESTION ANSWERING,research-problem
Open-domain question answering ( QA ) aims to answer questions from a broad range of domains by effectively marshalling evidence from large open - domain knowledge sources .,research-problem
Recent work on open - domain QA has focused on using unstructured text retrieved from the web to build machine comprehension models .,research-problem
Our code will be released under https://github.com/shuohangwang/mprc.,code
"In this paper , we describe a semantic parsing approach to the problem of KB QA .",approach
Semantic parses can be deterministically converted to a query to extract the answers from the KB .,approach
"That is , for each input question , we construct an explicit structural semantic parse ( semantic graph ) , as in .",approach
"In particular , we adapt Gated Graph Neural Networks ( GGNNs ) , described in , to process and score semantic parses .",approach
Modeling Semantics with Gated Graph Neural Networks for Knowledge Base Question Answering,research-problem
Knowledge base question answering ( QA ) is an important natural language processing problem .,research-problem
QA requires precise modeling of the question semantics through the entities and relations available in the KB in order to retrieve the correct answer .,research-problem
"In this paper , we describe a semantic parsing approach to the problem of KB QA .",research-problem
https://github.com/UKPLab/coling2018-graph-neural-networks-question-answering.,code
Focal Visual - Text Attention for Visual Question Answering,research-problem
"Visual question answering ( VQA ) is a successful direction utilizing both computer vision and natural language processing techniques to solve an interesting problem : given a pair of image and a question ( in natural language ) , the goal is to learn an inference model that can the answer questions according to cues discovered from the image .",research-problem
"Extending from VQA on a single image , this paper considers the following problem :",research-problem
"BIDAF ( ensemble ) achieves an EM score of 73.3 and an F 1 score of 81.1 , outperforming all previous approaches .",result
BI - DIRECTIONAL ATTENTION FLOW FOR MACHINE COMPREHENSION,research-problem
"Machine comprehension ( MC ) , answering a query about a given context paragraph , requires modeling complex interactions between the context and the query .",research-problem
"Recently , attention mechanisms have been successfully extended to MC .",research-problem
The tasks of machine comprehension ( MC ) and question answering ( QA ) have gained significant popularity over the past few years within the natural language processing and computer vision communities .,research-problem
Each paragraph and question are tokenized by a regular - expression - based word tokenizer ( PTB Tokenizer ) and fed into the model .,experimental-setup
The hidden state size ( d ) of the model is 100 .,experimental-setup
The model has about 2.6 million parameters .,experimental-setup
"A dropout ) rate of 0.2 is used for the CNN , all LSTM layers , and the linear transformation before the softmax for the answers .",experimental-setup
The training process takes roughly 20 hours on a single Titan X GPU .,experimental-setup
"We use 100 1D filters for CNN char embedding , each with a width of 5 .",experimental-setup
"We use the AdaDelta ( Zeiler , 2012 ) optimizer , with a minibatch size of 60 and an initial learning rate of 0.5 , for 12 epochs .",experimental-setup
"During training , the moving averages of all weights of the model are maintained with the exponential decay rate of 0.999 .",experimental-setup
"At test time , the moving averages instead of the raw weights are used .",experimental-setup
Both char - level and word - level embeddings contribute towards the model 's performance .,ablation-analysi
C2Q attention proves to be critical with a drop of more than 10 points on both metrics .,ablation-analysi
"Despite being a simpler attention mechanism , our proposed static attention outperforms the dynamically computed attention by more than 3 points .",ablation-analysi
"At the word embedding layer , query words such as When , Where and Who are not well aligned to possible answers in the context , but this dramatically changes in the contextual embedding layer which has access to context from surrounding words and is just 1 layer below the attention layer .",ablation-analysi
The Skip - Thought encoder for the model encodes sentences with length less than 30 words using 2400 GRU units with word vector dimensionality of 620 to produce 4800 - dimensional combineskip vectors . .,hyperparameter
"The combine - skip vectors , with the first 2400 dimensions being uni-skip model and the last 2400 bi-skip model , are used as they have been found to be the best performing in the experiments",hyperparameter
Generating Text through Adversarial Training using Skip - Thought Vectors,research-problem
Attempts have been made for utilizing GANs with word embeddings for text generation .,research-problem
Numerous efforts have been made in the field of natural language text generation for tasks such as sentiment analysis and machine translation .,research-problem
This work pro-Code available at : https://github.com/enigmaeth/skip-thought-gan poses an approach for text generation using Generative Adversarial Networks with Skip - Thought vectors .,code
"For dialogue generation , we set the maximum length to 15 words for each generated sentence .",hyperparameter
"Based on the performance on the validation set , we set the hidden size to 512 , embedding size to 64 and vocabulary size to 40 K for baseline models and the proposed model .",hyperparameter
"The parameters are updated by the Adam algorithm ( Kingma and Ba , 2014 ) and initialized by sampling from the uniform distribution ( [? 0.1 , 0.1 ] ) .",hyperparameter
The initial learning rate is 0.002 and the model is trained in minibatches with a batch size of 256 . ? 1 and ?,hyperparameter
"To address this problem , we propose a novel Auto - Encoder Matching model to learn utterance - level dependency .",model
"First , motivated by , we use two auto- encoders to learn the semantic representations of inputs and responses in an unsupervised style .",model
"Second , given the utterance - level representations , the mapping module is taught to learn the utterance - level dependency .",model
An Auto - Encoder Matching Model for Learning Utterance - Level Semantic Dependency in Dialogue Generation,research-problem
"Automatic dialogue generation task is of great importance to many applications , ranging from open - domain chatbots to goal - oriented technical support agents .",research-problem
"However , conversation generation is a much more complex and flexible task as there are less "" word - to - words "" relations between inputs and responses .",research-problem
"We propose the use of a dilated CNN as a decoder in VAE , inspired by the recent success of using CNNs for audio , image and language modeling ( van den .",model
"In contrast with prior work where extremely large CNNs are used , we exploit the dilated CNN for its flexibility in varying the amount of conditioning context .",model
Improved Variational Autoencoders for Text Modeling using Dilated Convolutions,research-problem
"Recent work on generative text modeling has found that variational autoencoders ( VAE ) with LSTM decoders perform worse than simpler LSTM language models ( Bowman et al. , 2015 ) .",research-problem
We can see that SCNN - VAE - Semi has the best classification accuracy of 65.5 .,ablation-analysi
"On the other hand , LCNN - VAE - Semi has the best NLL result .",ablation-analysi
"Since the evaluation metric is fundamentally instructive , we can see the impact of SeqGAN , which outperforms other baselines significantly .",result
"A significance T - test on the NLL oracle score distribution of the generated sequences from the compared models is also performed , which demonstrates the significant improvement of SeqGAN over all compared models .",result
"Additionally , SeqGAN outperforms PG - BLEU , which means the discriminative signal in GAN is more general and effective than a predefined score ( e.g. BLEU ) to guide the generative policy to capture the underlying distribution of the sequence data .        ",result
"After about 150 training epochs , both the maximum likelihood estimation and the schedule sampling methods converge to a relatively high NLL oracle score , whereas SeqGAN can improve the limit of the generator with the same structure as the baselines significantly .",result
This indicates the prospect of applying adversarial training strategies to discrete sequence generative models to breakthrough the limitations of MLE .,result
The first model is a random token generation .,baseline
The second one is the MLE trained LSTM G ? .,baseline
The third one is scheduled sampling .,baseline
The fourth one is the Policy Gradient with BLEU ( PG - BLEU ) .,baseline
"To setup the synthetic data experiments , we first initialize the parameters of an LSTM network following the normal distribution N ( 0 , 1 ) as the oracle describing the real data distribution G oracle ( x t |x 1 , . . . , x t?1 ) .",hyperparameter
"In SeqGAN algorithm , the training set for the discriminator is comprised by the generated examples with the label 0 and the instances from S with the label",hyperparameter
"In the scheduled sampling , the training process gradually changes from a fully guided scheme feeding the true previous tokens into LSTM , towards a less guided scheme which mostly feeds the LSTM with its generated tokens .",hyperparameter
"For different tasks , one should design specific structure for the convolutional layer and in our synthetic data experiments , the kernel size is from 1 to T and the number of each kernel size is between 100 to 200 3 .",hyperparameter
Dropout ) and L2 regularization are used to avoid over-fitting .,hyperparameter
A curriculum rate ? is used to control the probability of replacing the true tokens with the generated ones .,hyperparameter
"In this paper , to address the above two issues , we follow ) and consider the sequence generation procedure as a sequential decision making process .",model
The generative model is treated as an agent of reinforcement learning ( RL ) ; the state is the generated tokens so far and the action is the next token to be generated .,model
"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a discriminator to evaluate the sequence and feedback the evaluation to guide the learning of the generative model .",model
"To solve the problem that the gradient can not pass back to the generative model when the output is discrete , we regard the generative model as a stochastic parametrized policy .",model
"In our policy gradient , we employ Monte Carlo ( MC ) search to approximate the state - action value .",model
"We directly train the policy ( generative model ) via policy gradient , which naturally avoids the differentiation difficulty for discrete data in a conventional GAN .",model
"As a new way of training generative models , Generative Adversarial Net ( GAN ) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real - valued data .",research-problem
"However , it has limitations when the goal is for generating sequences of discrete tokens .",research-problem
Generating sequential synthetic data that mimics the real one is an important problem in unsupervised learning .,research-problem
"In this paper , we propose a new algorithmic framework called Leak GAN to address both the non-informativeness and the sparsity issues .",approach
LeakGAN is a new way of providing richer information from the discriminator to the generator by borrowing the recent advances in hierarchical reinforcement learning .,approach
The MANAGER is along shortterm memory network ( LSTM ) and serves as a mediator .,approach
"In each step , it receives generator D 's high - level feature representation , e.g. , the feature map of the CNN , and uses it to form the guiding goal for the WORKER module in that timestep . ",approach
"As illustrated in , we specifically introduce a hierarchical generator G , which consists of a high - level MANAGER module and a low - level WORKER module .",approach
"Next , given the goal embedding produced by the MAN - AGER , the WORKER first encodes current generated words with another LSTM , then combines the output of the LSTM and the goal embedding to take a final action at current state .",approach
Long Text Generation via Adversarial Training with Leaked Information,research-problem
"Automatically generating coherent and semantically meaningful text has many applications in machine translation , dialogue systems , image captioning , etc .",research-problem
"In this paper , we propose a novel adversarial learning framework , RankGAN , for generating highquality language descriptions .",model
"As opposed to performing a binary classification task , we propose to train the ranker to rank the machine - written sentences lower than human - written sentences with respect to a reference sentence which is human-written .",model
RankGAN learns the model from the relative ranking information between the machine - written and the human - written sentences in an adversarial framework .,model
"In the proposed RankGAN , we relax the training of the discriminator to a learning - to - rank optimization problem .",model
"Specifically , the proposed new adversarial network consists of two neural network models , a generator and a ranker .",model
"Accordingly , we train the generator to synthesize sentences which confuse the ranker so that machine - written sentences are ranked higher than human - written sentences in regard to the reference .",model
"During learning , we adopt the policy gradient technique to overcome the non-differentiable problem .",model
"Consequently , by viewing a set of data samples collectively and evaluating their quality through relative ranking , the discriminator is able to make better assessment of the quality of the samples , which in turn helps the generator to learn better .",model
Our method is suitable for language learning in comparison to conventional GANs .,model
Adversarial Ranking for Language Generation,research-problem
"Motivated by the success of transfer learning , we apply BERT to negation detection and scope resolution .",approach
"We explore the set of design choices involved , and experiment on all 3 public datasets available : the BioScope Corpus ( Abstracts and Full Papers ) , the Sherlock Dataset and the SFU Review Corpus .",approach
NegBERT : A Transfer Learning Approach for Negation Detection and Scope Resolution,research-problem
We use Google 's BERT as the base model to generate contextual embeddings for the sentence .,experimental-setup
"We then use a vector of dimension R H x N_C to compute scores per token , for the classification task at hand .",experimental-setup
"We use early stopping on dev data for 6 epochs as tolerance and F 1 score as the early stopping metric , use the Adam optimizer with an initial learning rate of 3 e - 5 , and the Categorical Cross Entropy Loss with class weights as described above to avoid training on the padded label outputs .",experimental-setup
"For all other corpuses , we use a default 70 - 15 - 15 split for the train - dev - test data .",experimental-setup
The input to the BERT model is a sequence of tokenized and encoded tokens of a sentence .,experimental-setup
"BERT outputs a vector of size R H per token of the input , which we feed to a common classification layer of dimen-sion R Hx5 for cue detection and R Hx2 for scope resolution .",experimental-setup
"We perform cue detection and scope resolution for all 3 datasets , and train on 1 and test on all datasets .",experimental-setup
"We trained the models on free GPUs available via Google Colaboratory , the training scripts are publicly available .",experimental-setup
"( 1 ) As shown in , our Evaluator - SLMbased method yields a large improvement over the baselines , demonstrating that the language - modelbased evaluator is effective as a post-hoc grammar checker for the compressed sentences .",result
"( 3 ) As for Google news dataset , LSTMs ( LSTM + pos+dep ) ( & 3 ) is a relatively strong baseline , suggesting that incorporating dependency relations and part - of - speech tags may help model learn the syntactic relations and thus make a better prediction .",result
"For Gigaword dataset with 1.02 million instances , the perplexity of the language model is 20.3 , while for the Google news dataset with 0.2 million instances , the perplexity is 76.5 .",result
"When further applying Evaluator - SLM , only a tiny improvement is observed ( &3 vs & 4 ) , not comparable to the improvement between # 3 and # 5 .",result
"The results shows that small improvements are observed on two datasets ( # 4 vs # 5 ; & 4 vs & 5 ) , suggesting that incorporating syntactic knowledge may help evaluator to encourage more unseen but reasonable word collocations .",result
We choose several strong baselines ; the first one is the dependency - tree - based method that considers the sentence compression task as an optimization problem by using integer linear programming 5 .,baseline
The second method is the long short - term memory networks ( LSTMs ) which showed strong promise in sentence compression by .,baseline
"The embedding size for word , part - of - speech tag , and the dependency relation is 128 .",hyperparameter
"The mini - batch size was chosen from [ 5 , 50 , 100 ] .",hyperparameter
"Vocabulary size was 50,000 .",hyperparameter
"The learning rate for neural language model is 2.5 e - 4 , and 1e - 05 for the policy network .",hyperparameter
We employed the vanilla RNN with a hidden size of 512 for both the policy network and neural language model .,hyperparameter
"For policy learning , we used the REINFORCE algorithm to update the parameters of the policy network and find an policy that maximizes the reward .",hyperparameter
"To answer the above questions , a syntax - based neural language model is trained on large - scale datasets as a readability evaluator .",model
The neural language model is supposed to learn the correct word collocations in terms of both syntax and semantics .,model
"The policy network performs either RETAIN or REMOVE action to form a compression , and receives a reward ( e.g. , readability score ) to update the network .",model
"Subsequently , we formulate the deletionbased sentence compression as a series of trialand - error deletion operations through a reinforcement learning framework .",model
A Language Model based Evaluator for Sentence Compression,research-problem
"We herein present a language - modelbased evaluator for deletion - based sentence compression , and viewed this task as a series of deletion - and - evaluation operations using the evaluator .",research-problem
6 https://github.com/code4conference/code4sc,code
We can see that indeed this abstractive method performed poorly in cross - domain settings .,result
"( 2 ) In the in - domain setting , with the same amount of training data ( 8,000 ) , our BiLSTM method with syntactic features ( BiLSTM + SynFeat and BiL - STM + SynFeat + ILP ) performs similarly to or better than the LSTM + method proposed by , in terms of both F1 and accuracy .",result
"( 4 ) In the out - of - domain setting , our BiLSTM + SynFeat and BiLSTM+SynFeat+ILP methods clearly outperform the LSTM and LSTM + methods .",result
This shows that our method is comparable to the LSTM + method in the in - domain setting .,result
"Therefore , our method works reasonably well for both in - domain and out - ofdomain data .",result
( 5 ) The Traditional ILP method also works better than the LSTM and LSTM + methods in the out - of - domain setting .,result
But the Traditional ILP method performs worse in the in - domain setting than both the LSTM and LSTM + methods and our methods .,result
"We also notice that on Google News , adding the ILP layer decreased the sentence compression performance .",result
"We can see that in the in - domain setting , our method does not have any advantage over the LSTM + method .",result
"But in the cross - domain setting , our method that uses ILP to impose syntax - based constraints clearly performs better than LSTM + when the amount of training data is relatively small .",result
"In the experiments , our model was trained using the Adam algorithm with a learning rate initialized at 0.001 .",hyperparameter
The dimension of the hidden layers of bi - LSTM is 100 .,hyperparameter
Word embeddings are initialized from GloVe 100 dimensional pre-trained embeddings .,hyperparameter
POS and dependency embeddings are randomly initialized with 40 - dimensional vectors .,hyperparameter
The embeddings are all updated during training .,hyperparameter
Dropping probability for dropout layers between stacked LSTM layers is 0.5 .,hyperparameter
The batch size is set as 30 .,hyperparameter
We utilize an open source ILP solver 4 in our method .,hyperparameter
"To this end , we extend the deletionbased LSTM model for sentence compression by .",model
"Specifically , we propose two major changes to the model by : We explicitly introduce POS embeddings and dependency relation embeddings into the neural network model .",model
"( 2 ) Inspired by a previous method , we formulate the final predictions as an Integer Linear Programming problem to incorporate constraints based on syntactic relations between words and expected lengths of the compressed sentences .",model
"In addition to the two major changes above , we also use bi-directional LSTM to include contextual information from both directions into the model .",model
Can Syntax Help ? Improving an LSTM - based Sentence Compression Model for New Domains,research-problem
"We observe that across all three datasets , including all three annotations of BROADCAST , gaze features lead to improvements over our baseline 3 - layer bi - LSTM .",result
"Also , CASCADED - LSTM is consistently better than MULTITASK - LSTM . : Results ( F1 ) .",result
"For all three datasets , the inclusion of gaze measures ( first pass duration ( FP ) and regression duration ( Regr. ) ) leads to improvements over the baseline .",result
"With the harder datasets , the impact of the gaze information becomes stronger , consistently favouring the cascaded architecture , and with improvements using both first pass duration and regression duration , the late measure associated with interpretation of content .",result
Both the baseline and our systems are three - layer bi - LSTM models trained for 30 iterations with pretrained ( SENNA ) embeddings .,hyperparameter
"The input and hidden layers are 50 dimensions , and at the output layer we predict sequences of two labels , indicating whether to delete the labeled word or not .",hyperparameter
We go beyond this by suggesting that eye - tracking recordings can be used to induce better models for sentence compression for text simplification .,model
"Specifically , we show how to use existing eye - tracking recordings to improve the induction of Long Short - Term Memory models ( LSTMs ) for sentence compression .",model
Our proposed model does not require that the gaze data and the compression data come from the same source .,model
"While not explored here , an intriguing potential of this work is in deriving sentence simplification models that are personalized for individual users , based on their reading behavior .",model
"Indeed , in this work we use gaze data from readers of the Dundee Corpus to improve sentence compression results on several datasets .",model
Improving sentence compression by learning to predict gaze,research-problem
"There is a significant difference in performance of the MIRA baseline and the LSTM models , both in terms of F1 - score and in accuracy .",result
More than 30 % of golden compressions could be fully regenerated by the LSTM systems which is in sharp contrast with the 20 % of MIRA .,result
"The differences in F- score between the three versions of LSTM are not significant , all scores are close to 0.81 .",result
"In particular , we will present a model which benefits from the very recent advances in deep learning and uses word embeddings and Long Short Term Memory models ( LSTMs ) to output surprisingly readable and informative compressions .",model
"Trained on a corpus of less than two million automatically extracted parallel sentences and using a standard tool to obtain word embeddings , in its best and most simple configuration it achieves 4.5 points out of 5 in readability and 3.8 points in informativeness in an extensive evaluation with human judges .",model
Sentence Compression by Deletion with LSTMs,research-problem
"We present an LSTM approach to deletion - based sentence compression where the task is to translate a sentence into a sequence of zeros and ones , corresponding to token deletion decisions .",research-problem
Sentence compression is a standard NLP task where the goal is to generate a shorter paraphrase of a sentence .,research-problem
"All the other methods are based on LSTM models and better than the Majority method , showing that LSTM has potentials in automatically generating representations and can all bring performance improvement for sentiment classification .",result
"The LSTM method gets the worst performance of all the neural network baseline methods , because it treats targets equally with other context words and does not make full use of the target information .",result
"TD - LSTM outperforms LSTM over 1 percent and 2 percent on the Restaurant and Laptop category respectively , since it develops from the standard LSTM and processes the left and right contexts with targets .",result
"Further , both AE - LSTM and ATAE - LSTM stably exceed the TD - LSTM method because of the introduction of attention mechanism .",result
"Compared with AE - LSTM , ATAE - LSTM especially enhance the interaction between the context words and target and thus has a better performance than AE - LSTM .",result
"The more attentions are paid to targets , the higher accuracy the system achieves .",result
"For AE - LSTM and ATAE - LSTM , they capture important information in the context with the supervision of target and generate more reasonable representations for aspect - level sentiment classification .",result
"We can also see that AE - LSTM and ATAE - LSTM further emphasize the modeling of targets via the addition of the aspect embedding , which is also the reason of performance improvement .",result
We can see that IAN achieves the best performance among all baselines .,result
"Compared with ATAE - LSTM model , IAN improves the performance about 1.4 % and 3.2 % on the Restaurant and Laptop categories respectively .",result
"Majority is a basic baseline method , which assigns the largest sentiment polarity in the training set to each sample in the test set .",baseline
LSTM only uses one LSTM network to model the context and get the hidden state of each word .,baseline
TD - LSTM adopts two long short - term memory ( LSTM ) networks to model the left context with target and the right context with target respectively .,baseline
AE - LSTM represents targets with aspect embeddings .,baseline
ATAE - LSTM is developed based on AE - LSTM .,baseline
"In our experiments , all word embeddings from context and target are initialized by GloVe 2 , and all out - of - vocabulary words are initialized by sampling from the uniform distribution U ( ?0.1 , 0.1 ) .",hyperparameter
"All weight matrices are given their initial values by sampling from uniform distribution U ( ?0.1 , 0.1 ) , and all biases are set to zeros .",hyperparameter
"The dimensions of word embeddings , attention vectors and LSTM hidden states are set to 300 as in .",hyperparameter
"The coefficient of L 2 normalization in the objective function is set to 10 ?5 , and the dropout rate is set to 0.5 .",hyperparameter
"To train the parameters of IAN , we employ the Momentum , which adds a fraction ? of the update vector in the prior step to the current update vector .",hyperparameter
"Based on the two points analyzed above , we propose an interactive attention network ( IAN ) model which is based on long - short term memory networks ( LSTM ) and attention mechanism .",model
IAN utilizes the attention mechanism associated with a target to get important information from the context and compute context representation for sentiment classification .,model
"Further , IAN makes use of the interactive information from context to supervise the modeling of the target which is helpful to judging sentiment .",model
"Finally , with both target representation and context representation concatenated , IAN predicts the sentiment polarity for the target within its context .",model
Interactive Attention Networks for Aspect - Level Sentiment Classification,research-problem
Previous approaches have realized the importance of targets in sentiment classification and developed various methods with the goal of precisely modeling their contexts via generating target - specific representations .,research-problem
"1 ) Feature - based SVM is still a strong baseline , our best model achieves competitive results on D1 and D2 without relying on so many manually - designed features and external resources .",result
4 ) The integrated full model over all achieves the best performance compared to using only one of the two proposed approaches .,result
"5 ) The proposed target representation is more helpful on restaurant domain ( D1 , D3 , and D4 ) than laptop domain ( D2 ) .",result
"2 ) Compared with all other neural baselines , our full model achieves statistically significant improvements ( p < 0.05 ) on both accuracies and macro - F1 scores for D1 , D3 , D4 .",result
"3 ) Compared with LSTM + ATT , all three settings of our model are able to achieve statistically significant improvements ( p < 0.05 ) on all datasets .",result
Effective Attention Modeling for Aspect - Level Sentiment Classification,research-problem
Aspect - level sentiment classification is an important task in fine - grained sentiment analysis .,research-problem
The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language .,dataset
"The corpus is based on the dataset introduced by and consists of 11,855 single sentences extracted from movie reviews .",dataset
"It was parsed with the Stanford parser and includes a total of 215,154 unique phrases from those parse trees , each annotated by 3 human judges .",dataset
This new dataset allows us to analyze the intricacies of sentiment and to capture complex linguistic phenomena .,dataset
The granularity and size of this dataset will enable the community to train compositional models that are based on supervised and structured machine learning techniques .,dataset
showed that a fine grained classification into 5 classes is a reasonable approximation to capture most of the data variation .,result
"The RNTN gets the highest performance , followed by the MV - RNN and RNN .",result
"The recursive models work very well on shorter phrases , where negation and composition are important , while bag of features baselines perform well only with longer sentences .",result
The combination of the new sentiment treebank and the RNTN pushes the state of the art on short phrases up to 85.4 % .,result
"We compare to commonly used methods that use bag of words features with Naive Bayes and SVMs , as well as Naive Bayes with bag of bigram features .",baseline
We also compare to a model that averages neural word vectors and ignores word order ( VecAvg ) .,baseline
Optimal performance for all models was achieved at word vector sizes between 25 and 35 dimensions and batch sizes between 20 and 30 .,hyperparameter
The RNTN would usually achieve its best performance on the dev set after training for 3 - 5 hours .,hyperparameter
"The sentences in the treebank were split into a train ( 8544 ) , dev ( 1101 ) and test splits ( 2210 ) and these splits are made available with the data release .",hyperparameter
We use f = tanh in all experiments .,hyperparameter
"In order to capture the compositional effects with higher accuracy , we propose a new model called the Recursive Neural Tensor Network ( RNTN ) .",model
Recursive Neural Tensor Networks take as input phrases of any length .,model
They represent a phrase through word vectors and a parse tree and then compute vectors for higher nodes in the tree using the same tensor - based composition function .,model
Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank,research-problem
Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition .,research-problem
"The RNTN has the highest reversal accuracy , showing its ability to structurally learn negation of positive sentences .",ablation-analysi
shows a typical case in which sentiment was made more positive by switching the main class from negative to neutral even though both not and dull were negative .,ablation-analysi
Therefore we can conclude that the RNTN is best able to identify the effect of negations upon both positive and negative sentiment sentences . :,ablation-analysi
memnet is an end - toend memory network .,baseline
cLSTM 4 classifies utterances using neighboring utterances ( of same speaker ) as context .,baseline
"TFN 5 models intra-and intermodality dynamics by explicitly aggregating uni - , bi- and trimodal interactions .",baseline
"MFN performs multi-view learning by using Delta - memory Attention Network , a fusion mechanism to learn cross - view interactions .",baseline
CMN models separate contexts for both speaker and listener to an utterance .,baseline
"We propose Interactive COnversational memory Network ( ICON ) , a multimodal network for identifying emotions in utterance - videos .",model
"First , it extracts multimodal features from all utterancevideos .",model
"Next , given a test utterance to be classified , ICON considers the preceding utterances of both speakers falling within a context - window and models their self - emotional influences using local gated recurrent units .",model
"Furthermore , to incorporate inter -speaker influences , a global representation is generated using a GRU that intakes output of the local GRUs .",model
"For each instance in the context - window , the output of this global GRU is stored as a memory cell .",model
These memories are then subjected to multiple read / write cycles that include attention mechanism for generating contextual summaries of the conversational history .,model
"At each iteration , the representation of the test utterance is improved with this summary representation and finally used for prediction .",model
ICON : Interactive Conversational Memory Network for Multimodal Emotion Detection,research-problem
Emotion recognition in conversations is crucial for building empathetic machines .,research-problem
"Analyzing emotional dynamics in conversations , however , poses complex challenges .",research-problem
20 % of the training set is used as validation set for hyper - parameter tuning .,experimental-setup
Termination of the training - phase is decided by early - stopping with a patience of 10 d = 100 dv = 512 dem = 100 K = 40 R = 3 epochs .,experimental-setup
The network is subjected to regularization in the form of Dropout and Gradient - clipping for a norm of 40 .,experimental-setup
"Finally , the best hyper - parameters are decided using a gridsearch .",experimental-setup
"We use the Adam optimizer ( Kingma and Ba , 2014 ) for training the parameters starting with an initial learning rate of 0.001 .",experimental-setup
"For multimodal feature extraction , we explore different designs for the employed CNNs .",experimental-setup
"For text , we find the single layer CNN to perform at par with deeper variants .",experimental-setup
"For visual features , however , a deeper CNN provides better representations .",experimental-setup
We also find that contextually conditioned features perform better than context - less features .,experimental-setup
"From the , the ASVAET is able to improve supervised performance consistently for all classifiers .",result
The ASVAET outperforms the compared semisupervised methods evidently .,result
The TNet - AS outperforms the other three models .,result
"For the MemNet , the test accuracy can be improved by about 2 % by the TSSVAE , and so as the Macro - averaged F1 .",result
"Compared with the other two semi-supervised methods , the ASVAET also shows better results .",result
The adoption of indomain pre-trained word vectors is beneficial for the performance compared with the Glove vectors .,result
"TC - LSTM : Two LSTMs are used to model the left and right context of the target separately , then the concatenation of two representations is used to predict the label .",baseline
"MemNet : It uses the attention mechanism over the word embedding over multiple rounds to aggregate the information in the sentence , the vector of the final round is used for the prediction .",baseline
IAN : IAN adopts two LSTMs to derive the representations of the context and the target phrase interactively and the concatenation is fed to the softmax layer .,baseline
BILSTM - ATT -G : It models left and right contexts using two attention - based LSTMs and makes use of a special gate layer to combine these two representations .,baseline
"TNet - AS : Without using an attention module , TNet adopts a convolutional layer to get salient features from the transformed word representations originated from a bidirectional LSTM layer .",baseline
"The number of units in the encoder and the decoder is 100 and the latent variable is of size 50 and the number of layers of both Transformer blocks is 2 , the number of selfattention heads is 8 .",hyperparameter
"In this work , the KL weight is set to be 1e - 4 .",hyperparameter
"In this paper , we proposed a classifier - agnostic framework which named Aspect - term Semi-supervised Variational Autoencoder ( Kingma and Welling , 2014 ) based on Transformer ( ASVAET ) .",model
The variational autoencoder offers the flexibility to customize the model structure .,model
"Specifically , the representation of the lexical context is extracted by the encoder and the aspect - term sentiment polarity is inferred from the specific ATSA classifier .",model
"By regarding the aspect sentiment polarity of the unlabeled data as the discrete latent variable , the model implicitly induces the sentiment polarity via the variational inference .",model
"In addition , by separating the representation of the input sentence , the classifier becomes an independent module in our framework , which endows the method with the ability to integrate different classifiers .",model
Variational Semi-supervised Aspect - term Sentiment Analysis via Transformer,research-problem
"Aspect based sentiment analysis ( ABSA ) has two sub - tasks , namely aspect - term sentiment analysis ( ATSA ) and aspect - category sentiment analysis ( ACSA ) .",research-problem
"ACSA is to infer the sentiment polarity with regard to the predefined categories , e.g. , the aspect food , price , ambience .",research-problem
"On the other hand , ATSA aims at classifying the sentiment polarity of a given aspect word or phrase in the text .",research-problem
The word embeddings are initialized with 200 - dimension GloVE vectors and fine - tuned during the training .,hyperparameter
The fc layer size is 300 .,hyperparameter
The Adam ( Kingma and Ba 2014 ) is used as the optimizer with the initial learning rate 10 ? 4 .,hyperparameter
Gradients with the 2 norm larger than 40 are normalized to be 40 .,hyperparameter
"All weights in networks are randomly initialized from a uniform distribution U ( ? 0.01 , 0.01 ) .",hyperparameter
"The batch sizes are 64 and 32 for source and target domains , respectively .",hyperparameter
"To alleviate overfitting , we apply dropout on the word embeddings of the context with dropout rate 0.5 .",hyperparameter
We also perform early stopping on the validation set during the training process .,hyperparameter
The hyperparameters are tuned on 10 % randomly held - out training data of the target domain in R1?L task and are fixed to be used in all transfer pairs .,hyperparameter
"To resolve the challenges , we propose a novel framework named Multi - Granularity Alignment Network ( MGAN ) to simultaneously align aspect granularity and aspect- specific feature representations across domains .",model
"Specifically , the MGAN consists of two networks for learning aspect - specific representations for the two domains , respectively .",model
"The CFA considers both semantic alignment by maximally ensuring the equivalent distributions from different domains but the same class , and semantic separation by guaranteeing distributions from both different classes and domains to be as dissimilar as possible .",model
"First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , "" salmon "" to the "" food "" ) .",model
"To prevent false alignment , we adopt the Contrastive Feature Alignment ( CFA ) ( Motiian et al. 2017 ) to semantically align aspect - specific representations .",model
Exploiting Coarse - to - Fine Task Transfer for Aspect - level Sentiment Classification,research-problem
"Aspect - level sentiment classification ( ASC ) aims at identifying sentiment polarities towards aspects in a sentence , where the aspect can behave as a general Aspect Category ( AC ) or a specific Aspect Term ( AT ) .",research-problem
"To model aspect - oriented sentiment analysis , equipping Recurrent Neural Networks ( RNNs ) with the attention Copyright c 2019 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) .",research-problem
"It is evident from the results that our IARM model outperforms all the baseline models , including the state of the art , in both of the domains .",result
"We obtained bigger improvement in laptop domain , of 1.7 % , compared to restaurant domain , of 1.4 % .",result
IARM : Inter-Aspect Relation Modeling with Memory Networks in Aspect - Based Sentiment Analysis,research-problem
Sentiment analysis has immense implications in modern businesses through user-feedback mining .,research-problem
Aspect - based sentiment analysis ( ABSA ) caters to these needs .,research-problem
The aim of the ABSA classifier is to learn these connections between the aspects and their sentiment bearing phrases .,research-problem
"In this work , we explore the implication of hand - crafted features for SER and compare the performance of lighter machine learning models with the heavily data - reliant deep learning models .",model
"Furthermore , we also combine features from the textual modality to understand the correlation between different modalities and aid ambiguity resolution .",model
"For both the approaches , we first extract handcrafted features from the time domain of the audio signal and train the respective models .",model
"In the first approach , we train traditional machine learning classifiers , namely , Random Forests , Gradient Boosting , Support Vector Machines , Naive - Bayes and Logistic Regression .",model
"In the second approach , we build a Multi - Layer Perceptron and an LSTM classifier to recognize emotion given a speech signal .",model
Multimodal Speech Emotion Recognition and Ambiguity Resolution,research-problem
Identifying emotion from speech is a nontrivial task pertaining to the ambiguous definition of emotion itself .,research-problem
"With the rise of deep learning algorithms , there have been multiple attempts to tackle the task of Speech Emotion Recognition ( SER ) as in [ 2 ] and .",research-problem
"In this work , we explore the implication of hand - crafted features for SER and compare the performance of lighter machine learning models with the heavily data - reliant deep learning models .",research-problem
"We use librosa , a Python library , to process the audio files and extract features from them .",experimental-setup
"We use scikit - learn and xgboost [ 25 ] , the machine learning libraries for Python , to implement all the ML classifiers ( RF , XGB , SVM , MNB , and LR ) and the MLP .",experimental-setup
We use PyTorch to implement the LSTM classifiers described earlier .,experimental-setup
"In order to regularize the hidden space of the LSTM classifiers , we use a shut - off mechanism , called dropout , where a fraction of neurons are not used for final prediction .",experimental-setup
We randomly split our dataset into a train ( 80 % ) and test ( 20 % ) set .,experimental-setup
The LSTM classifiers were trained on an NVIDIA Titan X GPU for faster processing .,experimental-setup
We stop the training when we do not see any improvement in validation performance for > 10 epochs .,experimental-setup
NRC - Canada is the top method in SemEval 2014 Task 4 for ACSA and ATSA task .,baseline
CNN is widely used on text classification task .,baseline
TD - LSTM uses two LSTM networks to model the preceding and following contexts of the target to generate target - dependent representation for sentiment prediction .,baseline
ATAE - LSTM is an attention - based LSTM for ACSA task .,baseline
"IAN stands for interactive attention network for ATSA task , which is also based on LSTM and attention mechanisms .",baseline
"RAM is a recurrent attention network for ATSA task , which uses LSTM and multiple attention mechanisms .",baseline
"GCN stands for gated convolutional neural network , in which GTRU does not have the aspect embedding as an additional input .",baseline
"In our experiments , word embedding vectors are initialized with 300 - dimension GloVe vectors which are pre-trained on unlabeled data of 840 billion tokens .",hyperparameter
"Words out of the vocabulary of Glo Ve are randomly initialized with a uniform distribution U ( ? 0.25 , 0.25 ) .",hyperparameter
All neural models are implemented in PyTorch .,hyperparameter
"We use Adagrad with a batch size of 32 instances , default learning rate of 1 e ? 2 , and maximal epochs of 30 .",hyperparameter
We only fine tune early stopping with 5 - fold cross validation on training datasets .,hyperparameter
"In this paper , we propose a fast and effective neural network for ACSA and ATSA based on convolutions and gating mechanisms , which has much less training time than LSTM based networks , but with better accuracy .",model
"For ACSA task , our model has two separate convolutional layers on the top of the embedding layer , whose outputs are combined by novel gating units .",model
"For ATSA task , where the aspect terms consist of multiple words , we extend our model to include another convolutional layer for the target expressions .",model
"The proposed gating units have two nonlinear gates , each of which is connected to one convolutional layer .",model
Aspect Based Sentiment Analysis with Gated Convolutional Networks,research-problem
"Aspect based sentiment analysis ( ABSA ) can provide more detailed information than general sentiment analysis , because it aims to predict the sentiment polarities of the given aspects or entities in text .",research-problem
We summarize previous approaches into two subtasks : aspect - category sentiment analysis ( ACSA ) and aspect - term sentiment analysis ( ATSA ) .,research-problem
"A number of models have been developed for ABSA , but there are two different subtasks , namely aspect - category sentiment analysis ( ACSA ) and aspect - term sentiment analysis ( ATSA ) .",research-problem
"The goal of ACSA is to predict the sentiment polarity with regard to the given aspect , which is one of a few predefined categories .",research-problem
"On the other hand , the goal of ATSA is to identify the sentiment polarity concerning the target entities that appear in the text instead , which could be a multi-word phrase or a single word .",research-problem
"For MOSEI dataset , we obtain better performance with text .",result
"For text - acoustic input pairs , we obtain the highest accuracies with 79. 74 % , 79.60 % and 79.32 % for MMMU - BA , MMUU - SA and MU - SA frameworks , respectively .",result
"Finally , we experiment with tri-modal inputs and observe an improved performance of 79. 80 % , 79.76 % and 79.63 % for MMMU - BA , MMUU - SA and MU - SA frameworks , respectively .",result
The performance improvement was also found to be statistically significant ( T-test ) than the bimodality and uni-modality inputs .,result
"Further , we observe that the MMMU - BA framework reports the best accuracy of 79 . 80 % for the MOSEI dataset , thus supporting our claim that multi-modal attention framework ( i.e. MMMU - BA ) captures more information than the self - attention frameworks ( i.e. MMUU - SA & MU - SA ) .",result
"We use Bi-directional GRUs having 300 neurons , each followed by a dense layer consisting of 100 neurons .",hyperparameter
"Utilizing the dense layer , we project the input features of all the three modalities to the same dimensions .",hyperparameter
We set dropout = 0.5 ( MOSI ) & 0.3 ( MOSEI ) as a measure of regularization .,hyperparameter
"In addition , we also use dropout = 0.4 ( MOSI ) & 0.3 ( MOSEI ) for the Bi - GRU layers .",hyperparameter
"We employ ReLu activation function in the dense layers , and softmax activation in the final classification layer .",hyperparameter
"For training the network we set the batch size = 32 , use Adam optimizer with cross - entropy loss function and train for 50 epochs .",hyperparameter
"In this paper , we propose a novel method that employs a recurrent neural network based multimodal multi-utterance attention framework for sentiment prediction .",model
To better address these concerns we propose a novel fusion method by focusing on inter-modality relations computed between the target utterance and its context .,model
The attention mechanism is then used to attend to the important contextual utterances having higher relatedness or similarity ( computed using inter-modality correlations ) with the target utterance .,model
"Unlike previous approaches that simply apply attentions over the contextual utterance for classification , we attend over the contextual utterances by computing correlations among the modalities of the target utterance and the context utterances .",model
The model facilitates this modality selection by attending over the contextual utterances and thus generates better multimodal feature representation when these modalities from the context are combined with the modalities of the target utterance .,model
Contextual Inter-modal Attention for Multi-modal Sentiment Analysis,research-problem
"Traditionally , sentiment analysis has been applied to a wide variety of texts .",research-problem
"In pursuit of this goal , we develop deep memory network for aspect level sentiment classification , which is inspired by the recent success of computational models with attention mechanism and explicit memory .",approach
"Our approach is data - driven , computationally efficient and does not rely on syntactic parser or sentiment lexicon .",approach
The approach consists of multiple computational layers with shared parameters .,approach
"Each layer is a content - and location - based attention model , which first learns the importance / weight of each context word and then utilizes this information to calculate continuous text representation .",approach
The text representation in the last layer is regarded as the feature for sentiment classification .,approach
"As every component is differentiable , the entire model could be efficiently trained end - toend with gradient descent , where the loss function is the cross - entropy error of sentiment classification .",approach
"We can find that feature - based SVM is an extremely strong performer and substantially outperforms other baseline methods , which demonstrates the importance of a powerful feature representation for aspect level sentiment classification .",result
"We can also find that the performance of Contex - tAVG is very poor , which means that assigning the same weight / importance to all the context words is not an effective way .",result
We can find that using multiple computational layers could consistently improve the classification accuracy in all these models .,result
"Among three recurrent models , TDLSTM performs better than LSTM , which indicates that taking into account of the aspect information is helpful .",result
"Among all our models from single hop to nine hops , we can observe that using more computational layers could generally lead to better performance , especially when the number of hops is less than six .",result
We consider that each hidden vector of TDLSTM encodes the semantics of word sequence until the current position .,result
"The best performances are achieved when the model contains seven and nine hops , respectively .",result
All these models perform comparably when the number of hops is larger than five .,result
"On both datasets , the proposed approach could obtain comparable accuracy compared to the state - of - art feature - based SVM system .",result
Aspect Level Sentiment Classification with Deep Memory Network,research-problem
Aspect level sentiment classification is a fundamental task in the field of sentiment analysis .,research-problem
Our proposed DialogueRNN system employs three gated recurrent units ( GRU ) to model these aspects .,model
"The incoming utterance is fed into two GRUs called global GRU and party GRU to update the context and party Copyright 2019 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) .",model
The global GRU encodes corresponding party information while encoding an utterance .,model
Attending over this GRU gives contextual representation that has information of all preceding utterances by different parties in the conversation .,model
The speaker state depends on this context through attention and the speaker 's previous state .,model
"Finally , the updated speaker state is fed into the emotion GRU to decode the emotion representation of the given utterance , which is used for emotion classification .",model
"This ensures that at time t , the speaker state directly gets information from the speaker 's previous state and global GRU which has information on the preceding parties .",model
"At time t , the emotion GRU cell gets the emotion representation of t ? 1 and the speaker state of t .",model
DialogueRNN : An Attentive RNN for Emotion Detection in Conversations,research-problem
"Among all the GloVe - based methods , the TD - LSTM approach performs worst because it takes the aspect information into consideration in a very coarse way .",result
"After taking the importance of the aspect into account with attention mechanism , they achieve a stable improvement comparing to the TD - LSTM .",result
"RAM achieves a better performance than other basic attention - based models , because it combines multiple attentions with a recurrent neural network to capture aspect - specific representations .",result
PBAN achieves a similar performance as RAM by employing a position embedding .,result
"To be specific , PBAN is better than RAM on Restaurant dataset , but worse than RAN on Laptop dataset .",result
"AEN is slightly better than TSN , but still worse than RAM and PBAN .",result
"Moreover , the two models ( SDGCN - A and SDGCN - G ) with position information gain a significant improvement compared to the two models without position information .",result
"Compared with RAM and PBAN , the over all performance of TSN is not perform well on both Restaurant dataset and Laptop dataset , which might because the framework of TSN is too simple to model the representations of context and aspect effectively .",result
"Comparing the results of SDGCN - A w/o position and SDGCN - G w/o position , SDGCN - A and SDGCN - G , respectively , we observe that the GCN built with global - relation is slightly higher than built with adjacent - relation in both accuracy and Macro - F1 measure .",result
"Benefits from the power of pre-trained BERT , BERT - based models have shown huge superiority over GloVe - based models .",result
"Furthermore , compared with AEN - BERT , on the Restaurant dataset , SDGCN - BERT achieves absolute increases of 1.09 % and 1.86 % in accuracy and Macro - F1 measure respectively , and gains absolute increases of 1.42 % and 2.03 % in accuracy and Macro - F1 measure respectively on the Laptop dataset .",result
"TD - LSTM constructs aspect-specific representation by the left context with aspect and the right context with aspect , then employs two LSTMs to model them respectively .",baseline
The last hidden states of the two LSTMs are finally concatenated for predicting the sentiment polarity of the aspect .,baseline
"ATAE - LSTM first attaches the aspect embedding to each word embedding to capture aspect - dependent information , and then employs attention mechanism to get the sentence representation for final classification .",baseline
Mem Net uses a deep memory network on the context word embeddings for sentence representation to capture the relevance between each context word and the aspect .,baseline
IAN generates the representations for aspect terms and contexts with two attention - based LSTM network separately .,baseline
"RAM [ 10 ] employs a gated recurrent unit network to model a multiple attention mechanism , and captures the relevance between each context word and the aspect .",baseline
PBAN appends the position embedding into each word embedding .,baseline
TSN is a two - stage framework for aspect - level sentiment analysis .,baseline
"AEN mainly consists of an embedding layer , an attentional encoder layer , an aspect - specific attention layer , and an output layer .",baseline
AEN - BERT is AEN with BERT embedding .,baseline
"In our implementation , we respectively use the GloVe 3 word vector and the pre-trained language model word representation BERT 4 to initialize the word embeddings .",hyperparameter
The dimension of each word vector is 300 for GloVe and 768 for BERT .,hyperparameter
"The number of LSTM hidden units is set to 300 , and the output dimension of GCN layer is set to 600 .",hyperparameter
"The weight matrix of last fully connect layer is randomly initialized by a normal distribution N ( 0 , 1 ) .",hyperparameter
"Besides the last fully connect layer , all the weight matrices are randomly initialized by a uniform distribution U ( ? 0.01 , 0.01 ) .",hyperparameter
"In addition , we add L2-regularization to the last fully connect layer with a weight of 0.01 .",hyperparameter
"During training , we set dropout to 0.5 , the batch size is set to 32 and the optimizer is Adam Optimizer with a learning rate of 0.001 .",hyperparameter
We implement our proposed model using Tensorflow 5 .,hyperparameter
"In this paper , we propose a novel method to model Sentiment Dependencies with Graph Convolutional Networks ( SDGCN ) for aspect - level sentiment classification .",model
"GCN is a simple and effective convolutional neural network operating on graphs , which can catch inter-dependent information from rich relational data .",model
"In our case , an aspect is treated as a node , and an edge represents the sentiment dependency relation of two nodes .",model
"For every node in graph , GCN encodes relevant information about its neighborhoods as a new feature representation vector .",model
Our model learns the sentiment dependencies of aspects via this graph structure .,model
"As far as we know , our work is the first to consider the sentiment dependencies between aspects in one sentence for aspect - level sentiment classification task .",model
"Furthermore , in order to capture the aspect - specific representations , our model applies bidirectional attention mechanism with position encoding before GCN .",model
Modeling Sentiment Dependencies with Graph Convolutional Networks for Aspect - level Sentiment Classification,research-problem
"It is a fine - grained task in sentiment analysis , which aims to infer the sentiment polarities of aspects in their context .",research-problem
"From , we observe that IMN ?d is able to significantly outperform other baselines on F1 - I .",result
"IMN further boosts the performance and outperforms the best F1 - I results from the baselines by 2.29 % , 1.77 % , and 2.61 % on D1 , D2 , and D3 .",result
IMN wo DE performs only marginally below IMN .,result
"IMN ?d is more affected without domain - specific embeddings , while it still outperforms all other baselines except DECNN - d Trans .",result
DECNN - dTrans is a very strong baseline as it exploits additional knowledge from larger corpora for both tasks .,result
"IMN ?d wo DE is competitive with DECNN - dTrans even without utilizing additional knowledge , which suggests the effectiveness of the proposed network structure .",result
"Specifically , for AE ( F1 - a and F1 - o ) , IMN ?d performs the best in most cases .",result
"For AS ( acc - s and F1 - s ) , IMN outperforms other methods by large margins .",result
We adopt the multi - layer - CNN structure from as the CNN - based encoders in our proposed network .,hyperparameter
"We adopt their released domainspecific embeddings for restaurant and laptop domains with 100 dimensions , which are trained on a large domain - specific corpus using fast Text .",hyperparameter
"For word embedding initialization , we concatenate a general - purpose embedding matrix and a domain - specific embedding matrix 7 following .",hyperparameter
The general - purpose embeddings are pre-trained Glove vectors with 300 dimensions .,hyperparameter
Learning rate and batch size are set to conventional values without specific tuning for our task .,hyperparameter
We tune the maximum number of iterations T in the message passing mechanism by training IMN ?d via cross validation on D1 .,hyperparameter
It is set to 2 .,hyperparameter
"We use Adam optimizer with learning rate set to 10 ? 4 , and we set batch size to 32 .",hyperparameter
"At training phase , we randomly sample 20 % of the training data from the aspect - level dataset as the development set and only use the remaining 80 % for training .",hyperparameter
"In this work , we propose an interactive multitask learning network ( IMN ) , which solves both tasks simultaneously , enabling the interactions between both tasks to be better exploited .",model
"Furthermore , IMN allows AE and AS to be trained together with related document - level tasks , exploiting the knowledge from larger document - level corpora .",model
"In addition , IMN allows fined - grained tokenlevel classification tasks to be trained together with document - level classification tasks .",model
"In contrast to most multi-task learning schemes which share information through learning a common feature representation , IMN not only allows shared features , but also explicitly models the interactions between tasks through the message passing mechanism , allowing different tasks to better influence each other .",model
The information is then combined with the shared latent representation and made available to all tasks for further processing .,model
IMN introduces a novel message passing mechanism that allows informative interactions between tasks .,model
"Specifically , it sends useful information from different tasks back to a shared latent representation .",model
"We incorporated two document - level classification tasks - sentiment classification ( DS ) and domain classification ( DD ) - to be jointly trained with AE and AS , allowing the aspect - level tasks to benefit from document - level information .",model
An Interactive Multi - Task Learning Network for End - to - End Aspect - Based Sentiment Analysis,research-problem
Aspect - based sentiment analysis ( ABSA ) aims to determine people 's attitude towards specific aspects in a review .,research-problem
"This is done by extracting explicit aspect mentions , referred to as aspect term extraction ( AE ) , and detecting the sentiment orientation towards each extracted aspect term , referred to as aspect - level sentiment classification ( AS ) .",research-problem
"We observe that + Message passing - a and + Message passing - d contribute to the performance gains the most , which demonstrates the effectiveness of the proposed message passing mechanism .",ablation-analysi
We also observe that simply adding documentlevel tasks ( + DS / DD ) with parameter sharing only marginally improves the performance of IMN ?d .,ablation-analysi
"However , + Message passing -d is still helpful with considerable performance gains , showing that aspect - level tasks can benefit from knowing predictions of the relevant document - level tasks .",ablation-analysi
The over all performance of TD - LSTM is not good since it only makes a rough treatment of the target words .,result
"ATAE - LSTM , IAN and RAM are attention based models , they stably exceed the TD - LSTM method on Restaurant and Laptop datasets .",result
"RAM is better than other RNN based models , but it does not perform well on Twitter dataset , which might because bidirectional LSTM is not good at modeling small and ungrammatical text .",result
"Feature - based SVM is still a competitive baseline , but relying on manually - designed features .",result
Rec - NN gets the worst performances among all neural network baselines as dependency parsing is not guaranteed to work well on ungrammatical short texts such as tweets and comments .,result
"Like AEN , Mem Net also eschews recurrence , but its over all performance is not good since it does not model the hidden semantic of embeddings , and the result of the last attention is essentially a linear combination of word embeddings .",result
shows the number of training and test instances in each category .,hyperparameter
"Word embeddings in AEN - Glo Ve do not get updated in the learning process , but we fine - tune pre-trained BERT 3 in AEN - BERT .",hyperparameter
Embedding dimension d dim is 300 for GloVe and is 768 for pretrained BERT .,hyperparameter
Dimension of hidden states d hid is set to 300 .,hyperparameter
The weights of our model are initialized with Glorot initialization .,hyperparameter
"Adam optimizer ( Kingma and Ba , 2014 ) is applied to update all the parameters .",hyperparameter
"During training , we set label smoothing parameter to 0.2 , the coefficient ? of L 2 regularization item is 10 ? 5 and dropout rate is 0.1 .",hyperparameter
This paper propose an attention based model to solve the problems above .,model
"Specifically , our model eschews recurrence and employs attention as a competitive alternative to draw the introspective and interactive semantics between target and context words .",model
"To deal with the label unreliability issue , we employ a label smoothing regularization to encourage the model to be less confident with fuzzy labels .",model
We also apply pre-trained BERT to this task and show our model enhances the performance of basic BERT model .,model
Attentional Encoder Network for Targeted Sentiment Classification,research-problem
"Targeted sentiment classification is a fine - grained sentiment analysis task , which aims at determining the sentiment polarities ( e.g. , negative , neutral , or positive ) of a sentence over "" opinion targets "" that explicitly appear in the sentence .",research-problem
"However , these neural network models are still in infancy to deal with the fine - grained targeted sentiment classification task .",research-problem
"Comparing the results of AEN - GloVe and AEN - Glo Ve w / o LSR , we observe that the accuracy of AEN - Glo Ve w / o LSR drops significantly on all three datasets .",ablation-analysi
"The over all performance of AEN - GloVe and AEN - Glo Ve - BiLSTM is relatively close , AEN - Glo Ve performs better on the Restaurant dataset .",ablation-analysi
"More importantly , AEN - Glo Ve has fewer parameters and is easier to parallelize .",ablation-analysi
"AEN - Glo Ve 's lightweight level ranks second , since it takes some more parameters than MemNet in modeling hidden states of sequences .",ablation-analysi
"As a comparison , the model size of AEN - Glo Ve - BiLSTM is more than twice that of AEN - GloVe , but does not bring any performance improvements .",ablation-analysi
"( 2 ) The TD - LSTM model , which has been shown to be better than LSTM , gets the worst performance of all RNN based models and the accuracy achieved by TD - LSTM is 2.94 % and 2.4 % lower than those by Bi - LSTM on the two datasets respectively .",result
"Our method achieves accuracies of 82.23 % as well as 77 . 27 % on the Restaurant and Laptop dataset respectively , which are 0.89 % and 2.03 % higher than the current best method .",result
"In addition , another observation is that Bi - GRU - PW performs even worse than Bi - GRU .",result
The accuracy achieved by Bi - GRU - PW is 0.72 % as well as 1.41 % lower than that by Bi - GRU on the Restaurant and Laptop dataset respectively .,result
HAPN achieves improvement of 0.35 % and 0.78 % on accuracy respectively on the two dataset .,result
( 1 ) The information fusion operation is only used to calculate the Source2context attention value .,result
The output of Source2aspect attention is only used for information fusion .,result
"And the achieved model is "" Bi - GRU - PE "" reported in the , achieving the accuracies of 80.89 % and 76.02 % on the two datasets respectively , which are 1.34 % and 1.25 % lower than the proposed model .",result
"( 3 ) Compared with the state - of - the - art methods , our model achieves the best performance , which illustrates the effectiveness of the proposed approach .",result
"After introducing the position embeddings , the accuracy has an increase of 0.62 % and 2.67 % on two datasets .",result
Majority assigns the sentiment polarity with most frequent occurrences in the training set to each sample in test set .,baseline
Bi - LSTM and Bi - GRU adopt a Bi - LSTM and a Bi - GRU network to model the sentence and use the hidden state of the final word for prediction respectively .,baseline
TD - LSTM adopts two LSTMs to model the left context with target and the right context with target respectively ; It takes the hidden states of LSTM at last time - step to represent the sentence for prediction .,baseline
"MemNet applies attention multiple times on the word embeddings , and the output of last attention is fed to softmax for prediction .",baseline
"IAN interactively learns attentions in the contexts and targets , and generates the representations for targets and contexts separately .",baseline
RAM ) is a multilayer architecture where each layer consists of attention - based aggregation of word features and a GRU cell to learn the sentence representation .,baseline
"LCR - Rot employs three Bi- LSTMs to model the left context , the target and the right context .",baseline
AOA - LSTM introduces an attention - over- attention ( AOA ) based network to model aspects and sentences in a joint way and explicitly capture the interaction between aspects and context sentences .,baseline
"We use 300 - dimension word vectors pre-trained by GloVe ( whose vocabulary size is 1.9M ) for our experiments , as previous works did .",hyperparameter
"All out - of - vocabulary words are initialized as zero vectors , and all biases are set to zero .",hyperparameter
The dimensions of hidden states and fused embeddings are set to 300 .,hyperparameter
The dimension of position embeddings is set to 50 .,hyperparameter
Keras is used for implementing our neural network model .,hyperparameter
The paired t- test is used for the significance testing .,hyperparameter
"In model training , we set the learning rate to 0.001 , the batch size to 64 , and dropout rate to 0.5 .",hyperparameter
"Based on the analysis above , in this paper , we propose a hierarchical attention based positionaware network ( HAPN ) for aspect - level sentiment classification .",model
A position - aware encoding layer is introduced for modelling the sentence to achieve the position - aware abstract representation of each word .,model
"On this basis , a succinct fusion mechanism is further proposed to fuse the information of aspects and the contexts , achieving the final sentence representation .",model
"Finally , we feed the achieved sentence representation into a softmax layer to predict the sentiment polarity .",model
Hierarchical Attention Based Position-aware Network for Aspect-level Sentiment Analysis,research-problem
"Aspect - level sentiment analysis is a fine - grained task in sentiment analysis , which aims to identify the sentiment polarity ( i.e. , negative , neutral , or positive ) of a specific opinion target expressed in a comment / review by a reviewer .",research-problem
We make our source code public at https://github.com/DUT-LiuYang/Aspect-Sentiment-Analysis.,code
"The relative performance of SuBiL - STM and SuBiLSTM - Tied are fairly close to each other , as shown by the relative gains in .",result
"SuBiLSTM - Tied works better on small datasets ( SST and TREC ) , probably owing to the regularizing effect of using the same LSTM to encode both suffixes and prefixes .",result
"The training complexity for both the models is similar and hence , with half the parameters , SuBILSTM - Tied should be the more favored model for sentence modeling tasks .",result
"For the larger datasets ( SNLI and QUORA ) , SuBILSTM slightly edges out the tied version owing to its larger capacity .",result
"For each of the tasks , we compare SuBiLSTM and SuBiLSTM - Tied with a single - layer BiLSTM and a 2 - layer BiLSTM encoder with the same hidden dimension .",baseline
"In this paper , we propose a simple , general and effective technique to compute contextual representations that capture long range dependencies .",model
"For each token t , we encode both its prefix and suffix in both the forward and reverse direction .",model
"Further , we combine the prefix and suffix representations by a simple max - pooling operation to produce a richer contextual representation of t in both the forward and reverse direction .",model
We call our model Suffix BiLSTM or SuBiLSTM in short .,model
Improved Sentence Modeling using Suffix Bidirectional LSTM,research-problem
"Recurrent neural networks have become ubiquitous in computing representations of sequential data , especially textual data in natural language processing .",research-problem
Using SuBiLSTM we achieve new state - of - the - art results for fine - grained sentiment classification and question classification .,research-problem
Recurrent Neural Networks ( RNN ) ) have emerged as a powerful tool for modeling sequential data .,research-problem
"First , our model brings a substantial improvement over the methods that do not leverage sentiment linguistic knowledge ( e.g. , RNTN , LSTM , BiLSTM , CNN and ID - LSTM ) on both datasets .",result
"Second , our model also consistently outperforms LR - Bi - LSTM which integrates linguistic roles of sentiment , negation and intensity words into neural networks via the linguistic regularization .",result
"For example , our model achieves 2.4 % improvements over the MR dataset and 0.8 % improvements over the SST dataset compared to LR - Bi - LSTM .",result
RNTN : Recursive Tensor Neural Network ) is used to model correlations between different dimensions of child nodes vectors .,baseline
LSTM / Bi-LSTM : Cho et al. ( 2014 ) employs Long Short - Term Memory and the bidirectional variant to capture sequential information .,baseline
"Tree-LSTM : Memory cells was introduced by Tree - Structured Long Short - Term Memory and gates into tree - structured neural network , which is beneficial to capture semantic relatedness by parsing syntax trees .",baseline
CNN : Convolutional Neural Networks ) is applied to generate task - specific sentence representation .,baseline
NCSL : designs a Neural Context - Sensitive Lexicon ( NSCL ) to obtain prior sentiment scores of words in the sentence .,baseline
LR - Bi-LSTM : imposes linguistic roles into neural networks by applying linguistic regularization on intermediate outputs with KL divergence .,baseline
Self - attention : proposes a selfattention mechanism to learn structured sentence embedding .,baseline
ID - LSTM : uses reinforcement learning to learn structured sentence representation for sentiment classification .,baseline
"In this work , we propose a Multi- sentimentresource Enhanced Attention Network ( MEAN ) for sentence - level sentiment classification to integrate many kinds of sentiment linguistic knowledge into deep neural networks via multi -path attention mechanism .",model
"Then , we propose a multisentiment - resource attention module to learn more comprehensive and meaningful sentiment - specific sentence representation by using the three types of sentiment resource words as attention sources attending to the context words respectively .",model
"In this way , we can attend to different sentimentrelevant information from different representation subspaces implied by different types of sentiment sources and capture the over all semantics of the sentiment , negation and intensity words for sentiment prediction .",model
"Specifically , we first design a coupled word embedding module to model the word representation from character - level and word - level semantics .",model
A Multi-sentiment - resource Enhanced Attention Network for Sentiment Classification,research-problem
( 1 ) Majority performs worst since it only utilizes the data distribution information .,result
Our method MGAN outperforms Majority and Feature + SVM since MGAN could learn the high quality representation for prediction .,result
( 2 ) ATAE - LSTM is better than LSTM since it employs attention mechanism on the hidden states and combines with attention embedding to generate the final representation .,result
"TD - LSTM performs slightly better than ATAE - LSTM , and it employs two LSTM networks to capture the left and right context of the aspect .",result
TD - LSTM performs worse than our method MGAN since it could not properly pay more attentions on the important parts of the context .,result
"( 3 ) IAN achieves slightly better results with the previous LSTM - based methods , which interactively learns the attended aspect and context vector as final representation .",result
Our method consistently performs better than IAN since we utilize the finegrained attention vectors to relieve the information loss in IAN .,result
"BILSTM - ATT - G models left context and right context using attention - based LSTMs , which achieves better performance than MemNet .",result
RAM performs better than other baselines .,result
"Our proposed MGAN consistently performs better than MemNet , BILSTM - ATT - G and RAM on all three datasets .",result
"Majority is the basic baseline , which chooses the largest sentiment polarity in the training set to each instance in the test set .",baseline
"MemNet applys multi-hop attentions on the word embeddings , learns the attention weights on context word vectors with respect to the averaged query vector .",baseline
"IAN interactively learns the coarse - grained attentions between the context and aspect , and concatenate the vectors for prediction .",baseline
"BILSTM - ATT -G ( Liu and Zhang , 2017 ) models left and right context with two attention - based LSTMs and utilizes gates to control the importance of left context , right context and the entire sentence for prediction .",baseline
"RAM learns multi-hop attentions on the hidden states of bidirectional LSTM networks for context words , and proposes to use GRU network to get the aggregated vector from the attentions .",baseline
"MGAN - C only employs the coarse - grained attentions for prediction , which is similar with IAN .",baseline
MGAN - F only utilizes the proposed fine - grained attentions for prediction .,baseline
"MGAN - CF adopts both the coarse - grained and fine - grained attentions , while without applying the aspect alignment loss .",baseline
MGAN is the complete multi-grained attention network model .,baseline
"In this paper , we propose a multi -grained attention network to address the above two issues in aspect level sentiment classification .",model
"Specifically , we propose a fine - grained attention mechanism ( i.e. F- Aspect2Context and F - Context2Aspect ) , which is employed to characterize the word - level interactions between aspect and context words , and relieve the information loss occurred in coarse - grained attention mechanism .",model
"In addition , we utilize the bidirectional coarsegrained attention ( i.e. C- Aspect2Context and C - Context2Aspect ) and combine them with finegrained attention vectors to compose the multigrained attention network for the final sentiment polarity prediction , which can leverage the advantages of them .",model
"More importantly , in order to make use of the valuable aspect - level interaction information , we design an aspect alignment loss in the objective function to enhance the difference of the attention weights towards the aspects which have the same context and different sentiment polarities .",model
Multi - grained Attention Network for Aspect - Level Sentiment Classification,research-problem
We propose a novel multi-grained attention network ( MGAN ) model for aspect level sentiment classification .,research-problem
"Comparing the 1 - hop memory networks ( first nine rows ) , we see significant performance gains achieved by CNP , CI , JCI , and JPI on both datasets , where each of them has p < 0.01 over the strongest baseline ( BL - MN ) from paired t- test using F1 - Macro .",result
"3 . Comparing all TMNs , we see that JCI works the best as it always obtains the top - three scores on two datasets and in two settings .",result
"2 . In the 3 - hop setting , TMNs achieve much better results on Restaurant .",result
"JCI , IT , and CI achieve the best scores , outperforming the strongest baseline AMN by 2.38 % , 2.18 % , and 2.03 % .",result
"On Laptop , BL - MN and most TMNs ( except CNP and JPI ) perform similarly .",result
CI and JPI also perform well in most cases .,result
"IT , NP , and CNP can achieve very good scores in some cases but are less stable .",result
We use the open - domain word embeddings 1 for the initialization of word vectors .,hyperparameter
"We initialize other model parameters from a uniform distribution U ( - 0.05 , 0.05 ) .",hyperparameter
The dimension of the word embedding and the size of the hidden layers are 300 .,hyperparameter
The learning rate is set to 0.01 and the dropout rate is set to 0.1 .,hyperparameter
Stochastic gradient descent is used as our optimizer .,hyperparameter
"We also compare the memory networks in their multiple computational layers version ( i.e. , multiple hops ) and the number of hops is set to 3 as used in the mentioned previous studies .",hyperparameter
"We implemented all models in the TensorFlow environment using same input , embedding size , dropout rate , optimizer , etc.",hyperparameter
"To address this problem , we propose target - sensitive memory networks ( TMNs ) , which can capture the sentiment interaction between targets and contexts .",model
Target - Sensitive Memory Networks for Aspect Sentiment Classification,research-problem
Aspect sentiment classification ( ASC ) is a fundamental task in sentiment analysis .,research-problem
"However , we found an important problem with the current MNs in performing the ASC task .",research-problem
"c LSTM performs reasonably well on short conversations ( i.e. , EC and DailyDialog ) , but the worst on long conversations ( i.e. , MELD , EmoryNLP and IEMOCAP ) .",result
BERT BASE achieves very competitive performance on all datasets except EC due to its strong representational power via bi-directional context modelling using the Transformer .,result
"In particular , DialogueRNN performs better than our model on IEMOCAP , which maybe attributed to its detailed speaker information for modelling the emotion dynamics in each speaker as the conversation flows .",result
Our KET variants KET SingleSelfAttn and KET StdAttn perform comparably with the best baselines on all datasets except IEMOCAP .,result
"However , both variants perform noticeably worse than KET on all datasets except EC , validating the importance of our proposed hierarchical self - attention and dynamic context - aware affective graph attention mechanism .",result
One observation worth mentioning is that these two variants perform on a par with the KET model on EC .,result
"In contrast , when the utterance - level LSTM in c LSTM is replaced by features extracted by CNN , i.e. , the CNN + c LSTM , the model performs significantly better than c LSTM on long conversations , which further validates that modelling long conversations using only RNN models may not be sufficient .",result
"This finding indicates that our model is robust across datasets with varying training sizes , context lengths and domains .",result
Knowledge - Enriched Transformer for Emotion Detection in Textual Conversations,research-problem
The task of detecting emotions in textual conversations leads to a wide range of applications such as opinion mining in social networks .,research-problem
"This work addresses the task of detecting emotions ( e.g. , happy , sad , angry , etc. ) in textual conversations , where the emotion of an utterance is detected in the conversational context .",research-problem
We preprocessed all datasets by lower - casing and tokenization using Spacy 2 .,experimental-setup
We use the released code for BERT BASE and DialogueRNN .,experimental-setup
We use Glo Ve embedding for initialization in the word and concept embedding layers,experimental-setup
"For each dataset , all models are fine - tuned based on their performance on the validation set .",experimental-setup
"For our model in all datasets , we use Adam optimization ( Kingma and Ba , 2014 ) with a batch size of 64 and learning rate of 0.0001 throughout the training process .",experimental-setup
"For the class weights in cross - entropy loss for each dataset , we set them as the ratio of the class distribution in the validation set to the class distribution in the training set .",experimental-setup
It is clear that both context and knowledge are essential to the strong performance of KET on all datasets .,ablation-analysi
"Note that removing context has a greater impact on long conversations than short conversations , which is expected because more contextual information is lost in long conversations .",ablation-analysi
SentiHood currently contains annotated sentences containing one or two location entity mentions .,dataset
2 Sen-tiHood contains 5215 sentences with 3862 sentences containing a single location and 1353 sentences containing multiple ( two ) locations .,dataset
""" Positive "" sentiment is dominant for aspects such as dining and shopping .",dataset
The general aspect is the most frequent aspect with over 2000 sentences while aspect touristy has occurred in less than 100 sentences .,dataset
"Notice that since each sentence can contain one or more opinions , the total number of opinions ( 5920 ) in the dataset is higher than the number of sentences .",dataset
"Location entity names are masked by location1 and location 2 in the whole dataset , so the task does not involve identification and segmentation of the named entities .",dataset
"As we can see , the n-gram representation with location masking achieves slightly better results over the left - right context .",result
"Also , by adding POS information , we gain an increase in the performance .",result
"Separating the left and the right context ( LR - Left - Right ) for BoW representation , does not improve the performance .",result
"Amongst the two variations of LSTM , the model with final state embeddings does slightly better than the model where we use the embeddings at the location index , however they are not significantly different ( with a p valueless than 0.01 ) .",result
"It is interesting to note that the best LSTM model is not superior to logistic regression model , especially in terms of AUC .",result
Another interesting observation is that the F 1 measure for logistic regression model with n-grams and POS information is very low while this model 's performance is superior to other models in terms of AUC .,result
SentiHood : Targeted Aspect Based Sentiment Analysis Dataset for Urban Neighbourhoods,research-problem
"In this paper , we introduce the task of targeted aspect - based sentiment analysis .",research-problem
Sentiment analysis is an important task in natural language processing .,research-problem
"Aspect - based sentiment analysis ( ABSA ) relates to the task of extracting fine - grained information by identifying the polarity towards different aspects of an entity in the same unit of text , and recognizing the polarity associated with each aspect separately .",research-problem
Targeted sentiment analysis investigates the classification of opinion polarities towards certain target entity mentions in given sentences ( often a tweet ) .,research-problem
This suggests that gathering contexts temporally through sequential processing is indeed a superior method over non-temporal memory representations .,result
CMN self which uses only single history channel also provides lesser performance when compared to CMN .,result
"Overall , predictions on valence and arousal levels also show similar results which reinforce our hypothesis of CMN 's ability to model emotional dynamics .",result
We use 10 % of the training set as a held - out validation set for hyperparameter tuning .,hyperparameter
"To optimize the parameters , we use Stochastic Gradient Descent ( SGD ) optimizer , starting with an initial learning Utterances whose history has atleast 3 similar emotion labels in either own history or the history of the other person , is counted in case 1 or 2 , respectively .",hyperparameter
An annealing approach halves the lr every 20 epochs and termination is decided using an early - stop measure with a patience of 12 by monitoring the validation loss .,hyperparameter
Gradient clipping is used for regularization with a norm set to 40 .,hyperparameter
The dimension size of the memory cells d is set as 50 .,hyperparameter
Hyperparameters are decided using a Random Search .,hyperparameter
"Based on validation performance , context window length K is set to be 40 and the number of hops R is fixed at 3 hops .",hyperparameter
"We propose a conversational memory network ( CMN ) , which uses a multimodal approach for emotion detection in utterances ( a unit of speech bound by breathes or pauses ) of such conversational videos .",model
Our proposed CMN incorporates these factors by using emotional context information present in the conversation history .,model
It improves speakerbased emotion modeling by using memory networks which are efficient in capturing long - term dependencies and summarizing task - specific details using attention models .,model
"Specifically , the memory cells of CMN are continuous vectors that store the context information found in the utterance histories .",model
CMN also models interplay of these memories to capture interspeaker dependencies .,model
"CMN first extracts multimodal features ( audio , visual , and text ) for all utterances in a video .",model
Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos,research-problem
Emotion recognition in conversations is crucial for the development of empathetic machines .,research-problem
"In this paper , we analyze emotion detection in videos of dyadic conversations .",research-problem
"Emotion detection from such resources can benefit numerous fields like counseling , public opinion mining , financial forecasting , and intelligent systems such as smart homes and chatbots .",research-problem
From here we see that using cosine similarity instead of dot product improves accuracy across the board .,result
However it is not to suggest that switching from dot product to cosine similarity alone improves accuracy as other minor ad - justments and hyperparameter tuning as explained was done .,result
"As seen during grid search , whenever the initial learning rate was 0.25 , accuracy was always poor .",result
"Introducing L2 regularization to dot product improves accuracy for all cases except a depreciation in the case of using unigrams only , lucikily cosine similarity does not suffer from this same depreciation .",result
Grid search was performed using 20 % of the training data as a validation set in order to determine the optimal hyperparameters as well as whether to use a constant learning rate or learning rate annealing .,hyperparameter
"The optimal learning rate in the case of cosine similarity is extremely small , suggesting a chaotic error surface .",hyperparameter
"The weights of the networks were initialized from a uniform distribution in the range of [ - 0.001 , 0.001 ] .",hyperparameter
"We did however tune the number of iterations from , learning rate from [ 0.25 , 0.025 , 0.0025 , 0.001 ] and ? from .",hyperparameter
"In the case of using L2 regularized dot product , ? ( regularization strength ) was chosen from [ 1 , 0.1 , 0.01 ] .",hyperparameter
The model in turn requires a larger number of epochs for convergence .,hyperparameter
"For the distribution for sampling negative words , we used the n-gram distribution raised to the 3 / 4 th power in accordance with .",hyperparameter
This paper aims to improve existing document embedding models by training document embeddings using cosine similarity instead of dot product .,model
"For example , in the basic model of trying to predict given a document - the words / n - grams in the document , instead of trying to maximize the dot product between a document vector and vectors of the words / n - grams in the document over the training set , we 'll be trying to maximize the cosine similarity instead .",model
Sentiment Classification using Document Embeddings trained with Cosine Similarity,research-problem
"In document - level sentiment classification , each document must be mapped to a fixed length vector .",research-problem
"In document classification tasks such as sentiment classification ( in this paper we focus on binary sentiment classification of long movie reviews , i.e. determining whether each review is positive or negative ) , the choice of document representation is usually more important than the choice of classifier .",research-problem
Code to reproduce all experiments is available at https://github.com/tanthongtan/dv-cosine.,code
"In this paper , we investigate several methods of constructing an auxiliary sentence and transform ( T ) ABSA into a sentence - pair classification task .",approach
We fine - tune the pre-trained model from BERT and achieve new state - of - the - art results on ( T ) ABSA task .,approach
"We find that BERT - single has achieved better results on these two subtasks , and BERT - pair has achieved further improvements over BERT - single .",result
The BERT - pair - NLI - B model achieves the best performance for aspect category detection .,result
"For aspect category polarity , BERTpair - QA - B performs best on all 4 - way , 3 - way , and binary settings .",result
LR : a logistic regression classifier with n-gram and pos-tag features .,baseline
LSTM - Final ) : a biLSTM model with the final state as a representation .,baseline
LSTM - Loc ) : a biLSTM model with the state associated with the target position as a representation .,baseline
LSTM + TA + SA ) : a biLSTM model which introduces complex target - level and sentence - level attention mechanisms .,baseline
SenticLSTM : an upgraded version of the LSTM + TA + SA model which introduces external information from Sentic - Net .,baseline
"Dmu - Entnet : a bidirectional EntNet with external "" memory chains "" with a delayed memory update mechanism to track entities .",baseline
We use the pre-trained uncased BERT - base model 5 for fine - tuning .,hyperparameter
"The number of Transformer blocks is 12 , the hidden layer size is 768 , the number of self - attention heads is 12 , and the total number of parameters for the pretrained model is 110M .",hyperparameter
"The initial learning rate is 2 e - 5 , and the batch size is 24 .",hyperparameter
"the dropout probability at 0.1 , set the number of epochs to 4 .",hyperparameter
Utilizing BERT for Aspect - Based Sentiment Analysis via Constructing Auxiliary Sentence,research-problem
"Aspect - based sentiment analysis ( ABSA ) , which aims to identify fine - grained opinion polarity towards a specific aspect , is a challenging subtask of sentiment analysis ( SA ) .",research-problem
"Both SA and ABSA are sentence - level or document - level tasks , but one comment may refer to more than one object , and sentence - level tasks can not handle sentences with multiple targets .",research-problem
"Therefore , introduce the task of targeted aspect - based sentiment analysis ( TABSA ) , which aims to identify fine - grained opinion polarity towards a specific aspect associated with a given target .",research-problem
The hyperparameters for our models were tuned on the development set for each task .,hyperparameter
We initialized our word representations using publicly available 300 - dimensional Glove vectors,hyperparameter
"For the sentiment classification task , word representations were updated during training with a learning rate of 0.1 .",hyperparameter
"For the semantic relatedness task , word representations were held fixed as we did not observe any significant improvement when the representations were tuned .",hyperparameter
Our models were trained using AdaGrad with a learning rate of 0.05 and a minibatch size of 25 .,hyperparameter
The model parameters were regularized with a per-minibatch L2 regularization strength of 10 ?4 .,hyperparameter
The sentiment classifier was additionally regularized using dropout with a dropout rate of 0.5 .,hyperparameter
"In this paper , we introduce a generalization of the standard LSTM architecture to tree - structured network topologies and show its superiority for representing sentence meaning over a sequential LSTM .",model
"While the standard LSTM composes its hidden state from the input at the current time step and the hidden state of the LSTM unit in the previous time step , the tree - structured LSTM , or Tree - LSTM , composes its state from an input vector and the hidden states of arbitrarily many child units .",model
Improved Semantic Representations From Tree - Structured Long Short - Term Memory Networks,research-problem
"Tree - LSTMs outperform all existing systems and strong LSTM baselines on two tasks : predicting the semantic relatedness of two sentences ( Sem Eval 2014 , Task 1 ) and sentiment classification ( Stanford Sentiment Treebank ) .",research-problem
Implementations of our models and experiments are available at https :// github.com/stanfordnlp/treelstm.,code
"As shown by the results in , our RAM consistently outperforms all compared methods on these four datasets .",result
"AC and AC - S perform poorly , because averaging context is equivalent to paying identical attention to each word which would hide the true sentiment word .",result
Rec - NN is better than TD - LSTM but not as good as our method .,result
"TD - LSTM performs less competitive than our method on all the datasets , particularly on the tweet dataset , because in this dataset sentiment words are usually far from person names , for which case the multiple - attention mechanism is designed to work .",result
"TD - LSTM - A also performs worse than our method , because it s two attentions , i.e. one for the text before the target and the other for the after , can not tackle some cases where more than one features being attended are at the same side of the target .",result
"MemNet adopts multiple attentions in order to improve the attention results , given the assumption that the result of an attention at a later hop should be better than that at the beginning .",result
"In this paper , we propose a novel framework to solve the above problems in target sentiment analysis .",model
"Specifically , our framework first adopts a bidirectional LSTM ( BLSTM ) to produce the memory ( i.e. the states of time steps generated by LSTM ) from the input , as bidirectional recurrent neural networks ( RNNs ) were found effective for a similar purpose in machine translation .",model
"The memory slices are then weighted according to their relative positions to the target , so that different targets from the same sentence have their own tailor - made memories .",model
Our framework introduces a novel way of applying multiple - attention mechanism to synthesize important features in difficult sentence structures .,model
"After that , we pay multiple attentions on the position - weighted memory and nonlinearly combine the attention results with a recurrent network , i.e. GRUs .",model
"Finally , we apply softmax on the output of the GRU network to predict the sentiment on the target .",model
Recurrent Attention Network on Memory for Aspect Sentiment Analysis,research-problem
"Compared with CNN embedding : Emo2 Vec works better than CNN embedding on 14 / 18 datasets , giving 2.6 % absolute accuracy improvement for the sentiment task and 1.6 % absolute f1score improvement on the other tasks .",result
"Compared with SSWE : Emo2 Vec works much better on all datasets except SS - T datasets , which gives 3.3 % accuracy improvement and 4.7 % f 1 score improvement respectively on sentiment and other tasks .",result
"Since Emo2 Vec is not trained by predicting contextual words , it is weak on capturing synthetic and semantic meaning .",result
"GloVe + Emo2 Vec achieves better performances on SOTA results on three datasets ( SE0714 , stress and tube tablet ) and comparable result to SOTA on dataset Previous SOTA results",result
It shows multi-task training helps to create better generalized word emotion representations than just using a single task .,result
"On average , it gives 1.3 % improvement in accuracy for the sentiment task and 1.1 % improvement of f 1 - score on the other tasks .",result
"Here , we want to highlight that solely using a simple classifier with good word representation can achieve promising results .",result
"Compared with GloVe+ DeepMoji , GloVe + Emo2 Vec achieves same or better results on 11 / 14 datasets , which on average gives 1.0 % improvement .",result
"Thus , to detect the corresponding emotion , more attention needs to be paid to words .",result
This work demonstrates the effectiveness of incorporating sentiment labels in a wordlevel information for sentiment - related tasks compared to other word embeddings .,model
"1 ) We propose Emo2Vec 1 which are word - level representations that encode emotional semantics into fixed - sized , real - valued vectors .",model
2 ) We propose to learn Emo2Vec with a multi-task learning framework by including six different emotion - related tasks .,model
Emo2 Vec : Learning Generalized Emotion Representation by Multi- task Training,research-problem
"Our official ranking is 1/38 ( tie ) in Subtask A , 2/24 in Subtask B , 2/16 in Subtask C , 2/16 in Subtask D and 11/12 in Subtask E.",result
"In this paper , we present two deep - learning systems that competed at SemEval - 2017 Task 4 .",model
Our first model is designed for addressing the problem of messagelevel sentiment analysis .,model
"We employ a 2 - layer Bidirectional LSTM , equipped with an attention mechanism .",model
"For the topic - based sentiment analysis tasks , we propose a Siamese Bidirectional LSTM with a contextaware attention mechanism .",model
DataStories at SemEval-2017 Task 4 : Deep LSTM with Attention for Message - level and Topic - based Sentiment Analysis,research-problem
"In this paper we present two deep - learning systems that competed at SemEval - 2017 Task 4 "" Sentiment Analysis in Twitter "" .",research-problem
"Sentiment analysis is an area in Natural Language Processing ( NLP ) , studying the identification and quantification of the sentiment expressed in text .",research-problem
"In this paper , we propose a novel progressive self - supervised attention learning approach for neural ASC models .",approach
"Our method is able to automatically and incrementally mine attention supervision information from a training corpus , which can be exploited to guide the training of attention mechanisms in ASC models .",approach
The basic idea behind our approach roots in the following fact : the context word with the maximum attention weight has the greatest impact on the sentiment prediction of an input sentence .,approach
"First , both of our reimplemented MN and TNet are comparable to their original models reported in .",result
"When we replace the CNN of TNet with an attention mechanism , TNet - ATT is slightly inferior to TNet .",result
"Moreover , when we perform additional K+1 - iteration of training on these models , their performance has not changed significantly , suggesting simply increasing training time is unable to enhance the performance of the neural ASC models .",result
"Finally , when we use both kinds of attention supervision information , no matter for which metric , MN ( + AS ) remarkably outperforms MN on all test sets .",result
We used pre-trained Glo Ve vectors to initialize the word embeddings with vector dimension 300 .,hyperparameter
"For out - of - vocabulary words , we randomly sampled their embeddings from the uniform distribution , as implemented in .",hyperparameter
"To alleviate overfitting , we employed dropout strategy ( Hinton et al. , 2012 ) on the input word embeddings of the LSTM and the ultimate aspect - related sentence representation .",hyperparameter
"Adam ( Kingma and Ba , 2015 ) was adopted as the optimizer with the learning rate 0.001 .",hyperparameter
All hyper - parameters were tuned on 20 % randomly held - out training data .,hyperparameter
"When implementing our approach , we empirically set the maximum iteration number K as 5 , ? in Equation 3 as 0.1 on LAPTOP data set , 0.5 on REST data set and 0.1 on TWITTER data set , respectively .",hyperparameter
Progressive Self - Supervised Attention Learning for Aspect - Level Sentiment Analysis,research-problem
"In aspect - level sentiment classification ( ASC ) , it is prevalent to equip dominant neural models with attention mechanisms , for the sake of acquiring the importance of each context word on the given aspect .",research-problem
"In this paper , we propose a progressive self - supervised attention learning approach for neural ASC models , which automatically mines useful attention supervision information from a training corpus to refine attention mechanisms .",research-problem
"Aspect - level sentiment classification ( ASC ) , as an indispensable task in sentiment analysis , aims at inferring the sentiment polarity of an input sentence in a certain aspect .",research-problem
"However , the existing attention mechanism in ASC suffers from a major drawback .",research-problem
"From the results , we see some improvement on Discourse Structure prediction when we are using a joint model but the improvement is statistically significant only for the Nuclearity and Relation predictions .",result
"The improvements on the Relation predictions were mainly on the Contrastive set , specifically the class of Contrast , Comparison and Cause relations as .",result
In the fine grained setting we compute the accuracy of exact match across five classes .,result
All the neural models presented in this paper were implemented using the Tensor Flow python pack - .,hyperparameter
We minimize the crossentropy error using the Adam optimizer and L2regularization on the set of weights .,hyperparameter
"For the individual models ( before joining ) , we use 200 training epochs and a batch size of 100 .",hyperparameter
Our framework consists of three main sub parts .,model
"Given a segmented sentence , the first step is to create meaningful vector representations for all the EDUs .",model
"Next , we devise three different Recursive Neural Net models , each designed for one of discourse structure prediction , discourse relation prediction and sentiment analysis .",model
"Finally , we join these Neural Nets in two different ways : Multitasking and Pre-training .",model
Exploring Joint Neural Model for Sentence Level Discourse Parsing and Sentiment Analysis,research-problem
"Thus , in this paper , we propose a novel neural network model BiHDM to learn the bi-hemispheric discrepancy for EEG emotion recognition .",model
"BiHDM aims to obtain the deep discrepant features between the left and right hemispheres , which is expected to contain more discriminative information to recognize the EEG emotion signals .",model
"Hence , to avoid losing this intrinsic graph structural information of EEG data , we can simplify the graph structure learning process by using the horizontal and vertical traversing RNNs , which will construct a complete relationship graph and generate discriminative deep features for all the EEG electrodes .",model
"After obtaining these deep features of each electrodes , we can extract the asymmetric discrepancy information between two hemispheres by performing specific pairwise operations for any paired symmetric electrodes .",model
A Novel Bi-hemispheric Discrepancy Model for EEG Emotion Recognition,research-problem
"Inspired by this study , in this paper , we propose a novel bi-hemispheric discrepancy model ( BiHDM ) to learn the asymmetric differences between two hemispheres for electroencephalograph ( EEG ) emotion recognition .",research-problem
"As the first step to make machines capture human emotions , emotion recognition has received substantial attention from human - machine - interaction ( HMI ) and pattern recognition research communities in recent years , , .",research-problem
"We use the released handcrafted features , i.e. , the differential entropy ( DE ) in SEED and SEED - IV , and the Short - Time Fourier Transform ( STFT ) in MPED , as the input to feed our model .",experimental-setup
"Thus the sizes d N of the input sample X t are 5 62 , 5 62 and 1 62 for these three datasets , respectively .",experimental-setup
"The learning rate , momentum and weight decay rate are set as 0.003 , 0.9 and 0.95 respectively .",experimental-setup
The network is trained using SGD with batch size of 200 .,experimental-setup
"Moreover , in the experiment , we respectively set the dimension d l of each electrode 's deep representation to 32 ; the parameters d g and K of the global high - level feature to 32 and 6 ; and the dimension do of the output feature to 16 without elaborate traversal .",experimental-setup
"Specifically , we implemented BiHDM using Tensor Flow on one Nvidia 1080 Ti GPU .",experimental-setup
"In addition , we adopt the subtraction as the pairwise operation of the BiHDM model in the experiment section , and discuss the other two types of operations in section III - D.",experimental-setup
"We can see that our model , despite being a simple architecture , performs better in terms of accuracy than many popular and sophisticated NLP models .",result
"In this paper , we use the pretrained BERT model and finetune it for the fine - grained sentiment classification task on the Stanford Sentiment Treebank ( SST ) dataset .",model
Fine - grained Sentiment Classification using BERT,research-problem
"Sentiment classification is an important process in understanding people 's perception towards a product , service , or topic .",research-problem
"Among the variants of the RNN function , we use GRUs as they yield comparable performance to that of the LSTM and include a smaller number of weight parameters .",hyperparameter
"We use a max encoder step of 750 for the audio input , based on the implementation choices presented in and 128 for the text input because it covers the maximum length of the transcripts .",hyperparameter
"In preparing the textual dataset , we first use the released transcripts of the IEMOCAP dataset for simplicity .",hyperparameter
"The vocabulary size of the dataset is 3,747 , including the "" UNK "" token , which represents unknown words , and the "" PAD "" token , which is used to indicate padding information added while preparing mini-batch data .",hyperparameter
"The number of hidden units and the number of layers in the RNN for each model ( ARE , TRE , MDRE and MDREA ) are selected based on extensive hyperparameter search experiments .    ",hyperparameter
The weights of the hidden units are initialized using orthogonal,hyperparameter
"weights ] , and the text embedding layer is initialized from pretrained word - embedding vectors .",hyperparameter
"To overcome these limitations , we propose a model that uses high - level text transcription , as well as low - level audio signals , to utilize the information contained within low - resource datasets to a greater degree .",model
"In this paper , we propose a novel deep dual recurrent encoder model that simultaneously utilizes audio and text data in recognizing emotions from speech .",model
MULTIMODAL SPEECH EMOTION RECOGNITION USING AUDIO AND TEXT,research-problem
"The ARE model ( ) incorrectly classifies most instances of happy as neutral ( 43.51 % ) ; thus , it shows reduced accuracy ( 35.15 % ) in predicting the the happy class .",ablation-analysi
"Overall , most of the emotion classes are frequently confused with the neutral class .",ablation-analysi
"Interestingly , the TRE model ( ) shows greater prediction gains in predicting the happy class when compared to the ARE model ( 35.15 % to 75. 73 % ) .",ablation-analysi
The MDRE model ) compensates for the weaknesses of the previous two models ( ARE and TRE ) and benefits from their strengths to a surprising degree .,ablation-analysi
"Furthermore , the occurrence of the incorrect "" sad - to - happy "" cases in the TRE model is reduced from 16 . 20 % to 9.15 % .",ablation-analysi
"We initialise our model with GloVe ( 300 - D , trained on 42B tokens , 1.9 M vocab , not updated during training : ) 4 and pre-process the corpus with tokenisation using NLTK ) and case folding .",hyperparameter
Training is carried out over 800 epochs with the FTRL optimiser and a batch size of 128 and learning rate of 0.05 .,hyperparameter
Dropout is applied to the output of ? in the final classifier ( Equation ) with a rate of 0.2 .,hyperparameter
"We use the following hyper - parameters for weight matrices in both directions : R ? R 3003 , H , U , V , Ware all matrices of size R 300300 , v ? R 300 , and hidden size of the GRU in Equation is 300 .",hyperparameter
"Lastly , to curb overfitting , we regularise the last layer ( Equation ) with an L 2 penalty on its weights : ?",hyperparameter
"We empirically set the number of memory chains to 6 , with the keys of two of them set to the same embeddings as the target words LOC1 and LOC2 , resp. , and the other 4 chains with free key embeddings which are updated during training , and therefore free to capture any entities .",hyperparameter
"In this work , we propose a novel model architecture for TABSA , augmented with multiple "" memory chains "" , and equipped with a delayed memory update mechanism , to keep track of numerous entities independently .",model
Recurrent Entity Networks with Delayed Memory Update for Targeted Aspect - based Sentiment Analysis,research-problem
"While neural networks have been shown to achieve impressive results for sentence - level sentiment analysis , targeted aspect - based sentiment analysis ( TABSA ) - extraction of finegrained opinion polarity w.r.t. a pre-defined set of aspects - remains a difficult task .",research-problem
"In our experiments , all word embedding are initialized by the pre-trained Glove vector 2 .",hyperparameter
"All the weight matrices are given the initial value by sampling from the uniform distribution U ( ?0.1 , 0.1 ) , and all the biases are set to zero .",hyperparameter
"The dimension of position embedding is set to 100 , which is randomly initialized and updated during the training process .",hyperparameter
"The dimension of the word embedding and aspect term embedding are set to 300 , and the number of the hidden units are set to 200 .",hyperparameter
"We use Tensorflow to implement our proposed model and employ the Momentum as the training method , whose momentum parameter ? is set to 0.9 , ? is set to 10 ? 6 , and the initial learning rate is set to 0.01 .",hyperparameter
"Inspired by this , we go one step further and propose a position - aware bidirectional attention network ( PBAN ) based on bidirectional Gated Recurrent Units ( Bi - GRU ) .",model
"1 ) Obtaining position information of each word in corresponding sentence based on the current aspect term , then converting the position information into position embedding .",model
2 ) The PBAN composes of two Bi - GRU networks focusing on extracting the aspectlevel features and sentence - level features respectively .,model
3 ) Using the bidirectional attention mechanism to model the mutual relation between aspect term and its corresponding sentence .,model
A Position - aware Bidirectional Attention Network for Aspect - level Sentiment Analysis,research-problem
"Sentiment analysis , also known as opinion mining , is a vital task in Natural Language Processing ( NLP ) .",research-problem
LSTM - based models relying on sequential information can perform well for formal sentences by capturing more useful context features ;,result
"For ungrammatical text , CNN - based models may have some advantages because CNN aims to extract the most informative n-gram features and is thus less sensitive to informal texts without strong sequential patterns .",result
"We propose a new architecture , named Target - Specific Transformation Networks ( TNet ) , to solve the above issues in the task of target sentiment classification .",model
TNet firstly encodes the context information into word embeddings and generates the contextualized word representations with LSTMs .,model
"To integrate the target information into the word representations , TNet introduces a novel Target - Specific Transformation ( TST ) component for generating the target - specific word representations .",model
"Contrary to the previous attention - based approaches which apply the same target representation to determine the attention scores of individual context words , TST firstly generates different representations of the target conditioned on individual context words , then it consolidates each context word with its tailor - made target representation to obtain the transformed word representation .",model
"As the context information carried by the representations from the LSTM layer will be lost after the non-linear TST , we design a contextpreserving mechanism to contextualize the generated target - specific word representations .",model
"To help the CNN feature extractor locate sentiment indicators more accurately , we adopt a proximity strategy to scale the input of convolutional layer with positional relevance between a word and the target .",model
Transformation Networks for Target - Oriented Sentiment Classification,research-problem
"After removing the deep transformation ( i.e. , the techniques introduced in Section 2.2 ) , both TNet - LF and TNet - AS are reduced to TNet w/o transformation ( where position relevance is kept ) , and their results in both accuracy and F 1 measure are incomparable with those of TNet .",ablation-analysi
It shows that the integration of target information into the word - level representations is crucial for good performance .,ablation-analysi
"Comparing the results of TNet and TNet w/o context ( where TST and position relevance are kept ) , we observe that the performance of TNet w/o context drops significantly on LAPTOP and REST 7 , while on TWITTER , TNet w/o context performs very competitive ( p- values with TNet - LF and TNet - AS are 0.066 and 0.053 respectively for Accuracy ) .",ablation-analysi
"TNet w/o context performs consistently better than TNet w/o transformation , which verifies the efficacy of the target specific transformation ( TST ) , before applying context - preserving .",ablation-analysi
"All of the produced p-values are less than 0.05 , suggesting that the improvements brought in by position information are significant .",ablation-analysi
"This work first builds an RRC dataset called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",dataset
"To answer RQ1 , we observed that the proposed joint post - training ( BERT - PT ) has the best performance over all tasks in all domains , which show the benefits of having two types of knowledge .",result
"Rest. Methods EM F1 EM F1 DrQA 38.26 50.99 49.52 63.73 DrQA+MRC 40 To answer RQ2 , to our surprise we found that the vanilla pre-trained weights of BERT do not work well for review - based tasks , although it achieves state - of - the - art results on many other NLP tasks .",result
"To answer RQ3 , we noticed that the roles of domain knowledge and task knowledge vary for different tasks and domains .",result
"For RRC , we found that the performance gain of BERT - PT mostly comes from task - awareness ( MRC ) post -training ( as indicated by BERT - MRC ) .",result
"For AE , we found that great performance boost comes mostly from domain knowledge posttraining , which indicates that contextualized representations of domain knowledge are very important for AE .",result
"For AE , errors mostly come from annotation inconsistency and boundaries of aspects ( e.g. , apple OS is predicted as OS ) .",result
"For ASC , we observed that large - scale annotated MRC data is very useful .",result
We further investigated the examples improved by BERT - MRC and found that the boundaries of spans ( especially short spans ) were greatly improved .,result
"BERT - MRC has almost no improvement on restaurant , which indicates Wikipedia may have no knowledge about aspects of restaurant .",result
The errors on RRC mainly come from boundaries of spans that are not concise enough and incorrect location of spans that may have certain nearby words related to the question .,result
"ASC tends to have more errors as the decision boundary between the negative and neutral examples is unclear ( e.g. , even annotators may not sure whether the reviewer shows no opinion or slight negative opinion when mentioning an aspect ) .",result
We adopt BERT BASE ( uncased ) as the basis for all experiments,hyperparameter
"10 . Since post - training may take a large footprint on GPU memory ( as BERT pretraining ) , we leverage FP16 computation 11 to reduce the size of both the model and hidden representations of data .",hyperparameter
"We set a static loss scale of 2 in FP16 , which can avoid any over / under - flow of floating point computation .",hyperparameter
The maximum length of post -training is set to 320 with a batch size of 16 for each type of knowledge .,hyperparameter
"The number of subbatch u is set to 2 , which is good enough to store each sub - batch iteration into a GPU memory of 11G .",hyperparameter
We use Adam optimizer and set the learning rate to be 3e - 5 .,hyperparameter
"We train 70,000 steps for the laptop domain and 140,000 steps for the restaurant domain , which roughly have one pass over the preprocessed data on the respective domain .",hyperparameter
This work adopts BERT ) as the base model as it achieves the state - of the - art performance on MRC .,model
"To address all the above challenges , we propose a novel joint post - training technique that takes BERT 's pre-trained weights as the initialization 4 for basic language understanding and adapt BERT with both domain knowledge and task ( MRC ) knowledge before fine - tuning using the domain end task annotated data for the domain RRC .",model
"This technique leverages knowledge from two sources : unsupervised domain reviews and supervised ( yet out - of - domain ) MRC data 5 , where the former enhances domain - awareness and the latter strengthens MRC task - awareness .",model
BERT Post - Training for Review Reading Comprehension and Aspect - based Sentiment Analysis,research-problem
"To show the generality of the approach , the proposed post - training is also applied to some other review - based tasks such as aspect extraction and aspect sentiment classification in aspect - based sentiment analysis .",research-problem
"In particular , CNN - RNF - LSTM achieves 53.4 % and 90.0 % accuracies on the fine - grained and binary sentiment classification tasks respectively , which match the state - of the - art results on the Stanford Sentiment Treebank .",result
"CNN - RNF - LSTM also obtains competitive results on answer sentence selection datasets , despite the simple model architecture compared to state - of - the - art systems .",result
"Conventional RNN models clearly benefit from max pooling , especially on the task of answer sentence selection .",result
"As a result , RNF - based CNN models perform consistently better than max - pooled RNN models .",result
We consider CNN variants with linear filters and RNFs.,baseline
"For RNFs , we adopt two implementations based on GRUs and LSTMs respectively .",baseline
"We also compare against the following RNN variants : GRU , LSTM , GRU with max pooling , and LSTM with max pooling .",baseline
"To overcome this , we propose to employ recurrent neural networks ( RNNs ) as convolution filters of CNN systems for various NLP tasks .",model
"Our recurrent neural filters ( RNFs ) can naturally deal with language compositionality with a recurrent function that models word relations , and they are also able to implicitly model long - term dependencies .",model
"RNFs are typically applied to word sequences of moderate lengths , which alleviates some well - known drawbacks of RNNs , including their vulnerability to the gradient vanishing and exploding problems .",model
"As a result , RNF - based CNN models can be 3 - 8 x faster than their RNN counterparts .",model
"As in conventional CNNs , the computation of the convolution operation with RNFs can be easily parallelized .",model
We present two RNF - based CNN architectures for sentence classification and answer sentence selection problems .,model
Convolutional Neural Networks with Recurrent Neural Filters,research-problem
"In this work , we model convolution filters with RNNs that naturally capture compositionality and long - term dependencies in language .",research-problem
"As shown in the table , Mazajak model outperformed the current state - of - the - art models on the SemEval and ASTD datasets .",result
"In addition , it achieved a high performance on the ArSAS dataset .",result
"Our reported scores are higher than current top systems for all the evaluation scores , including average recall , F P N , and accuracy .",result
These results confirm that our model choice for our tool represents the current state - of - the - art for Arabic SA .,result
The best performing system in the SemEval 2017 task is the one described in which achieved an F P N of 0.61 .,baseline
"For the ASTD , the best reported results are by who used an ensemble system combining output of CNN and Bi - LSTM architectures , which achieved an F P N of 0.71 .",baseline
"In this paper , we present Mazajak 2 , an Online Arabic sentiment analysis system that utilises deep learning and massive Arabic word embeddings .",model
The system is available as an online API that can be used by other researchers .,model
Mazajak : An Online Arabic Sentiment Analyser,research-problem
Sentiment analysis ( SA ) is one of the most useful natural language processing applications .,research-problem
Sentiment analysis is one of the vital approaches to extract public opinion from large corpora of text .,research-problem
"Work on SA started in early 2000s , particularly with the work of , where they studied the sentiment of movies ' reviews .",research-problem
"We can find that the Majority method is the worst , which means the majority sentiment polarity occupies 53.50 % , 65.00 % and 50 % of all samples on the Restaurant , Laptop and Twitter testing datasets respectively .",result
The Simple SVM model performs better than Majority .,result
Our model achieves significantly better results than feature - enhanced SVM .,result
"Among LSTM based neural networks described in this paper , the basic LSTM approach performs the worst .",result
TD - LSTM obtains an improvement of 1 - 2 % over LSTM when target signals are taken into consideration .,result
"MemNet achieves better results than other models on the Restaurant dataset , since it considers not only the contexts of targets but also the position of each context word related to the target .",result
IAN considers separate representations of targets and obtains better result on the Laptop dataset .,result
GRNN - G3 achieves competitive results on all datasets because of its three - way structure and special gated - RNN model .,result
"In the contrast , our LCR - Rot model achieves the best results on the all datasets among all models .",result
"With the help of feature engineering , the Feature - enhanced SVM achieves much better results .",result
"Majority assigns the sentiment polarity that has the largest probability in the training set ; 2 . Simple SVM is a SVM classifier with simple features such as unigrams and bigrams ; 3 . Feature - enhanced SVM is a SVM classifier with a state - of - the - art feature template which contains n-gram features , parse features and lexicon features ; 4 . LSTM represents a standard LSTM for aspect - based sentiment classification task ; 5 . TD - LSTM adopts two LSTMs to model the left context with target and the right context with target respectively ; 74.30 66.50 66.50 TD- LSTM 75.60 68.10 70.80 AE - LSTM 76.60 68.90 - ATAE - LSTM 77.20 68.70 - GRNN- G3 79.55 * 71.47 * 70.09 * MemNet 79.98 * 70.33 * 70.52 * IAN 78.60 72.10 - LCR - Rot ( our approach ) 81.34 75.24 72.69 : The performance ( classification accuracy ) of different methods on three datasets .",baseline
6 . AE - LSTM is an upgraded version of LSTM .,baseline
7 . ATAE - LSTM is developed based on AE - LSTM .,baseline
8 . GRNN - G3 adopts a Gated - RNN to represent sentence and use a three - way structure to leverage contexts .,baseline
MemNet is a deep memory network which considers the content and position of target .,baseline
"IAN interactively learns attentions in the contexts and targets , and generate the representations for targets and contexts separately .",baseline
"In our work , the dimension of word embedding vectors and hidden state vectors is 300 .",hyperparameter
"All out - ofvocabulary words and weight matrices are randomly initialized by a uniform distribution U ( - 0.1 , 0.1 ) , and all bias are set to zero .",hyperparameter
Tensor Flow is used for implementing our neural network model .,hyperparameter
The paired t- test is used for the significance testing .,hyperparameter
"We use GloVe 2 vectors with 300 dimensions to initialize the word embeddings , the same as .",hyperparameter
"In model training , the learning rate is set to 0.1 , the weight for L 2 - norm regularization is set to 1 e - 5 , and dropout rate is set to 0.5 .",hyperparameter
We train the model use stochastic gradient descent optimizer with momentum of 0.9 .,hyperparameter
"With the attempt to better address the two problems , in this paper we propose a left - center - right separated neural network with rotatory attention mechanism ( LCR - Rot ) .",model
"On this basis , we further propose a rotatory attention mechanism to take into account the interaction between targets and contexts to better represent targets and contexts .",model
"Specifically , we design a left - center - right separated LSTMs that contains three LSTMs , i.e. , left - , center - and right - LSTM , respectively modeling the three parts of a review ( left context , target phrase and right context ) .",model
The target2context attention is used to capture the most indicative sentiment words in left / right contexts .,model
"Subsequently , the context2target attention is used to capture the most important word in the target .",model
This leads to a two - side representation of the target : left - aware target and right - aware target .,model
"Finally , we concatenate the component representations as the final representation of the sentence and feed it into a softmax layer to predict the sentiment polarity .",model
Left - Center - Right Separated Neural Network for Aspect - based Sentiment Analysis with Rotatory Attention,research-problem
"Aspect - based sentiment analysis is a fine - grained classification task in sentiment analysis , identifying sentiment polarity of a sentence expressed toward a target .",research-problem
"In the early studies , methods for the aspect - based sentiment classification task were similar as that used in standard sentiment classification task .",research-problem
"Because of advantages of neural networks , we approach this aspect level sentiment classification problem based on long short - term memory ( LSTM ) neural networks .",approach
"Previous LSTM - based methods mainly focus on modeling texts separately , while our approach models aspects and texts simultaneously using LSTMs .",approach
"Furthermore , the target representation and text representation generated from LSTMs interact with each other by an attention - over - attention ( AOA ) module .",approach
AOA automatically generates mutual attentions not only from aspect - to - text but also text - to - aspect .,approach
That is why we choose AOA to attend to the most important parts in both aspect and sentence .,approach
"In our implementation , we found that the performance fluctuates with different random initialization , which is a well - known issue in training neural networks .",result
"On average , our algorithm is better than these baseline methods and our best trained model outperforms them in a large margin .",result
"Majority is a basic baseline method , which assigns the largest sentiment polarity in the training set to each sample in the test set .",baseline
"LSTM uses one LSTM network to model the sentence , and the last hidden state is used as the sentence representation for the final classification .",baseline
TD - LSTM uses two LSTM networks to model the preceding and following contexts surrounding the aspect term .,baseline
AT - LSTM first models the sentence via a LSTM model .,baseline
ATAE - LSTM further extends AT - LSTM by appending the aspect embedding into each word vector .,baseline
IAN uses two LSTM networks to model the sentence and aspect term respectively .,baseline
"In experiments , we first randomly select 20 % of training data as validation set to tune the hyperparameters .",hyperparameter
"All weight matrices are randomly initialized from uniform distribution U ( ?10 ?4 , 10 ?4 ) and all bias terms are set to zero .",hyperparameter
The L 2 regularization coefficient is set to 10 ? 4 and the dropout keep rate is set to 0.2 .,hyperparameter
The word embeddings are initialized with 300 - dimensional Glove vectors and are fixed during training .,hyperparameter
The dimension of LSTM hidden states is set to 150 .,hyperparameter
The initial learning rate is 0.01 for the Adam optimizer .,hyperparameter
"If the training loss does not drop after every three epochs , we decrease the learning rate by half .",hyperparameter
The batch size is set as 25 .,hyperparameter
"For the out of vocabulary words we initialize them randomly from uniform distribution U ( ? 0.01 , 0.01 ) .",hyperparameter
Aspect Level Sentiment Classification with Attention - over - Attention Neural Networks,research-problem
Aspect - level sentiment classification aims to identify the sentiment expressed towards some aspects given context sentences .,research-problem
"Our performance matches the prior state of the art , however the comparison is not fair .",result
"For the text transcript of each of the utterance we use pretrained Glove embeddings of dimension 300 , along with the maximum sequence length of 500 to obtain a ( 500,300 ) vector for each utterance .",hyperparameter
"For the Mocap data , for each different mode such as face , hand , head rotation we sample all the feature values between the start and finish time values and split them into 200 partitioned arrays .",hyperparameter
"We then average each of the 200 arrays along the columns ( 165 for faces , 18 for hands , and 6 for rotation ) , and finally concatenate all of them to obtain ( 200,189 ) dimension vector for each utterance .",hyperparameter
MULTI - MODAL EMOTION RECOGNITION ON IEMOCAP WITH NEURAL NETWORKS,research-problem
Emotion recognition has become an important field of research in human computer interactions and there is a growing need for automatic emotion recognition systems .,research-problem
"For our RGNN in all experiments , we empirically set the number of convolutional layers L = 2 , dropout rate of 0.7 at the output fully - connected layer , and batch size of 16 .",hyperparameter
"We use Adam optimization with default values , i.e. , ? 1 = 0.9 and ? 2 = 0.999 .",hyperparameter
"We only tune the output feature dimension d , label noise level , learning rate ? , L1 regularization factor ? , and L2 regularization for each experiment .",hyperparameter
"In this paper , we propose a regularized graph neural network ( RGNN ) aiming to address all three aforementioned challenges .",model
"Inspired by , , we consider each channel in EEG signals as a node in our graph .",model
"Our RGNN model extends the simple graph convolution network ( SGC ) and leverages the topological structure of EEG signals , i.e. , according to the economy of brain network organization , we propose a biologically supported sparse adjacency matrix to capture both local and global inter-channel relations .",model
"Local interchannel relations connect nearby groups of neurons and may reveal anatomical connectivity at macroscale , .",model
"Global inter-channel relations connect distant groups of neurons between the left and right hemispheres and may reveal emotion - related functional connectivity , .",model
EEG - Based Emotion Recognition Using Regularized Graph Neural Networks,research-problem
"E MOTION recognition is an important subarea of affective computing , which focuses on recognizing human emotions based on a variety of modalities , such as audio- visual expressions , body language , physiological signals , etc .",research-problem
"The two major designs in our adjacency matrix A , i.e. , global connection and symmetric adjacency matrix designs , are helpful in recognizing emotions .",ablation-analysi
"The global connection models the asymmetric difference between neuronal activities in the left and right hemispheres and have been shown to reveal certain emotions , , .",ablation-analysi
"Our NodeDAT regularizer has a noticeable positive impact on the performance of our model , which demonstrates that domain adaptation is significantly helpful in crosssubject classification .",ablation-analysi
DL regularizer improves performance of our model by around 3 % in accuracy on both datasets .,ablation-analysi
"In addition , if NodeDAT is removed , the performance of our model has a greater variance , demonstrating the importance of NodeDAT in improving the robustness of our model against cross - subject variations .",ablation-analysi
"In our d-TBCNN model , the number of units is 300 for convolution and 200 for the last hidden layer .",hyperparameter
Dropout is further applied to both weights and embeddings .,hyperparameter
"All hidden layers are dropped out by 50 % , and embeddings 40 % .",hyperparameter
"Word embeddings are 300 dimensional , pretrained ourselves using word2vec To train our model , we compute gradient by back - propagation and apply stochastic gradient descent with mini-batch 200 .",hyperparameter
We use ReLU as the activation function .,hyperparameter
"For regularization , we add 2 penalty for weights with a coefficient of 10 ?5 .",hyperparameter
"In this paper , we propose a novel neural architecture for discriminative sentence modeling , called the Tree - Based Convolutional Neural Network ( TBCNN ) .",model
"Our models can leverage different sentence parsing trees , e.g. , constituency trees and dependency trees .",model
"The model variants are denoted as c- TBCNN and d - TBCNN , respectively .",model
"The idea of tree - based convolution is to apply a set of subtree feature detectors , sliding over the entire parsing tree of a sentence ; then pooling aggregates these extracted feature vectors by taking the maximum value in each dimension .",model
Discriminative Neural Sentence Modeling by Tree - Based Convolution,research-problem
"Comparing this to CNNs with GloVe / fastText embeddings , where Glo Ve is used for English , and fastText is used for all other languages , we observe substantial improvements across all datasets .",result
This shows that word vectors do tend to convey pertinent word semantics signals that enable models to generalize better .,result
Note also that the accuracy using GloVe on the English movies review dataset is consistent with numbers reported in previous work .,result
"Next , we consider our DM - MCNNs with their dual - module mechanism to take advantage of transfer learning .",result
We observe fairly consistent and sometimes quite substan - tial gains over CNNs with just the GloVe / fastText vectors .,result
"Although the automatically projected cross - lingual embeddings are very noisy and limited in their coverage , particularly with respect to inflected forms , our model succeeds in exploiting them to obtain substantial gains in several different languages and domains .",result
"In this paper , we investigate how extrinsic signals can be incorporated into deep neural networks for sentiment analysis .",model
"In our paper , we instead consider word embeddings specifically specialized for the task of sentiment analysis , studying how they can lead to stronger and more consistent gains , despite the fact that the embeddings were obtained using out - of - domain data .",model
We instead propose a bespoke convolutional neural network architecture with a separate memory module dedicated to the sentiment embeddings .,model
A Helping Hand : Transfer Learning for Deep Sentiment Analysis,research-problem
"Over the past decades , sentiment analysis has grown from an academic endeavour to an essential analytics tool .",research-problem
"In recent years , deep neural architectures based on convolutional or recurrent layers have become established as the preeminent models for supervised sentiment polarity classification .",research-problem
"Aiming to automatically extract aspects from the text efficiently and analyze the sentiment polarity of aspects simultaneously , this paper proposes a multi-task learning model for aspect - based sentiment analysis .",model
The LCF - ATEPC 3 model proposed in this paper is a novel multilingual and multi-task - oriented model .,model
"The proposed model is based on multi-head self - attention ( MHSA ) and integrates the pre-trained and the local context focus mechanism , namely LCF - ATEPC .",model
"By training on a small amount of annotated data of aspect and their polarity , the model can be adapted to a large - scale dataset , automatically extracting the aspects and predicting the sentiment polarities .",model
Graphical Abstract A Multi-task Learning Model for Chinese - oriented Aspect Polarity Classification and Aspect Term Extraction Highlights A Multi-task Learning Model for Chinese - oriented Aspect Polarity Classification and Aspect Term Extraction A Multi-task Learning Model for Chinese - oriented Aspect Polarity Classification and Aspect Term Extraction,research-problem
Aspect - based sentiment analysis ( ABSA ) task is a multi - grained task of natural language processing and consists of two subtasks : aspect term extraction ( ATE ) and aspect polarity classification ( APC ) .,research-problem
Most of the existing work focuses on the subtask of aspect term polarity inferring and ignores the significance of aspect term extraction .,research-problem
"By integrating the domain - adapted BERT model , the LCF - ATEPC model achieved the state - of the - art performance of aspect term extraction and aspect polarity classification in four Chinese review datasets .",research-problem
"Aspect - based sentiment analysis ; Pontiki , Galanis , Papageorgiou , Androutsopoulos , Manandhar , AL - Smadi , Al - Ayyoub , Zhao , Qin , De Clercq , Hoste , Apidianaki , Tannier , Loukachevitch , Kotelnikov , Bel , Jimnez - Zafra and Eryigit ( 2016 ) ( ABSA ) is a fine - grained task compared with traditional sentiment analysis , which requires the model to be able to automatic extract the aspects and predict the polarities of all the aspects .",research-problem
The APC task is a kind of classification problem .,research-problem
"The researches concerning APC tasks is more abundant than the ATE task , and a large number of deep learning - based models have been proposed to solve APC problems , such as the models ; ; ; based on long short - term memory ( LSTM ) and the methodologies based on transformer .",research-problem
The codes for this paper are available at https://github.com/yangheng95/LCF-ATEPC,code
"LSTM : Standard LSTM can not capture any aspect information in sentence , so it must get the same ( a ) the aspect of this sentence : service ( b ) the aspect of this sentence : food : Attention Visualizations .",result
"Since it can not take advantage of the aspect information , not surprisingly the model has worst performance .",result
TD - LSTM : TD - LSTM can improve the performance of sentiment classifier by treating an aspect as a target .,result
"Since there is no attention mechanism in TD - LSTM , it can not "" know "" which words are important for a given aspect .",result
It is worth noting that TC - LSTM performs worse than LSTM and TD - LSTM in .,result
"ATAE - LSTM not only addresses the shortcoming of the unconformity between word vectors and aspect embeddings , but also can capture the most important information in response to a given aspect .",result
We apply the proposed model to aspect - level sentiment classification .,hyperparameter
"In our experiments , all word vectors are initialized by Glove 1 .",hyperparameter
The word embedding vectors are pre-trained on an unlabeled corpus whose size is about 840 billion .,hyperparameter
"The other parameters are initialized by sampling from a uniform distribution U (?? , ? ) .",hyperparameter
"The dimension of word vectors , aspect embeddings and the size of hidden layer are 300 .",hyperparameter
The length of attention weights is the same as the length of sentence .,hyperparameter
Theano is used for implementing our neural network models .,hyperparameter
"We trained all models with a batch size of 25 examples , and a momentum of 0.9 , L 2 - regularization weight of 0.001 and initial learning rate of 0.01 for AdaGrad .",hyperparameter
"In this paper , we propose an attention mechanism to enforce the model to attend to the important part of a sentence , in response to a specific aspect .",model
We design an aspect - tosentence attention mechanism that can concentrate on the key part of a sentence given the aspect .,model
We explore the potential correlation of aspect and sentiment polarity in aspect - level sentiment classification .,model
"In order to capture important information in response to a given aspect , we design an attentionbased LSTM .",model
Attention - based LSTM for Aspect - level Sentiment Classification,research-problem
Aspect - level sentiment classification is a finegrained task in sentiment analysis .,research-problem
"In this paper , we deal with aspect - level sentiment classification and we find that the sentiment polarity of a sentence is highly dependent on both content and aspect .",research-problem
"Our two models achieve the best performance when compared to these baselines as shown in , which shows that our proposed neural units effectively captures the aspect - specific features .",result
"Surprisingly , a vanilla CNN works quite well on this problem .",result
"It even beats these welldesigned LSTM models , which further proves that using CNN - based methods is a direction worth exploring .",result
"Compared to one recently proposed model AF - LSTM , our method achieve 2 % - 5 % improvements .",result
TD - LSTM uses two LSTM networks to model the preceding and following contexts surrounding the aspect term .,baseline
AT - LSTM combines the sentence hidden states from a LSTM with the aspect term embedding to generate the attention vector .,baseline
ATAE - LSTM further extends AT - LSTM by appending the aspect embedding into each word vector .,baseline
AF - LSTM introduces a word - aspect fusion attention to learn associative relationships between aspect and context words .,baseline
CNN uses the architecture proposed in without explicitly considering aspect .,baseline
"We use rectifier as non-linear function f in the CNN g , CNN t and sigmoid in the CNN s , filter window sizes of 1 , 2 , 3 , 4 with 100 feature maps each , l 2 regularization term of 0.001 and minibatch size of 25 .",hyperparameter
Parameterized filters and gates have the same size and number as normal filters .,hyperparameter
"They are generated uniformly by CNN with window sizes of 1 , 2 , 3 , 4 , eg. among 100 parameterized filters with size 3 , 25 of them are generated by aspect CNN with filter size 1 , 2 , 3 , 4 respectively .",hyperparameter
The word embeddings are initialized with 300 - dimensional Glove vectors and are fixed during training .,hyperparameter
The dropout rate is chosen as 0.3 .,hyperparameter
Training is done through mini-batch stochastic gradient descent with Adam update rule .,hyperparameter
The initial learning rate is 0.001 .,hyperparameter
"If the training loss does not drop after every three epochs , we decrease the learning rate by half .",hyperparameter
"For the out of vocabulary words we initialize them randomly from uniform distribution U ( ? 0.01 , 0.01 ) .",hyperparameter
We apply dropout on the final classification features of PG - CNN .,hyperparameter
We adopt early stopping based on the validation loss on development sets .,hyperparameter
Parameterized Convolutional Neural Networks for Aspect Level Sentiment Classification,research-problem
Continuous growing of user generated text in social media platforms such as Twitter drives sentiment classification increasingly popular .,research-problem
"Differing from general sentiment classification , aspect level sentiment classification identifies opinions from text about specific entities and their aspects .",research-problem
"We observe that PRET is very helpful , and consistently gives a 1 - 3 % increase in accuracy over LSTM + ATT across all datasets .",result
"MULT gives similar performance as LSTM + ATT on D1 and D2 , but improvements can be clearly observed for D3 and D4 .",result
The combination ( PRET + MULT ) over all yields better results .,result
( 2 ) The numbers of neutral examples in the test sets of D3 and D4 are very small .,result
"In all experiments , 300 - dimension Glo Ve vectors are used to initialize E and E when pretraining is not conducted for weight initialization .",hyperparameter
These vectors are also used for initializing E in the pretraining phase .,hyperparameter
We randomly sample 20 % of the original training data from the aspectlevel dataset as the development set and only use the remaining 80 % for training .,hyperparameter
"For all experiments , the dimension of LSTM hidden vectors is set to 300 , ?",hyperparameter
"is set to 0.1 , and we use dropout with probability 0.5 on sentence / document representations before the output layer .",hyperparameter
We use RMSProp as the optimizer with the decay rate set to 0.9 and the base learning rate set to 0.001 .,hyperparameter
The mini - batch size is set to 32 .,hyperparameter
"Specifically , we explore two transfer methods to incorporate this sort of knowledge - pretraining and multi-task learning .",model
Exploiting Document Knowledge for Aspect - level Sentiment Classification,research-problem
Our source code can be obtained from https://github.com/ruidan/Aspect-level-sentiment.,code
"( 2 ) Overall , transfers of the LSTM and embedding layer are more useful than the output layer .",ablation-analysi
( 3 ) Transfer of the embedding layer is more helpful on D3 and D4 .,ablation-analysi
Sentiment information is not adequately captured by Glo Ve word embeddings .,ablation-analysi
We focus in on the task of sentiment analysis and attempt to learn an unsupervised representation that accurately contains this concept .,approach
"As an approach , we consider the popular research benchmark of byte ( character ) level language modelling due to its further simplicity and generality .",approach
We train on a very large corpus picked to have a similar distribution as our task of interest .,approach
Representation learning ) plays a critical role in many modern machine learning systems .,research-problem
We focus in on the task of sentiment analysis and attempt to learn an unsupervised representation that accurately contains this concept .,research-problem
"As expected , trained contextual unimodal features help the hierarchical fusion framework to outperform the non-hierarchical framework .",result
"The non-hierarchical model outperforms the baseline uni - SVM , which confirms that it is the contextsensitive learning paradigm that plays the key role in improving performance over the baseline .",result
"Since bc - LSTM has access to both the preceding and following information of the utterance sequence , it performs consistently better on all the datasets over sc - LSTM .",result
The performance improvement is in the range of 0.3 % to 1.5 % on MOSI and MOUD datasets .,result
Every LSTM network variant has outperformed the baseline uni - SVM on all the datasets by the margin of 2 % to 5 % ( see ) .,result
It is to be noted that both sc - LSTM and bc - LSTM perform quite well on the multimodal emotion recognition and sentiment analysis datasets .,result
"On the IEMOCAP dataset , the performance improvement of bc - LSTM and sc - LSTM over h- LSTM is in the range of 1 % to 5 % .",result
Experimental results in show that the proposed method outperformes by a significant margin .,result
"In this paper , we discard such an oversimplifying hypothesis and develop a framework based on long shortterm memory ( LSTM ) that takes a sequence of utterances as input and extracts contextual utterancelevel features .",model
"Our model preserves the sequential order of utterances and enables consecutive utterances to share information , thus providing contextual information to the utterance - level sentiment classification process .",model
Context - Dependent Sentiment Analysis in User- Generated Videos,research-problem
"Multimodal sentiment analysis is a developing area of research , which involves the identification of sentiments in videos .",research-problem
"Sentiment analysis is a ' suitcase ' research problem that requires tackling many NLP sub - tasks , e.g. , aspect extraction , named entity recognition , concept extraction , sarcasm detection , personality recognition , and more .",research-problem
"Emotion recognition further breaks down the inferred polarity into a set of emotions conveyed by the subjective data , e.g. , positive sentiment can be caused by joy or anticipation , while negative sentiment can be caused by fear or disgust .",research-problem
"Recently , a number of approaches to multimodal sentiment analysis , producing interesting results , have been proposed .",research-problem
We then build a neural network model with four components .,model
The model uses ELMo ( which stands for Embeddings from Language Models ) pre-trained contextual embeddings as an input and builds sequence representation vectors that are used to predict the relation between the question pairs .,model
Tha3aroon at NSURL - 2019 Task 8 : Semantic Question Similarity in Arabic,research-problem
"In this paper , we describe our team 's effort on the semantic text question similarity task of NSURL 2019 .",research-problem
Semantic Text Similarity ( STS ) problems are both real - life and challenging .,research-problem
"For example , in the paraphrase identification task , STS is used to predict if one sentence is a paraphrase of the other or not .",research-problem
A new task has been proposed by Mawdoo3 1 company with a new dataset provided by their data annotation team for Semantic Question Similarity ( SQS ) for the Arabic language .,research-problem
"SQS is a variant of STS , which aims to compare a pair of questions and determine whether they have the same meaning or not .",research-problem
All experiments discussed in this work have been done on the Google Colab 7 environment using Tesla T4 GPU accelerator with the following hyperparameters :,experimental-setup
"We construct and publicly release a dataset of 25,100 queries annotated with the probability of being a well - formed natural language question ( 2.1 ) .",dataset
Our dataset ise available for download at http://goo.gl/language/ query-wellformedness .,dataset
"The best performance obtained is 70.7 % while using word - 1 , 2 - grams and POS - 1 , 2 , 3 - grams as features .",result
"Although character - 3 , 4 grams gave improvement over word unigrams and bigrams , the performance did not sustain when combined with POS tags .",result
Using POS n-grams gave a strong boost of 5.2 points over word unigrams and bigrams .,result
The majority class baseline is 61.5 % which corresponds to all queries being classified non-wellformed .,baseline
The question word baseline that classifies any query starting with a question word word n-grams char n-grams POS n -grams pwf ( q ) :,baseline
"Thus , in this paper we present a model to predict whether a given query is a well - formed natural language question .",model
We then train a feed - forward neural network classifier that uses the lexical and syntactic features extracted from the query on this data ( 2.2 ) .,model
Identifying Well - formed Natural Language Questions,research-problem
"To address these limitations , we introduce Sci - Cite , a new dataset of citation intents that is significantly larger , more coarse - grained and generaldomain compared with existing datasets .",dataset
"We consider three intent categories outlined in : BACK - GROUND , METHOD and RESULTCOMPARISON .",dataset
Citation intent of sentence extractions was labeled through the crowdsourcing platform .,dataset
"Citation contexts were annotated by 850 crowdsource workers who made a total of 29,926 annotations and individually made between 4 and 240 annotations .",dataset
"Each sentence was annotated , on average , 3.74 times .",dataset
"This resulted in a total 9,159 crowdsourced instances which were divided to training and validation sets with 90 % of the data used for the training set .",dataset
We observe that our scaffold - enhanced models achieve clear improvements over the state - of - the - art approach on this task .,result
Generally we observe that results on categories with more number of instances are higher .,result
"Starting with the ' BiLSTM - Attn ' baseline with a macro F1 score of 51.8 , adding the first scaffold task in ' BiLSTM - Attn + section title scaffold ' improves the F1 score to 56.9 (?= 5.1 ) .",result
Adding the second scaffold in ' BiLSTM - Attn + citation worthiness scaffold ' also results in similar improvements : 56.3 (?= 4.5 ) .,result
Adding both scaffolds results in further improvements .,result
"When both scaffolds are used simultaneously in ' BiLSTM - Attn + both scaffolds ' , the F1 score further improves to 63.1 ( ?= 11.3 ) , suggesting that the two tasks provide complementary signal that is useful for citation intent prediction .",result
"The best result is achieved when we also add ELMo vectors to the input representations in ' BiLSTM - Attn w / ELMo + both scaffolds ' , achieving an F1 of 67.9 , a major improvement from the previous state - of - the - art results of 54.6 ( ?= 13.3 ) .",result
And the best results are obtained by using ELMo representation in addition to both scaffolds .,result
Each scaffold task improves model performance .,result
"We note that the scaffold tasks provide major contributions on top of the ELMo - enabled baseline ( ?= 13.6 ) , demonstrating the efficacy of using structural scaffolds for citation intent prediction .",result
"We also experimented with adding features used in to our best model and not only we did not see any improvements , but we observed at least 1.7 % decline in performance .",result
"For example on ACL - ARC , the results on the BACKGROUND category are the highest as this category is the most common .",result
"Conversely , the results on the FUTUREWORK category are the lowest .",result
This category has the fewest data points ( see distribution of the categories in ) and thus it is harder for the model to learn the optimal parameters for correct classification in this category .,result
We implement our proposed scaffold framework using the AllenNLP library .,hyperparameter
"For word representations , we use 100 - dimensional GloVe vectors trained on a corpus of 6B tokens from Wikipedia and Gigaword .",hyperparameter
"For contextual representations , we use ELMo vectors released by with output dimension size of 1,024 which have been trained on a dataset of 5.5 B tokens .",hyperparameter
"For each of scaffold tasks , we use a single - layer MLP with 20 hidden nodes , ReLU activation and a Dropout rate of 0.2 between the hidden and input layers .",hyperparameter
We use a single - layer BiLSTM with a hidden dimension size of 50 for each direction 11 .,hyperparameter
We use Beaker 12 for running the experiments .,hyperparameter
Batch size is 8 for ACL - ARC dataset and 32 for SciCite dataset ( recall that SciCite is larger than ACL - ARC ) .,hyperparameter
"To this end , we propose a neural multitask learning framework to incorporate knowledge into citations from the structure of scientific papers .",model
"In particular , we propose two auxiliary tasks as structural scaffolds to improve citation intent prediction : 1 ( 1 ) predicting the section title in which the citation occurs and ( 2 ) predicting whether a sentence needs a citation .",model
"Our contributions are : ( i ) we propose a neural scaffold framework for citation intent classification to incorporate into citations knowledge from structure of scientific papers ; ( ii ) we achieve a new state - of - the - art of 67.9 % F1 on the ACL - ARC citations benchmark , an absolute 13.3 % increase over the previous state - of - the - art ; and ( iii ) we introduce SciCite , a new dataset of citation intents which is at least five times as large as existing datasets and covers a variety of scientific domains .",model
"On two datasets , we show that the proposed neural scaffold model outperforms existing methods by large margins .",model
Structural Scaffolds for Citation Intent Classification in Scientific Publications,research-problem
"Identifying the intent of a citation in scientific papers ( e.g. , background information , use of methods , comparing results ) is critical for machine reading of individual publications and automated analysis of the scientific literature .",research-problem
"In this work , we approach the problem of citation intent classification by modeling the language expressed in the citation context .",research-problem
Our code and data are available at : https://github.com/ allenai/scicite .,code
"In this paper , we propose the usage of translations as compelling and effective domain - free contexts , or contexts that are always available no matter what the task domain is .",approach
"In this paper , we propose a method to mitigate the possible problems when using translated sentences as context based on the following observations .",approach
"Based on these observations , we present a neural attentionbased multiple context fixing attachment ( MCFA ) .",approach
"MCFA is a series of modules that uses all the sentence vectors ( e.g. Arabic , English , Korean , etc. ) as context to fix a sentence vector ( e.g. Korean ) .",approach
MCFA computes two sentence usability metrics to control the noise when fixing vectors : ( a ) self usability ? i ( a ) weighs the confidence of using sentence a in solving the task .,approach
"( b ) relative usability ? r ( a , b ) weighs the confidence of using sentence a in fixing sentence b.",approach
"( 1 ) MCFA is attached after encoding the sentence , which makes it widely adaptable to other models .",approach
"( 3 ) MCFA moves the vectors inside the same space , thus preserves the meaning of vector dimensions .",approach
Tokenization is done using the polyglot library 7 .,hyperparameter
Training is done via stochastic gradient descent over shuffled mini-batches with the Adadelta update rule .,hyperparameter
We experiment on using only one additional context ( N = 1 ) and using all ten languages at once ( N = 10 ) .,hyperparameter
"For our CNN , we use rectified linear units and three filters with different window sizes h = 3 , 4 , 5 with 100 feature maps each , following .",hyperparameter
"For the final sentence vector , we concatenate the feature maps to get a 300 - dimension vector .",hyperparameter
We use dropout on all nonlinear connections with a dropout rate of 0.5 .,hyperparameter
"We also use an l 2 constraint of 3 , following for accurate comparisons .",hyperparameter
We use FastText pre-trained vectors 8 for all our data sets and their corresponding additional context .,hyperparameter
"During training , we use mini-batch size of 50 .",hyperparameter
We perform early stopping using a random 10 % of the training set as the development set .,hyperparameter
Translations as Additional Contexts for Sentence Classification,research-problem
"The token embeddings were pre-trained on a large corpus combining Wikipedia , PubMed , and PMC texts ( Moen and Ananiadou , 2013 ) using the word2vec tool 4 ( denoted as "" Word2vec- wiki+P.M. "" ) .",hyperparameter
The learning rate is initially set as 0.003 and decayed by 0.9 after each epoch .,hyperparameter
"The window sizes of the CNN encoder in the sentence encoding layer are 2 , 3 , 4 and 5 .",hyperparameter
They are fixed during the training phase to avoid over-fitting .,hyperparameter
"The model is trained using the Adam optimization method ( Kingma and Ba , 2014 ) .",hyperparameter
"For regularization , dropout ( Srivastava et al. , 2014 ) is applied to each layer .",hyperparameter
"To reduce this gap , we adopted the dropout with expectation - linear regularization introduced by to explicitly control the inference gap and thus improve the generaliza - tion performance .",hyperparameter
Hyperparameters were optimized via grid search based on the validation set and the best configuration is shown in .,hyperparameter
"In this work , we present a hierarchical neural network model for the sequential sentence classification task , which we call a hierarchical sequential labeling network ( HSLN ) .",model
"Our model first uses a RNN or CNN layer to individually encode the sentence representation from the sequence of word embeddings , then uses another bi - LSTM layer to take as input the individual sentence representation and output the contextualized sentence representation , subsequently uses a single - hidden - layer feed - forward network to transform the sentence representation to the probability vector , and finally optimizes the predicted label sequence jointly via a CRF layer .",model
Hierarchical Neural Networks for Sequential Sentence Classification in Medical Scientific Abstracts,research-problem
"This hampers the traditional sentence classification approaches to the problem of sequential sentence classification , where structured prediction is needed for better over all classification performance .",research-problem
"As can be seen from , our HSLN - CNN model uni-formly suffers a little more from the component removal than the HSLN - RNN model , indicating that the HSLN - RNN model is more robust .",ablation-analysi
"Last but not the least , the dropout regularization and attention - based pooling components we add to our system can help further improve the model in a limited extent . :",ablation-analysi
"When the context enriching layer is removed , both models experience the most significant performance drop and can only be on par with the previous stateof - the - art results , strongly demonstrating that this proposed component is the key to the performance improvement of our model .",ablation-analysi
"Furthermore , even without the label sequence optimization layer , our model still significantly outperforms the best published methods that are empowered by this layer , indicating that the context enriching layer we propose can help optimize the label sequence by considering the context information from the surrounding sentences .",ablation-analysi
We report averaged scores across five different runs of the model training .,result
"Overall , our span - based ensemble model using ELMo achieved the best F1 scores , 87.4 F1 and 87.0 F1 on the CoNLL - 2005 and CoNLL - 2012 datasets , respectively .",result
Our single and ensemble models using ELMO achieved the best F 1 scores on all the test sets except the Brown test set .,result
"In comparison with the CRF - based single model , our span - based single model consistently yielded better F 1 scores regardless of the word embeddings , SENNA and ELMO .",result
"For comparison , as a model based on BIO tagging approaches , we use the BiLSTM - CRF model proposed by .",baseline
"As the base function f base , we use 4 BiLSTM layers with 300 dimensional hidden units .",hyperparameter
"As the objective function , we use the crossentropy L ? in Eq. 3 with L2 weight decay ,",hyperparameter
"To optimize the model parameters , we use Adam .",hyperparameter
"To validate the model performance , we use two types of word embeddings .",hyperparameter
"Typical word embeddings , SENNA 6 ( Collobert et al. , 2011 ) Contextualized word embeddings , ELMo 7 SENNA and ELMo can be regarded as different types of embeddings in terms of the context sensitivity .",hyperparameter
These embeddings are fixed during training .,hyperparameter
"To fill this gap , this paper presents a simple and accurate span - based model .",model
"Inspired by recent span - based models in syntactic parsing and coreference resolution , our model directly scores all possible labeled spans based on span representations induced from neural networks .",model
"At decoding time , we greedily select higher scoring labeled spans .",model
The model parameters are learned by optimizing loglikelihood of correct labeled spans .,model
A Span Selection Model for Semantic Role Labeling,research-problem
We present a simple and accurate span - based model for semantic role labeling ( SRL ) .,research-problem
"Given a sentence and a target predicate , SRL systems have to predict semantic arguments of the predicate .",research-problem
"In Remarkably , we get 74.1 F 1 score on the out - of - domain dataset , which outperforms the previous state - of - the - art system by 2.0 F 1 score .",result
"When ensembling 5 models with FFN nonlinear sub - layers , our approach achieves an F 1 score of 84.6 and 83.9 on the two datasets respectively , which has an absolute improvement of 1.4 and 0.5 over the previous state - of - the - art .",result
These results are consistent with our intuition that the self - attention layers is helpful to capture structural information and long distance dependencies .,result
We initialize the weights of all sub-layers as random orthogonal matrices .,hyperparameter
"For other parameters , we initialize them by sampling each element from a Gaussian distribution with mean 0 and variance 1 ? d .",hyperparameter
The embedding layer can be initialized randomly or using pre-trained word embeddings .,hyperparameter
The dimension of word embeddings and predicate mask embeddings is set to 100 and the number of hidden layers is set to 10 .,hyperparameter
The number of heads h is set to 8 .,hyperparameter
Dropout layers are added before residual connections with a keep probability of 0.8 .,hyperparameter
"Dropout is also applied before the attention softmax layer and the feed - froward ReLU hidden layer , and the keep probabilities are set to 0.9 .",hyperparameter
Learning Parameter optimization is performed using stochastic gradient descent .,hyperparameter
Each SGD contains a mini-batch of approximately 4096 tokens for the CoNLL - 2005 dataset and 8192 tokens for the CoNLL - 2012 dataset .,hyperparameter
The learning rate is initialized to 1.0 .,hyperparameter
We set the number of hidden units d to 200 .,hyperparameter
We also employ label smoothing technique with a smoothing value of 0.1 during training .,hyperparameter
We adopt Adadelta ) ( = 10 6 and ? = 0.95 ) as the optimizer .,hyperparameter
"To avoid exploding gradients problem , we clip the norm of gradients with a predefined threshold 1.0 .",hyperparameter
"After training 400 k steps , we halve the learning rate every 100 K steps .",hyperparameter
We train all models for 600 K steps .,hyperparameter
"For DEEP - ATT with FFN sub - layers , the whole training stage takes about two days to finish on a single Titan X GPU , which is 2.5 times faster than the previous approach ) .",hyperparameter
"To address these problems above , we present a deep attentional neural network ( DEEPATT ) for the task of SRL 1 .",model
Our models rely on the self - attention mechanism which directly draws the global dependencies of the inputs .,model
"In contrast to RNNs , a major advantage of self - attention is that it conducts direct connections between two arbitrary tokens in a sentence .",model
"Therefore , distant elements can interact with each other by shorter paths ( O ( 1 ) v.s. O ( n ) ) , which allows unimpeded information flow through the network .",model
"Self - attention also provides a more flexible way to select , represent and synthesize the information of the inputs and is complementary to RNN based models .",model
"Along with self - attention , DEEP - ATT comes with three variants which uses recurrent ( RNN ) , convolutional ( CNN ) and feed - forward ( FFN ) neural network to further enhance the representations .",model
Deep Semantic Role Labeling with Self - Attention,research-problem
Semantic Role Labeling ( SRL ) is believed to be a crucial step towards natural language understanding and has been widely studied .,research-problem
"In this paper , we present a simple and effective architecture for SRL which aims to address these problems .",research-problem
"Semantic Role Labeling is a shallow semantic parsing task , whose goal is to determine essentially "" who did what to whom "" , "" when "" and "" where "" .",research-problem
"As shown in , 2 our joint model outperforms the previous best pipeline system by an F1 difference of anywhere between 1.3 and 6.0 in every setting .",result
"On all datasets , our model is able to predict over 40 % of the sentences completely correctly .",result
We propose an end - to - end approach for predicting all the predicates and their argument spans in one forward pass .,model
"Our model builds on a recent coreference resolution model , by making central use of learned , contextualized span representations .",model
We use these representations to predict SRL graphs directly over text spans .,model
"Each edge is identified by independently predicting which role , if any , holds between every possible pair of text spans , while using aggressive beam 1 Code and models : https://github.com/luheng/lsgn pruning for efficiency .",model
The final graph is simply the union of predicted SRL roles ( edges ) and their associated text spans ( nodes ) .,model
"The span representations also generalize the token - level representations in BIObased models , letting the model dynamically decide which spans and roles to include , without using previously standard syntactic features .",model
"To the best of our knowledge , this is the first span - based SRL model that does not assume that predicates are given .",model
Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling,research-problem
"Semantic role labeling ( SRL ) captures predicateargument relations , such as "" who did what to whom . """,research-problem
"Recent high - performing SRL models are BIO - taggers , labeling argument spans for a single predicate at a time ( as shown in .",research-problem
"Each edge is identified by independently predicting which role , if any , holds between every possible pair of text spans , while using aggressive beam 1 Code and models : https://github.com/luheng/lsgn pruning for efficiency .",code
Our ensemble ( PoE ) has an absolute improvement of 2.1 F1 on both CoNLL 2005 and CoNLL 2012 over the previous state of the art .,result
Our single model also achieves more than a 0.4 improvement on both datasets .,result
"In comparison with the best reported results , our percentage of completely correct predicates improves by 5.9 points .",result
"While the continuing trend of improving SRL without syntax seems to suggest that neural end - to - end systems no longer needs parsers , our analysis in Section 4.4 will show that accurate syntactic information can improve these deep models .",result
"Our network consists of 8 BiLSTM layers ( 4 forward LSTMs and 4 reversed LSTMs ) with 300 dimensional hidden units , and a softmax layer for predicting the output distribution .",hyperparameter
"All the weight matrices in BiL - STMs are initialized with random orthonormal matrices as described in. ,",hyperparameter
All tokens are lower - cased and initialized with 100 - dimensional GloVe embeddings pre-trained on 6B tokens and updated during training .,hyperparameter
Tokens that are not covered by GloVe are replaced with a randomly initialized UNK embedding .,hyperparameter
All the models are trained for 500 epochs with early stopping based on development results .,hyperparameter
"Training We use Adadelta ( Zeiler , 2012 ) with = 1e ?6 and ? = 0.95 and mini-batches of size 80 .",hyperparameter
We set RNN - dropout probability to 0.1 and clip gradients with norm larger than 1 .,hyperparameter
Deep Semantic Role Labeling : What Works and What 's Next,research-problem
"We introduce a new deep learning model for semantic role labeling ( SRL ) that significantly improves the state of the art , along with detailed analyses to reveal its strengths and limitations .",research-problem
Recently breakthroughs involving end - to - end deep models for SRL without syntactic input seem to overturn the long - held belief that syntactic parsing is a prerequisite for this task .,research-problem
"Without dropout , the model overfits at around 300 epochs at 78 F1 .",ablation-analysi
"Orthonormal parameter initialization is surprisingly important - without this , the model achieves only 65 F1 within the first 50 epochs .",ablation-analysi
All 8 layer ablations suffer a loss of more than 1.7 in absolute F 1 compared to the full model .,ablation-analysi
"We present results on the CoNLL - 2005 shared task and the CoNLL - 2012 English subset of OntoNotes 5.0 , achieving state - of - the - art results for a single model with predicted predicates on both corpora .",result
"We demonstrate that our models benefit from injecting state - of - the - art predicted parses at test time ( + D&M ) by fixing the attention to parses predicted by Dozat and Manning ( 2017 ) , the winner of the 2017 CoNLL shared task which we re-train using ELMo embeddings .",result
"For models using GloVe embeddings , our syntax - free SA model already achieves a new state - of - the - art by jointly predicting predicates , POS and SRL .",result
"LISA with it s own parses performs comparably to SA , but when supplied with D&M parses LISA out - performs the previous state - of - the - art by 2.5 F1 points .",result
"On the out - ofdomain Brown test set , LISA also performs comparably to its syntax - free counterpart with its own parses , but with D&M parses LISA performs exceptionally well , more than 3.5 F1 points higher than He et al ..",result
Incorporating ELMo em-beddings improves all scores .,result
"We train the model using Nadam ( Dozat , 2016 ) SGD combined with the learning rate schedule in .",hyperparameter
"In addition to MTL , we regularize our model using dropout .",hyperparameter
We use gradient clipping to avoid exploding gradients .,hyperparameter
"In response , we propose linguistically - informed self - attention ( LISA ) : a model that combines multi-task learning with stacked layers of multi-head self - attention ; the model is trained to : ( 1 ) jointly predict parts of speech and predicates ; ( 2 ) perform parsing ; and ( 3 ) attend to syntactic parse parents , while ( 4 ) assigning semantic role labels .",model
"Whereas prior work typically requires separate models to provide linguistic analysis , including most syntaxfree neural models which still rely on external predicate detection , our model is truly end - to - end : earlier layers are trained to predict prerequisite parts - of - speech and predicates , the latter of which are supplied to later layers for scoring .",model
"Though prior work re-encodes each sentence to predict each desired task and again with respect to each predicate to perform SRL , we more efficiently encode each sentence only once , predict its predicates , part - of - speech tags and labeled syntactic parse , then predict the semantic roles for all predicates in the sentence in parallel .",model
Linguistically - Informed Self - Attention for Semantic Role Labeling,research-problem
Current state - of - the - art semantic role labeling ( SRL ) uses a deep neural network with no explicit linguistic features .,research-problem
"However , prior work has shown that gold syntax trees can dramatically improve SRL decoding , suggesting the possibility of increased accuracy from explicit modeling of syntax .",research-problem
Token - Level Ensemble Distillation for Grapheme - to - Phoneme Conversion,research-problem
Grapheme - to - phoneme ( G2P ) conversion is an important task in automatic speech recognition and text - to - speech systems .,research-problem
Our approach is to decouple speaker modeling from speech synthesis by independently training a speaker - discriminative embedding network that captures the space of speaker characteristics and training a high quality TTS model on a smaller dataset conditioned on the representation learned by the first network .  ,approach
We train the speaker embedding network on a speaker verification task to determine if two different utterances were spoken by the same speaker .,approach
"In contrast to the subsequent TTS model , this network is trained on untranscribed speech containing reverberation and background noise from a large number of speakers .",approach
Transfer Learning from Speaker Verification to Multispeaker Text - To - Speech Synthesis,research-problem
"We describe a neural network - based system for text - to - speech ( TTS ) synthesis that is able to generate speech audio in the voice of different speakers , including those unseen during training .",research-problem
The goal of this work is to build a TTS system which can generate natural speech for a variety of speakers in a data efficient manner .,research-problem
"FastSpeech : Fast , Robust and Controllable Text to Speech",research-problem
Neural network based end - to - end text to speech ( TTS ) has significantly improved the quality of synthesized speech .,research-problem
Text to speech ( TTS ) has attracted a lot of attention in recent years due to the advance in deep learning .,research-problem
Neural network based TTS has outperformed conventional concatenative and statistical parametric approaches in terms of speech quality .,research-problem
"We first train the autoregressive Transformer TTS model on 4 NVIDIA V100 GPUs , with batchsize of 16 sentences on each GPU .",experimental-setup
"We use the Adam optimizer with ? 1 = 0.9 , ? 2 = 0.98 , ? = 10 ?9 and follow the same learning rate schedule in .",experimental-setup
"In addition , we also leverage sequence - level knowledge distillation that has achieved good performance in non-autoregressive machine translation to transfer the knowledge from the teacher model to the student model .",experimental-setup
"In the inference process , the output mel-spectrograms of our FastSpeech model are transformed into audio samples using the pretrained WaveGlow [ 20 ] 5 .",experimental-setup
"To this question , we investigate a neural network model for output label sequences .",model
This makes our task essentially to represent a full - exponential search space without making Markov assumptions .,model
"In particular , we represent each possible label using an embedding vector , and aim to encode sequences of label distributions using a recurrent neural network .",model
Hierarchically - Refined Label Attention Network for Sequence Labeling,research-problem
CRF has been used as a powerful model for statistical sequence labeling .,research-problem
"Finally , by adding CRF layer for joint decoding we achieve significant improvements over BLSTM - CNN models for both POS tagging and NER on all metrics .",result
"Comparing with traditional statistical models , our system achieves state - of - the - art accuracy , obtaining 0.05 % improvement over the previously best reported results by .",result
"Similar to the observations of POS tagging , our model achieves significant improvements over Senna and the other three neural models , namely the LSTM - CRF proposed by , LSTM - CNNs pro- :",result
"We compare the performance with three baseline systems - BRNN , the bi-direction RNN ; BLSTM , the bidirection LSTM , and BLSTM - CNNs , the combination of BLSTM with CNN to model characterlevel information .",baseline
Parameter optimization is performed with minibatch stochastic gradient descent ( SGD ) with batch size 10 and momentum 0.9 .,hyperparameter
"The "" best "" parameters appear at around 50 epochs , according to our experiments .",hyperparameter
"We choose an initial learning rate of ? 0 ( ? 0 = 0.01 for POS tagging , and 0.015 for NER , see Section 3.3 . ) , and the learning rate is updated on each epoch of training as ? t = ? 0 / ( 1 + ?t ) , with decay rate ? =",hyperparameter
"To reduce the effects of "" gradient exploding "" , we use a gradient clipping of 5.0 .",hyperparameter
We use early stopping based on performance on validation sets .,hyperparameter
"For each of the embeddings , we fine - tune initial embeddings , modifying them during gradient updates of the neural network model by back - propagating gradients .",hyperparameter
"To mitigate overfitting , we apply the dropout method ( Srivastava et al. , 2014 ) to regularize our model .",hyperparameter
"As shown in and 3 , we apply dropout on character embeddings before inputting to CNN , and on both the input and output vectors of BLSTM .",hyperparameter
We fix dropout rate at 0.5 for all dropout layers through all the experiments .,hyperparameter
"In this paper , we propose a neural network architecture for sequence labeling .",model
"It is a truly endto - end model requiring no task - specific resources , feature engineering , or data pre-processing beyond pre-trained word embeddings on unlabeled corpora .",model
"Thus , our model can be easily applied to a wide range of sequence labeling tasks on different languages and domains .",model
We first use convolutional neural networks ( CNNs ) to encode character - level information of a word into its character - level representation .,model
Then we combine character - and word - level representations and feed them into bi-directional LSTM ( BLSTM ) to model context information of each word .,model
"On top of BLSTM , we use a sequential CRF to jointly decode labels for the whole sentence .",model
End - to - end Sequence Labeling via Bi-directional LSTM-CNNs-CRF,research-problem
State - of - the - art sequence labeling systems traditionally require large amounts of taskspecific knowledge in the form of handcrafted features and data pre-processing .,research-problem
"Linguistic sequence labeling , such as part - ofspeech ( POS ) tagging and named entity recognition ( NER ) , is one of the first stages in deep language understanding and its importance has been well recognized in the natural language processing community .",research-problem
The word embeddings are initialized with zero values and the pre-trained embeddings are not updated during training .,hyperparameter
The dropout used on the embeddings is achieved by a RRIE is the relative reduction in error .,hyperparameter
We propose a novel model where we learn context sensitive initial character and word representations through two separate sentence - level recurrent models .,model
These are then combined via a meta-BiLSTM model that builds a unified representation of each word that is then used for syntactic tagging .,model
Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive Token Encodings,research-problem
shows that separately optimized models are significantly more accurate on average than jointly optimized models .,ablation-analysi
Separate optimization leads to better accuracy for 34 out of 40 treebanks for the morphological features task and for 30 out of 39 treebanks for xpos tagging .,ablation-analysi
"Separate optimization outperformed joint optimization by up to 2.1 percent absolute , while joint never out - performed separate by more than 0.5 % absolute .",ablation-analysi
The examples show that the combined model has significantly higher accuracy compared with either the character and word models individually .,ablation-analysi
"For all of the network sizes in the grid search , we still observed during training that the accuracy reach a high value and degrades with more iterations for the character and word model .",ablation-analysi
"We train the model parameters and word / character embeddings by the mini-batch stochastic gradient descent ( SGD ) with batch size 10 , momentum 0.9 , initial learning rate 0.01 and decay rate 0.05 .",hyperparameter
We also use a gradient clipping of 5.0 .,hyperparameter
The models are trained with early stopping ) based on the development performance .,hyperparameter
"In this paper , spotlighting a well - studied core problem of NLP , we propose and carefully analyze a neural part - of - speech ( POS ) tagging model that exploits adversarial training .",model
"With a BiLSTM - CRF model as our baseline POS tagger , we apply adversarial training by considering perturbations to input word / character embeddings .",model
Robust Multilingual Part - of - Speech Tagging via Adversarial Training,research-problem
"In this paper , we propose and analyze a neural POS tagging model that exploits AT .",research-problem
"We present a transfer learning approach based on a deep hierarchical recurrent neural network , which shares the hidden feature repre-sentation and part of the model parameters between the source task and the target task .",approach
Our approach combines the objectives of the two tasks and uses gradient - based methods for efficient training .,approach
TRANSFER LEARNING FOR SEQUENCE TAGGING WITH HIERARCHICAL RECURRENT NETWORKS,research-problem
1 Code is available at https://github.com/kimiyoung/transfer 1 ar Xiv:1703.06345v1 [ cs.CL ],code
"In an initial investigation , we compared Tnt , HunPos and TreeTagger and found Tnt to be consistently better than Treetagger , Hunpos followed closely but crashed on some languages ( e.g. , Arabic ) .",result
"The combined word + character representation model is the best representation , outperforming the baseline on all except one language ( Indonesian ) , providing strong results already without pre-trained embeddings .",result
This model ( w + c ) reaches the biggest improvement ( more than + 2 % accuracy ) on Hebrew and Slovene .,result
The over all best system is the multi-task bi - LSTM FREQBIN ( it uses w + c and POLYGLOT initialization for w ) .,result
Initializing the word embeddings ( + POLYGLOT ) with off - the - shelf languagespecific embeddings further improves accuracy .,result
"epochs , default learning rate ( 0.1 ) , 128 dimensions for word embeddings , 100 for character and byte embeddings , 100 hidden states and Gaussian noise with ?= 0.2 .",hyperparameter
"As training is stochastic in nature , we use a fixed seed throughout .",hyperparameter
In that case we use offthe - shelf polyglot embeddings .,hyperparameter
"Finally , we introduce a novel model , a bi - LSTM trained with auxiliary loss .",model
The model jointly predicts the POS and the log frequency of the word .,model
Multilingual Part - of - Speech Tagging with Bidirectional Long Short - Term Memory Models and Auxiliary Loss,research-problem
"We address these issues and evaluate bi - LSTMs with word , character , and unicode byte embeddings for POS tagging .",research-problem
The code is released at : https : //github.com/bplank/bilstm-aux,code
"Firstly , we use LSTM - CRF with randomly initialized word embeddings as our initial baseline .",baseline
"We adopt two state - of - the - art methods in sequence labeling , denoted as char - LSTM and char - CNN .",baseline
"We add more layers to the char - CNN model and refer to that as char - CNN - 5 and char - CNN - 9 , respectively for 5 and 9 convolutional layers .",baseline
"Furthermore , we add residual connections to the char - CNN - 9 and refer it as char - ResNet .",baseline
"Also , we apply 3 dense blocks based on char - ResNet which we refer to as char - DenseNet , to compare the difference between residual connection and dense connection .",baseline
The size of the dimensions of character embeddings is 32 which are randomly initialized using a uniform distribution .,hyperparameter
The state size of the bi-directional LSTMs is set to 256 .,hyperparameter
"0.05 is the decay ratio , the value of gradient clipping is 5 .",hyperparameter
"Dropout is applied on the input of IntNet , LSTMs , and CRF , and its ratio 0.5 is fixed , but with no dropout inside of IntNet .",hyperparameter
"The number of convolutional layers are 5 and 9 for IntNet - 5 and IntNet - 9 , respectively , and we have adopted the same weight initialization as that of ResNet .",hyperparameter
We adopt the same initialization method for randomly initialized word embeddings that are updated during training .,hyperparameter
We adopt standard BIOES tagging scheme for NER and Chunking .,hyperparameter
"For IntNet , the filter size of the initial convolution is 32 and that of other convolutions is 16 .",hyperparameter
"We use pre-trained word embeddings for initialization , GloVe 100 - dimension word embeddings for English , and fastText 300 dimension word embeddings for Spanish , Dutch , and German .",hyperparameter
We employ mini-batch stochastic gradient descent with momentum .,hyperparameter
"Furthermore , we propose IntNet , a funnel - shaped wide convolutional neural network for learning the internal structure of words by composing their characters .",model
"Unlike previous CNN - based approaches , our funnel - shaped Int - Net explores deeper and wider architecture with no down - sampling for learning character - to - word representations from limited supervised training corpora .",model
"Lastly , we combine our IntNet model with LSTM - CRF , which captures both word shape and context information , and jointly decode tags for sequence labeling .",model
Learning Better Internal Structure of Words for Sequence Labeling,research-problem
"In terms of dependency parsing , in most cases , our model jPTDP outperforms Stack - propagation .",result
It is somewhat unexpected that our model produces about 7 % absolute lower LAS score than Stack - propagation on Dutch ( nl ) .,result
"Without taking "" nl "" into account , our averaged LAS score over all remaining languages is 1.1 % absolute higher than Stack - propagation 's .",result
"The last row in shows an absolute LAS improvement of 4.4 % on average when comparing our jPTDP with its simplified version of not using characterbased representations : specifically , morphologically rich languages get an averaged improvement of 9.3 % , vice versa 2.6 % for others .",result
"So , our jPDTP is particularly good for morphologically rich languages , with 1.7 % higher averaged LAS than Stack - propagation over these languages .",result
"In this paper , we propose a novel neural architecture for joint POS tagging and graph - based dependency parsing .",model
Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTMthe bidirectional LSTM .,model
A Novel Neural Network Model for Joint POS Tagging and Graph - based Dependency Parsing,research-problem
Our jPTDP is implemented using DYNET v 2.0 .,experimental-setup
"We optimize the objective function using Adam ( Kingma and Ba , 2014 ) with default DYNET parameter settings and no mini-batches .",experimental-setup
"Following Kiperwasser and Goldberg ( 2016 b ) and , we apply a word dropout rate of 0.25 and Gaussian noise with ? = 0.2 .",experimental-setup
"For training , we run for 30 epochs , and evaluate the mixed accuracy of correctly assigning POS tag together with dependency arc and relation type on the development set after each training epoch .",experimental-setup
We perform a minimal grid search of hyper - parameters on English .,experimental-setup
"compares the POS tagging and dependency parsing results of our model jPTDP with results reported in prior work , using the same experimental setup .",experimental-setup
Our code is open - source and available together with pre-trained models at : https://github.com/ datquocnguyen/jPTDP .,code
