subject,predicate,object,label,file path
Contribution,has,Dataset,datase,/content/training-data/semantic_parsing/0/triples/dataset.txt
Dataset,for,new complex and cross-domain semantic parsing task,datase,/content/training-data/semantic_parsing/0/triples/dataset.txt
new complex and cross-domain semantic parsing task,introduce,Spider,datase,/content/training-data/semantic_parsing/0/triples/dataset.txt
Spider,consists of,200 databases,datase,/content/training-data/semantic_parsing/0/triples/dataset.txt
200 databases,with,multiple tables,datase,/content/training-data/semantic_parsing/0/triples/dataset.txt
200 databases,with,"10,181 questions",datase,/content/training-data/semantic_parsing/0/triples/dataset.txt
200 databases,with,"5,693 corresponding complex SQL queries",datase,/content/training-data/semantic_parsing/0/triples/dataset.txt
Dataset,creates and annotates,questions,datase,/content/training-data/semantic_parsing/0/triples/dataset.txt
Dataset,creates and annotates,SQL queries,datase,/content/training-data/semantic_parsing/0/triples/dataset.txt
SQL queries,including,different SQL clauses,datase,/content/training-data/semantic_parsing/0/triples/dataset.txt
different SQL clauses,such as,joining and nested query,datase,/content/training-data/semantic_parsing/0/triples/dataset.txt
Contribution,Code,https://yale-lily. github.io/ spider,code,/content/training-data/semantic_parsing/0/triples/code.txt
Contribution,has research problem,Complex and Cross - Domain Semantic Parsing,research-problem,/content/training-data/semantic_parsing/0/triples/research-problem.txt
Contribution,has research problem,Text - to - SQL,research-problem,/content/training-data/semantic_parsing/0/triples/research-problem.txt
Contribution,has research problem,Semantic parsing ( SP ,research-problem,/content/training-data/semantic_parsing/0/triples/research-problem.txt
Contribution,has research problem,SP,research-problem,/content/training-data/semantic_parsing/0/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/semantic_parsing/0/triples/results.txt
Results,has,over all performances,results,/content/training-data/semantic_parsing/0/triples/results.txt
over all performances,of,all models,results,/content/training-data/semantic_parsing/0/triples/results.txt
all models,are,low,results,/content/training-data/semantic_parsing/0/triples/results.txt
Results,has,performances,results,/content/training-data/semantic_parsing/0/triples/results.txt
performances,of,SQLNet and TypeSQL,results,/content/training-data/semantic_parsing/0/triples/results.txt
SQLNet and TypeSQL,utilize,SQL structure information,results,/content/training-data/semantic_parsing/0/triples/results.txt
SQL structure information,to guide,SQL generation process,results,/content/training-data/semantic_parsing/0/triples/results.txt
SQLNet and TypeSQL,significantly outperform,other Seq2Seq models,results,/content/training-data/semantic_parsing/0/triples/results.txt
performances,of,Seq2Seq - based basic models,results,/content/training-data/semantic_parsing/0/triples/results.txt
Seq2Seq - based basic models,including,Seq2Seq,results,/content/training-data/semantic_parsing/0/triples/results.txt
Seq2Seq - based basic models,including,Seq2Seq + Attention,results,/content/training-data/semantic_parsing/0/triples/results.txt
Seq2Seq - based basic models,including,Seq2Seq + Copying,results,/content/training-data/semantic_parsing/0/triples/results.txt
Seq2Seq - based basic models,are,very low,results,/content/training-data/semantic_parsing/0/triples/results.txt
Results,has,all models,results,/content/training-data/semantic_parsing/0/triples/results.txt
all models,struggle with,WHERE clause prediction,results,/content/training-data/semantic_parsing/0/triples/results.txt
Contribution,Code,https :// github.com/donglixp/coarse2fine,code,/content/training-data/semantic_parsing/2/triples/code.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
Hyperparameters,appended,10 - dimensional part - of - speech tag vectors,hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
10 - dimensional part - of - speech tag vectors,to,embeddings,hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
embeddings,of,question words,hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
question words,in,WIKISQL,hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
Hyperparameters,has,part - of - speech tags,hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
part - of - speech tags,obtained by,spaCy toolkit,hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
Hyperparameters,has,learning rate,hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
learning rate,selected from,"{ 0.002 , 0.005 }",hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
Hyperparameters,has,Dimensions,hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
Dimensions,of,hidden vectors and word embeddings,hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
hidden vectors and word embeddings,selected from,"{ 250 , 300 } and { 150 , 200 , 250 , 300 }",hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
Hyperparameters,has,smoothing parameter,hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
smoothing parameter,set to,0.1,hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
Hyperparameters,has,batch size,hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
batch size,was,200,hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
200,for,WIKISQL,hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
batch size,was,64,hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
64,for,other datasets,hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
Hyperparameters,has,dropout rate,hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
dropout rate,selected from,"{ 0.3 , 0.5 }",hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
Hyperparameters,has,Early stopping,hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
Early stopping,to determine,number of epochs,hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
Hyperparameters,has,Label smoothing,hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
Label smoothing,employed for,GEO,hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
Label smoothing,employed for,ATIS,hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
Hyperparameters,has,Word embeddings,hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
Word embeddings,shared by,table encoder,hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
Word embeddings,shared by,input encoder,hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
Word embeddings,initialized by,GloVe,hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
Hyperparameters,used,RMSProp optimizer,hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
RMSProp optimizer,to train,models,hyperparameters,/content/training-data/semantic_parsing/2/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/semantic_parsing/2/triples/model.txt
Model,has,second decoder,model,/content/training-data/semantic_parsing/2/triples/model.txt
second decoder,fills in,missing details,model,/content/training-data/semantic_parsing/2/triples/model.txt
missing details,by conditioning on,natural language input,model,/content/training-data/semantic_parsing/2/triples/model.txt
missing details,by conditioning on,sketch,model,/content/training-data/semantic_parsing/2/triples/model.txt
Model,has,explicitly share,model,/content/training-data/semantic_parsing/2/triples/model.txt
explicitly share,has,knowledge,model,/content/training-data/semantic_parsing/2/triples/model.txt
knowledge,of,coarse structures,model,/content/training-data/semantic_parsing/2/triples/model.txt
coarse structures,for,"examples that have the same sketch ( i.e. , basic meaning ",model,/content/training-data/semantic_parsing/2/triples/model.txt
Model,has,first decoder,model,/content/training-data/semantic_parsing/2/triples/model.txt
first decoder,focuses on,predicting,model,/content/training-data/semantic_parsing/2/triples/model.txt
predicting,has,rough sketch,model,/content/training-data/semantic_parsing/2/triples/model.txt
rough sketch,of,meaning representation,model,/content/training-data/semantic_parsing/2/triples/model.txt
Model,has,decomposition,model,/content/training-data/semantic_parsing/2/triples/model.txt
decomposition,disentangles,high - level from low - level semantic information,model,/content/training-data/semantic_parsing/2/triples/model.txt
high - level from low - level semantic information,enables,decoders,model,/content/training-data/semantic_parsing/2/triples/model.txt
decoders,to model,meaning,model,/content/training-data/semantic_parsing/2/triples/model.txt
meaning,at,different levels of granularity,model,/content/training-data/semantic_parsing/2/triples/model.txt
Model,has,generating,model,/content/training-data/semantic_parsing/2/triples/model.txt
generating,has,sketch,model,/content/training-data/semantic_parsing/2/triples/model.txt
sketch,has,decoder,model,/content/training-data/semantic_parsing/2/triples/model.txt
decoder,knows,basic meaning of the utterance,model,/content/training-data/semantic_parsing/2/triples/model.txt
sketch,use it,model,model,/content/training-data/semantic_parsing/2/triples/model.txt
model,as,global context,model,/content/training-data/semantic_parsing/2/triples/model.txt
global context,to improve,prediction,model,/content/training-data/semantic_parsing/2/triples/model.txt
prediction,of,final details,model,/content/training-data/semantic_parsing/2/triples/model.txt
Model,has,sketch,model,/content/training-data/semantic_parsing/2/triples/model.txt
sketch,encoded into,vectors,model,/content/training-data/semantic_parsing/2/triples/model.txt
vectors,to guide,decoding,model,/content/training-data/semantic_parsing/2/triples/model.txt
sketch,constrains,generation process,model,/content/training-data/semantic_parsing/2/triples/model.txt
Model,propose to decompose,decoding process,model,/content/training-data/semantic_parsing/2/triples/model.txt
decoding process,into,two stages,model,/content/training-data/semantic_parsing/2/triples/model.txt
Contribution,has research problem,Neural Semantic Parsing,research-problem,/content/training-data/semantic_parsing/2/triples/research-problem.txt
Contribution,has research problem,Semantic parsing,research-problem,/content/training-data/semantic_parsing/2/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/semantic_parsing/2/triples/results.txt
Results,find,tableaware input encoder,results,/content/training-data/semantic_parsing/2/triples/results.txt
tableaware input encoder,is,critical,results,/content/training-data/semantic_parsing/2/triples/results.txt
critical,for,doing well,results,/content/training-data/semantic_parsing/2/triples/results.txt
doing well,on,this task,results,/content/training-data/semantic_parsing/2/triples/results.txt
Results,has,our method,results,/content/training-data/semantic_parsing/2/triples/results.txt
our method,performs,competitively,results,/content/training-data/semantic_parsing/2/triples/results.txt
competitively,despite the use of,relatively simple decoders,results,/content/training-data/semantic_parsing/2/triples/results.txt
Results,has,COARSE2FINE 's accuracies,results,/content/training-data/semantic_parsing/2/triples/results.txt
COARSE2FINE 's accuracies,on,aggregation agg op and agg col,results,/content/training-data/semantic_parsing/2/triples/results.txt
aggregation agg op and agg col,are,90.2 % and 92.0 %,results,/content/training-data/semantic_parsing/2/triples/results.txt
90.2 % and 92.0 %,comparable to,SQLNET,results,/content/training-data/semantic_parsing/2/triples/results.txt
Results,has,Our model,results,/content/training-data/semantic_parsing/2/triples/results.txt
Our model,superior to,ONESTAGE,results,/content/training-data/semantic_parsing/2/triples/results.txt
Our model,superior to,previous best performing systems,results,/content/training-data/semantic_parsing/2/triples/results.txt
Results,has,most gain,results,/content/training-data/semantic_parsing/2/triples/results.txt
most gain,obtained by,improved decoder,results,/content/training-data/semantic_parsing/2/triples/results.txt
improved decoder,of,WHERE clause,results,/content/training-data/semantic_parsing/2/triples/results.txt
Results,has,Sketches,results,/content/training-data/semantic_parsing/2/triples/results.txt
Sketches,produced by,COARSE2FINE,results,/content/training-data/semantic_parsing/2/triples/results.txt
COARSE2FINE,are,more accurate,results,/content/training-data/semantic_parsing/2/triples/results.txt
Results,observe,COARSE2FINE,results,/content/training-data/semantic_parsing/2/triples/results.txt
COARSE2FINE,suggests,disentangling,results,/content/training-data/semantic_parsing/2/triples/results.txt
disentangling,has,high - level,results,/content/training-data/semantic_parsing/2/triples/results.txt
high - level,from,low - level information,results,/content/training-data/semantic_parsing/2/triples/results.txt
COARSE2FINE,outperforms,ONESTAGE,results,/content/training-data/semantic_parsing/2/triples/results.txt
Results,observe,sketch encoder,results,/content/training-data/semantic_parsing/2/triples/results.txt
sketch encoder,there is,8.9 point difference,results,/content/training-data/semantic_parsing/2/triples/results.txt
8.9 point difference,in accuracy between,COARSE2FINE and the oracle,results,/content/training-data/semantic_parsing/2/triples/results.txt
sketch encoder,is,beneficial,results,/content/training-data/semantic_parsing/2/triples/results.txt
Results,predicting,sketch,results,/content/training-data/semantic_parsing/2/triples/results.txt
sketch,has,correctly,results,/content/training-data/semantic_parsing/2/triples/results.txt
sketch,boosts,performance,results,/content/training-data/semantic_parsing/2/triples/results.txt
Results,On,WIKISQL,results,/content/training-data/semantic_parsing/2/triples/results.txt
WIKISQL,has,sketches,results,/content/training-data/semantic_parsing/2/triples/results.txt
sketches,are,marginally better,results,/content/training-data/semantic_parsing/2/triples/results.txt
marginally better,compared with,ONESTAGE,results,/content/training-data/semantic_parsing/2/triples/results.txt
sketches,predicted by,COARSE2FINE,results,/content/training-data/semantic_parsing/2/triples/results.txt
Contribution,has,Model,model,/content/training-data/semantic_parsing/1/triples/model.txt
Model,has,TRANX,model,/content/training-data/semantic_parsing/1/triples/model.txt
TRANX,designed with,following principles,model,/content/training-data/semantic_parsing/1/triples/model.txt
following principles,has,Effectiveness,model,/content/training-data/semantic_parsing/1/triples/model.txt
Effectiveness,test,TRANX,model,/content/training-data/semantic_parsing/1/triples/model.txt
TRANX,on,code generation,model,/content/training-data/semantic_parsing/1/triples/model.txt
code generation,name,DJANGO,model,/content/training-data/semantic_parsing/1/triples/model.txt
code generation,name,WIKISQL,model,/content/training-data/semantic_parsing/1/triples/model.txt
TRANX,on,semantic parsing,model,/content/training-data/semantic_parsing/1/triples/model.txt
semantic parsing,name,ATIS,model,/content/training-data/semantic_parsing/1/triples/model.txt
semantic parsing,name,GEO,model,/content/training-data/semantic_parsing/1/triples/model.txt
following principles,has,Extensibility,model,/content/training-data/semantic_parsing/1/triples/model.txt
Extensibility,uses,simple transition system,model,/content/training-data/semantic_parsing/1/triples/model.txt
simple transition system,to parse,NL utterances,model,/content/training-data/semantic_parsing/1/triples/model.txt
following principles,has,Generalization ability,model,/content/training-data/semantic_parsing/1/triples/model.txt
Generalization ability,employs,ASTs,model,/content/training-data/semantic_parsing/1/triples/model.txt
ASTs,as,general - purpose intermediate meaning representation,model,/content/training-data/semantic_parsing/1/triples/model.txt
Generalization ability,has,task - dependent grammar,model,/content/training-data/semantic_parsing/1/triples/model.txt
task - dependent grammar,provided to,system,model,/content/training-data/semantic_parsing/1/triples/model.txt
system,as,external knowledge,model,/content/training-data/semantic_parsing/1/triples/model.txt
external knowledge,to guide,parsing process,model,/content/training-data/semantic_parsing/1/triples/model.txt
Model,developed,TRANX,model,/content/training-data/semantic_parsing/1/triples/model.txt
TRANX,name,TRANsition - based abstract syntaX parser,model,/content/training-data/semantic_parsing/1/triples/model.txt
TRANX,for,semantic parsing and code generation,model,/content/training-data/semantic_parsing/1/triples/model.txt
Contribution,has research problem,Semantic Parsing and Code Generation,research-problem,/content/training-data/semantic_parsing/1/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/semantic_parsing/1/triples/results.txt
Results,has,Semantic Parsing,results,/content/training-data/semantic_parsing/1/triples/results.txt
Semantic Parsing,has,Our system,results,/content/training-data/semantic_parsing/1/triples/results.txt
Our system,outperforms,existing neural network - based approaches,results,/content/training-data/semantic_parsing/1/triples/results.txt
Semantic Parsing,has,model,results,/content/training-data/semantic_parsing/1/triples/results.txt
model,achieves,slightly better accuracy,results,/content/training-data/semantic_parsing/1/triples/results.txt
slightly better accuracy,on,GEO,results,/content/training-data/semantic_parsing/1/triples/results.txt
model,without,parent feeding,results,/content/training-data/semantic_parsing/1/triples/results.txt
Results,has,Code Generation,results,/content/training-data/semantic_parsing/1/triples/results.txt
Code Generation,find,parent feeding,results,/content/training-data/semantic_parsing/1/triples/results.txt
parent feeding,yields,+ 1 point gain,results,/content/training-data/semantic_parsing/1/triples/results.txt
+ 1 point gain,in,accuracy,results,/content/training-data/semantic_parsing/1/triples/results.txt
Code Generation,find,TRANX,results,/content/training-data/semantic_parsing/1/triples/results.txt
TRANX,achieves,impressive results,results,/content/training-data/semantic_parsing/1/triples/results.txt
TRANX,outperforms,many task - specific methods,results,/content/training-data/semantic_parsing/1/triples/results.txt
Code Generation,has,TRANX,results,/content/training-data/semantic_parsing/1/triples/results.txt
TRANX,achieves,state - of - the - art results,results,/content/training-data/semantic_parsing/1/triples/results.txt
state - of - the - art results,on,DJANGO,results,/content/training-data/semantic_parsing/1/triples/results.txt
Contribution,has,Dataset,datase,/content/training-data/prosody_prediction/0/triples/dataset.txt
Dataset,will be,largest publicly available dataset,datase,/content/training-data/prosody_prediction/0/triples/dataset.txt
largest publicly available dataset,with,prosodic annotations,datase,/content/training-data/prosody_prediction/0/triples/dataset.txt
Dataset,for predicting,prosodic prominence,datase,/content/training-data/prosody_prediction/0/triples/dataset.txt
prosodic prominence,based on,recently published Libri TTS corpus,datase,/content/training-data/prosody_prediction/0/triples/dataset.txt
prosodic prominence,containing,automatically generated prosodic prominence labels,datase,/content/training-data/prosody_prediction/0/triples/dataset.txt
automatically generated prosodic prominence labels,read by,1230 different speakers,datase,/content/training-data/prosody_prediction/0/triples/dataset.txt
automatically generated prosodic prominence labels,for,over 260 hours,datase,/content/training-data/prosody_prediction/0/triples/dataset.txt
automatically generated prosodic prominence labels,for,2.8 million words,datase,/content/training-data/prosody_prediction/0/triples/dataset.txt
2.8 million words,of,English audio books,datase,/content/training-data/prosody_prediction/0/triples/dataset.txt
prosodic prominence,from,text,datase,/content/training-data/prosody_prediction/0/triples/dataset.txt
Contribution,Code,https://github.com/Helsinki - NLP / prosody,code,/content/training-data/prosody_prediction/0/triples/code.txt
Contribution,has research problem,Predicting Prosodic Prominence from Text,research-problem,/content/training-data/prosody_prediction/0/triples/research-problem.txt
Contribution,has research problem,predicting prosodic prominence from written text,research-problem,/content/training-data/prosody_prediction/0/triples/research-problem.txt
Contribution,has research problem,Prosody prediction,research-problem,/content/training-data/prosody_prediction/0/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
Experimental setup,add,one fullyconnected classifier layer,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
one fullyconnected classifier layer,mapping,representation,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
representation,of,each word,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
each word,to,labels,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
one fullyconnected classifier layer,on top of,BiLSTM,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
Experimental setup,take,last hidden layer,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
last hidden layer,of,BERT,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
last hidden layer,train,single fully - connected classifier layer,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
single fully - connected classifier layer,mapping,representation,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
representation,of,each word,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
each word,to,labels,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
single fully - connected classifier layer,on,top,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
Experimental setup,use,Huggingface PyTorch implementation,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
Huggingface PyTorch implementation,of,BERT,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
BERT,available in,pytorch transformers library,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
BERT,further,fine - tune,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
fine - tune,during,training,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
Experimental setup,use,dropout,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
dropout,of,0.2,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
0.2,between,layers of the BiLSTM,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
Experimental setup,use,batch size,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
batch size,of,32,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
Experimental setup,use,smaller BERT - base model,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
smaller BERT - base model,using,uncased alternative,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
Experimental setup,fine - tune,model,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
model,for,2 epochs,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
Experimental setup,For,conditional random field ( CRF ) model,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
conditional random field ( CRF ) model,use,MarMot,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
MarMot,with,default configuration,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
Experimental setup,For,SVM,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
SVM,use,Minitagger 4 implementation,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
Minitagger 4 implementation,using,each dimension,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
each dimension,of,pre-trained 300D Glo Ve 840B word embeddings,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
pre-trained 300D Glo Ve 840B word embeddings,with,context - size 1,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
pre-trained 300D Glo Ve 840B word embeddings,as,features,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
Experimental setup,For,BiLSTM,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
BiLSTM,use,pre-trained 300D Glo Ve 840B word embeddings,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
Experimental setup,has,models,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
models,name,BERT - base uncased,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
models,name,3 - layer 600D Bidirectional Long Short - Term Memory ( BiLSTM ,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
models,name,Minitagger ( SVM ) ) + GloVe,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
models,name,MarMoT ( CRF ,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
models,name,Majority class per word,experimental-setup,/content/training-data/prosody_prediction/0/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/prosody_prediction/0/triples/results.txt
Results,with,manually annotated test set from The Boston University radio news corpus,results,/content/training-data/prosody_prediction/0/triples/results.txt
manually annotated test set from The Boston University radio news corpus,has,good results,results,/content/training-data/prosody_prediction/0/triples/results.txt
good results,provide,further support,results,/content/training-data/prosody_prediction/0/triples/results.txt
further support,for,quality of the new dataset,results,/content/training-data/prosody_prediction/0/triples/results.txt
manually annotated test set from The Boston University radio news corpus,has,difference,results,/content/training-data/prosody_prediction/0/triples/results.txt
difference,has,much bigger,results,/content/training-data/prosody_prediction/0/triples/results.txt
difference,between,BERT and BiLSTM,results,/content/training-data/prosody_prediction/0/triples/results.txt
Results,For,most of the models,results,/content/training-data/prosody_prediction/0/triples/results.txt
most of the models,has,biggest improvement,results,/content/training-data/prosody_prediction/0/triples/results.txt
biggest improvement,in,performance,results,/content/training-data/prosody_prediction/0/triples/results.txt
biggest improvement,achieved when,moving,results,/content/training-data/prosody_prediction/0/triples/results.txt
moving,from,1 %,results,/content/training-data/prosody_prediction/0/triples/results.txt
1 %,of,training examples,results,/content/training-data/prosody_prediction/0/triples/results.txt
1 %,to,5 %,results,/content/training-data/prosody_prediction/0/triples/results.txt
Results,has,Minitagger SVM model 's test accuracies,results,/content/training-data/prosody_prediction/0/triples/results.txt
Minitagger SVM model 's test accuracies,are,slightly lower,results,/content/training-data/prosody_prediction/0/triples/results.txt
slightly lower,than,CRF 's,results,/content/training-data/prosody_prediction/0/triples/results.txt
CRF 's,with,80.8 % and 65.4 % test accuracies,results,/content/training-data/prosody_prediction/0/triples/results.txt
Results,has,BERTbased model,results,/content/training-data/prosody_prediction/0/triples/results.txt
BERTbased model,gets,highest accuracy,results,/content/training-data/prosody_prediction/0/triples/results.txt
highest accuracy,of,83.2 % and 68.6 %,results,/content/training-data/prosody_prediction/0/triples/results.txt
83.2 % and 68.6 %,in,2 - way and 3 - way classification tasks,results,/content/training-data/prosody_prediction/0/triples/results.txt
Results,has,3layer BiLSTM,results,/content/training-data/prosody_prediction/0/triples/results.txt
3layer BiLSTM,achieves,82.1 %,results,/content/training-data/prosody_prediction/0/triples/results.txt
82.1 %,in,2 - way classification,results,/content/training-data/prosody_prediction/0/triples/results.txt
3layer BiLSTM,achieves,66.4 %,results,/content/training-data/prosody_prediction/0/triples/results.txt
66.4 %,in,3 - way classification task,results,/content/training-data/prosody_prediction/0/triples/results.txt
Results,has,traditional feature - based classifiers,results,/content/training-data/prosody_prediction/0/triples/results.txt
traditional feature - based classifiers,perform,slightly below,results,/content/training-data/prosody_prediction/0/triples/results.txt
slightly below,has,neural network models,results,/content/training-data/prosody_prediction/0/triples/results.txt
neural network models,with,CRF,results,/content/training-data/prosody_prediction/0/triples/results.txt
CRF,obtaining,81.8 % and 66.4 %,results,/content/training-data/prosody_prediction/0/triples/results.txt
81.8 % and 66.4 %,for,two classification tasks,results,/content/training-data/prosody_prediction/0/triples/results.txt
Results,has,All models,results,/content/training-data/prosody_prediction/0/triples/results.txt
All models,reached close to,full predictive capacity,results,/content/training-data/prosody_prediction/0/triples/results.txt
full predictive capacity,with,only 10 % of the training examples,results,/content/training-data/prosody_prediction/0/triples/results.txt
All models,has,3 - way classification accuracy,results,/content/training-data/prosody_prediction/0/triples/results.txt
3 - way classification accuracy,stays below,70 %,results,/content/training-data/prosody_prediction/0/triples/results.txt
All models,reach over,80 %,results,/content/training-data/prosody_prediction/0/triples/results.txt
80 %,in,2 - way classification task,results,/content/training-data/prosody_prediction/0/triples/results.txt
Results,taking,simple majority class per word,results,/content/training-data/prosody_prediction/0/triples/results.txt
simple majority class per word,gives,80.2 %,results,/content/training-data/prosody_prediction/0/triples/results.txt
80.2 %,for,2 - way classification task,results,/content/training-data/prosody_prediction/0/triples/results.txt
simple majority class per word,gives,62.4 %,results,/content/training-data/prosody_prediction/0/triples/results.txt
62.4 %,for,3 - way classification task,results,/content/training-data/prosody_prediction/0/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/question_generation/0/triples/baselines.txt
Baselines,has,PCFG - Trans,baselines,/content/training-data/question_generation/0/triples/baselines.txt
PCFG - Trans,has,rule - based system,baselines,/content/training-data/question_generation/0/triples/baselines.txt
Baselines,has,NQG,baselines,/content/training-data/question_generation/0/triples/baselines.txt
NQG,extend,s 2s+ att,baselines,/content/training-data/question_generation/0/triples/baselines.txt
s 2s+ att,with,feature - rich encoder,baselines,/content/training-data/question_generation/0/triples/baselines.txt
feature - rich encoder,to build,NQG system,baselines,/content/training-data/question_generation/0/triples/baselines.txt
Baselines,has,NQG + Pretrain,baselines,/content/training-data/question_generation/0/triples/baselines.txt
NQG + Pretrain,Based on,NQG +,baselines,/content/training-data/question_generation/0/triples/baselines.txt
NQG +,initialize,word embedding matrix,baselines,/content/training-data/question_generation/0/triples/baselines.txt
word embedding matrix,with,pre-trained GloVe vectors,baselines,/content/training-data/question_generation/0/triples/baselines.txt
Baselines,has,NQG ++,baselines,/content/training-data/question_generation/0/triples/baselines.txt
NQG ++,Based on,NQG +,baselines,/content/training-data/question_generation/0/triples/baselines.txt
NQG +,use,pre-train word embedding and STshare methods,baselines,/content/training-data/question_generation/0/triples/baselines.txt
pre-train word embedding and STshare methods,to further improve,performance,baselines,/content/training-data/question_generation/0/triples/baselines.txt
Baselines,has,s 2 s+ att,baselines,/content/training-data/question_generation/0/triples/baselines.txt
s 2 s+ att,implement,seq2seq with attention,baselines,/content/training-data/question_generation/0/triples/baselines.txt
Baselines,has,NQG +,baselines,/content/training-data/question_generation/0/triples/baselines.txt
NQG +,incorporate,copy mechanism,baselines,/content/training-data/question_generation/0/triples/baselines.txt
copy mechanism,deal with,rare words problem,baselines,/content/training-data/question_generation/0/triples/baselines.txt
Baselines,has,NQG + STshare,baselines,/content/training-data/question_generation/0/triples/baselines.txt
NQG + STshare,Based on,NQG +,baselines,/content/training-data/question_generation/0/triples/baselines.txt
NQG + STshare,make,encoder and decoder,baselines,/content/training-data/question_generation/0/triples/baselines.txt
encoder and decoder,share,same embedding matrix,baselines,/content/training-data/question_generation/0/triples/baselines.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/question_generation/0/triples/ablation-analysis.txt
Ablation analysis,has,answer position indicator,ablation-analysis,/content/training-data/question_generation/0/triples/ablation-analysis.txt
answer position indicator,plays,crucial role,ablation-analysis,/content/training-data/question_generation/0/triples/ablation-analysis.txt
crucial role,in,answer focused question generation,ablation-analysis,/content/training-data/question_generation/0/triples/ablation-analysis.txt
Ablation analysis,show that,"word case , POS and NER tag features",ablation-analysis,/content/training-data/question_generation/0/triples/ablation-analysis.txt
"word case , POS and NER tag features",contributes to,question generation,ablation-analysis,/content/training-data/question_generation/0/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/question_generation/0/triples/model.txt
Model,denoted as,Neural Question Generation ( NQG ) framework,model,/content/training-data/question_generation/0/triples/model.txt
Neural Question Generation ( NQG ) framework,to generate,natural language questions from text,model,/content/training-data/question_generation/0/triples/model.txt
natural language questions from text,without,pre-defined rules,model,/content/training-data/question_generation/0/triples/model.txt
Model,has,answer position feature,model,/content/training-data/question_generation/0/triples/model.txt
answer position feature,denotes,answer span,model,/content/training-data/question_generation/0/triples/model.txt
answer span,in,input sentence,model,/content/training-data/question_generation/0/triples/model.txt
Model,has,lexical features,model,/content/training-data/question_generation/0/triples/model.txt
lexical features,to help produce,better sentence encoding,model,/content/training-data/question_generation/0/triples/model.txt
better sentence encoding,include,part - of - speech ( POS ,model,/content/training-data/question_generation/0/triples/model.txt
better sentence encoding,include,named entity ( NER ) tags,model,/content/training-data/question_generation/0/triples/model.txt
Model,has,decoder,model,/content/training-data/question_generation/0/triples/model.txt
decoder,with,attention mechanism,model,/content/training-data/question_generation/0/triples/model.txt
decoder,generates,answer specific question,model,/content/training-data/question_generation/0/triples/model.txt
answer specific question,of,sentence,model,/content/training-data/question_generation/0/triples/model.txt
Model,has,Neural Question Generation framework,model,/content/training-data/question_generation/0/triples/model.txt
Neural Question Generation framework,extends,sequence - to - sequence models,model,/content/training-data/question_generation/0/triples/model.txt
sequence - to - sequence models,by enriching,encoder,model,/content/training-data/question_generation/0/triples/model.txt
encoder,with,answer,model,/content/training-data/question_generation/0/triples/model.txt
encoder,with,lexical features,model,/content/training-data/question_generation/0/triples/model.txt
encoder,to generate,answer focused questions,model,/content/training-data/question_generation/0/triples/model.txt
Model,has,encoder,model,/content/training-data/question_generation/0/triples/model.txt
encoder,reads,input sentence,model,/content/training-data/question_generation/0/triples/model.txt
encoder,reads,answer position indicator,model,/content/training-data/question_generation/0/triples/model.txt
encoder,reads,lexical features,model,/content/training-data/question_generation/0/triples/model.txt
Contribution,has research problem,Neural Question Generation from Text,research-problem,/content/training-data/question_generation/0/triples/research-problem.txt
Contribution,has research problem,Automatic question generation,research-problem,/content/training-data/question_generation/0/triples/research-problem.txt
Contribution,has research problem,Automatic question generation from natural language text,research-problem,/content/training-data/question_generation/0/triples/research-problem.txt
Contribution,has research problem,question generation,research-problem,/content/training-data/question_generation/0/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/question_generation/0/triples/results.txt
Results,has,NQG framework,results,/content/training-data/question_generation/0/triples/results.txt
NQG framework,outperforms,PCFG - Trans and s 2s + att baselines,results,/content/training-data/question_generation/0/triples/results.txt
PCFG - Trans and s 2s + att baselines,by,large margin,results,/content/training-data/question_generation/0/triples/results.txt
Results,has,NQG ++,results,/content/training-data/question_generation/0/triples/results.txt
NQG ++,has,1.11 BLEU score gain,results,/content/training-data/question_generation/0/triples/results.txt
1.11 BLEU score gain,over,NQG +,results,/content/training-data/question_generation/0/triples/results.txt
Results,With the help of,copy mechanism,results,/content/training-data/question_generation/0/triples/results.txt
copy mechanism,has,NQG +,results,/content/training-data/question_generation/0/triples/results.txt
NQG +,has,2.05 BLEU improvement,results,/content/training-data/question_generation/0/triples/results.txt
Contribution,has,Approach,approach,/content/training-data/question_generation/1/triples/approach.txt
Approach,use,context,approach,/content/training-data/question_generation/1/triples/approach.txt
context,by considering,exemplars,approach,/content/training-data/question_generation/1/triples/approach.txt
Approach,use,difference,approach,/content/training-data/question_generation/1/triples/approach.txt
difference,between,relevant and irrelevant exemplars,approach,/content/training-data/question_generation/1/triples/approach.txt
Approach,uses,differential context,approach,/content/training-data/question_generation/1/triples/approach.txt
differential context,obtained through,supporting and contrasting exemplars,approach,/content/training-data/question_generation/1/triples/approach.txt
supporting and contrasting exemplars,to obtain,differentiable embedding,approach,/content/training-data/question_generation/1/triples/approach.txt
differentiable embedding,used by,question decoder,approach,/content/training-data/question_generation/1/triples/approach.txt
question decoder,to decode,appropriate question,approach,/content/training-data/question_generation/1/triples/approach.txt
Approach,consider,different contexts,approach,/content/training-data/question_generation/1/triples/approach.txt
different contexts,in the form of,"Location , Caption , and Part of Speech tags",approach,/content/training-data/question_generation/1/triples/approach.txt
Approach,propose,multimodal differential network,approach,/content/training-data/question_generation/1/triples/approach.txt
multimodal differential network,to solve,task,approach,/content/training-data/question_generation/1/triples/approach.txt
task,of,visual question generation,approach,/content/training-data/question_generation/1/triples/approach.txt
Contribution,has research problem,Visual Question Generation,research-problem,/content/training-data/question_generation/1/triples/research-problem.txt
Contribution,has research problem,Generating natural questions from an image,research-problem,/content/training-data/question_generation/1/triples/research-problem.txt
Contribution,has research problem,generating natural questions for an image,research-problem,/content/training-data/question_generation/1/triples/research-problem.txt
Contribution,has,Model,model,/content/training-data/smile_recognition/0/triples/model.txt
Model,In,second stage,model,/content/training-data/smile_recognition/0/triples/model.txt
second stage,has,output layer,model,/content/training-data/smile_recognition/0/triples/model.txt
output layer,consists of,two units,model,/content/training-data/smile_recognition/0/triples/model.txt
two units,for,smile or no smile,model,/content/training-data/smile_recognition/0/triples/model.txt
second stage,has,regular neural network,model,/content/training-data/smile_recognition/0/triples/model.txt
regular neural network,follows,convolutions,model,/content/training-data/smile_recognition/0/triples/model.txt
convolutions,to discriminate,features,model,/content/training-data/smile_recognition/0/triples/model.txt
features,learned by,convolutions,model,/content/training-data/smile_recognition/0/triples/model.txt
Model,has,novelty,model,/content/training-data/smile_recognition/0/triples/model.txt
novelty,is,"exact number of convolutions , number of hidden layers and size of hidden layers",model,/content/training-data/smile_recognition/0/triples/model.txt
"exact number of convolutions , number of hidden layers and size of hidden layers",subject to,model selection,model,/content/training-data/smile_recognition/0/triples/model.txt
"exact number of convolutions , number of hidden layers and size of hidden layers",are,not fixed,model,/content/training-data/smile_recognition/0/triples/model.txt
Model,has,convolution,model,/content/training-data/smile_recognition/0/triples/model.txt
convolution,followed by,more convolutions,model,/content/training-data/smile_recognition/0/triples/model.txt
more convolutions,to become,more invariant,model,/content/training-data/smile_recognition/0/triples/model.txt
more invariant,to,distortions,model,/content/training-data/smile_recognition/0/triples/model.txt
distortions,in,input,model,/content/training-data/smile_recognition/0/triples/model.txt
Model,has,input images,model,/content/training-data/smile_recognition/0/triples/model.txt
input images,fed into,convolution,model,/content/training-data/smile_recognition/0/triples/model.txt
convolution,comprising,convolutional and a subsampling layer,model,/content/training-data/smile_recognition/0/triples/model.txt
Contribution,has research problem,Smile Recognition,research-problem,/content/training-data/smile_recognition/0/triples/research-problem.txt
Contribution,has research problem,facial expression recognition,research-problem,/content/training-data/smile_recognition/0/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/smile_recognition/0/triples/experimental-setup.txt
Experimental setup,contains,four parameters,experimental-setup,/content/training-data/smile_recognition/0/triples/experimental-setup.txt
four parameters,name,number of convolutions,experimental-setup,/content/training-data/smile_recognition/0/triples/experimental-setup.txt
four parameters,name,number of hidden layers,experimental-setup,/content/training-data/smile_recognition/0/triples/experimental-setup.txt
four parameters,name,number of units per hidden layer,experimental-setup,/content/training-data/smile_recognition/0/triples/experimental-setup.txt
four parameters,name,dropout factor,experimental-setup,/content/training-data/smile_recognition/0/triples/experimental-setup.txt
four parameters,to be,optimized,experimental-setup,/content/training-data/smile_recognition/0/triples/experimental-setup.txt
Experimental setup,generated,CUDA code,experimental-setup,/content/training-data/smile_recognition/0/triples/experimental-setup.txt
CUDA code,executed on,Tesla K40c,experimental-setup,/content/training-data/smile_recognition/0/triples/experimental-setup.txt
Experimental setup,apply to,momentum,experimental-setup,/content/training-data/smile_recognition/0/triples/experimental-setup.txt
momentum,fixed to,= 0.9,experimental-setup,/content/training-data/smile_recognition/0/triples/experimental-setup.txt
Experimental setup,implemented using,Lasagne,experimental-setup,/content/training-data/smile_recognition/0/triples/experimental-setup.txt
Experimental setup,has,All layers,experimental-setup,/content/training-data/smile_recognition/0/triples/experimental-setup.txt
All layers,except of,softmax,experimental-setup,/content/training-data/smile_recognition/0/triples/experimental-setup.txt
softmax,used in,output layer,experimental-setup,/content/training-data/smile_recognition/0/triples/experimental-setup.txt
All layers,use,ReLU units,experimental-setup,/content/training-data/smile_recognition/0/triples/experimental-setup.txt
Experimental setup,has,learning rate,experimental-setup,/content/training-data/smile_recognition/0/triples/experimental-setup.txt
learning rate,not subject to,model selection,experimental-setup,/content/training-data/smile_recognition/0/triples/experimental-setup.txt
learning rate,fixed to,? = 0.01,experimental-setup,/content/training-data/smile_recognition/0/triples/experimental-setup.txt
Experimental setup,has,model,experimental-setup,/content/training-data/smile_recognition/0/triples/experimental-setup.txt
model,trained for,50 epochs,experimental-setup,/content/training-data/smile_recognition/0/triples/experimental-setup.txt
50 epochs,in,model selection,experimental-setup,/content/training-data/smile_recognition/0/triples/experimental-setup.txt
Experimental setup,has,entire database,experimental-setup,/content/training-data/smile_recognition/0/triples/experimental-setup.txt
entire database,randomly split into,60% / 20 % / 20 % training / validation / test ratio,experimental-setup,/content/training-data/smile_recognition/0/triples/experimental-setup.txt
Experimental setup,has,Stochastic gradient descent,experimental-setup,/content/training-data/smile_recognition/0/triples/experimental-setup.txt
Stochastic gradient descent,with,batch size,experimental-setup,/content/training-data/smile_recognition/0/triples/experimental-setup.txt
batch size,of,500,experimental-setup,/content/training-data/smile_recognition/0/triples/experimental-setup.txt
Experimental setup,has,some parameters,experimental-setup,/content/training-data/smile_recognition/0/triples/experimental-setup.txt
some parameters,fixed to,reasonable and empirical values,experimental-setup,/content/training-data/smile_recognition/0/triples/experimental-setup.txt
reasonable and empirical values,such as,"size of convolutions ( 5 5 pixels , 32 feature maps ",experimental-setup,/content/training-data/smile_recognition/0/triples/experimental-setup.txt
reasonable and empirical values,such as,size of subsamplings ( 2 2 pixels using max pooling ,experimental-setup,/content/training-data/smile_recognition/0/triples/experimental-setup.txt
Contribution,has,Baselines,baselines,/content/training-data/natural_language_inference/32/triples/baselines.txt
Baselines,applied,BiDAF ++,baselines,/content/training-data/natural_language_inference/32/triples/baselines.txt
BiDAF ++,has,strong extractive QA model,baselines,/content/training-data/natural_language_inference/32/triples/baselines.txt
strong extractive QA model,to,QuAC dataset,baselines,/content/training-data/natural_language_inference/32/triples/baselines.txt
Baselines,has,- QHIER - RNN,baselines,/content/training-data/natural_language_inference/32/triples/baselines.txt
- QHIER - RNN,removes,hierarchical LSTM layers,baselines,/content/training-data/natural_language_inference/32/triples/baselines.txt
hierarchical LSTM layers,on,final question vectors,baselines,/content/training-data/natural_language_inference/32/triples/baselines.txt
Baselines,has,- FLOW,baselines,/content/training-data/natural_language_inference/32/triples/baselines.txt
- FLOW,removes,flow component,baselines,/content/training-data/natural_language_inference/32/triples/baselines.txt
flow component,from,IF layer,baselines,/content/training-data/natural_language_inference/32/triples/baselines.txt
Contribution,Code,https://github.com/momohuang/FlowQA,code,/content/training-data/natural_language_inference/32/triples/code.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/32/triples/model.txt
Model,feed,entire hidden representations,model,/content/training-data/natural_language_inference/32/triples/model.txt
entire hidden representations,generated during,process,model,/content/training-data/natural_language_inference/32/triples/model.txt
process,of,answering previous questions,model,/content/training-data/natural_language_inference/32/triples/model.txt
Model,has,FLOWQA,model,/content/training-data/natural_language_inference/32/triples/model.txt
FLOWQA,consists of,two main components,model,/content/training-data/natural_language_inference/32/triples/model.txt
two main components,has,base neural model,model,/content/training-data/natural_language_inference/32/triples/model.txt
base neural model,for,single - turn MC,model,/content/training-data/natural_language_inference/32/triples/model.txt
two main components,has,FLOW mechanism,model,/content/training-data/natural_language_inference/32/triples/model.txt
FLOW mechanism,encodes,conversation history,model,/content/training-data/natural_language_inference/32/triples/model.txt
Model,has,information transfer,model,/content/training-data/natural_language_inference/32/triples/model.txt
information transfer,happens for,each context word,model,/content/training-data/natural_language_inference/32/triples/model.txt
information transfer,allowing,rich information,model,/content/training-data/natural_language_inference/32/triples/model.txt
rich information,in,reasoning process,model,/content/training-data/natural_language_inference/32/triples/model.txt
rich information,to,flow,model,/content/training-data/natural_language_inference/32/triples/model.txt
Model,has,FLOW mechanism,model,/content/training-data/natural_language_inference/32/triples/model.txt
FLOW mechanism,is,remarkably effective,model,/content/training-data/natural_language_inference/32/triples/model.txt
remarkably effective,at tracking,world states,model,/content/training-data/natural_language_inference/32/triples/model.txt
world states,for,sequential instruction understanding,model,/content/training-data/natural_language_inference/32/triples/model.txt
FLOW mechanism,can be,viewed,model,/content/training-data/natural_language_inference/32/triples/model.txt
viewed,as,stacking single - turn QA models,model,/content/training-data/natural_language_inference/32/triples/model.txt
stacking single - turn QA models,along,dialog progression,model,/content/training-data/natural_language_inference/32/triples/model.txt
stacking single - turn QA models,building,information flow,model,/content/training-data/natural_language_inference/32/triples/model.txt
information flow,along,dialog,model,/content/training-data/natural_language_inference/32/triples/model.txt
Model,present,FLOWQA,model,/content/training-data/natural_language_inference/32/triples/model.txt
FLOWQA,designed for,conversational machine comprehension,model,/content/training-data/natural_language_inference/32/triples/model.txt
Model,propose,alternating parallel processing structure,model,/content/training-data/natural_language_inference/32/triples/model.txt
alternating parallel processing structure,which,alternates,model,/content/training-data/natural_language_inference/32/triples/model.txt
alternates,between,sequentially processing,model,/content/training-data/natural_language_inference/32/triples/model.txt
sequentially processing,has,one dimension,model,/content/training-data/natural_language_inference/32/triples/model.txt
one dimension,in parallel of,other dimension,model,/content/training-data/natural_language_inference/32/triples/model.txt
sequentially processing,has,speeds up,model,/content/training-data/natural_language_inference/32/triples/model.txt
speeds up,has,training,model,/content/training-data/natural_language_inference/32/triples/model.txt
training,has,significantly,model,/content/training-data/natural_language_inference/32/triples/model.txt
Contribution,has research problem,CONVERSATIONAL MACHINE COMPREHENSION,research-problem,/content/training-data/natural_language_inference/32/triples/research-problem.txt
Contribution,has research problem,conversational machine comprehension ( MC ,research-problem,/content/training-data/natural_language_inference/32/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/32/triples/results.txt
Results,find that,FLOW,results,/content/training-data/natural_language_inference/32/triples/results.txt
FLOW,is,critical component,results,/content/training-data/natural_language_inference/32/triples/results.txt
Results,Removing,QHier - RNN,results,/content/training-data/natural_language_inference/32/triples/results.txt
QHier - RNN,has,minor impact,results,/content/training-data/natural_language_inference/32/triples/results.txt
minor impact,has,0.1 %,results,/content/training-data/natural_language_inference/32/triples/results.txt
minor impact,on,both datasets,results,/content/training-data/natural_language_inference/32/triples/results.txt
Results,Based on,training time,results,/content/training-data/natural_language_inference/32/triples/results.txt
training time,has,speedup,results,/content/training-data/natural_language_inference/32/triples/results.txt
speedup,is,4.2 x,results,/content/training-data/natural_language_inference/32/triples/results.txt
4.2 x,on,QuAC,results,/content/training-data/natural_language_inference/32/triples/results.txt
speedup,is,8.1x,results,/content/training-data/natural_language_inference/32/triples/results.txt
8.1x,on,CoQA,results,/content/training-data/natural_language_inference/32/triples/results.txt
training time,takes,each epoch,results,/content/training-data/natural_language_inference/32/triples/results.txt
Results,comparing,0 - Ans and 1 - Ans,results,/content/training-data/natural_language_inference/32/triples/results.txt
0 - Ans and 1 - Ans,providing,gold answers,results,/content/training-data/natural_language_inference/32/triples/results.txt
gold answers,is,more crucial,results,/content/training-data/natural_language_inference/32/triples/results.txt
more crucial,for,QuAC,results,/content/training-data/natural_language_inference/32/triples/results.txt
0 - Ans and 1 - Ans,on,two datasets,results,/content/training-data/natural_language_inference/32/triples/results.txt
Results,has,FLOWQA,results,/content/training-data/natural_language_inference/32/triples/results.txt
FLOWQA,yields,substantial improvement,results,/content/training-data/natural_language_inference/32/triples/results.txt
substantial improvement,over,existing models,results,/content/training-data/natural_language_inference/32/triples/results.txt
substantial improvement,has,+ 4.0 % F 1,results,/content/training-data/natural_language_inference/32/triples/results.txt
+ 4.0 % F 1,on,QuAC,results,/content/training-data/natural_language_inference/32/triples/results.txt
substantial improvement,has,+ 7.2 % F 1,results,/content/training-data/natural_language_inference/32/triples/results.txt
+ 7.2 % F 1,on,CoQA,results,/content/training-data/natural_language_inference/32/triples/results.txt
substantial improvement,on,both datasets,results,/content/training-data/natural_language_inference/32/triples/results.txt
Results,removing,FLOW,results,/content/training-data/natural_language_inference/32/triples/results.txt
FLOW,results in,substantial performance drop,results,/content/training-data/natural_language_inference/32/triples/results.txt
substantial performance drop,with or without using,QHierRNN,results,/content/training-data/natural_language_inference/32/triples/results.txt
substantial performance drop,has,2 - 3 %,results,/content/training-data/natural_language_inference/32/triples/results.txt
2 - 3 %,on,QuAC,results,/content/training-data/natural_language_inference/32/triples/results.txt
substantial performance drop,has,4.1 %,results,/content/training-data/natural_language_inference/32/triples/results.txt
4.1 %,on,CoQA,results,/content/training-data/natural_language_inference/32/triples/results.txt
Contribution,has,Experiments,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
Experiments,for,Answer Sentence Selection,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
Answer Sentence Selection,has,Hyperparameters,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
Hyperparameters,in,prediction,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
prediction,use,20 samples,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
20 samples,to calculate,expectation,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
expectation,of,lower bound,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
Hyperparameters,use,LSTMs,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
LSTMs,with,3 layers,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
LSTMs,with,50 hidden units,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
LSTMs,apply,40 % dropout,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
40 % dropout,after,embedding layer,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
Hyperparameters,For the construction of,inference network,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
inference network,use,MLP,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
MLP,with,2 layers,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
inference network,use,tanh units,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
tanh units,of,50 dimension,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
Hyperparameters,has,word embeddings ( K = 50 ,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
word embeddings ( K = 50 ),are,obtained,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
obtained,by running,word2vec tool,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
word2vec tool,on,English Wikipedia dump,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
word2vec tool,on,AQUAINT 5 corpus,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
Hyperparameters,for modelling,joint representation,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
joint representation,has,MLP,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
MLP,with,2 layers,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
joint representation,has,tanh units,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
tanh units,of,150 dimension,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
Hyperparameters,During,training,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
training,carry out,stochastic estimation,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
stochastic estimation,by taking,one sample,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
one sample,for computing,gradients,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
Answer Sentence Selection,has,Results,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
Results,including,word count feature,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
word count feature,has,our models,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
our models,achieve,state - of - the - art,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
our models,has,improve further,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
Results,has,LSTM + Att,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
LSTM + Att,performs,slightly better,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
slightly better,than,vanilla LSTM model,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
slightly better,has,our NASM,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
our NASM,improves,results,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
Results,has,our best model,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
our best model,after combining with,co-occurrence word count feature,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
our best model,outperforms,all the previous models,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
all the previous models,including,neural network based models,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
all the previous models,including,classifiers,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
classifiers,with,set,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
set,of,hand - crafted features,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
Results,on,Wik - iQA dataset,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
Wik - iQA dataset,has,all of our models,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
all of our models,outperform,previous distributional models,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
previous distributional models,by,large margin,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
Experiments,on,Document Modelling,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
Document Modelling,has,Results,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
Results,For,experiments,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
experiments,on,RCV1 - v2 dataset,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
RCV1 - v2 dataset,has,NVDM,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
NVDM,with,latent variable,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
latent variable,performs,even better,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
even better,than,fDARN,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
fDARN,with,200 dimension,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
latent variable,of,50 dimension,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
Results,indicate,NVDM,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
NVDM,achieves,best performance,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
best performance,on,both datasets,experiments,/content/training-data/natural_language_inference/18/triples/experiments.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/18/triples/model.txt
Model,using,reparameteris ation method,model,/content/training-data/natural_language_inference/18/triples/model.txt
reparameteris ation method,has,inference network,model,/content/training-data/natural_language_inference/18/triples/model.txt
inference network,trained through,back - propagating,model,/content/training-data/natural_language_inference/18/triples/model.txt
back - propagating,has,unbiased and low variance gradients,model,/content/training-data/natural_language_inference/18/triples/model.txt
unbiased and low variance gradients,w.r.t.,latent variables,model,/content/training-data/natural_language_inference/18/triples/model.txt
Model,introduces,neural variational framework,model,/content/training-data/natural_language_inference/18/triples/model.txt
neural variational framework,for,generative models,model,/content/training-data/natural_language_inference/18/triples/model.txt
generative models,of,text,model,/content/training-data/natural_language_inference/18/triples/model.txt
neural variational framework,inspired by,variational autoencoder,model,/content/training-data/natural_language_inference/18/triples/model.txt
Model,build,inference network,model,/content/training-data/natural_language_inference/18/triples/model.txt
inference network,implemented by,deep neural network,model,/content/training-data/natural_language_inference/18/triples/model.txt
deep neural network,conditioned on,text,model,/content/training-data/natural_language_inference/18/triples/model.txt
inference network,to approximate,intractable distributions,model,/content/training-data/natural_language_inference/18/triples/model.txt
intractable distributions,over,latent variables,model,/content/training-data/natural_language_inference/18/triples/model.txt
Model,has,primary feature,model,/content/training-data/natural_language_inference/18/triples/model.txt
primary feature,of,NVDM,model,/content/training-data/natural_language_inference/18/triples/model.txt
NVDM,is,each word,model,/content/training-data/natural_language_inference/18/triples/model.txt
each word,generated directly from,dense continuous document representation,model,/content/training-data/natural_language_inference/18/triples/model.txt
dense continuous document representation,instead of,more common binary semantic vector,model,/content/training-data/natural_language_inference/18/triples/model.txt
Model,has,NASM,model,/content/training-data/natural_language_inference/18/triples/model.txt
NASM,is,supervised conditional model,model,/content/training-data/natural_language_inference/18/triples/model.txt
supervised conditional model,imbues,LSTMs,model,/content/training-data/natural_language_inference/18/triples/model.txt
LSTMs,with,latent stochastic attention mechanism,model,/content/training-data/natural_language_inference/18/triples/model.txt
LSTMs,to model,semantics,model,/content/training-data/natural_language_inference/18/triples/model.txt
semantics,of,question - answer pairs,model,/content/training-data/natural_language_inference/18/triples/model.txt
NASM,has,attention model,model,/content/training-data/natural_language_inference/18/triples/model.txt
attention model,designed to,focus,model,/content/training-data/natural_language_inference/18/triples/model.txt
focus,on,phrases,model,/content/training-data/natural_language_inference/18/triples/model.txt
phrases,of,answer,model,/content/training-data/natural_language_inference/18/triples/model.txt
attention model,modelled by,latent distribution,model,/content/training-data/natural_language_inference/18/triples/model.txt
attention model,strongly connected to,question semantics,model,/content/training-data/natural_language_inference/18/triples/model.txt
NASM,has,Bayesian inference,model,/content/training-data/natural_language_inference/18/triples/model.txt
Bayesian inference,provides,natural safeguard,model,/content/training-data/natural_language_inference/18/triples/model.txt
natural safeguard,against,overfitting,model,/content/training-data/natural_language_inference/18/triples/model.txt
Model,has,neural variational inference,model,/content/training-data/natural_language_inference/18/triples/model.txt
neural variational inference,learns to model,posterior probability,model,/content/training-data/natural_language_inference/18/triples/model.txt
Model,propose,Neural Answer Selection Model ( NASM ,model,/content/training-data/natural_language_inference/18/triples/model.txt
Neural Answer Selection Model ( NASM ),for,question answering,model,/content/training-data/natural_language_inference/18/triples/model.txt
Model,propose,Neural Variational Document Model ( NVDM ,model,/content/training-data/natural_language_inference/18/triples/model.txt
Neural Variational Document Model ( NVDM ),for,document modelling,model,/content/training-data/natural_language_inference/18/triples/model.txt
Contribution,has research problem,Neural Variational Inference,research-problem,/content/training-data/natural_language_inference/18/triples/research-problem.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
Ablation analysis,shows,attentive information,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
attentive information,functioning as,soft - alignment,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
attentive information,is,significantly effective,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
significantly effective,in,semantic sentence matching,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
Ablation analysis,shows,connection,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
connection,endowing,more representational power,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
connection,is,essential,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
connection,has,dense connection,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
dense connection,is,more effective,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
more effective,than,residual connection,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
connection,between,layers,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
Ablation analysis,shows,connections,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
connections,among,layers,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
connections,important to help,gradient flow,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
Ablation analysis,removed,dense connections,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
dense connections,over,both co-attentive and recurrent features,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
dense connections,has,performance,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
performance,has,degraded,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
degraded,to,88.5 %,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
Ablation analysis,could see,performance,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
performance,because of,regularization effect,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
performance,was,rather higher,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
Ablation analysis,has,result,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
result,shows,dense connections,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
dense connections,over,attentive features,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
dense connections,are,more effective,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
Ablation analysis,has,models,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
models,with,dense connections,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
dense connections,have,higher performance,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
dense connections,rather than,residual connections,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
models,which have,connections,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
connections,are,more robust,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
more robust,to,increased depth of network,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
connections,between,layers,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
models,has,performances,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
performances,tend to,degrade,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
degrade,as,layers,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
layers,get,deeper,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
Ablation analysis,demonstrate,dense connection,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
dense connection,using,concatenation operation,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
concatenation operation,over,deeper layers,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
dense connection,has,more powerful capability,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
more powerful capability,retaining,collective knowledge,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
collective knowledge,to learn,textual semantics,ablation-analysis,/content/training-data/natural_language_inference/15/triples/ablation-analysis.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
Hyperparameters,randomly initialized,character embedding,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
character embedding,with,16d vector,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
Hyperparameters,set,1000 hidden units,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
1000 hidden units,with respect to,fullyconnected layers,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
Hyperparameters,For,bottleneck component,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
bottleneck component,set,200 hidden units,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
200 hidden units,with,dropout rate,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
dropout rate,of,0.2,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
200 hidden units,as,encoded features,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
encoded features,of,autoencoder,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
Hyperparameters,For,densely - connected recurrent layers,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
densely - connected recurrent layers,stacked,5 layers,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
5 layers,each of which have,100 hidden units,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
Hyperparameters,initialized,word embedding,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
word embedding,with,300d Glo Ve vectors,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
Hyperparameters,has,sequence lengths,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
sequence lengths,are,all different,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
all different,for,each dataset,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
each dataset,has,55,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
55,for,MultiNLI,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
each dataset,has,35,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
35,for,SNLI,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
each dataset,has,25,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
25,for,Quora question pair,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
each dataset,has,50,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
50,for,TrecQA,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
sequence lengths,of,sentence,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
Hyperparameters,has,batch normalization,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
batch normalization,applied on,fully - connected layers,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
batch normalization,for,one - way type datasets,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
Hyperparameters,has,dropout,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
dropout,applied before,fully - connected layers,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
fully - connected layers,with,keep rate,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
keep rate,of,0.8,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
dropout,applied after,word and character embedding layers,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
word and character embedding layers,with,keep rate,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
keep rate,of,0.5,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
Hyperparameters,has,learning rate,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
learning rate,decreased by,factor,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
factor,of,0.85,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
factor,when,dev accuracy,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
dev accuracy,has,does not improve,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
Hyperparameters,has,RMSProp optimizer,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
RMSProp optimizer,with,initial learning rate,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
initial learning rate,of,0.001,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
Hyperparameters,has,word embeddings,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
word embeddings,for,out - of - vocabulary words,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
word embeddings,initialized,randomly,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
Hyperparameters,has,weights,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
weights,constrained by,L2 regularization,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
L2 regularization,with,regularization constant ? = 10 ?6,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
weights,except,embedding matrices,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
Hyperparameters,extracted,32d character representation,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
32d character representation,with,convolutional network,hyperparameters,/content/training-data/natural_language_inference/15/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/15/triples/model.txt
Model,called,DRCN,model,/content/training-data/natural_language_inference/15/triples/model.txt
DRCN,abbreviation for,Densely - connected Recurrent and Co -attentive neural Network,model,/content/training-data/natural_language_inference/15/triples/model.txt
Model,proposed,DRCN,model,/content/training-data/natural_language_inference/15/triples/model.txt
DRCN,utilize,increased representational power,model,/content/training-data/natural_language_inference/15/triples/model.txt
increased representational power,of,deeper recurrent networks,model,/content/training-data/natural_language_inference/15/triples/model.txt
increased representational power,of,attentive information,model,/content/training-data/natural_language_inference/15/triples/model.txt
Model,instead of,conventional summation operation,model,/content/training-data/natural_language_inference/15/triples/model.txt
conventional summation operation,used,concatenation operation,model,/content/training-data/natural_language_inference/15/triples/model.txt
concatenation operation,in combination with,attention mechanism,model,/content/training-data/natural_language_inference/15/triples/model.txt
concatenation operation,to preserve,co-attentive information,model,/content/training-data/natural_language_inference/15/triples/model.txt
co-attentive information,has,better,model,/content/training-data/natural_language_inference/15/triples/model.txt
Model,propose,densely - connected recurrent network,model,/content/training-data/natural_language_inference/15/triples/model.txt
densely - connected recurrent network,where,recurrent hidden features,model,/content/training-data/natural_language_inference/15/triples/model.txt
recurrent hidden features,retained to,uppermost layer,model,/content/training-data/natural_language_inference/15/triples/model.txt
Model,adopted,autoencoder,model,/content/training-data/natural_language_inference/15/triples/model.txt
Model,forwarded,fixed length vector,model,/content/training-data/natural_language_inference/15/triples/model.txt
fixed length vector,to,higher layer recurrent module,model,/content/training-data/natural_language_inference/15/triples/model.txt
Contribution,has research problem,Semantic Sentence Matching,research-problem,/content/training-data/natural_language_inference/15/triples/research-problem.txt
Contribution,has research problem,Sentence matching,research-problem,/content/training-data/natural_language_inference/15/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/15/triples/results.txt
Results,in case of,encoding - based method,results,/content/training-data/natural_language_inference/15/triples/results.txt
encoding - based method,obtain,best performance,results,/content/training-data/natural_language_inference/15/triples/results.txt
best performance,of,86.5 %,results,/content/training-data/natural_language_inference/15/triples/results.txt
best performance,without,co-attention and exact match flag,results,/content/training-data/natural_language_inference/15/triples/results.txt
Results,of,MultiNLI dataset,results,/content/training-data/natural_language_inference/15/triples/results.txt
MultiNLI dataset,has,Our plain DRCN,results,/content/training-data/natural_language_inference/15/triples/results.txt
Our plain DRCN,has,competitive performance,results,/content/training-data/natural_language_inference/15/triples/results.txt
competitive performance,without,contextualized knowledge,results,/content/training-data/natural_language_inference/15/triples/results.txt
MultiNLI dataset,by combining,DRCN,results,/content/training-data/natural_language_inference/15/triples/results.txt
DRCN,with,ELMo,results,/content/training-data/natural_language_inference/15/triples/results.txt
DRCN,has,our model,results,/content/training-data/natural_language_inference/15/triples/results.txt
our model,with,fewer parameters,results,/content/training-data/natural_language_inference/15/triples/results.txt
fewer parameters,of,61 m,results,/content/training-data/natural_language_inference/15/triples/results.txt
our model,outperforms,LM - Transformer,results,/content/training-data/natural_language_inference/15/triples/results.txt
LM - Transformer,which has,85 m parameters,results,/content/training-data/natural_language_inference/15/triples/results.txt
Results,has,ensemble model,results,/content/training-data/natural_language_inference/15/triples/results.txt
ensemble model,with,53 m parameters ( 6.7 m 8 ,results,/content/training-data/natural_language_inference/15/triples/results.txt
53 m parameters ( 6.7 m 8 ),outperforms,LM - Transformer,results,/content/training-data/natural_language_inference/15/triples/results.txt
LM - Transformer,whose,number of parameters,results,/content/training-data/natural_language_inference/15/triples/results.txt
number of parameters,is,85 m,results,/content/training-data/natural_language_inference/15/triples/results.txt
ensemble model,achieves,accuracy,results,/content/training-data/natural_language_inference/15/triples/results.txt
accuracy,sets,new state - of the - art performance,results,/content/training-data/natural_language_inference/15/triples/results.txt
accuracy,of,90.1 %,results,/content/training-data/natural_language_inference/15/triples/results.txt
Results,has,proposed DRCN,results,/content/training-data/natural_language_inference/15/triples/results.txt
proposed DRCN,obtains,accuracy,results,/content/training-data/natural_language_inference/15/triples/results.txt
accuracy,of,88.9 %,results,/content/training-data/natural_language_inference/15/triples/results.txt
88.9 %,is,competitive score,results,/content/training-data/natural_language_inference/15/triples/results.txt
Results,on,Quora question pair dataset,results,/content/training-data/natural_language_inference/15/triples/results.txt
Quora question pair dataset,obtained,accuracies,results,/content/training-data/natural_language_inference/15/triples/results.txt
accuracies,of,90.15 % and 91.30 %,results,/content/training-data/natural_language_inference/15/triples/results.txt
90.15 % and 91.30 %,in,single and ensemble methods,results,/content/training-data/natural_language_inference/15/triples/results.txt
90.15 % and 91.30 %,surpassing,previous state - of - the - art model,results,/content/training-data/natural_language_inference/15/triples/results.txt
previous state - of - the - art model,of,DIIN,results,/content/training-data/natural_language_inference/15/triples/results.txt
Results,on,TrecQA and SelQA datasets,results,/content/training-data/natural_language_inference/15/triples/results.txt
TrecQA and SelQA datasets,has,proposed DRCN,results,/content/training-data/natural_language_inference/15/triples/results.txt
proposed DRCN,using,collective attentions,results,/content/training-data/natural_language_inference/15/triples/results.txt
collective attentions,over,multiple layers,results,/content/training-data/natural_language_inference/15/triples/results.txt
proposed DRCN,achieves,new state - of the - art performance,results,/content/training-data/natural_language_inference/15/triples/results.txt
new state - of the - art performance,exceeding,current state - of - the - art performance,results,/content/training-data/natural_language_inference/15/triples/results.txt
current state - of - the - art performance,has,significantly,results,/content/training-data/natural_language_inference/15/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/4/triples/ablation-analysis.txt
Ablation analysis,note,deep residual coattention,ablation-analysis,/content/training-data/natural_language_inference/4/triples/ablation-analysis.txt
deep residual coattention,yielded,highest contribution,ablation-analysis,/content/training-data/natural_language_inference/4/triples/ablation-analysis.txt
highest contribution,followed by,mixed objective,ablation-analysis,/content/training-data/natural_language_inference/4/triples/ablation-analysis.txt
highest contribution,to,model performance,ablation-analysis,/content/training-data/natural_language_inference/4/triples/ablation-analysis.txt
Ablation analysis,has,sparse mixture,ablation-analysis,/content/training-data/natural_language_inference/4/triples/ablation-analysis.txt
sparse mixture,of,experts layer,ablation-analysis,/content/training-data/natural_language_inference/4/triples/ablation-analysis.txt
experts layer,in,decoder,ablation-analysis,/content/training-data/natural_language_inference/4/triples/ablation-analysis.txt
experts layer,added,minor improvements,ablation-analysis,/content/training-data/natural_language_inference/4/triples/ablation-analysis.txt
minor improvements,to,model performance,ablation-analysis,/content/training-data/natural_language_inference/4/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/4/triples/model.txt
Model,obtain,latter objective,model,/content/training-data/natural_language_inference/4/triples/model.txt
latter objective,using,self - critical policy learning,model,/content/training-data/natural_language_inference/4/triples/model.txt
self - critical policy learning,in which,reward,model,/content/training-data/natural_language_inference/4/triples/model.txt
reward,based on,word overlap,model,/content/training-data/natural_language_inference/4/triples/model.txt
word overlap,between,proposed answer,model,/content/training-data/natural_language_inference/4/triples/model.txt
word overlap,between,ground truth answer,model,/content/training-data/natural_language_inference/4/triples/model.txt
Model,extend,Dynamic Coattention Network ( DCN ,model,/content/training-data/natural_language_inference/4/triples/model.txt
Dynamic Coattention Network ( DCN ),with,deep residual coattention encoder,model,/content/training-data/natural_language_inference/4/triples/model.txt
Model,propose,mixed objective,model,/content/training-data/natural_language_inference/4/triples/model.txt
mixed objective,combines,traditional cross entropy loss,model,/content/training-data/natural_language_inference/4/triples/model.txt
traditional cross entropy loss,over,positions,model,/content/training-data/natural_language_inference/4/triples/model.txt
traditional cross entropy loss,with,measure of word overlap,model,/content/training-data/natural_language_inference/4/triples/model.txt
measure of word overlap,trained with,reinforcement learning,model,/content/training-data/natural_language_inference/4/triples/model.txt
Contribution,has research problem,QUESTION ANSWERING,research-problem,/content/training-data/natural_language_inference/4/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/4/triples/experimental-setup.txt
Experimental setup,preprocess,corpus,experimental-setup,/content/training-data/natural_language_inference/4/triples/experimental-setup.txt
corpus,use,reversible tokenizer,experimental-setup,/content/training-data/natural_language_inference/4/triples/experimental-setup.txt
reversible tokenizer,from,Stanford CoreNLP,experimental-setup,/content/training-data/natural_language_inference/4/triples/experimental-setup.txt
Experimental setup,swap,first maxout layer,experimental-setup,/content/training-data/natural_language_inference/4/triples/experimental-setup.txt
first maxout layer,with,sparse mixture,experimental-setup,/content/training-data/natural_language_inference/4/triples/experimental-setup.txt
sparse mixture,of,experts layer,experimental-setup,/content/training-data/natural_language_inference/4/triples/experimental-setup.txt
first maxout layer,of,highway maxout network,experimental-setup,/content/training-data/natural_language_inference/4/triples/experimental-setup.txt
highway maxout network,in,DCN decoder,experimental-setup,/content/training-data/natural_language_inference/4/triples/experimental-setup.txt
Experimental setup,For,out of vocabulary words,experimental-setup,/content/training-data/natural_language_inference/4/triples/experimental-setup.txt
out of vocabulary words,set,embeddings and context vectors,experimental-setup,/content/training-data/natural_language_inference/4/triples/experimental-setup.txt
embeddings and context vectors,to,zero,experimental-setup,/content/training-data/natural_language_inference/4/triples/experimental-setup.txt
Experimental setup,For,word embeddings,experimental-setup,/content/training-data/natural_language_inference/4/triples/experimental-setup.txt
word embeddings,concatenate,these embeddings,experimental-setup,/content/training-data/natural_language_inference/4/triples/experimental-setup.txt
these embeddings,with,context vectors ( CoVe ,experimental-setup,/content/training-data/natural_language_inference/4/triples/experimental-setup.txt
word embeddings,use,GloVe embeddings,experimental-setup,/content/training-data/natural_language_inference/4/triples/experimental-setup.txt
GloVe embeddings,pretrained on,840B Common Crawl corpus,experimental-setup,/content/training-data/natural_language_inference/4/triples/experimental-setup.txt
Experimental setup,perform,word dropout,experimental-setup,/content/training-data/natural_language_inference/4/triples/experimental-setup.txt
word dropout,on,document,experimental-setup,/content/training-data/natural_language_inference/4/triples/experimental-setup.txt
document,zeros,word embedding,experimental-setup,/content/training-data/natural_language_inference/4/triples/experimental-setup.txt
word embedding,with,probability 0.075,experimental-setup,/content/training-data/natural_language_inference/4/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/4/triples/results.txt
Results,Comparison to,baseline DCN with CoVe,results,/content/training-data/natural_language_inference/4/triples/results.txt
baseline DCN with CoVe,has,DCN +,results,/content/training-data/natural_language_inference/4/triples/results.txt
DCN +,outperforms,baseline,results,/content/training-data/natural_language_inference/4/triples/results.txt
baseline,on,SQuAD development set,results,/content/training-data/natural_language_inference/4/triples/results.txt
SQuAD development set,by,3.2 % exact match accuracy,results,/content/training-data/natural_language_inference/4/triples/results.txt
SQuAD development set,by,3.2 % F1,results,/content/training-data/natural_language_inference/4/triples/results.txt
Results,shows,consistent performance gain,results,/content/training-data/natural_language_inference/4/triples/results.txt
consistent performance gain,over,baseline,results,/content/training-data/natural_language_inference/4/triples/results.txt
consistent performance gain,across,question types,results,/content/training-data/natural_language_inference/4/triples/results.txt
consistent performance gain,across,question lengths,results,/content/training-data/natural_language_inference/4/triples/results.txt
consistent performance gain,across,answer lengths,results,/content/training-data/natural_language_inference/4/triples/results.txt
consistent performance gain,of,DCN +,results,/content/training-data/natural_language_inference/4/triples/results.txt
Results,has,DCN +,results,/content/training-data/natural_language_inference/4/triples/results.txt
DCN +,provides,significant advantage,results,/content/training-data/natural_language_inference/4/triples/results.txt
significant advantage,for,long questions,results,/content/training-data/natural_language_inference/4/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
Ablation analysis,Removing,hypothesis - based attention ( model 24 ,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
hypothesis - based attention ( model 24 ),has,decrease,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
decrease,has,accuracy,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
accuracy,to,86.5 %,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
Ablation analysis,has,Each tree node,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
Each tree node,implemented with,tree - LSTM block,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
tree - LSTM block,performance,drops,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
drops,to,88.2 %,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
Ablation analysis,replacing,bidirectional LSTMs,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
bidirectional LSTMs,in,inference composition,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
Ablation analysis,replacing,reduces,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
reduces,has,accuracy,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
accuracy,to,87.3 % and 86.3 %,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
Ablation analysis,replacing,input encoding,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
input encoding,with,feedforward neural network,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
Ablation analysis,remove,premise - based attention,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
premise - based attention,from,ESIM,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
premise - based attention,has,accuracy,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
accuracy,has,drops,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
drops,to,87.2 %,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
drops,on,test set,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
Ablation analysis,remove,pooling layer,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
pooling layer,in,inference composition,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
pooling layer,replace it with,summation,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
summation,has,accuracy,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
accuracy,has,drops,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
drops,to,87.1 %,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
Ablation analysis,remove,difference and elementwise product,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
difference and elementwise product,from,local inference enhancement layer,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
local inference enhancement layer,has,accuracy,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
accuracy,has,drops,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
drops,to,87.0 %,ablation-analysis,/content/training-data/natural_language_inference/85/triples/ablation-analysis.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/natural_language_inference/85/triples/hyperparameters.txt
Hyperparameters,use,dropout,hyperparameters,/content/training-data/natural_language_inference/85/triples/hyperparameters.txt
dropout,with,rate,hyperparameters,/content/training-data/natural_language_inference/85/triples/hyperparameters.txt
rate,of,0.5,hyperparameters,/content/training-data/natural_language_inference/85/triples/hyperparameters.txt
dropout,applied to,all feedforward connections,hyperparameters,/content/training-data/natural_language_inference/85/triples/hyperparameters.txt
Hyperparameters,use,Adam method,hyperparameters,/content/training-data/natural_language_inference/85/triples/hyperparameters.txt
Adam method,for,optimization,hyperparameters,/content/training-data/natural_language_inference/85/triples/hyperparameters.txt
Hyperparameters,use,pre-trained 300 - D Glove 840B vectors,hyperparameters,/content/training-data/natural_language_inference/85/triples/hyperparameters.txt
pre-trained 300 - D Glove 840B vectors,to initialize,our word embeddings,hyperparameters,/content/training-data/natural_language_inference/85/triples/hyperparameters.txt
Hyperparameters,has,initial learning rate,hyperparameters,/content/training-data/natural_language_inference/85/triples/hyperparameters.txt
initial learning rate,is,0.0004,hyperparameters,/content/training-data/natural_language_inference/85/triples/hyperparameters.txt
Hyperparameters,has,Out - of - vocabulary ( OOV ) words,hyperparameters,/content/training-data/natural_language_inference/85/triples/hyperparameters.txt
Out - of - vocabulary ( OOV ) words,initialized,randomly,hyperparameters,/content/training-data/natural_language_inference/85/triples/hyperparameters.txt
randomly,with,Gaussian samples,hyperparameters,/content/training-data/natural_language_inference/85/triples/hyperparameters.txt
Hyperparameters,has,first momentum,hyperparameters,/content/training-data/natural_language_inference/85/triples/hyperparameters.txt
first momentum,set to be,0.9,hyperparameters,/content/training-data/natural_language_inference/85/triples/hyperparameters.txt
Hyperparameters,has,batch size,hyperparameters,/content/training-data/natural_language_inference/85/triples/hyperparameters.txt
batch size,is,32,hyperparameters,/content/training-data/natural_language_inference/85/triples/hyperparameters.txt
Hyperparameters,has,hidden states,hyperparameters,/content/training-data/natural_language_inference/85/triples/hyperparameters.txt
hidden states,of,LSTMs,hyperparameters,/content/training-data/natural_language_inference/85/triples/hyperparameters.txt
hidden states,of,tree - LSTMs,hyperparameters,/content/training-data/natural_language_inference/85/triples/hyperparameters.txt
hidden states,of,word embeddings,hyperparameters,/content/training-data/natural_language_inference/85/triples/hyperparameters.txt
hidden states,have,300 dimensions,hyperparameters,/content/training-data/natural_language_inference/85/triples/hyperparameters.txt
Hyperparameters,has,second,hyperparameters,/content/training-data/natural_language_inference/85/triples/hyperparameters.txt
second,has,0.999,hyperparameters,/content/training-data/natural_language_inference/85/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/85/triples/model.txt
Model,enhancing,sequential inference models,model,/content/training-data/natural_language_inference/85/triples/model.txt
sequential inference models,based on,chain models,model,/content/training-data/natural_language_inference/85/triples/model.txt
Model,explicitly encoding,parsing information,model,/content/training-data/natural_language_inference/85/triples/model.txt
parsing information,with,recursive networks,model,/content/training-data/natural_language_inference/85/triples/model.txt
parsing information,in,local inference modeling,model,/content/training-data/natural_language_inference/85/triples/model.txt
parsing information,in,inference composition,model,/content/training-data/natural_language_inference/85/triples/model.txt
Contribution,has research problem,Natural Language Inference,research-problem,/content/training-data/natural_language_inference/85/triples/research-problem.txt
Contribution,has research problem,Reasoning and inference,research-problem,/content/training-data/natural_language_inference/85/triples/research-problem.txt
Contribution,has research problem,natural language inference ( NLI ,research-problem,/content/training-data/natural_language_inference/85/triples/research-problem.txt
Contribution,has research problem,NLI,research-problem,/content/training-data/natural_language_inference/85/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/85/triples/results.txt
Results,adding,intra-sentence attention,results,/content/training-data/natural_language_inference/85/triples/results.txt
intra-sentence attention,yields,further improvement,results,/content/training-data/natural_language_inference/85/triples/results.txt
Results,ensemble,our ESIM model,results,/content/training-data/natural_language_inference/85/triples/results.txt
our ESIM model,with,syntactic tree - LSTMs,results,/content/training-data/natural_language_inference/85/triples/results.txt
syntactic tree - LSTMs,based on,syntactic parse trees,results,/content/training-data/natural_language_inference/85/triples/results.txt
our ESIM model,achieve,significant improvement,results,/content/training-data/natural_language_inference/85/triples/results.txt
significant improvement,over,our best sequential encoding model ESIM,results,/content/training-data/natural_language_inference/85/triples/results.txt
significant improvement,attaining,accuracy,results,/content/training-data/natural_language_inference/85/triples/results.txt
accuracy,of,88.6 %,results,/content/training-data/natural_language_inference/85/triples/results.txt
Results,has,our ESIM model,results,/content/training-data/natural_language_inference/85/triples/results.txt
our ESIM model,achieves,accuracy,results,/content/training-data/natural_language_inference/85/triples/results.txt
accuracy,of,88.0 %,results,/content/training-data/natural_language_inference/85/triples/results.txt
our ESIM model,has,outperformed,results,/content/training-data/natural_language_inference/85/triples/results.txt
outperformed,has,previous models,results,/content/training-data/natural_language_inference/85/triples/results.txt
previous models,including,more complicated network architectures,results,/content/training-data/natural_language_inference/85/triples/results.txt
Results,has,Our final model,results,/content/training-data/natural_language_inference/85/triples/results.txt
Our final model,achieves,accuracy,results,/content/training-data/natural_language_inference/85/triples/results.txt
accuracy,of,88.6 %,results,/content/training-data/natural_language_inference/85/triples/results.txt
88.6 %,has,best result,results,/content/training-data/natural_language_inference/85/triples/results.txt
best result,observed on,SNLI,results,/content/training-data/natural_language_inference/85/triples/results.txt
Results,has,our enhanced sequential encoding model,results,/content/training-data/natural_language_inference/85/triples/results.txt
our enhanced sequential encoding model,attains,accuracy,results,/content/training-data/natural_language_inference/85/triples/results.txt
accuracy,of,88.0 %,results,/content/training-data/natural_language_inference/85/triples/results.txt
88.0 %,has,outperform,results,/content/training-data/natural_language_inference/85/triples/results.txt
outperform,has,previous models,results,/content/training-data/natural_language_inference/85/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/89/triples/ablation-analysis.txt
Ablation analysis,Ablating,two auxiliary losses,ablation-analysis,/content/training-data/natural_language_inference/89/triples/ablation-analysis.txt
two auxiliary losses,leads to,over all degradation,ablation-analysis,/content/training-data/natural_language_inference/89/triples/ablation-analysis.txt
over all degradation,on,curve,ablation-analysis,/content/training-data/natural_language_inference/89/triples/ablation-analysis.txt
two auxiliary losses,leads to,outperforms,ablation-analysis,/content/training-data/natural_language_inference/89/triples/ablation-analysis.txt
outperforms,by,large margin,ablation-analysis,/content/training-data/natural_language_inference/89/triples/ablation-analysis.txt
outperforms,has,baseline,ablation-analysis,/content/training-data/natural_language_inference/89/triples/ablation-analysis.txt
Ablation analysis,Ablating,independent no - answer loss ( indep - II ,ablation-analysis,/content/training-data/natural_language_inference/89/triples/ablation-analysis.txt
independent no - answer loss ( indep - II ),leads to,severe decline,ablation-analysis,/content/training-data/natural_language_inference/89/triples/ablation-analysis.txt
severe decline,on,no - answer accuracy ( NoAns ACC ,ablation-analysis,/content/training-data/natural_language_inference/89/triples/ablation-analysis.txt
independent no - answer loss ( indep - II ),causes,little influence,ablation-analysis,/content/training-data/natural_language_inference/89/triples/ablation-analysis.txt
little influence,on,HasAns,ablation-analysis,/content/training-data/natural_language_inference/89/triples/ablation-analysis.txt
Ablation analysis,deleting,both of two losses,ablation-analysis,/content/training-data/natural_language_inference/89/triples/ablation-analysis.txt
both of two losses,causes,degradation,ablation-analysis,/content/training-data/natural_language_inference/89/triples/ablation-analysis.txt
degradation,of,more than 1.5 points,ablation-analysis,/content/training-data/natural_language_inference/89/triples/ablation-analysis.txt
more than 1.5 points,on,over all performance,ablation-analysis,/content/training-data/natural_language_inference/89/triples/ablation-analysis.txt
over all performance,with or without,ELMo embeddings,ablation-analysis,/content/training-data/natural_language_inference/89/triples/ablation-analysis.txt
over all performance,in terms of,F1,ablation-analysis,/content/training-data/natural_language_inference/89/triples/ablation-analysis.txt
Ablation analysis,find that,improvement,ablation-analysis,/content/training-data/natural_language_inference/89/triples/ablation-analysis.txt
improvement,is,significant,ablation-analysis,/content/training-data/natural_language_inference/89/triples/ablation-analysis.txt
improvement,on,noanswer accuracy,ablation-analysis,/content/training-data/natural_language_inference/89/triples/ablation-analysis.txt
Ablation analysis,Removing,independent span loss ( indep - I ,ablation-analysis,/content/training-data/natural_language_inference/89/triples/ablation-analysis.txt
independent span loss ( indep - I ),results in,performance drop,ablation-analysis,/content/training-data/natural_language_inference/89/triples/ablation-analysis.txt
performance drop,for,all answerable questions ( HasAns ,ablation-analysis,/content/training-data/natural_language_inference/89/triples/ablation-analysis.txt
Ablation analysis,Adding,ELMo embeddings,ablation-analysis,/content/training-data/natural_language_inference/89/triples/ablation-analysis.txt
ELMo embeddings,does not,boost,ablation-analysis,/content/training-data/natural_language_inference/89/triples/ablation-analysis.txt
boost,has,performance,ablation-analysis,/content/training-data/natural_language_inference/89/triples/ablation-analysis.txt
Ablation analysis,observe,RMR + ELMo + Verifier,ablation-analysis,/content/training-data/natural_language_inference/89/triples/ablation-analysis.txt
RMR + ELMo + Verifier,achieves,best precision,ablation-analysis,/content/training-data/natural_language_inference/89/triples/ablation-analysis.txt
best precision,when,recall,ablation-analysis,/content/training-data/natural_language_inference/89/triples/ablation-analysis.txt
recall,is,less than 80,ablation-analysis,/content/training-data/natural_language_inference/89/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/89/triples/model.txt
Model,augment,existing readers,model,/content/training-data/natural_language_inference/89/triples/model.txt
existing readers,with,two auxiliary losses,model,/content/training-data/natural_language_inference/89/triples/model.txt
Model,leverage,multi-head pointer network,model,/content/training-data/natural_language_inference/89/triples/model.txt
multi-head pointer network,to generate,two pairs of span scores,model,/content/training-data/natural_language_inference/89/triples/model.txt
two pairs of span scores,where,other,model,/content/training-data/natural_language_inference/89/triples/model.txt
other,used for,auxiliary loss,model,/content/training-data/natural_language_inference/89/triples/model.txt
two pairs of span scores,where,one pair,model,/content/training-data/natural_language_inference/89/triples/model.txt
one pair,is,normalized,model,/content/training-data/natural_language_inference/89/triples/model.txt
normalized,with,no -answer score,model,/content/training-data/natural_language_inference/89/triples/model.txt
Model,introduce,additional answer verifying phase,model,/content/training-data/natural_language_inference/89/triples/model.txt
additional answer verifying phase,aims at,finding,model,/content/training-data/natural_language_inference/89/triples/model.txt
finding,has,local entailment,model,/content/training-data/natural_language_inference/89/triples/model.txt
local entailment,supports,answer,model,/content/training-data/natural_language_inference/89/triples/model.txt
Model,introducing,independent span loss,model,/content/training-data/natural_language_inference/89/triples/model.txt
independent span loss,aims to,concentrate,model,/content/training-data/natural_language_inference/89/triples/model.txt
concentrate,on,answer extraction task,model,/content/training-data/natural_language_inference/89/triples/model.txt
Model,has,our system,model,/content/training-data/natural_language_inference/89/triples/model.txt
our system,consists of,two components,model,/content/training-data/natural_language_inference/89/triples/model.txt
two components,has,no-answer reader,model,/content/training-data/natural_language_inference/89/triples/model.txt
no-answer reader,for extracting,candidate answers,model,/content/training-data/natural_language_inference/89/triples/model.txt
no-answer reader,detecting,unanswerable questions,model,/content/training-data/natural_language_inference/89/triples/model.txt
two components,has,answer verifier,model,/content/training-data/natural_language_inference/89/triples/model.txt
answer verifier,for deciding,extracted candidate,model,/content/training-data/natural_language_inference/89/triples/model.txt
extracted candidate,is,legitimate,model,/content/training-data/natural_language_inference/89/triples/model.txt
Model,investigate,three different architectures,model,/content/training-data/natural_language_inference/89/triples/model.txt
three different architectures,for,answer verifying task,model,/content/training-data/natural_language_inference/89/triples/model.txt
three different architectures,has,first one,model,/content/training-data/natural_language_inference/89/triples/model.txt
first one,is,sequential model,model,/content/training-data/natural_language_inference/89/triples/model.txt
sequential model,takes,two sentences,model,/content/training-data/natural_language_inference/89/triples/model.txt
two sentences,as,along sequence,model,/content/training-data/natural_language_inference/89/triples/model.txt
three different architectures,has,second one,model,/content/training-data/natural_language_inference/89/triples/model.txt
second one,capture,interactions,model,/content/training-data/natural_language_inference/89/triples/model.txt
interactions,between,two sentences,model,/content/training-data/natural_language_inference/89/triples/model.txt
three different architectures,has,last one,model,/content/training-data/natural_language_inference/89/triples/model.txt
last one,is,hybrid model,model,/content/training-data/natural_language_inference/89/triples/model.txt
hybrid model,that combines,above two models,model,/content/training-data/natural_language_inference/89/triples/model.txt
Model,present,another independent noanswer loss,model,/content/training-data/natural_language_inference/89/triples/model.txt
another independent noanswer loss,to further alleviate,confliction,model,/content/training-data/natural_language_inference/89/triples/model.txt
another independent noanswer loss,focusing on,no-answer detection task,model,/content/training-data/natural_language_inference/89/triples/model.txt
no-answer detection task,without,shared normalization,model,/content/training-data/natural_language_inference/89/triples/model.txt
shared normalization,of,answer extraction,model,/content/training-data/natural_language_inference/89/triples/model.txt
Model,propose,read - then - verify system,model,/content/training-data/natural_language_inference/89/triples/model.txt
read - then - verify system,aims to be,robust,model,/content/training-data/natural_language_inference/89/triples/model.txt
robust,to,unanswerable questions,model,/content/training-data/natural_language_inference/89/triples/model.txt
Contribution,has research problem,Machine Reading Comprehension,research-problem,/content/training-data/natural_language_inference/89/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/89/triples/experimental-setup.txt
Experimental setup,use,300D embeddings,experimental-setup,/content/training-data/natural_language_inference/89/triples/experimental-setup.txt
300D embeddings,for,Model - II,experimental-setup,/content/training-data/natural_language_inference/89/triples/experimental-setup.txt
300D embeddings,for,Model - III,experimental-setup,/content/training-data/natural_language_inference/89/triples/experimental-setup.txt
Experimental setup,use,Glo Ve 100D embeddings,experimental-setup,/content/training-data/natural_language_inference/89/triples/experimental-setup.txt
Glo Ve 100D embeddings,for,reader,experimental-setup,/content/training-data/natural_language_inference/89/triples/experimental-setup.txt
Experimental setup,utilize,nltk tokenizer,experimental-setup,/content/training-data/natural_language_inference/89/triples/experimental-setup.txt
nltk tokenizer,split,sentences,experimental-setup,/content/training-data/natural_language_inference/89/triples/experimental-setup.txt
nltk tokenizer,to preprocess,passages and questions,experimental-setup,/content/training-data/natural_language_inference/89/triples/experimental-setup.txt
Experimental setup,For,Model - II,experimental-setup,/content/training-data/natural_language_inference/89/triples/experimental-setup.txt
Model - II,has,Adam optimizer ( Kingma and Ba 2014 ,experimental-setup,/content/training-data/natural_language_inference/89/triples/experimental-setup.txt
Adam optimizer ( Kingma and Ba 2014 ),with,learning rate,experimental-setup,/content/training-data/natural_language_inference/89/triples/experimental-setup.txt
learning rate,of,0.0008,experimental-setup,/content/training-data/natural_language_inference/89/triples/experimental-setup.txt
Model - II,has,dropout,experimental-setup,/content/training-data/natural_language_inference/89/triples/experimental-setup.txt
dropout,of,0.3,experimental-setup,/content/training-data/natural_language_inference/89/triples/experimental-setup.txt
0.3,applied for,preventing overfitting,experimental-setup,/content/training-data/natural_language_inference/89/triples/experimental-setup.txt
Model - II,has,hidden size,experimental-setup,/content/training-data/natural_language_inference/89/triples/experimental-setup.txt
hidden size,set as,300,experimental-setup,/content/training-data/natural_language_inference/89/triples/experimental-setup.txt
Experimental setup,run,grid search,experimental-setup,/content/training-data/natural_language_inference/89/triples/experimental-setup.txt
grid search,among,"[ 0.1 , 0.3 , 0.5 , 0.7 , 1 , 2 ]",experimental-setup,/content/training-data/natural_language_inference/89/triples/experimental-setup.txt
Experimental setup,has,batch size,experimental-setup,/content/training-data/natural_language_inference/89/triples/experimental-setup.txt
batch size,is,48,experimental-setup,/content/training-data/natural_language_inference/89/triples/experimental-setup.txt
48,for,reader,experimental-setup,/content/training-data/natural_language_inference/89/triples/experimental-setup.txt
batch size,is,64,experimental-setup,/content/training-data/natural_language_inference/89/triples/experimental-setup.txt
64,for,Model - II,experimental-setup,/content/training-data/natural_language_inference/89/triples/experimental-setup.txt
batch size,is,32,experimental-setup,/content/training-data/natural_language_inference/89/triples/experimental-setup.txt
32,for,Model - I,experimental-setup,/content/training-data/natural_language_inference/89/triples/experimental-setup.txt
32,for,Model - III,experimental-setup,/content/training-data/natural_language_inference/89/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/89/triples/results.txt
Results,Notice that,SLQA +,results,/content/training-data/natural_language_inference/89/triples/results.txt
SLQA +,reached,comparable result,results,/content/training-data/natural_language_inference/89/triples/results.txt
comparable result,compared to,our approach,results,/content/training-data/natural_language_inference/89/triples/results.txt
Results,has,our system,results,/content/training-data/natural_language_inference/89/triples/results.txt
our system,obtains,state - of the - art results,results,/content/training-data/natural_language_inference/89/triples/results.txt
state - of the - art results,on,test set,results,/content/training-data/natural_language_inference/89/triples/results.txt
test set,by achieving,F 1 score,results,/content/training-data/natural_language_inference/89/triples/results.txt
F 1 score,of,74.2,results,/content/training-data/natural_language_inference/89/triples/results.txt
test set,by achieving,EM score,results,/content/training-data/natural_language_inference/89/triples/results.txt
EM score,of,71.7,results,/content/training-data/natural_language_inference/89/triples/results.txt
Contribution,has,Approach,approach,/content/training-data/natural_language_inference/52/triples/approach.txt
Approach,that,learns,approach,/content/training-data/natural_language_inference/52/triples/approach.txt
learns,to,select and explain answers,approach,/content/training-data/natural_language_inference/52/triples/approach.txt
learns,when,only supervision available,approach,/content/training-data/natural_language_inference/52/triples/approach.txt
only supervision available,is,which answer is correct,approach,/content/training-data/natural_language_inference/52/triples/approach.txt
which answer is correct,not,how to explain it,approach,/content/training-data/natural_language_inference/52/triples/approach.txt
Approach,chooses,justifications,approach,/content/training-data/natural_language_inference/52/triples/approach.txt
justifications,that provide,most help,approach,/content/training-data/natural_language_inference/52/triples/approach.txt
most help,towards,ranking,approach,/content/training-data/natural_language_inference/52/triples/approach.txt
ranking,has,correct answers,approach,/content/training-data/natural_language_inference/52/triples/approach.txt
correct answers,has,higher,approach,/content/training-data/natural_language_inference/52/triples/approach.txt
higher,than,incorrect ones,approach,/content/training-data/natural_language_inference/52/triples/approach.txt
Approach,formally,our neural network approach,approach,/content/training-data/natural_language_inference/52/triples/approach.txt
our neural network approach,has,alternates,approach,/content/training-data/natural_language_inference/52/triples/approach.txt
alternates,using,current model,approach,/content/training-data/natural_language_inference/52/triples/approach.txt
current model,with,max - pooling,approach,/content/training-data/natural_language_inference/52/triples/approach.txt
current model,to choose,highest scoring justifications,approach,/content/training-data/natural_language_inference/52/triples/approach.txt
highest scoring justifications,for,correct answers,approach,/content/training-data/natural_language_inference/52/triples/approach.txt
alternates,optimizing,answer ranking model,approach,/content/training-data/natural_language_inference/52/triples/approach.txt
answer ranking model,given,justifications,approach,/content/training-data/natural_language_inference/52/triples/approach.txt
Contribution,has,Baselines,baselines,/content/training-data/natural_language_inference/52/triples/baselines.txt
Baselines,has,IR Baseline,baselines,/content/training-data/natural_language_inference/52/triples/baselines.txt
IR Baseline,rank,answer candidates,baselines,/content/training-data/natural_language_inference/52/triples/baselines.txt
answer candidates,using,unboosted query,baselines,/content/training-data/natural_language_inference/52/triples/baselines.txt
unboosted query,of,question and answer terms,baselines,/content/training-data/natural_language_inference/52/triples/baselines.txt
answer candidates,by,maximum tf .idf document retrieval score,baselines,/content/training-data/natural_language_inference/52/triples/baselines.txt
Baselines,has,IR ++,baselines,/content/training-data/natural_language_inference/52/triples/baselines.txt
IR ++,with,only the IR ++ feature group,baselines,/content/training-data/natural_language_inference/52/triples/baselines.txt
IR ++,uses,same architecture,baselines,/content/training-data/natural_language_inference/52/triples/baselines.txt
same architecture,as,full model,baselines,/content/training-data/natural_language_inference/52/triples/baselines.txt
Contribution,has research problem,Answer Justification,research-problem,/content/training-data/natural_language_inference/52/triples/research-problem.txt
Contribution,has research problem,Developing interpretable machine learning ( ML ) models,research-problem,/content/training-data/natural_language_inference/52/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/52/triples/results.txt
Results,has,QA Performance,results,/content/training-data/natural_language_inference/52/triples/results.txt
QA Performance,tackle,AI2 Kaggle question set,results,/content/training-data/natural_language_inference/52/triples/results.txt
AI2 Kaggle question set,By way of,loose comparison,results,/content/training-data/natural_language_inference/52/triples/results.txt
loose comparison,has,our model,results,/content/training-data/natural_language_inference/52/triples/results.txt
our model,has,approximately 5 % higher performance,results,/content/training-data/natural_language_inference/52/triples/results.txt
QA Performance,has,Our full model,results,/content/training-data/natural_language_inference/52/triples/results.txt
Our full model,combines,"IR ++ , lexical overlap , discourse , and embeddings - based features",results,/content/training-data/natural_language_inference/52/triples/results.txt
"IR ++ , lexical overlap , discourse , and embeddings - based features",has,P@1,results,/content/training-data/natural_language_inference/52/triples/results.txt
P@1,of,53.3 %,results,/content/training-data/natural_language_inference/52/triples/results.txt
53.3 %,has,absolute gain,results,/content/training-data/natural_language_inference/52/triples/results.txt
absolute gain,over,strong IR baseline,results,/content/training-data/natural_language_inference/52/triples/results.txt
absolute gain,of,6.3 %,results,/content/training-data/natural_language_inference/52/triples/results.txt
QA Performance,In comparison to,other systems,results,/content/training-data/natural_language_inference/52/triples/results.txt
other systems,that,competed,results,/content/training-data/natural_language_inference/52/triples/results.txt
competed,in,Kaggle challenge,results,/content/training-data/natural_language_inference/52/triples/results.txt
Kaggle challenge,has,our system,results,/content/training-data/natural_language_inference/52/triples/results.txt
our system,comes in,7th place,results,/content/training-data/natural_language_inference/52/triples/results.txt
7th place,out of,170 competitors,results,/content/training-data/natural_language_inference/52/triples/results.txt
Results,has,Justification Performance,results,/content/training-data/natural_language_inference/52/triples/results.txt
Justification Performance,has,61 %,results,/content/training-data/natural_language_inference/52/triples/results.txt
61 %,of,top - ranked justifications,results,/content/training-data/natural_language_inference/52/triples/results.txt
top - ranked justifications,from,our system,results,/content/training-data/natural_language_inference/52/triples/results.txt
top - ranked justifications,rated as,Good,results,/content/training-data/natural_language_inference/52/triples/results.txt
Good,compared to,52 %,results,/content/training-data/natural_language_inference/52/triples/results.txt
52 %,from,IR baseline,results,/content/training-data/natural_language_inference/52/triples/results.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/48/triples/model.txt
Model,deploys,iterative inference process,model,/content/training-data/natural_language_inference/48/triples/model.txt
iterative inference process,involves,novel alternating attention mechanism,model,/content/training-data/natural_language_inference/48/triples/model.txt
novel alternating attention mechanism,then finds,corresponding matches,model,/content/training-data/natural_language_inference/48/triples/model.txt
corresponding matches,by attending,document,model,/content/training-data/natural_language_inference/48/triples/model.txt
novel alternating attention mechanism,first attends,some parts,model,/content/training-data/natural_language_inference/48/triples/model.txt
some parts,of,query,model,/content/training-data/natural_language_inference/48/triples/model.txt
novel alternating attention mechanism,has,result,model,/content/training-data/natural_language_inference/48/triples/model.txt
result,fed back into,iterative inference process,model,/content/training-data/natural_language_inference/48/triples/model.txt
iterative inference process,to seed,next search step,model,/content/training-data/natural_language_inference/48/triples/model.txt
iterative inference process,to uncover,inferential links,model,/content/training-data/natural_language_inference/48/triples/model.txt
inferential links,between,missing query word,model,/content/training-data/natural_language_inference/48/triples/model.txt
inferential links,between,query,model,/content/training-data/natural_language_inference/48/triples/model.txt
inferential links,between,document,model,/content/training-data/natural_language_inference/48/triples/model.txt
iterative inference process,After,fixed number of iterations,model,/content/training-data/natural_language_inference/48/triples/model.txt
fixed number of iterations,has,model,model,/content/training-data/natural_language_inference/48/triples/model.txt
model,uses,summary,model,/content/training-data/natural_language_inference/48/triples/model.txt
summary,of,inference process,model,/content/training-data/natural_language_inference/48/triples/model.txt
summary,to predict,answer,model,/content/training-data/natural_language_inference/48/triples/model.txt
Model,propose,novel neural attention - based inference model,model,/content/training-data/natural_language_inference/48/triples/model.txt
novel neural attention - based inference model,to perform,machine reading comprehension tasks,model,/content/training-data/natural_language_inference/48/triples/model.txt
Model,first reads,document and the query,model,/content/training-data/natural_language_inference/48/triples/model.txt
document and the query,using,recurrent neural network,model,/content/training-data/natural_language_inference/48/triples/model.txt
Contribution,has research problem,Machine Reading,research-problem,/content/training-data/natural_language_inference/48/triples/research-problem.txt
Contribution,has research problem,machine comprehension,research-problem,/content/training-data/natural_language_inference/48/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
Experimental setup,set,batch size,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
batch size,to,32,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
Experimental setup,to stabilize,learning,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
learning,clip,gradients,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
gradients,if,norm,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
norm,greater than,5,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
Experimental setup,To train,our model,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
our model,used,stochastic gradient descent,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
stochastic gradient descent,with,ADAM optimizer,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
stochastic gradient descent,with,initial learning rate,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
initial learning rate,of,0.001,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
Experimental setup,decay,learning rate,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
learning rate,by,0.8,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
learning rate,if,accuracy,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
accuracy,on,validation set,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
validation set,has,not increase,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
not increase,after,half - epoch,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
half - epoch,i.e.,2000 batches,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
2000 batches,for,CBT,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
half - epoch,i.e.,5000 batches,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
5000 batches,for,CNN,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
Experimental setup,initialize,all weights,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
all weights,of,our model,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
all weights,by,sampling,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
sampling,from,"normal distribution N ( 0 , 0.05 ",experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
Experimental setup,has,biases,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
biases,are,initialized,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
initialized,to,zero,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
Experimental setup,has,Our model,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
Our model,implemented in,Theano,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
Theano,using,Keras library,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
Experimental setup,has,GRU recurrent weights,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
GRU recurrent weights,are,initialized,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
initialized,to be,orthogonal,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
Experimental setup,setting,embedding regularization,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
embedding regularization,worked,robustly,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
robustly,across,datasets,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
embedding regularization,to,0.0001,experimental-setup,/content/training-data/natural_language_inference/48/triples/experimental-setup.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/65/triples/model.txt
Model,has,attention vectors,model,/content/training-data/natural_language_inference/65/triples/model.txt
attention vectors,to perform,pooling,model,/content/training-data/natural_language_inference/65/triples/model.txt
Model,has,AP,model,/content/training-data/natural_language_inference/65/triples/model.txt
AP,consists of,learning,model,/content/training-data/natural_language_inference/65/triples/model.txt
learning,has,similarity measure,model,/content/training-data/natural_language_inference/65/triples/model.txt
similarity measure,over,projected segments ( e.g. trigrams ,model,/content/training-data/natural_language_inference/65/triples/model.txt
projected segments ( e.g. trigrams ),using,similarity scores,model,/content/training-data/natural_language_inference/65/triples/model.txt
similarity scores,to compute,attention vectors,model,/content/training-data/natural_language_inference/65/triples/model.txt
attention vectors,in,both directions,model,/content/training-data/natural_language_inference/65/triples/model.txt
similarity scores,between,segments,model,/content/training-data/natural_language_inference/65/triples/model.txt
projected segments ( e.g. trigrams ),of,two items,model,/content/training-data/natural_language_inference/65/triples/model.txt
two items,in,input pair,model,/content/training-data/natural_language_inference/65/triples/model.txt
AP,enables,pooling layer,model,/content/training-data/natural_language_inference/65/triples/model.txt
pooling layer,to be aware of,current input pair,model,/content/training-data/natural_language_inference/65/triples/model.txt
current input pair,in a way that,information,model,/content/training-data/natural_language_inference/65/triples/model.txt
information,from,two input items,model,/content/training-data/natural_language_inference/65/triples/model.txt
information,directly influence,computation,model,/content/training-data/natural_language_inference/65/triples/model.txt
computation,of,each other 's representations,model,/content/training-data/natural_language_inference/65/triples/model.txt
Model,propose,Attentive Pooling ( AP ,model,/content/training-data/natural_language_inference/65/triples/model.txt
Attentive Pooling ( AP ),has,two - way attention mechanism,model,/content/training-data/natural_language_inference/65/triples/model.txt
two - way attention mechanism,that,significantly improves,model,/content/training-data/natural_language_inference/65/triples/model.txt
significantly improves,has,discriminative models ' performance,model,/content/training-data/natural_language_inference/65/triples/model.txt
discriminative models ' performance,by enabling,joint learning,model,/content/training-data/natural_language_inference/65/triples/model.txt
joint learning,of,representations,model,/content/training-data/natural_language_inference/65/triples/model.txt
representations,of,inputs,model,/content/training-data/natural_language_inference/65/triples/model.txt
inputs,as well as,their similarity measurement,model,/content/training-data/natural_language_inference/65/triples/model.txt
discriminative models ' performance,on,pair - wise ranking or classification,model,/content/training-data/natural_language_inference/65/triples/model.txt
Contribution,has research problem,Neural networks ( NN ) with attention mechanisms,research-problem,/content/training-data/natural_language_inference/65/triples/research-problem.txt
Contribution,has research problem,attention mechanisms,research-problem,/content/training-data/natural_language_inference/65/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/65/triples/experimental-setup.txt
Experimental setup,use,learning rate schedule,experimental-setup,/content/training-data/natural_language_inference/65/triples/experimental-setup.txt
learning rate schedule,that,decreases,experimental-setup,/content/training-data/natural_language_inference/65/triples/experimental-setup.txt
decreases,has,learning rate,experimental-setup,/content/training-data/natural_language_inference/65/triples/experimental-setup.txt
learning rate,For,AP - CNN,experimental-setup,/content/training-data/natural_language_inference/65/triples/experimental-setup.txt
learning rate,For,AP - biLSTM,experimental-setup,/content/training-data/natural_language_inference/65/triples/experimental-setup.txt
learning rate,For,QA - LSTM,experimental-setup,/content/training-data/natural_language_inference/65/triples/experimental-setup.txt
Experimental setup,use,context window,experimental-setup,/content/training-data/natural_language_inference/65/triples/experimental-setup.txt
context window,of,size,experimental-setup,/content/training-data/natural_language_inference/65/triples/experimental-setup.txt
size,has,3,experimental-setup,/content/training-data/natural_language_inference/65/triples/experimental-setup.txt
3,for,Insurance QA,experimental-setup,/content/training-data/natural_language_inference/65/triples/experimental-setup.txt
size,has,4,experimental-setup,/content/training-data/natural_language_inference/65/triples/experimental-setup.txt
4,for,TREC - QA,experimental-setup,/content/training-data/natural_language_inference/65/triples/experimental-setup.txt
4,for,Wiki QA,experimental-setup,/content/training-data/natural_language_inference/65/triples/experimental-setup.txt
Experimental setup,has,best results,experimental-setup,/content/training-data/natural_language_inference/65/triples/experimental-setup.txt
best results,achieved using,between 15 and 25 training epochs,experimental-setup,/content/training-data/natural_language_inference/65/triples/experimental-setup.txt
Experimental setup,has,four NN architectures,experimental-setup,/content/training-data/natural_language_inference/65/triples/experimental-setup.txt
four NN architectures,name,QA - CNN,experimental-setup,/content/training-data/natural_language_inference/65/triples/experimental-setup.txt
four NN architectures,name,AP - CNN,experimental-setup,/content/training-data/natural_language_inference/65/triples/experimental-setup.txt
four NN architectures,name,QA - biLSTM,experimental-setup,/content/training-data/natural_language_inference/65/triples/experimental-setup.txt
four NN architectures,name,AP - biLSTM,experimental-setup,/content/training-data/natural_language_inference/65/triples/experimental-setup.txt
four NN architectures,implemented using,Theano,experimental-setup,/content/training-data/natural_language_inference/65/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/65/triples/results.txt
Results,for,TREC - QA dataset,results,/content/training-data/natural_language_inference/65/triples/results.txt
TREC - QA dataset,has,AP - CNN,results,/content/training-data/natural_language_inference/65/triples/results.txt
AP - CNN,has,outperforms,results,/content/training-data/natural_language_inference/65/triples/results.txt
outperforms,has,QA - CNN,results,/content/training-data/natural_language_inference/65/triples/results.txt
QA - CNN,by,large margin,results,/content/training-data/natural_language_inference/65/triples/results.txt
large margin,in,both metrics,results,/content/training-data/natural_language_inference/65/triples/results.txt
outperforms,has,state - of - the - art systems,results,/content/training-data/natural_language_inference/65/triples/results.txt
state - of - the - art systems,in,both metrics,results,/content/training-data/natural_language_inference/65/triples/results.txt
both metrics,name,MAP and MRR,results,/content/training-data/natural_language_inference/65/triples/results.txt
TREC - QA dataset,has,AP - biLSTM,results,/content/training-data/natural_language_inference/65/triples/results.txt
AP - biLSTM,has,outperforms,results,/content/training-data/natural_language_inference/65/triples/results.txt
outperforms,has,QA - biLSTM,results,/content/training-data/natural_language_inference/65/triples/results.txt
QA - biLSTM,has,performance,results,/content/training-data/natural_language_inference/65/triples/results.txt
performance,has,not as good,results,/content/training-data/natural_language_inference/65/triples/results.txt
not as good,as,AP - CNN,results,/content/training-data/natural_language_inference/65/triples/results.txt
Results,for,Insurance QA dataset,results,/content/training-data/natural_language_inference/65/triples/results.txt
Insurance QA dataset,see that,AP - CNN,results,/content/training-data/natural_language_inference/65/triples/results.txt
AP - CNN,has,outperforms,results,/content/training-data/natural_language_inference/65/triples/results.txt
outperforms,by,large margin,results,/content/training-data/natural_language_inference/65/triples/results.txt
large margin,in,both test sets,results,/content/training-data/natural_language_inference/65/triples/results.txt
large margin,in,dev set,results,/content/training-data/natural_language_inference/65/triples/results.txt
outperforms,has,QA - CNN,results,/content/training-data/natural_language_inference/65/triples/results.txt
Insurance QA dataset,has,AP - CNN and AP - biLSTM,results,/content/training-data/natural_language_inference/65/triples/results.txt
AP - CNN and AP - biLSTM,have,similar performance,results,/content/training-data/natural_language_inference/65/triples/results.txt
AP - CNN and AP - biLSTM,has,outperform,results,/content/training-data/natural_language_inference/65/triples/results.txt
outperform,has,state - of - the - art systems,results,/content/training-data/natural_language_inference/65/triples/results.txt
Results,for,WikiQA dataset,results,/content/training-data/natural_language_inference/65/triples/results.txt
WikiQA dataset,has,difference of performance,results,/content/training-data/natural_language_inference/65/triples/results.txt
difference of performance,between,AP - CNN and QA - CNN,results,/content/training-data/natural_language_inference/65/triples/results.txt
AP - CNN and QA - CNN,is,smaller,results,/content/training-data/natural_language_inference/65/triples/results.txt
smaller,than,Insurance QA dataset,results,/content/training-data/natural_language_inference/65/triples/results.txt
WikiQA dataset,has,AP - CNN,results,/content/training-data/natural_language_inference/65/triples/results.txt
AP - CNN,has,outperforms,results,/content/training-data/natural_language_inference/65/triples/results.txt
outperforms,has,QA - CNN,results,/content/training-data/natural_language_inference/65/triples/results.txt
WikiQA dataset,has,AP - biLSTM,results,/content/training-data/natural_language_inference/65/triples/results.txt
AP - biLSTM,has,outperforms,results,/content/training-data/natural_language_inference/65/triples/results.txt
outperforms,has,QA - biLSTM,results,/content/training-data/natural_language_inference/65/triples/results.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/35/triples/model.txt
Model,extend,pointer - generator,model,/content/training-data/natural_language_inference/35/triples/model.txt
pointer - generator,introducing,artificial token,model,/content/training-data/natural_language_inference/35/triples/model.txt
artificial token,corresponding to,each style,model,/content/training-data/natural_language_inference/35/triples/model.txt
pointer - generator,to,conditional decoder,model,/content/training-data/natural_language_inference/35/triples/model.txt
Model,extend,mechanism,model,/content/training-data/natural_language_inference/35/triples/model.txt
mechanism,to,Transformer based one,model,/content/training-data/natural_language_inference/35/triples/model.txt
Transformer based one,that allows,words,model,/content/training-data/natural_language_inference/35/triples/model.txt
words,to be generated from,vocabulary,model,/content/training-data/natural_language_inference/35/triples/model.txt
words,to be copied from,question,model,/content/training-data/natural_language_inference/35/triples/model.txt
words,to be copied from,passages,model,/content/training-data/natural_language_inference/35/triples/model.txt
Model,introduce,pointer - generator mechanism,model,/content/training-data/natural_language_inference/35/triples/model.txt
pointer - generator mechanism,for generating,abstractive answer,model,/content/training-data/natural_language_inference/35/triples/model.txt
abstractive answer,from,question,model,/content/training-data/natural_language_inference/35/triples/model.txt
abstractive answer,from,multiple passages,model,/content/training-data/natural_language_inference/35/triples/model.txt
multiple passages,covers,various answer styles,model,/content/training-data/natural_language_inference/35/triples/model.txt
Model,introduce,multi-style learning,model,/content/training-data/natural_language_inference/35/triples/model.txt
multi-style learning,that enables,our model,model,/content/training-data/natural_language_inference/35/triples/model.txt
our model,to control,answer styles,model,/content/training-data/natural_language_inference/35/triples/model.txt
our model,has,improves,model,/content/training-data/natural_language_inference/35/triples/model.txt
improves,has,RC,model,/content/training-data/natural_language_inference/35/triples/model.txt
RC,for,all styles involved,model,/content/training-data/natural_language_inference/35/triples/model.txt
Model,For,each decoding step,model,/content/training-data/natural_language_inference/35/triples/model.txt
each decoding step,controls,mixture weights,model,/content/training-data/natural_language_inference/35/triples/model.txt
mixture weights,over,three distributions,model,/content/training-data/natural_language_inference/35/triples/model.txt
Model,propose,Masque,model,/content/training-data/natural_language_inference/35/triples/model.txt
Masque,has,generative model,model,/content/training-data/natural_language_inference/35/triples/model.txt
generative model,for,multi-passage RC,model,/content/training-data/natural_language_inference/35/triples/model.txt
Contribution,has research problem,Generative Reading Comprehension,research-problem,/content/training-data/natural_language_inference/35/triples/research-problem.txt
Contribution,has research problem,generative reading comprehension ( RC ,research-problem,/content/training-data/natural_language_inference/35/triples/research-problem.txt
Contribution,has research problem,reading comprehension ( RC ,research-problem,/content/training-data/natural_language_inference/35/triples/research-problem.txt
Contribution,has research problem,RC,research-problem,/content/training-data/natural_language_inference/35/triples/research-problem.txt
Contribution,has research problem,multi-passage RC,research-problem,/content/training-data/natural_language_inference/35/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/35/triples/results.txt
Results,has,our single model,results,/content/training-data/natural_language_inference/35/triples/results.txt
our single model,pushed forward,state - of - the - art,results,/content/training-data/natural_language_inference/35/triples/results.txt
state - of - the - art,by,significant margin,results,/content/training-data/natural_language_inference/35/triples/results.txt
our single model,controlled with,NQA style,results,/content/training-data/natural_language_inference/35/triples/results.txt
our single model,trained with,two styles,results,/content/training-data/natural_language_inference/35/triples/results.txt
Results,has,evaluation scores,results,/content/training-data/natural_language_inference/35/triples/results.txt
evaluation scores,of,model,results,/content/training-data/natural_language_inference/35/triples/results.txt
model,were,low,results,/content/training-data/natural_language_inference/35/triples/results.txt
model,controlled with,NLG style,results,/content/training-data/natural_language_inference/35/triples/results.txt
Results,has,our model,results,/content/training-data/natural_language_inference/35/triples/results.txt
our model,has,outperformed,results,/content/training-data/natural_language_inference/35/triples/results.txt
outperformed,has,baselines,results,/content/training-data/natural_language_inference/35/triples/results.txt
outperformed,in terms of,ROUGE - L,results,/content/training-data/natural_language_inference/35/triples/results.txt
our model,without,multi-style learning,results,/content/training-data/natural_language_inference/35/triples/results.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/natural_language_inference/37/triples/hyperparameters.txt
Hyperparameters,for,TriviaQA,hyperparameters,/content/training-data/natural_language_inference/37/triples/hyperparameters.txt
TriviaQA,using,larger dimensionality,hyperparameters,/content/training-data/natural_language_inference/37/triples/hyperparameters.txt
larger dimensionality,of,280,hyperparameters,/content/training-data/natural_language_inference/37/triples/hyperparameters.txt
280,for,linear layers,hyperparameters,/content/training-data/natural_language_inference/37/triples/hyperparameters.txt
linear layers,is,beneficial,hyperparameters,/content/training-data/natural_language_inference/37/triples/hyperparameters.txt
larger dimensionality,of,140,hyperparameters,/content/training-data/natural_language_inference/37/triples/hyperparameters.txt
140,for,each GRU,hyperparameters,/content/training-data/natural_language_inference/37/triples/hyperparameters.txt
Hyperparameters,has,Glo Ve 300 dimensional word vectors,hyperparameters,/content/training-data/natural_language_inference/37/triples/hyperparameters.txt
Glo Ve 300 dimensional word vectors,used for,word embeddings,hyperparameters,/content/training-data/natural_language_inference/37/triples/hyperparameters.txt
Hyperparameters,During,training,hyperparameters,/content/training-data/natural_language_inference/37/triples/hyperparameters.txt
training,maintain,exponential moving average,hyperparameters,/content/training-data/natural_language_inference/37/triples/hyperparameters.txt
exponential moving average,with,decay rate,hyperparameters,/content/training-data/natural_language_inference/37/triples/hyperparameters.txt
decay rate,of,0.999,hyperparameters,/content/training-data/natural_language_inference/37/triples/hyperparameters.txt
exponential moving average,of,weights,hyperparameters,/content/training-data/natural_language_inference/37/triples/hyperparameters.txt
Hyperparameters,train,model,hyperparameters,/content/training-data/natural_language_inference/37/triples/hyperparameters.txt
model,with,"Adadelta optimizer ( Zeiler , 2012 ",hyperparameters,/content/training-data/natural_language_inference/37/triples/hyperparameters.txt
"Adadelta optimizer ( Zeiler , 2012 )",with,batch size,hyperparameters,/content/training-data/natural_language_inference/37/triples/hyperparameters.txt
batch size,has,45,hyperparameters,/content/training-data/natural_language_inference/37/triples/hyperparameters.txt
45,for,SQuAD,hyperparameters,/content/training-data/natural_language_inference/37/triples/hyperparameters.txt
batch size,has,60,hyperparameters,/content/training-data/natural_language_inference/37/triples/hyperparameters.txt
60,for,Triv - ia QA,hyperparameters,/content/training-data/natural_language_inference/37/triples/hyperparameters.txt
Hyperparameters,On,SQuAD,hyperparameters,/content/training-data/natural_language_inference/37/triples/hyperparameters.txt
SQuAD,use,dimensionality,hyperparameters,/content/training-data/natural_language_inference/37/triples/hyperparameters.txt
dimensionality,of size,100,hyperparameters,/content/training-data/natural_language_inference/37/triples/hyperparameters.txt
100,for,GRUs,hyperparameters,/content/training-data/natural_language_inference/37/triples/hyperparameters.txt
dimensionality,of size,200,hyperparameters,/content/training-data/natural_language_inference/37/triples/hyperparameters.txt
200,for,linear layers,hyperparameters,/content/training-data/natural_language_inference/37/triples/hyperparameters.txt
linear layers,employed after,each attention mechanism,hyperparameters,/content/training-data/natural_language_inference/37/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/37/triples/model.txt
Model,proposing,improved pipelined method,model,/content/training-data/natural_language_inference/37/triples/model.txt
Model,introduce,method,model,/content/training-data/natural_language_inference/37/triples/model.txt
method,for training,models,model,/content/training-data/natural_language_inference/37/triples/model.txt
models,to produce,accurate per-paragraph confidence scores,model,/content/training-data/natural_language_inference/37/triples/model.txt
Model,use,summed objective function,model,/content/training-data/natural_language_inference/37/triples/model.txt
summed objective function,that marginalizes,model 's output,model,/content/training-data/natural_language_inference/37/triples/model.txt
model 's output,over,all locations,model,/content/training-data/natural_language_inference/37/triples/model.txt
all locations,occurs,answer text,model,/content/training-data/natural_language_inference/37/triples/model.txt
Model,use,shared - normalization objective,model,/content/training-data/natural_language_inference/37/triples/model.txt
shared - normalization objective,where,paragraphs,model,/content/training-data/natural_language_inference/37/triples/model.txt
paragraphs,processed,independently,model,/content/training-data/natural_language_inference/37/triples/model.txt
paragraphs,has,probability,model,/content/training-data/natural_language_inference/37/triples/model.txt
probability,of,answer candidate,model,/content/training-data/natural_language_inference/37/triples/model.txt
probability,is,marginalized,model,/content/training-data/natural_language_inference/37/triples/model.txt
marginalized,over,all paragraphs,model,/content/training-data/natural_language_inference/37/triples/model.txt
all paragraphs,sampled from,same document,model,/content/training-data/natural_language_inference/37/triples/model.txt
Model,sampling,paragraphs,model,/content/training-data/natural_language_inference/37/triples/model.txt
paragraphs,including,paragraphs,model,/content/training-data/natural_language_inference/37/triples/model.txt
paragraphs,do not contain,answer,model,/content/training-data/natural_language_inference/37/triples/model.txt
answer,to,train on,model,/content/training-data/natural_language_inference/37/triples/model.txt
paragraphs,from,context documents,model,/content/training-data/natural_language_inference/37/triples/model.txt
Model,propose,TF - IDF heuristic,model,/content/training-data/natural_language_inference/37/triples/model.txt
TF - IDF heuristic,to select,paragraphs,model,/content/training-data/natural_language_inference/37/triples/model.txt
paragraphs,to,train and test on,model,/content/training-data/natural_language_inference/37/triples/model.txt
Contribution,has research problem,Multi - Paragraph Reading Comprehension,research-problem,/content/training-data/natural_language_inference/37/triples/research-problem.txt
Contribution,has research problem,neural paragraph - level question answering,research-problem,/content/training-data/natural_language_inference/37/triples/research-problem.txt
Contribution,has research problem,answering questions given a related paragraph,research-problem,/content/training-data/natural_language_inference/37/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/37/triples/results.txt
Results,has,SQuAD,results,/content/training-data/natural_language_inference/37/triples/results.txt
SQuAD,has,our variations,results,/content/training-data/natural_language_inference/37/triples/results.txt
our variations,to handle,multi-paragraph setting,results,/content/training-data/natural_language_inference/37/triples/results.txt
multi-paragraph setting,cause,minor loss,results,/content/training-data/natural_language_inference/37/triples/results.txt
minor loss,of,performance,results,/content/training-data/natural_language_inference/37/triples/results.txt
SQuAD,has,shared - norm approach,results,/content/training-data/natural_language_inference/37/triples/results.txt
shared - norm approach,able to,reach,results,/content/training-data/natural_language_inference/37/triples/results.txt
reach,has,peak performance,results,/content/training-data/natural_language_inference/37/triples/results.txt
peak performance,given,15 paragraphs,results,/content/training-data/natural_language_inference/37/triples/results.txt
peak performance,of,72.37 F1,results,/content/training-data/natural_language_inference/37/triples/results.txt
peak performance,of,64.08 EM,results,/content/training-data/natural_language_inference/37/triples/results.txt
SQuAD,has,shared - norm model,results,/content/training-data/natural_language_inference/37/triples/results.txt
shared - norm model,is,strongest,results,/content/training-data/natural_language_inference/37/triples/results.txt
shared - norm model,has,not lose,results,/content/training-data/natural_language_inference/37/triples/results.txt
not lose,has,performance,results,/content/training-data/natural_language_inference/37/triples/results.txt
performance,as,large numbers of paragraphs,results,/content/training-data/natural_language_inference/37/triples/results.txt
large numbers of paragraphs,are,used,results,/content/training-data/natural_language_inference/37/triples/results.txt
SQuAD,has,all our approaches,results,/content/training-data/natural_language_inference/37/triples/results.txt
all our approaches,has,some benefit,results,/content/training-data/natural_language_inference/37/triples/results.txt
SQuAD,has,base model,results,/content/training-data/natural_language_inference/37/triples/results.txt
base model,starts to,drop,results,/content/training-data/natural_language_inference/37/triples/results.txt
drop,in,performance,results,/content/training-data/natural_language_inference/37/triples/results.txt
drop,once,more than two paragraphs,results,/content/training-data/natural_language_inference/37/triples/results.txt
more than two paragraphs,are,used,results,/content/training-data/natural_language_inference/37/triples/results.txt
SQuAD,has,Our paragraph - level model,results,/content/training-data/natural_language_inference/37/triples/results.txt
Our paragraph - level model,is,competitive,results,/content/training-data/natural_language_inference/37/triples/results.txt
Results,has,Trivia QA Unfiltered,results,/content/training-data/natural_language_inference/37/triples/results.txt
Trivia QA Unfiltered,has,base model,results,/content/training-data/natural_language_inference/37/triples/results.txt
base model,starts to,lose,results,/content/training-data/natural_language_inference/37/triples/results.txt
lose,as,more paragraphs,results,/content/training-data/natural_language_inference/37/triples/results.txt
more paragraphs,are,used,results,/content/training-data/natural_language_inference/37/triples/results.txt
lose,has,performance,results,/content/training-data/natural_language_inference/37/triples/results.txt
Results,has,Trivia QA Web,results,/content/training-data/natural_language_inference/37/triples/results.txt
Trivia QA Web,find,effective,results,/content/training-data/natural_language_inference/37/triples/results.txt
effective,to be,TF - IDF ranking,results,/content/training-data/natural_language_inference/37/triples/results.txt
effective,to be,sum objective,results,/content/training-data/natural_language_inference/37/triples/results.txt
Trivia QA Web,Using,refined model,results,/content/training-data/natural_language_inference/37/triples/results.txt
refined model,has,increases,results,/content/training-data/natural_language_inference/37/triples/results.txt
increases,has,gain,results,/content/training-data/natural_language_inference/37/triples/results.txt
gain,by,another 4 points,results,/content/training-data/natural_language_inference/37/triples/results.txt
Trivia QA Web,has,"shared - norm , merge , and no-answer training methods",results,/content/training-data/natural_language_inference/37/triples/results.txt
"shared - norm , merge , and no-answer training methods",has,improve,results,/content/training-data/natural_language_inference/37/triples/results.txt
improve,with,shared - norm method,results,/content/training-data/natural_language_inference/37/triples/results.txt
shared - norm method,being,tied,results,/content/training-data/natural_language_inference/37/triples/results.txt
tied,with,merge approach,results,/content/training-data/natural_language_inference/37/triples/results.txt
merge approach,on,general set,results,/content/training-data/natural_language_inference/37/triples/results.txt
shared - norm method,being,significantly ahead,results,/content/training-data/natural_language_inference/37/triples/results.txt
significantly ahead,of,others,results,/content/training-data/natural_language_inference/37/triples/results.txt
others,on,verified set,results,/content/training-data/natural_language_inference/37/triples/results.txt
improve,has,model 's ability,results,/content/training-data/natural_language_inference/37/triples/results.txt
model 's ability,to utilize,more text,results,/content/training-data/natural_language_inference/37/triples/results.txt
Contribution,has,Experiments,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
Experiments,has,Tasks,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
Tasks,has,Pretty - CLEVR,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
Pretty - CLEVR,has,Results,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
Results,has,MLP,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
MLP,has,perfectly solves,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
perfectly solves,has,non-relational questions,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
MLP,has,struggle,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
struggle,with,single jump questions,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
Results,has,relational network,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
relational network,solves,non-relational questions,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
relational network,has,accuracy,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
accuracy,has,sharply drops off,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
sharply drops off,with,more jumps,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
relational network,as well as,ones,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
ones,requiring,single jump,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
Tasks,has,Sudoku,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
Sudoku,has,Results,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
Results,At,64 steps,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
64 steps,has,accuracy,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
accuracy,for,17 givens puzzles,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
accuracy,has,increases,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
increases,to,96.6 %,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
Results,has,outperform,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
outperform,Park,has,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
has,treats,Sudoku,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
Sudoku,as,9x9 image,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
outperform,Park,10 convolutional layers,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
Results,has,Our network,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
Our network,learns to,solve,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
solve,has,94.1 %,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
94.1 %,of,even the hardest 17 - givens Sudokus,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
94.1 %,after,32 steps,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
Our network,has,outperforms,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
outperforms,has,loopy belief propagation,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
loopy belief propagation,with,parallel and random messages,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
parallel and random messages,passing,updates,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
outperforms,has,version,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
version,of,loopy belief propagation,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
loopy belief propagation,has,modified specifically,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
modified specifically,for solving,Sudokus,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
Sudokus,that uses,250 steps,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
Results,see that,harder,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
harder,has,17 givens,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
harder,continue to,improve,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
improve,after,32 steps,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
Results,see that,even simple Sudokus,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
even simple Sudokus,with,33 givens,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
even simple Sudokus,require,upwards of 10 steps,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
upwards of 10 steps,of,relational reasoning,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
Tasks,has,bAbI question - answering tasks,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
bAbI question - answering tasks,has,Results,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
Results,need,single step of relational reasoning,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
single step of relational reasoning,to solve,all the bAbI tasks,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
Results,appears,multiple steps of relational reasoning,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
multiple steps of relational reasoning,not,important,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
important,for,bAbI dataset,experiments,/content/training-data/natural_language_inference/56/triples/experiments.txt
Contribution,Code,github.com/rasmusbergpalm/recurrent-relationalnetworks.,code,/content/training-data/natural_language_inference/56/triples/code.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/56/triples/model.txt
Model,introduces,composite function,model,/content/training-data/natural_language_inference/56/triples/model.txt
composite function,serves,modular component,model,/content/training-data/natural_language_inference/56/triples/model.txt
modular component,for,many - step relational reasoning,model,/content/training-data/natural_language_inference/56/triples/model.txt
many - step relational reasoning,in,end - to - end differentiable learning systems,model,/content/training-data/natural_language_inference/56/triples/model.txt
composite function,Toward generally realizing,ability,model,/content/training-data/natural_language_inference/56/triples/model.txt
ability,to methodically reason about,objects,model,/content/training-data/natural_language_inference/56/triples/model.txt
ability,to methodically reason about,interactions,model,/content/training-data/natural_language_inference/56/triples/model.txt
interactions,over,many steps,model,/content/training-data/natural_language_inference/56/triples/model.txt
composite function,name,recurrent relational network,model,/content/training-data/natural_language_inference/56/triples/model.txt
composite function,encodes,inductive biases,model,/content/training-data/natural_language_inference/56/triples/model.txt
inductive biases,that,objects,model,/content/training-data/natural_language_inference/56/triples/model.txt
objects,exists in,world,model,/content/training-data/natural_language_inference/56/triples/model.txt
objects,affect,each other,model,/content/training-data/natural_language_inference/56/triples/model.txt
each other,invariant to,time,model,/content/training-data/natural_language_inference/56/triples/model.txt
objects,described by,properties,model,/content/training-data/natural_language_inference/56/triples/model.txt
properties,changeover,time,model,/content/training-data/natural_language_inference/56/triples/model.txt
Model,decompose,function,model,/content/training-data/natural_language_inference/56/triples/model.txt
function,into,two components,model,/content/training-data/natural_language_inference/56/triples/model.txt
two components,trained,jointly end - to - end,model,/content/training-data/natural_language_inference/56/triples/model.txt
two components,name,perceptual front - end,model,/content/training-data/natural_language_inference/56/triples/model.txt
perceptual front - end,to recognize,objects,model,/content/training-data/natural_language_inference/56/triples/model.txt
objects,in,raw input,model,/content/training-data/natural_language_inference/56/triples/model.txt
perceptual front - end,represent them as,vectors,model,/content/training-data/natural_language_inference/56/triples/model.txt
two components,name,relational reasoning module,model,/content/training-data/natural_language_inference/56/triples/model.txt
relational reasoning module,uses,representation,model,/content/training-data/natural_language_inference/56/triples/model.txt
representation,to reason about,objects and their interactions,model,/content/training-data/natural_language_inference/56/triples/model.txt
function,for,relational reasoning,model,/content/training-data/natural_language_inference/56/triples/model.txt
Model,has,relational reasoning module,model,/content/training-data/natural_language_inference/56/triples/model.txt
relational reasoning module,implements,interface,model,/content/training-data/natural_language_inference/56/triples/model.txt
interface,operates on,graph,model,/content/training-data/natural_language_inference/56/triples/model.txt
graph,of,nodes and directed edges,model,/content/training-data/natural_language_inference/56/triples/model.txt
graph,where,nodes,model,/content/training-data/natural_language_inference/56/triples/model.txt
nodes,represented by,real valued vectors,model,/content/training-data/natural_language_inference/56/triples/model.txt
nodes,is,differentiable,model,/content/training-data/natural_language_inference/56/triples/model.txt
Contribution,has research problem,Recurrent Relational Networks,research-problem,/content/training-data/natural_language_inference/56/triples/research-problem.txt
Contribution,has research problem,recurrent relational network,research-problem,/content/training-data/natural_language_inference/56/triples/research-problem.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/8/triples/model.txt
Model,show,neural network - based sentence model,model,/content/training-data/natural_language_inference/8/triples/model.txt
neural network - based sentence model,can be applied to,task of answer sentence selection,model,/content/training-data/natural_language_inference/8/triples/model.txt
Model,construct,two distributional sentence models,model,/content/training-data/natural_language_inference/8/triples/model.txt
two distributional sentence models,first,bag - of - words model,model,/content/training-data/natural_language_inference/8/triples/model.txt
two distributional sentence models,second,bigram model,model,/content/training-data/natural_language_inference/8/triples/model.txt
bigram model,based on,convolutional neural network,model,/content/training-data/natural_language_inference/8/triples/model.txt
Model,present,enhanced version,model,/content/training-data/natural_language_inference/8/triples/model.txt
enhanced version,combines,signal,model,/content/training-data/natural_language_inference/8/triples/model.txt
signal,of,distributed matching algorithm,model,/content/training-data/natural_language_inference/8/triples/model.txt
distributed matching algorithm,with,two simple word matching features,model,/content/training-data/natural_language_inference/8/triples/model.txt
Model,train,supervised model,model,/content/training-data/natural_language_inference/8/triples/model.txt
supervised model,to learn,semantic matching,model,/content/training-data/natural_language_inference/8/triples/model.txt
semantic matching,Assuming a set of,pre-trained semantic word embeddings,model,/content/training-data/natural_language_inference/8/triples/model.txt
semantic matching,between,question and answer pairs,model,/content/training-data/natural_language_inference/8/triples/model.txt
Contribution,has research problem,Answer Sentence Selection,research-problem,/content/training-data/natural_language_inference/8/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/8/triples/experimental-setup.txt
Experimental setup,use,AdaGrad algorithm,experimental-setup,/content/training-data/natural_language_inference/8/triples/experimental-setup.txt
AdaGrad algorithm,for,training,experimental-setup,/content/training-data/natural_language_inference/8/triples/experimental-setup.txt
Experimental setup,used,word embeddings ( d = 50 ,experimental-setup,/content/training-data/natural_language_inference/8/triples/experimental-setup.txt
word embeddings ( d = 50 ),computed using,Collobert and Weston 's neural language model,experimental-setup,/content/training-data/natural_language_inference/8/triples/experimental-setup.txt
Experimental setup,has,All hyperparameters,experimental-setup,/content/training-data/natural_language_inference/8/triples/experimental-setup.txt
All hyperparameters,optimised via,grid search,experimental-setup,/content/training-data/natural_language_inference/8/triples/experimental-setup.txt
grid search,on,MAP score on the development data,experimental-setup,/content/training-data/natural_language_inference/8/triples/experimental-setup.txt
Experimental setup,has,other model weights,experimental-setup,/content/training-data/natural_language_inference/8/triples/experimental-setup.txt
other model weights,has,randomly intitialised,experimental-setup,/content/training-data/natural_language_inference/8/triples/experimental-setup.txt
randomly intitialised,using,"Gaussian distribution ( = 0 , ? = 0.01 ",experimental-setup,/content/training-data/natural_language_inference/8/triples/experimental-setup.txt
Experimental setup,has,L - BFGS,experimental-setup,/content/training-data/natural_language_inference/8/triples/experimental-setup.txt
L - BFGS,to train,logistic regression classifier,experimental-setup,/content/training-data/natural_language_inference/8/triples/experimental-setup.txt
logistic regression classifier,with,L2 regulariser,experimental-setup,/content/training-data/natural_language_inference/8/triples/experimental-setup.txt
L2 regulariser,of,0.01,experimental-setup,/content/training-data/natural_language_inference/8/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/8/triples/results.txt
Results,has,bigram model,results,/content/training-data/natural_language_inference/8/triples/results.txt
bigram model,performs,better,results,/content/training-data/natural_language_inference/8/triples/results.txt
better,than,unigram model,results,/content/training-data/natural_language_inference/8/triples/results.txt
bigram model,addition of,IDF - weighted word count features,results,/content/training-data/natural_language_inference/8/triples/results.txt
IDF - weighted word count features,has,significantly improve,results,/content/training-data/natural_language_inference/8/triples/results.txt
significantly improve,by,10 % - 15 %,results,/content/training-data/natural_language_inference/8/triples/results.txt
significantly improve,has,performance,results,/content/training-data/natural_language_inference/8/triples/results.txt
performance,for,both models,results,/content/training-data/natural_language_inference/8/triples/results.txt
Results,has,best models ( bigram + count ,results,/content/training-data/natural_language_inference/8/triples/results.txt
best models ( bigram + count ),outperform,all baselines,results,/content/training-data/natural_language_inference/8/triples/results.txt
Contribution,has,Dataset,datase,/content/training-data/natural_language_inference/69/triples/dataset.txt
Dataset,has,WIKIHOP,datase,/content/training-data/natural_language_inference/69/triples/dataset.txt
WIKIHOP,uses,sets,datase,/content/training-data/natural_language_inference/69/triples/dataset.txt
sets,of,WIKIPEDIA articles,datase,/content/training-data/natural_language_inference/69/triples/dataset.txt
WIKIPEDIA articles,where,answers,datase,/content/training-data/natural_language_inference/69/triples/dataset.txt
answers,to,queries,datase,/content/training-data/natural_language_inference/69/triples/dataset.txt
queries,about,specific properties,datase,/content/training-data/natural_language_inference/69/triples/dataset.txt
specific properties,of,entity,datase,/content/training-data/natural_language_inference/69/triples/dataset.txt
specific properties,can not be located in,entity 's article,datase,/content/training-data/natural_language_inference/69/triples/dataset.txt
Dataset,has,MEDHOP,datase,/content/training-data/natural_language_inference/69/triples/dataset.txt
MEDHOP,has,goal,datase,/content/training-data/natural_language_inference/69/triples/dataset.txt
goal,to establish,drug - drug interactions,datase,/content/training-data/natural_language_inference/69/triples/dataset.txt
drug - drug interactions,based on,scientific findings,datase,/content/training-data/natural_language_inference/69/triples/dataset.txt
scientific findings,about,drugs and proteins and their interactions,datase,/content/training-data/natural_language_inference/69/triples/dataset.txt
drugs and proteins and their interactions,found across,multiple MEDLINE abstracts,datase,/content/training-data/natural_language_inference/69/triples/dataset.txt
Dataset,draw upon,existing Knowledge Bases ( KBs ,datase,/content/training-data/natural_language_inference/69/triples/dataset.txt
existing Knowledge Bases ( KBs ),name,WIKIDATA and DRUG - BANK,datase,/content/training-data/natural_language_inference/69/triples/dataset.txt
Contribution,has,Baselines,baselines,/content/training-data/natural_language_inference/69/triples/baselines.txt
Baselines,has,TF - IDF,baselines,/content/training-data/natural_language_inference/69/triples/baselines.txt
TF - IDF,has,Retrieval - based models,baselines,/content/training-data/natural_language_inference/69/triples/baselines.txt
Retrieval - based models,known to be,strong QA baselines,baselines,/content/training-data/natural_language_inference/69/triples/baselines.txt
strong QA baselines,if,candidate answers,baselines,/content/training-data/natural_language_inference/69/triples/baselines.txt
candidate answers,are,provided,baselines,/content/training-data/natural_language_inference/69/triples/baselines.txt
Baselines,has,Random,baselines,/content/training-data/natural_language_inference/69/triples/baselines.txt
Random,Selects,random candidate,baselines,/content/training-data/natural_language_inference/69/triples/baselines.txt
Baselines,has,Majority - candidate - per-query - type,baselines,/content/training-data/natural_language_inference/69/triples/baselines.txt
Majority - candidate - per-query - type,Predicts,candidate,baselines,/content/training-data/natural_language_inference/69/triples/baselines.txt
candidate,that was,most frequently observed,baselines,/content/training-data/natural_language_inference/69/triples/baselines.txt
most frequently observed,as,true answer,baselines,/content/training-data/natural_language_inference/69/triples/baselines.txt
Baselines,has,Max- mention,baselines,/content/training-data/natural_language_inference/69/triples/baselines.txt
Max- mention,Predicts,most frequently mentioned candidate,baselines,/content/training-data/natural_language_inference/69/triples/baselines.txt
most frequently mentioned candidate,in,support documents,baselines,/content/training-data/natural_language_inference/69/triples/baselines.txt
Baselines,has,Document - cue,baselines,/content/training-data/natural_language_inference/69/triples/baselines.txt
Document - cue,has,effect,baselines,/content/training-data/natural_language_inference/69/triples/baselines.txt
effect,that,correct candidate,baselines,/content/training-data/natural_language_inference/69/triples/baselines.txt
correct candidate,indicated solely by,presence of certain documents,baselines,/content/training-data/natural_language_inference/69/triples/baselines.txt
Contribution,has research problem,Multi-hop Reading Comprehension,research-problem,/content/training-data/natural_language_inference/69/triples/research-problem.txt
Contribution,has research problem,Reading Comprehension,research-problem,/content/training-data/natural_language_inference/69/triples/research-problem.txt
Contribution,has research problem,Reading Comprehension ( RC ,research-problem,/content/training-data/natural_language_inference/69/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/69/triples/results.txt
Results,In,masked setup,results,/content/training-data/natural_language_inference/69/triples/results.txt
masked setup,has,all baseline models,results,/content/training-data/natural_language_inference/69/triples/results.txt
all baseline models,fail,in the face of the randomized answer expressions,results,/content/training-data/natural_language_inference/69/triples/results.txt
all baseline models,reliant on,lexical cues,results,/content/training-data/natural_language_inference/69/triples/results.txt
Results,for,open - domain setting,results,/content/training-data/natural_language_inference/69/triples/results.txt
open - domain setting,of,WIKIHOP,results,/content/training-data/natural_language_inference/69/triples/results.txt
open - domain setting,has,reduction,results,/content/training-data/natural_language_inference/69/triples/results.txt
reduction,of,answer vocabulary,results,/content/training-data/natural_language_inference/69/triples/results.txt
answer vocabulary,to,100 random single - token mask expressions,results,/content/training-data/natural_language_inference/69/triples/results.txt
100 random single - token mask expressions,helps,model,results,/content/training-data/natural_language_inference/69/triples/results.txt
model,in selecting,candidate span,results,/content/training-data/natural_language_inference/69/triples/results.txt
model,compared to,multi-token candidate expressions,results,/content/training-data/natural_language_inference/69/triples/results.txt
multi-token candidate expressions,in,unmasked setting,results,/content/training-data/natural_language_inference/69/triples/results.txt
Results,has,Document - cue baseline,results,/content/training-data/natural_language_inference/69/triples/results.txt
Document - cue baseline,predict,more than a third of the samples,results,/content/training-data/natural_language_inference/69/triples/results.txt
more than a third of the samples,after sub - sampling,frequent document - answer pairs,results,/content/training-data/natural_language_inference/69/triples/results.txt
frequent document - answer pairs,for,WIKIHOP,results,/content/training-data/natural_language_inference/69/triples/results.txt
more than a third of the samples,has,correctly,results,/content/training-data/natural_language_inference/69/triples/results.txt
Results,has,Both neural RC models,results,/content/training-data/natural_language_inference/69/triples/results.txt
Both neural RC models,able to,largely retain or even improve,results,/content/training-data/natural_language_inference/69/triples/results.txt
largely retain or even improve,has,strong performance,results,/content/training-data/natural_language_inference/69/triples/results.txt
strong performance,when,answers,results,/content/training-data/natural_language_inference/69/triples/results.txt
answers,are,masked,results,/content/training-data/natural_language_inference/69/triples/results.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/68/triples/model.txt
Model,extend,boundary model,model,/content/training-data/natural_language_inference/68/triples/model.txt
boundary model,with,search mechanism,model,/content/training-data/natural_language_inference/68/triples/model.txt
Model,adopt,match - LSTM model,model,/content/training-data/natural_language_inference/68/triples/model.txt
Model,adopt,Pointer Net ( Ptr - Net ) model,model,/content/training-data/natural_language_inference/68/triples/model.txt
Pointer Net ( Ptr - Net ) model,enables,predictions,model,/content/training-data/natural_language_inference/68/triples/model.txt
predictions,of,tokens,model,/content/training-data/natural_language_inference/68/triples/model.txt
tokens,from,input sequence only,model,/content/training-data/natural_language_inference/68/triples/model.txt
Model,propose,two ways,model,/content/training-data/natural_language_inference/68/triples/model.txt
two ways,name,sequence model,model,/content/training-data/natural_language_inference/68/triples/model.txt
two ways,name,boundary model,model,/content/training-data/natural_language_inference/68/triples/model.txt
two ways,to apply,Ptr - Net model,model,/content/training-data/natural_language_inference/68/triples/model.txt
Ptr - Net model,for,our task,model,/content/training-data/natural_language_inference/68/triples/model.txt
Contribution,has research problem,MACHINE COMPREHENSION,research-problem,/content/training-data/natural_language_inference/68/triples/research-problem.txt
Contribution,has research problem,Machine comprehension of text,research-problem,/content/training-data/natural_language_inference/68/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/68/triples/experimental-setup.txt
Experimental setup,use,ADAMAX,experimental-setup,/content/training-data/natural_language_inference/68/triples/experimental-setup.txt
ADAMAX,with,coefficients,experimental-setup,/content/training-data/natural_language_inference/68/triples/experimental-setup.txt
coefficients,to optimize,model,experimental-setup,/content/training-data/natural_language_inference/68/triples/experimental-setup.txt
coefficients,has,? 1 = 0.9 and ? 2 = 0.999,experimental-setup,/content/training-data/natural_language_inference/68/triples/experimental-setup.txt
Experimental setup,use,word embeddings,experimental-setup,/content/training-data/natural_language_inference/68/triples/experimental-setup.txt
word embeddings,to initialize,model,experimental-setup,/content/training-data/natural_language_inference/68/triples/experimental-setup.txt
word embeddings,from,GloVe,experimental-setup,/content/training-data/natural_language_inference/68/triples/experimental-setup.txt
Experimental setup,has,dimensionality l,experimental-setup,/content/training-data/natural_language_inference/68/triples/experimental-setup.txt
dimensionality l,set to be,150 or 300,experimental-setup,/content/training-data/natural_language_inference/68/triples/experimental-setup.txt
dimensionality l,of,hidden layers,experimental-setup,/content/training-data/natural_language_inference/68/triples/experimental-setup.txt
Experimental setup,has,Words,experimental-setup,/content/training-data/natural_language_inference/68/triples/experimental-setup.txt
Words,not found in,Glo Ve,experimental-setup,/content/training-data/natural_language_inference/68/triples/experimental-setup.txt
Words,initialized as,zero vectors,experimental-setup,/content/training-data/natural_language_inference/68/triples/experimental-setup.txt
Experimental setup,tokenize,"all the passages , questions and answers",experimental-setup,/content/training-data/natural_language_inference/68/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/68/triples/results.txt
Results,adding,Bi - Ans - Ptr,results,/content/training-data/natural_language_inference/68/triples/results.txt
Bi - Ans - Ptr,with,bi-directional pre-processing LSTM,results,/content/training-data/natural_language_inference/68/triples/results.txt
Bi - Ans - Ptr,get,1.2 % improvement,results,/content/training-data/natural_language_inference/68/triples/results.txt
1.2 % improvement,in,F1,results,/content/training-data/natural_language_inference/68/triples/results.txt
Results,has,our boundary model,results,/content/training-data/natural_language_inference/68/triples/results.txt
our boundary model,achieving,exact match score,results,/content/training-data/natural_language_inference/68/triples/results.txt
exact match score,of,61.1 %,results,/content/training-data/natural_language_inference/68/triples/results.txt
our boundary model,achieving,F1 score,results,/content/training-data/natural_language_inference/68/triples/results.txt
F1 score,of,71.2 %,results,/content/training-data/natural_language_inference/68/triples/results.txt
our boundary model,has,outperformed,results,/content/training-data/natural_language_inference/68/triples/results.txt
outperformed,has,sequence model,results,/content/training-data/natural_language_inference/68/triples/results.txt
Results,in terms of,exact match score,results,/content/training-data/natural_language_inference/68/triples/results.txt
exact match score,has,boundary model,results,/content/training-data/natural_language_inference/68/triples/results.txt
boundary model,has,clear advantage,results,/content/training-data/natural_language_inference/68/triples/results.txt
clear advantage,over,sequence model,results,/content/training-data/natural_language_inference/68/triples/results.txt
Contribution,Code,https://github.com/soskek/der-network,code,/content/training-data/natural_language_inference/82/triples/code.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/natural_language_inference/82/triples/hyperparameters.txt
Hyperparameters,For,preprocessing,hyperparameters,/content/training-data/natural_language_inference/82/triples/hyperparameters.txt
preprocessing,segment,sentences,hyperparameters,/content/training-data/natural_language_inference/82/triples/hyperparameters.txt
preprocessing,segment,punctuation marks,hyperparameters,/content/training-data/natural_language_inference/82/triples/hyperparameters.txt
punctuation marks,has,.,hyperparameters,/content/training-data/natural_language_inference/82/triples/hyperparameters.txt
punctuation marks,has,!,hyperparameters,/content/training-data/natural_language_inference/82/triples/hyperparameters.txt
punctuation marks,has,?,hyperparameters,/content/training-data/natural_language_inference/82/triples/hyperparameters.txt
Hyperparameters,train,model,hyperparameters,/content/training-data/natural_language_inference/82/triples/hyperparameters.txt
model,with,hyper - parameters,hyperparameters,/content/training-data/natural_language_inference/82/triples/hyperparameters.txt
hyper - parameters,has,lightly tuned,hyperparameters,/content/training-data/natural_language_inference/82/triples/hyperparameters.txt
lightly tuned,on,validation set,hyperparameters,/content/training-data/natural_language_inference/82/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/82/triples/model.txt
Model,implement,reader,model,/content/training-data/natural_language_inference/82/triples/model.txt
reader,dynamically builds,meaning representations,model,/content/training-data/natural_language_inference/82/triples/model.txt
meaning representations,for,each entity,model,/content/training-data/natural_language_inference/82/triples/model.txt
meaning representations,by gathering and accumulating,information,model,/content/training-data/natural_language_inference/82/triples/model.txt
information,as it reads,document,model,/content/training-data/natural_language_inference/82/triples/model.txt
information,on,entity,model,/content/training-data/natural_language_inference/82/triples/model.txt
Contribution,has research problem,Machine Reading,research-problem,/content/training-data/natural_language_inference/82/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/82/triples/results.txt
Results,note that,initializing,results,/content/training-data/natural_language_inference/82/triples/results.txt
initializing,has,our model,results,/content/training-data/natural_language_inference/82/triples/results.txt
our model,with,pre-trained word vectors,results,/content/training-data/natural_language_inference/82/triples/results.txt
pre-trained word vectors,is,helpful,results,/content/training-data/natural_language_inference/82/triples/results.txt
Results,note that,our model,results,/content/training-data/natural_language_inference/82/triples/results.txt
our model,shows,best results,results,/content/training-data/natural_language_inference/82/triples/results.txt
best results,compared to,several previous reader models,results,/content/training-data/natural_language_inference/82/triples/results.txt
our model,name,full DER Network,results,/content/training-data/natural_language_inference/82/triples/results.txt
Results,has,99 % confidence intervals,results,/content/training-data/natural_language_inference/82/triples/results.txt
99 % confidence intervals,of,results,results,/content/training-data/natural_language_inference/82/triples/results.txt
results,of,full DER Network and the one initialized by word2vec,results,/content/training-data/natural_language_inference/82/triples/results.txt
full DER Network and the one initialized by word2vec,were,"[ 0.700 , 0.740 ]",results,/content/training-data/natural_language_inference/82/triples/results.txt
full DER Network and the one initialized by word2vec,were,"[ 0.708 , 0.749 ]",results,/content/training-data/natural_language_inference/82/triples/results.txt
full DER Network and the one initialized by word2vec,on,test set,results,/content/training-data/natural_language_inference/82/triples/results.txt
Results,has,Max - pooling,results,/content/training-data/natural_language_inference/82/triples/results.txt
Max - pooling,has,drastically improves,results,/content/training-data/natural_language_inference/82/triples/results.txt
drastically improves,has,performance,results,/content/training-data/natural_language_inference/82/triples/results.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/natural_language_inference/25/triples/hyperparameters.txt
Hyperparameters,use,pre-trained GloVe embeddings,hyperparameters,/content/training-data/natural_language_inference/25/triples/hyperparameters.txt
pre-trained GloVe embeddings,of dimension d,w = 300,hyperparameters,/content/training-data/natural_language_inference/25/triples/hyperparameters.txt
Hyperparameters,produce,character - based word representations,hyperparameters,/content/training-data/natural_language_inference/25/triples/hyperparameters.txt
character - based word representations,via,dc = 100,hyperparameters,/content/training-data/natural_language_inference/25/triples/hyperparameters.txt
dc = 100,has,convolutional filters,hyperparameters,/content/training-data/natural_language_inference/25/triples/hyperparameters.txt
convolutional filters,over,character embeddings,hyperparameters,/content/training-data/natural_language_inference/25/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/25/triples/model.txt
Model,take,model,model,/content/training-data/natural_language_inference/25/triples/model.txt
model,carries out,only basic question - document interaction,model,/content/training-data/natural_language_inference/25/triples/model.txt
only basic question - document interaction,prepend to,module,model,/content/training-data/natural_language_inference/25/triples/model.txt
module,that produces,token embeddings,model,/content/training-data/natural_language_inference/25/triples/model.txt
token embeddings,by,explicitly gating,model,/content/training-data/natural_language_inference/25/triples/model.txt
explicitly gating,between,contextual and non-contextual representations,model,/content/training-data/natural_language_inference/25/triples/model.txt
Model,turn to,semisupervised setting,model,/content/training-data/natural_language_inference/25/triples/model.txt
semisupervised setting,leverage,language model,model,/content/training-data/natural_language_inference/25/triples/model.txt
language model,pre-trained on,large amounts of data,model,/content/training-data/natural_language_inference/25/triples/model.txt
large amounts of data,as,sequence encoder,model,/content/training-data/natural_language_inference/25/triples/model.txt
sequence encoder,forcibly facilitates,context utilization,model,/content/training-data/natural_language_inference/25/triples/model.txt
Contribution,has research problem,Reading Comprehension,research-problem,/content/training-data/natural_language_inference/25/triples/research-problem.txt
Contribution,has research problem,Reading comprehension ( RC ,research-problem,/content/training-data/natural_language_inference/25/triples/research-problem.txt
Contribution,has research problem,RC,research-problem,/content/training-data/natural_language_inference/25/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/25/triples/results.txt
Results,has,performance,results,/content/training-data/natural_language_inference/25/triples/results.txt
performance,due to,utilizing,results,/content/training-data/natural_language_inference/25/triples/results.txt
utilizing,has,LM hidden states,results,/content/training-data/natural_language_inference/25/triples/results.txt
LM hidden states,of,first LSTM layer,results,/content/training-data/natural_language_inference/25/triples/results.txt
LM hidden states,has,significantly surpasses,results,/content/training-data/natural_language_inference/25/triples/results.txt
significantly surpasses,has,other two variants,results,/content/training-data/natural_language_inference/25/triples/results.txt
Results,has,less frequent,results,/content/training-data/natural_language_inference/25/triples/results.txt
less frequent,is,word - type,results,/content/training-data/natural_language_inference/25/triples/results.txt
word - type,has,gate activations,results,/content/training-data/natural_language_inference/25/triples/results.txt
gate activations,are,smaller,results,/content/training-data/natural_language_inference/25/triples/results.txt
Results,showing,benefit,results,/content/training-data/natural_language_inference/25/triples/results.txt
benefit,of,training,results,/content/training-data/natural_language_inference/25/triples/results.txt
training,in,semisupervised fashion,results,/content/training-data/natural_language_inference/25/triples/results.txt
semisupervised fashion,with,large language model,results,/content/training-data/natural_language_inference/25/triples/results.txt
training,has,QA model,results,/content/training-data/natural_language_inference/25/triples/results.txt
training,observe,significant improvement,results,/content/training-data/natural_language_inference/25/triples/results.txt
Results,observe,superior performance,results,/content/training-data/natural_language_inference/25/triples/results.txt
superior performance,illustrating,benefit,results,/content/training-data/natural_language_inference/25/triples/results.txt
benefit,of,contextualization,results,/content/training-data/natural_language_inference/25/triples/results.txt
superior performance,by,contextual one,results,/content/training-data/natural_language_inference/25/triples/results.txt
Results,Supplementing,calculation,results,/content/training-data/natural_language_inference/25/triples/results.txt
calculation,of,token reembeddings,results,/content/training-data/natural_language_inference/25/triples/results.txt
token reembeddings,with,hidden states,results,/content/training-data/natural_language_inference/25/triples/results.txt
hidden states,of,strong language model,results,/content/training-data/natural_language_inference/25/triples/results.txt
token reembeddings,proves to be,highly effective,results,/content/training-data/natural_language_inference/25/triples/results.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/90/triples/model.txt
Model,relies on,alignment,model,/content/training-data/natural_language_inference/90/triples/model.txt
Model,apply,intra-sentence attention,model,/content/training-data/natural_language_inference/90/triples/model.txt
intra-sentence attention,to endow,model,model,/content/training-data/natural_language_inference/90/triples/model.txt
model,with,richer encoding,model,/content/training-data/natural_language_inference/90/triples/model.txt
richer encoding,prior to,alignment step,model,/content/training-data/natural_language_inference/90/triples/model.txt
richer encoding,of,substructures,model,/content/training-data/natural_language_inference/90/triples/model.txt
Model,Given,two sentences,model,/content/training-data/natural_language_inference/90/triples/model.txt
two sentences,use,( soft ) alignment,model,/content/training-data/natural_language_inference/90/triples/model.txt
 soft ) alignment,to decompose,task,model,/content/training-data/natural_language_inference/90/triples/model.txt
task,into,subproblems,model,/content/training-data/natural_language_inference/90/triples/model.txt
two sentences,create,soft alignment matrix,model,/content/training-data/natural_language_inference/90/triples/model.txt
soft alignment matrix,using,neural attention,model,/content/training-data/natural_language_inference/90/triples/model.txt
two sentences,has,each word,model,/content/training-data/natural_language_inference/90/triples/model.txt
each word,repre-sented by,embedding vector,model,/content/training-data/natural_language_inference/90/triples/model.txt
two sentences,has,results,model,/content/training-data/natural_language_inference/90/triples/model.txt
results,of,subproblems,model,/content/training-data/natural_language_inference/90/triples/model.txt
results,has,merged,model,/content/training-data/natural_language_inference/90/triples/model.txt
merged,to produce,final classification,model,/content/training-data/natural_language_inference/90/triples/model.txt
Model,has,fully computationally decomposable,model,/content/training-data/natural_language_inference/90/triples/model.txt
fully computationally decomposable,with respect to,input text,model,/content/training-data/natural_language_inference/90/triples/model.txt
Contribution,has research problem,Natural Language Inference,research-problem,/content/training-data/natural_language_inference/90/triples/research-problem.txt
Contribution,has research problem,Natural language inference ( NLI ,research-problem,/content/training-data/natural_language_inference/90/triples/research-problem.txt
Contribution,has research problem,NLI,research-problem,/content/training-data/natural_language_inference/90/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
Experimental setup,implemented in,TensorFlow,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
Experimental setup,use,300 dimensional GloVe embeddings,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
300 dimensional GloVe embeddings,to represent,words,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
Experimental setup,has,Each hyperparameter setting,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
Each hyperparameter setting,using,Adagrad,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
Adagrad,with,default initial accumulator value,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
default initial accumulator value,of,0.1,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
Adagrad,for,optimization,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
Each hyperparameter setting,run on,single machine,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
single machine,with,10 asynchronous gradient - update threads,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
Experimental setup,has,Each embedding vector,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
Each embedding vector,has,projected down,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
projected down,to,200 dimensions,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
Each embedding vector,has,normalized,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
normalized,to have,2 norm of 1,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
Experimental setup,has,Out - of - vocabulary ( OOV ) words,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
Out - of - vocabulary ( OOV ) words,hashed to,one of 100 random embeddings,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
one of 100 random embeddings,initialized to,mean 0,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
one of 100 random embeddings,initialized to,standard deviation 1,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
Experimental setup,has,other parameter weights ( hidden layers etc. ,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
other parameter weights ( hidden layers etc. ),initialized from,random Gaussians,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
random Gaussians,with,mean 0,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
random Gaussians,with,standard deviation 0.01,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
Experimental setup,has,dropout ratio,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
dropout ratio,has,0.2,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
Experimental setup,has,learning rate,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
learning rate,has,intra-attention,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
intra-attention,has,0.025,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
learning rate,has,vanilla,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
vanilla,has,0.05,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
Experimental setup,has,Dropout regularization,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
Dropout regularization,used for,all ReLU layers,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
all ReLU layers,not for,final linear layer,experimental-setup,/content/training-data/natural_language_inference/90/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/90/triples/results.txt
Results,Adding,intra-sentence attention,results,/content/training-data/natural_language_inference/90/triples/results.txt
intra-sentence attention,gives,considerable improvement,results,/content/training-data/natural_language_inference/90/triples/results.txt
considerable improvement,of,0.5 percentage points,results,/content/training-data/natural_language_inference/90/triples/results.txt
0.5 percentage points,over,existing state of the art,results,/content/training-data/natural_language_inference/90/triples/results.txt
Results,has,Our vanilla approach,results,/content/training-data/natural_language_inference/90/triples/results.txt
Our vanilla approach,achieves,state - of - theart results,results,/content/training-data/natural_language_inference/90/triples/results.txt
state - of - theart results,with,almost an order of magnitude fewer parameters,results,/content/training-data/natural_language_inference/90/triples/results.txt
almost an order of magnitude fewer parameters,than,LSTMN,results,/content/training-data/natural_language_inference/90/triples/results.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/natural_language_inference/100/triples/hyperparameters.txt
Hyperparameters,set,NNM - 2,hyperparameters,/content/training-data/natural_language_inference/100/triples/hyperparameters.txt
NNM - 2,has,word embedding dimension,hyperparameters,/content/training-data/natural_language_inference/100/triples/hyperparameters.txt
word embedding dimension,as,700,hyperparameters,/content/training-data/natural_language_inference/100/triples/hyperparameters.txt
NNM - 2,has,number of bins,hyperparameters,/content/training-data/natural_language_inference/100/triples/hyperparameters.txt
number of bins,as,200,hyperparameters,/content/training-data/natural_language_inference/100/triples/hyperparameters.txt
Hyperparameters,set,NNM - 1,hyperparameters,/content/training-data/natural_language_inference/100/triples/hyperparameters.txt
NNM - 1,has,word embedding dimension,hyperparameters,/content/training-data/natural_language_inference/100/triples/hyperparameters.txt
word embedding dimension,as,700,hyperparameters,/content/training-data/natural_language_inference/100/triples/hyperparameters.txt
NNM - 1,has,number of bins,hyperparameters,/content/training-data/natural_language_inference/100/triples/hyperparameters.txt
number of bins,as,600,hyperparameters,/content/training-data/natural_language_inference/100/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/100/triples/model.txt
Model,has,Deep neural network with value - shared weights,model,/content/training-data/natural_language_inference/100/triples/model.txt
Deep neural network with value - shared weights,introduce,novel value - shared weighting scheme,model,/content/training-data/natural_language_inference/100/triples/model.txt
novel value - shared weighting scheme,in,deep neural networks,model,/content/training-data/natural_language_inference/100/triples/model.txt
Model,has,Incorporate attention scheme over question terms,model,/content/training-data/natural_language_inference/100/triples/model.txt
Incorporate attention scheme over question terms,incorporate,attention scheme,model,/content/training-data/natural_language_inference/100/triples/model.txt
attention scheme,over,question terms,model,/content/training-data/natural_language_inference/100/triples/model.txt
question terms,using,gating function,model,/content/training-data/natural_language_inference/100/triples/model.txt
Model,propose,attention based neural matching model ( a NMM ,model,/content/training-data/natural_language_inference/100/triples/model.txt
Contribution,has research problem,question answering,research-problem,/content/training-data/natural_language_inference/100/triples/research-problem.txt
Contribution,has research problem,Question answering ( QA ,research-problem,/content/training-data/natural_language_inference/100/triples/research-problem.txt
Contribution,has research problem,QA,research-problem,/content/training-data/natural_language_inference/100/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/100/triples/results.txt
Results,see,NMM,results,/content/training-data/natural_language_inference/100/triples/results.txt
NMM,beats,all the previous state - of - the art systems,results,/content/training-data/natural_language_inference/100/triples/results.txt
all the previous state - of - the art systems,including,both methods,results,/content/training-data/natural_language_inference/100/triples/results.txt
both methods,using,feature engineering and deep learning models,results,/content/training-data/natural_language_inference/100/triples/results.txt
NMM,trained with,TRAIN - ALL set,results,/content/training-data/natural_language_inference/100/triples/results.txt
Results,without combining,additional features,results,/content/training-data/natural_language_inference/100/triples/results.txt
additional features,has,NMM,results,/content/training-data/natural_language_inference/100/triples/results.txt
NMM,performs,well,results,/content/training-data/natural_language_inference/100/triples/results.txt
well,for,answer ranking,results,/content/training-data/natural_language_inference/100/triples/results.txt
well,showing,significant improvements,results,/content/training-data/natural_language_inference/100/triples/results.txt
significant improvements,over,previous deep learning model,results,/content/training-data/natural_language_inference/100/triples/results.txt
previous deep learning model,with no,additional features,results,/content/training-data/natural_language_inference/100/triples/results.txt
previous deep learning model,with no,linguistic feature engineering methods,results,/content/training-data/natural_language_inference/100/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/95/triples/ablation-analysis.txt
Ablation analysis,achieve,improvement,ablation-analysis,/content/training-data/natural_language_inference/95/triples/ablation-analysis.txt
improvement,of,nearly 3 points,ablation-analysis,/content/training-data/natural_language_inference/95/triples/ablation-analysis.txt
nearly 3 points,without,yes / no classification,ablation-analysis,/content/training-data/natural_language_inference/95/triples/ablation-analysis.txt
Ablation analysis,For,ablation,ablation-analysis,/content/training-data/natural_language_inference/95/triples/ablation-analysis.txt
ablation,of,content model,ablation-analysis,/content/training-data/natural_language_inference/95/triples/ablation-analysis.txt
content model,has,analyze,ablation-analysis,/content/training-data/natural_language_inference/95/triples/ablation-analysis.txt
analyze,violate,verification model,ablation-analysis,/content/training-data/natural_language_inference/95/triples/ablation-analysis.txt
analyze,not only affect,content score itself,ablation-analysis,/content/training-data/natural_language_inference/95/triples/ablation-analysis.txt
Ablation analysis,see that,answer verification,ablation-analysis,/content/training-data/natural_language_inference/95/triples/ablation-analysis.txt
answer verification,makes,great contribution,ablation-analysis,/content/training-data/natural_language_inference/95/triples/ablation-analysis.txt
great contribution,confirms,our hypothesis,ablation-analysis,/content/training-data/natural_language_inference/95/triples/ablation-analysis.txt
our hypothesis,that,cross - passage answer verification,ablation-analysis,/content/training-data/natural_language_inference/95/triples/ablation-analysis.txt
cross - passage answer verification,has,useful,ablation-analysis,/content/training-data/natural_language_inference/95/triples/ablation-analysis.txt
useful,for,multi-passage MRC,ablation-analysis,/content/training-data/natural_language_inference/95/triples/ablation-analysis.txt
great contribution,to,over all improvement,ablation-analysis,/content/training-data/natural_language_inference/95/triples/ablation-analysis.txt
Ablation analysis,jointly training,three models,ablation-analysis,/content/training-data/natural_language_inference/95/triples/ablation-analysis.txt
three models,provide,great benefits,ablation-analysis,/content/training-data/natural_language_inference/95/triples/ablation-analysis.txt
great benefits,shows that,three tasks,ablation-analysis,/content/training-data/natural_language_inference/95/triples/ablation-analysis.txt
three tasks,can,boost,ablation-analysis,/content/training-data/natural_language_inference/95/triples/ablation-analysis.txt
boost,has,each other,ablation-analysis,/content/training-data/natural_language_inference/95/triples/ablation-analysis.txt
each other,with,shared representations,ablation-analysis,/content/training-data/natural_language_inference/95/triples/ablation-analysis.txt
shared representations,at,bottom layers,ablation-analysis,/content/training-data/natural_language_inference/95/triples/ablation-analysis.txt
three tasks,are,closely related,ablation-analysis,/content/training-data/natural_language_inference/95/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/95/triples/model.txt
Model,consists of,three modules,model,/content/training-data/natural_language_inference/95/triples/model.txt
three modules,conduct,answer verification,model,/content/training-data/natural_language_inference/95/triples/model.txt
answer verification,by enabling,each answer candidate,model,/content/training-data/natural_language_inference/95/triples/model.txt
each answer candidate,to attend,other candidates,model,/content/training-data/natural_language_inference/95/triples/model.txt
other candidates,based on,their representations,model,/content/training-data/natural_language_inference/95/triples/model.txt
three modules,use,content scores,model,/content/training-data/natural_language_inference/95/triples/model.txt
content scores,to measure,quality,model,/content/training-data/natural_language_inference/95/triples/model.txt
quality,of,candidates,model,/content/training-data/natural_language_inference/95/triples/model.txt
three modules,model,meanings,model,/content/training-data/natural_language_inference/95/triples/model.txt
meanings,of,answer candidates,model,/content/training-data/natural_language_inference/95/triples/model.txt
answer candidates,extracted from,those passages,model,/content/training-data/natural_language_inference/95/triples/model.txt
three modules,follow,boundary - based MRC models,model,/content/training-data/natural_language_inference/95/triples/model.txt
boundary - based MRC models,to find,answer candidate,model,/content/training-data/natural_language_inference/95/triples/model.txt
answer candidate,by identifying,start and end position,model,/content/training-data/natural_language_inference/95/triples/model.txt
start and end position,of,answer,model,/content/training-data/natural_language_inference/95/triples/model.txt
answer candidate,for,each passage,model,/content/training-data/natural_language_inference/95/triples/model.txt
Model,has,final answer,model,/content/training-data/natural_language_inference/95/triples/model.txt
final answer,determined by,three factors,model,/content/training-data/natural_language_inference/95/triples/model.txt
three factors,modeled using,different modules,model,/content/training-data/natural_language_inference/95/triples/model.txt
different modules,can be,jointly trained,model,/content/training-data/natural_language_inference/95/triples/model.txt
jointly trained,in,our end - to - end framework,model,/content/training-data/natural_language_inference/95/triples/model.txt
three factors,has,boundary,model,/content/training-data/natural_language_inference/95/triples/model.txt
three factors,has,content,model,/content/training-data/natural_language_inference/95/triples/model.txt
three factors,has,crosspassage answer verification,model,/content/training-data/natural_language_inference/95/triples/model.txt
Contribution,has research problem,Multi - Passage Machine Reading Comprehension,research-problem,/content/training-data/natural_language_inference/95/triples/research-problem.txt
Contribution,has research problem,Machine reading comprehension ( MRC ,research-problem,/content/training-data/natural_language_inference/95/triples/research-problem.txt
Contribution,has research problem,MRC,research-problem,/content/training-data/natural_language_inference/95/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/95/triples/experimental-setup.txt
Experimental setup,For,MS - MARCO,experimental-setup,/content/training-data/natural_language_inference/95/triples/experimental-setup.txt
MS - MARCO,employ,300 - D pre-trained Glove embeddings,experimental-setup,/content/training-data/natural_language_inference/95/triples/experimental-setup.txt
300 - D pre-trained Glove embeddings,has,fixed,experimental-setup,/content/training-data/natural_language_inference/95/triples/experimental-setup.txt
fixed,during,training,experimental-setup,/content/training-data/natural_language_inference/95/triples/experimental-setup.txt
MS - MARCO,preprocess,corpus,experimental-setup,/content/training-data/natural_language_inference/95/triples/experimental-setup.txt
corpus,with,reversible tokenizer,experimental-setup,/content/training-data/natural_language_inference/95/triples/experimental-setup.txt
reversible tokenizer,from,Stanford CoreNLP,experimental-setup,/content/training-data/natural_language_inference/95/triples/experimental-setup.txt
MS - MARCO,has,character embeddings,experimental-setup,/content/training-data/natural_language_inference/95/triples/experimental-setup.txt
character embeddings,with,dimension,experimental-setup,/content/training-data/natural_language_inference/95/triples/experimental-setup.txt
dimension,as,30,experimental-setup,/content/training-data/natural_language_inference/95/triples/experimental-setup.txt
character embeddings,are,randomly initialized,experimental-setup,/content/training-data/natural_language_inference/95/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/95/triples/results.txt
Results,on,DuReader,results,/content/training-data/natural_language_inference/95/triples/results.txt
DuReader,see that,paragraph ranking,results,/content/training-data/natural_language_inference/95/triples/results.txt
paragraph ranking,boost,BiDAF baseline,results,/content/training-data/natural_language_inference/95/triples/results.txt
paragraph ranking,boost,significantly,results,/content/training-data/natural_language_inference/95/triples/results.txt
DuReader,has,our system ( single model ,results,/content/training-data/natural_language_inference/95/triples/results.txt
our system ( single model ),achieves,further improvement,results,/content/training-data/natural_language_inference/95/triples/results.txt
further improvement,by,large margin,results,/content/training-data/natural_language_inference/95/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
Ablation analysis,obtain,73.2,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
73.2,for,matched score,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
Ablation analysis,obtain,73.6,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
73.6,on,mismatched data,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
Ablation analysis,use,addition,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
addition,as,skip connection,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
skip connection,has,performance increase,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
performance increase,to,77.3 and 76.3,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
addition,of,representation,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
representation,after,highway network,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
highway network,and,representation,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
representation,after,self - attention,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
Ablation analysis,After removing,exact match binary feature,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
exact match binary feature,find,performance,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
performance,has,degrade,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
degrade,to,78.0,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
78.0,on,mismatched score,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
degrade,to,78.2,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
78.2,on,matched score,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
matched score,on,development set,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
Ablation analysis,remove,encoding layer completely,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
encoding layer completely,obtain,73.2,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
73.2,for,mismatched score,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
encoding layer completely,obtain,73.5,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
73.5,for,matched score,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
encoding layer completely,demonstrate,feature extraction layer,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
feature extraction layer,have,powerful capability,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
powerful capability,to capture,semantic feature,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
Ablation analysis,remove,both self - attention and fuse gate,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
both self - attention and fuse gate,retaining only,highway network,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
both self - attention and fuse gate,has,result,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
result,has,improves,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
improves,to,77.7 and 77.3,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
77.7 and 77.3,on,matched and mismatched development set,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
Ablation analysis,remove,fuse gate,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
fuse gate,has,performance,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
performance,has,degrade,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
degrade,to,73.5,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
73.5,for,matched score,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
degrade,to,73.8,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
73.8,for,mismatched,ablation-analysis,/content/training-data/natural_language_inference/49/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/49/triples/model.txt
Model,dub,general framework,model,/content/training-data/natural_language_inference/49/triples/model.txt
general framework,as,Interactive Inference Network ( IIN ,model,/content/training-data/natural_language_inference/49/triples/model.txt
Model,has,interaction tensor,model,/content/training-data/natural_language_inference/49/triples/model.txt
interaction tensor,encodes,high - order alignment relationship,model,/content/training-data/natural_language_inference/49/triples/model.txt
high - order alignment relationship,between,sentences pair,model,/content/training-data/natural_language_inference/49/triples/model.txt
Model,push,multi-head attention,model,/content/training-data/natural_language_inference/49/triples/model.txt
multi-head attention,by building,word - by - word dimension - wise alignment tensor,model,/content/training-data/natural_language_inference/49/triples/model.txt
word - by - word dimension - wise alignment tensor,call,interaction tensor,model,/content/training-data/natural_language_inference/49/triples/model.txt
multi-head attention,to,extreme,model,/content/training-data/natural_language_inference/49/triples/model.txt
Contribution,has research problem,NATURAL LANGUAGE INFERENCE,research-problem,/content/training-data/natural_language_inference/49/triples/research-problem.txt
Contribution,has research problem,NLI,research-problem,/content/training-data/natural_language_inference/49/triples/research-problem.txt
Contribution,has research problem,recognizing textual entiailment,research-problem,/content/training-data/natural_language_inference/49/triples/research-problem.txt
Contribution,has research problem,RTE,research-problem,/content/training-data/natural_language_inference/49/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
Experimental setup,use,exponential decayed keep rate,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
exponential decayed keep rate,where,initial keep rate,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
initial keep rate,is,1.0,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
exponential decayed keep rate,where,decay rate,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
decay rate,is,0.977,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
0.977,for,"every 10,000 step",experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
exponential decayed keep rate,during,training,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
Experimental setup,implement,our algorithm,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
our algorithm,with,Tensorflow framework,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
Experimental setup,has,sequence length,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
sequence length,set as,hard cutoff,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
hard cutoff,on,all experiments,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
all experiments,has,24,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
24,for,Quora Question Pair Dataset,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
all experiments,has,48,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
48,for,MultiNLI,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
all experiments,has,32,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
32,for,SNLI,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
Experimental setup,has,Dropout layers,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
Dropout layers,applied before,all linear layers,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
Dropout layers,after,word - embedding layer,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
Experimental setup,has,transitional scale down ratio,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
transitional scale down ratio,set to,0.5,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
Experimental setup,has,character embeddings,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
character embeddings,are,randomly initialized,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
randomly initialized,with,100D,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
Experimental setup,has,initial learning rate,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
initial learning rate,set to,0.5,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
Experimental setup,has,out - of - vocabulary word,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
out - of - vocabulary word,are,randomly initialized,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
randomly initialized,with,uniform distribution,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
Experimental setup,has,batch size,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
batch size,to,70,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
Experimental setup,has,first scale down ratio,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
first scale down ratio,in,feature extraction layer,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
feature extraction layer,set to,0.3,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
Experimental setup,has,Adadelta optimizer,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
Adadelta optimizer,with,? as 0.95 and as 1e ? 8,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
? as 0.95 and as 1e ? 8,used to,optimize,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
optimize,has,all the trainable weights,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
Experimental setup,has,All weights,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
All weights,constraint by,L2 regularization,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
Experimental setup,has,not improve,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
not improve,has,best in domain performance,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
best in domain performance,for,"30,000 steps",experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
not improve,has,SGD optimizer,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
SGD optimizer,with,learning rate,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
learning rate,of,3e ? 4,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
3e ? 4,to help,model,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
model,find,better local optimum,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
Experimental setup,has,initialize,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
initialize,has,word embeddings,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
word embeddings,with,pre-trained 300D Glo Ve 840B vectors,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
Experimental setup,has,crop or pad,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
crop or pad,each,token,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
token,to have,16 characters,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
Experimental setup,has,1D convolution kernel size,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
1D convolution kernel size,for,character embedding,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
1D convolution kernel size,is,5,experimental-setup,/content/training-data/natural_language_inference/49/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/49/triples/results.txt
Results,has,MULTINLI,results,/content/training-data/natural_language_inference/49/triples/results.txt
MULTINLI,find,out - of - domain test performance,results,/content/training-data/natural_language_inference/49/triples/results.txt
out - of - domain test performance,is,consistently lower,results,/content/training-data/natural_language_inference/49/triples/results.txt
consistently lower,than,in - domain test performance,results,/content/training-data/natural_language_inference/49/triples/results.txt
MULTINLI,has,Our approach,results,/content/training-data/natural_language_inference/49/triples/results.txt
Our approach,achieves,new state - of - the - art performance,results,/content/training-data/natural_language_inference/49/triples/results.txt
new state - of - the - art performance,of,80.0 %,results,/content/training-data/natural_language_inference/49/triples/results.txt
80.0 %,exceeding,current state - of - the - art performance,results,/content/training-data/natural_language_inference/49/triples/results.txt
current state - of - the - art performance,by,more than 5 %,results,/content/training-data/natural_language_inference/49/triples/results.txt
Our approach,without using,recurrent structure,results,/content/training-data/natural_language_inference/49/triples/results.txt
Results,has,SNLI,results,/content/training-data/natural_language_inference/49/triples/results.txt
SNLI,show,"our model , DIIN",results,/content/training-data/natural_language_inference/49/triples/results.txt
"our model , DIIN",achieves,state - of - the - art performance,results,/content/training-data/natural_language_inference/49/triples/results.txt
state - of - the - art performance,on,competitive leaderboard,results,/content/training-data/natural_language_inference/49/triples/results.txt
Results,has,QUORA QUESTION PAIR DATASET,results,/content/training-data/natural_language_inference/49/triples/results.txt
QUORA QUESTION PAIR DATASET,has,DECATT word and DECATT char,results,/content/training-data/natural_language_inference/49/triples/results.txt
DECATT word and DECATT char,uses,automatically collected in - domain paraphrase data,results,/content/training-data/natural_language_inference/49/triples/results.txt
automatically collected in - domain paraphrase data,to noisy pretrain,n-gram word embedding and ngram subword embedding,results,/content/training-data/natural_language_inference/49/triples/results.txt
n-gram word embedding and ngram subword embedding,on,decomposable attention model,results,/content/training-data/natural_language_inference/49/triples/results.txt
QUORA QUESTION PAIR DATASET,has,BIMPM,results,/content/training-data/natural_language_inference/49/triples/results.txt
BIMPM,models,different perspective of matching,results,/content/training-data/natural_language_inference/49/triples/results.txt
different perspective of matching,between,sentence pair,results,/content/training-data/natural_language_inference/49/triples/results.txt
sentence pair,on,both direction,results,/content/training-data/natural_language_inference/49/triples/results.txt
BIMPM,aggregates,matching vector,results,/content/training-data/natural_language_inference/49/triples/results.txt
matching vector,with,LSTM,results,/content/training-data/natural_language_inference/49/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/78/triples/ablation-analysis.txt
Ablation analysis,explore,utility,ablation-analysis,/content/training-data/natural_language_inference/78/triples/ablation-analysis.txt
utility,of using,character and syntactic embeddings,ablation-analysis,/content/training-data/natural_language_inference/78/triples/ablation-analysis.txt
character and syntactic embeddings,found to,helped,ablation-analysis,/content/training-data/natural_language_inference/78/triples/ablation-analysis.txt
helped,has,CAFE,ablation-analysis,/content/training-data/natural_language_inference/78/triples/ablation-analysis.txt
Ablation analysis,Using,ReLU,ablation-analysis,/content/training-data/natural_language_inference/78/triples/ablation-analysis.txt
ReLU,seems to be,worse,ablation-analysis,/content/training-data/natural_language_inference/78/triples/ablation-analysis.txt
worse,than,nonlinear FC layers,ablation-analysis,/content/training-data/natural_language_inference/78/triples/ablation-analysis.txt
Ablation analysis,replace,LSTM encoder,ablation-analysis,/content/training-data/natural_language_inference/78/triples/ablation-analysis.txt
LSTM encoder,with,BiLSTM,ablation-analysis,/content/training-data/natural_language_inference/78/triples/ablation-analysis.txt
LSTM encoder,observing,adding bi-directionality,ablation-analysis,/content/training-data/natural_language_inference/78/triples/ablation-analysis.txt
adding bi-directionality,has,did not improve,ablation-analysis,/content/training-data/natural_language_inference/78/triples/ablation-analysis.txt
did not improve,has,performance,ablation-analysis,/content/training-data/natural_language_inference/78/triples/ablation-analysis.txt
performance,for,our model,ablation-analysis,/content/training-data/natural_language_inference/78/triples/ablation-analysis.txt
Ablation analysis,has,1 - layer linear setting,ablation-analysis,/content/training-data/natural_language_inference/78/triples/ablation-analysis.txt
1 - layer linear setting,performs,best,ablation-analysis,/content/training-data/natural_language_inference/78/triples/ablation-analysis.txt
Ablation analysis,remove,inter-attention alignment features,ablation-analysis,/content/training-data/natural_language_inference/78/triples/ablation-analysis.txt
inter-attention alignment features,has,naturally impact,ablation-analysis,/content/training-data/natural_language_inference/78/triples/ablation-analysis.txt
naturally impact,has,model performance,ablation-analysis,/content/training-data/natural_language_inference/78/triples/ablation-analysis.txt
model performance,has,significantly,ablation-analysis,/content/training-data/natural_language_inference/78/triples/ablation-analysis.txt
Ablation analysis,observe,Sub and Concat compositions,ablation-analysis,/content/training-data/natural_language_inference/78/triples/ablation-analysis.txt
Sub and Concat compositions,were,more important,ablation-analysis,/content/training-data/natural_language_inference/78/triples/ablation-analysis.txt
more important,than,Mul composition,ablation-analysis,/content/training-data/natural_language_inference/78/triples/ablation-analysis.txt
Ablation analysis,observe,both highway layers,ablation-analysis,/content/training-data/natural_language_inference/78/triples/ablation-analysis.txt
both highway layers,have,marginally helped,ablation-analysis,/content/training-data/natural_language_inference/78/triples/ablation-analysis.txt
marginally helped,has,over all performance,ablation-analysis,/content/training-data/natural_language_inference/78/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/78/triples/model.txt
Model,achieve,efficient propagation,model,/content/training-data/natural_language_inference/78/triples/model.txt
efficient propagation,of,alignment features,model,/content/training-data/natural_language_inference/78/triples/model.txt
efficient propagation,propose,alignment factorization layers,model,/content/training-data/natural_language_inference/78/triples/model.txt
alignment factorization layers,to reduce,each alignment vector,model,/content/training-data/natural_language_inference/78/triples/model.txt
each alignment vector,to,single scalar valued feature,model,/content/training-data/natural_language_inference/78/triples/model.txt
Model,has,several new novel components,model,/content/training-data/natural_language_inference/78/triples/model.txt
Model,has,scalar valued feature,model,/content/training-data/natural_language_inference/78/triples/model.txt
scalar valued feature,used to,augment,model,/content/training-data/natural_language_inference/78/triples/model.txt
augment,has,base word representation,model,/content/training-data/natural_language_inference/78/triples/model.txt
Model,propose,"compare , compress and propagate ( Com Prop ) architecture",model,/content/training-data/natural_language_inference/78/triples/model.txt
"compare , compress and propagate ( Com Prop ) architecture",has,compressed alignment features,model,/content/training-data/natural_language_inference/78/triples/model.txt
compressed alignment features,for enhancing,representation learning,model,/content/training-data/natural_language_inference/78/triples/model.txt
compressed alignment features,propagated to,upper layers,model,/content/training-data/natural_language_inference/78/triples/model.txt
upper layers,such as,RNN - based encoder,model,/content/training-data/natural_language_inference/78/triples/model.txt
Contribution,has research problem,Natural Language Inference,research-problem,/content/training-data/natural_language_inference/78/triples/research-problem.txt
Contribution,has research problem,Natural Language Inference ( NLI ,research-problem,/content/training-data/natural_language_inference/78/triples/research-problem.txt
Contribution,has research problem,NLI,research-problem,/content/training-data/natural_language_inference/78/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/78/triples/experimental-setup.txt
Experimental setup,train them on,Nvidia P100 GPUs,experimental-setup,/content/training-data/natural_language_inference/78/triples/experimental-setup.txt
Experimental setup,use,Adam optimizer,experimental-setup,/content/training-data/natural_language_inference/78/triples/experimental-setup.txt
Adam optimizer,with,initial learning rate,experimental-setup,/content/training-data/natural_language_inference/78/triples/experimental-setup.txt
initial learning rate,of,0.0003,experimental-setup,/content/training-data/natural_language_inference/78/triples/experimental-setup.txt
Experimental setup,implement,our model,experimental-setup,/content/training-data/natural_language_inference/78/triples/experimental-setup.txt
our model,in,TensorFlow,experimental-setup,/content/training-data/natural_language_inference/78/triples/experimental-setup.txt
Experimental setup,has,batch order,experimental-setup,/content/training-data/natural_language_inference/78/triples/experimental-setup.txt
batch order,( randomly ) sorted within,buckets,experimental-setup,/content/training-data/natural_language_inference/78/triples/experimental-setup.txt
Experimental setup,has,size,experimental-setup,/content/training-data/natural_language_inference/78/triples/experimental-setup.txt
size,of,hidden layers,experimental-setup,/content/training-data/natural_language_inference/78/triples/experimental-setup.txt
hidden layers,of,highway network layers,experimental-setup,/content/training-data/natural_language_inference/78/triples/experimental-setup.txt
highway network layers,set to,300,experimental-setup,/content/training-data/natural_language_inference/78/triples/experimental-setup.txt
Experimental setup,has,L2 regularization,experimental-setup,/content/training-data/natural_language_inference/78/triples/experimental-setup.txt
L2 regularization,set to,10 ?6,experimental-setup,/content/training-data/natural_language_inference/78/triples/experimental-setup.txt
Experimental setup,has,batch size,experimental-setup,/content/training-data/natural_language_inference/78/triples/experimental-setup.txt
batch size,tuned amongst,"{ 128 , 256 , 512 }",experimental-setup,/content/training-data/natural_language_inference/78/triples/experimental-setup.txt
Experimental setup,has,number of latent factors k,experimental-setup,/content/training-data/natural_language_inference/78/triples/experimental-setup.txt
number of latent factors k,for,factorization layer,experimental-setup,/content/training-data/natural_language_inference/78/triples/experimental-setup.txt
factorization layer,tuned amongst,"{ 5 , 10 , 50 , 100 , 150 }",experimental-setup,/content/training-data/natural_language_inference/78/triples/experimental-setup.txt
Experimental setup,has,Sequence lengths,experimental-setup,/content/training-data/natural_language_inference/78/triples/experimental-setup.txt
Sequence lengths,padded to,batch - wise maximum,experimental-setup,/content/training-data/natural_language_inference/78/triples/experimental-setup.txt
Experimental setup,has,parameters,experimental-setup,/content/training-data/natural_language_inference/78/triples/experimental-setup.txt
parameters,initialized with,xavier initialization,experimental-setup,/content/training-data/natural_language_inference/78/triples/experimental-setup.txt
Experimental setup,has,Word embeddings,experimental-setup,/content/training-data/natural_language_inference/78/triples/experimental-setup.txt
Word embeddings,fixed during,training,experimental-setup,/content/training-data/natural_language_inference/78/triples/experimental-setup.txt
Word embeddings,preloaded with,300d Glo Ve embeddings,experimental-setup,/content/training-data/natural_language_inference/78/triples/experimental-setup.txt
Experimental setup,has,Dropout,experimental-setup,/content/training-data/natural_language_inference/78/triples/experimental-setup.txt
Dropout,with,keep probability,experimental-setup,/content/training-data/natural_language_inference/78/triples/experimental-setup.txt
keep probability,of,0.8,experimental-setup,/content/training-data/natural_language_inference/78/triples/experimental-setup.txt
keep probability,applied after,"each fullyconnected , recurrent or highway layer",experimental-setup,/content/training-data/natural_language_inference/78/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/78/triples/results.txt
Results,on,MultiNLI,results,/content/training-data/natural_language_inference/78/triples/results.txt
MultiNLI,has,CAFE,results,/content/training-data/natural_language_inference/78/triples/results.txt
CAFE,has,outperform,results,/content/training-data/natural_language_inference/78/triples/results.txt
outperform,has,ESIM + Read model,results,/content/training-data/natural_language_inference/78/triples/results.txt
CAFE,has,significantly outperforms,results,/content/training-data/natural_language_inference/78/triples/results.txt
significantly outperforms,has,ESIM,results,/content/training-data/natural_language_inference/78/triples/results.txt
MultiNLI,has,ensemble of CAFE models,results,/content/training-data/natural_language_inference/78/triples/results.txt
ensemble of CAFE models,achieve,competitive re-sult,results,/content/training-data/natural_language_inference/78/triples/results.txt
Results,on,SNLI benchmark,results,/content/training-data/natural_language_inference/78/triples/results.txt
SNLI benchmark,has,CAFE,results,/content/training-data/natural_language_inference/78/triples/results.txt
CAFE,obtains,88.5 % accuracy,results,/content/training-data/natural_language_inference/78/triples/results.txt
88.5 % accuracy,has,extremely competitive score,results,/content/training-data/natural_language_inference/78/triples/results.txt
88.5 % accuracy,on,SNLI test set,results,/content/training-data/natural_language_inference/78/triples/results.txt
CAFE,achieves,88.3 % and 88.1 % test accuracy,results,/content/training-data/natural_language_inference/78/triples/results.txt
88.3 % and 88.1 % test accuracy,with,only 3.5 M and 1.5 M parameters,results,/content/training-data/natural_language_inference/78/triples/results.txt
SNLI benchmark,has,CAFE + ELMo,results,/content/training-data/natural_language_inference/78/triples/results.txt
CAFE + ELMo,achieves,89.0 score,results,/content/training-data/natural_language_inference/78/triples/results.txt
89.0 score,on,SNLI,results,/content/training-data/natural_language_inference/78/triples/results.txt
CAFE + ELMo,has,lightweight adaptation,results,/content/training-data/natural_language_inference/78/triples/results.txt
lightweight adaptation,achieves,87.7 %,results,/content/training-data/natural_language_inference/78/triples/results.txt
CAFE + ELMo,has,outperforms,results,/content/training-data/natural_language_inference/78/triples/results.txt
outperforms,has,state - of - theart ESIM and DIIN models,results,/content/training-data/natural_language_inference/78/triples/results.txt
state - of - theart ESIM and DIIN models,with,fraction,results,/content/training-data/natural_language_inference/78/triples/results.txt
fraction,of,parameter cost,results,/content/training-data/natural_language_inference/78/triples/results.txt
SNLI benchmark,has,ensemble,results,/content/training-data/natural_language_inference/78/triples/results.txt
ensemble,of,5 CAFE models,results,/content/training-data/natural_language_inference/78/triples/results.txt
5 CAFE models,achieves,89.3 % test accuracy,results,/content/training-data/natural_language_inference/78/triples/results.txt
89.3 % test accuracy,has,best test scores,results,/content/training-data/natural_language_inference/78/triples/results.txt
SNLI benchmark,On,cross sentence ( single model setting ,results,/content/training-data/natural_language_inference/78/triples/results.txt
cross sentence ( single model setting ),has,performance,results,/content/training-data/natural_language_inference/78/triples/results.txt
performance,of,proposed CAFE model,results,/content/training-data/natural_language_inference/78/triples/results.txt
performance,is,extremely competitive,results,/content/training-data/natural_language_inference/78/triples/results.txt
Results,On,SciTail,results,/content/training-data/natural_language_inference/78/triples/results.txt
SciTail,has,CAFE,results,/content/training-data/natural_language_inference/78/triples/results.txt
CAFE,has,outperforms,results,/content/training-data/natural_language_inference/78/triples/results.txt
outperforms,by,significant margin,results,/content/training-data/natural_language_inference/78/triples/results.txt
significant margin,of,5 %,results,/content/training-data/natural_language_inference/78/triples/results.txt
outperforms,has,DGEM,results,/content/training-data/natural_language_inference/78/triples/results.txt
SciTail,has,proposed CAFE model,results,/content/training-data/natural_language_inference/78/triples/results.txt
proposed CAFE model,achieves,state - of - the - art performance,results,/content/training-data/natural_language_inference/78/triples/results.txt
SciTail,has,performance gain,results,/content/training-data/natural_language_inference/78/triples/results.txt
performance gain,over,strong baselines,results,/content/training-data/natural_language_inference/78/triples/results.txt
strong baselines,such as,DecompAtt,results,/content/training-data/natural_language_inference/78/triples/results.txt
strong baselines,such as,ESIM,results,/content/training-data/natural_language_inference/78/triples/results.txt
performance gain,are,10 % ? 13 %,results,/content/training-data/natural_language_inference/78/triples/results.txt
10 % ? 13 %,in terms of,accuracy,results,/content/training-data/natural_language_inference/78/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/99/triples/ablation-analysis.txt
Ablation analysis,see,full features integration,ablation-analysis,/content/training-data/natural_language_inference/99/triples/ablation-analysis.txt
full features integration,obtain,best performance,ablation-analysis,/content/training-data/natural_language_inference/99/triples/ablation-analysis.txt
Ablation analysis,replace,our input gate mechanism,ablation-analysis,/content/training-data/natural_language_inference/99/triples/ablation-analysis.txt
our input gate mechanism,into,simplified feature concatenation strategy,ablation-analysis,/content/training-data/natural_language_inference/99/triples/ablation-analysis.txt
our input gate mechanism,has,performance,ablation-analysis,/content/training-data/natural_language_inference/99/triples/ablation-analysis.txt
performance,has,drops,ablation-analysis,/content/training-data/natural_language_inference/99/triples/ablation-analysis.txt
drops,has,nearly 2.3 %,ablation-analysis,/content/training-data/natural_language_inference/99/triples/ablation-analysis.txt
nearly 2.3 %,on,EM score,ablation-analysis,/content/training-data/natural_language_inference/99/triples/ablation-analysis.txt
Ablation analysis,Among,all the feature ablations,ablation-analysis,/content/training-data/natural_language_inference/99/triples/ablation-analysis.txt
all the feature ablations,has,drop,ablation-analysis,/content/training-data/natural_language_inference/99/triples/ablation-analysis.txt
drop,much more than,other features,ablation-analysis,/content/training-data/natural_language_inference/99/triples/ablation-analysis.txt
drop,has,Part - Of - Speech,ablation-analysis,/content/training-data/natural_language_inference/99/triples/ablation-analysis.txt
drop,has,Exact Match,ablation-analysis,/content/training-data/natural_language_inference/99/triples/ablation-analysis.txt
drop,has,Qtype features,ablation-analysis,/content/training-data/natural_language_inference/99/triples/ablation-analysis.txt
Ablation analysis,has,final ablation,ablation-analysis,/content/training-data/natural_language_inference/99/triples/ablation-analysis.txt
final ablation,of,POS and NER,ablation-analysis,/content/training-data/natural_language_inference/99/triples/ablation-analysis.txt
POS and NER,can see,performance decays,ablation-analysis,/content/training-data/natural_language_inference/99/triples/ablation-analysis.txt
performance decays,over,3 % point,ablation-analysis,/content/training-data/natural_language_inference/99/triples/ablation-analysis.txt
Ablation analysis,employing,question influence,ablation-analysis,/content/training-data/natural_language_inference/99/triples/ablation-analysis.txt
question influence,on,passage encoding,ablation-analysis,/content/training-data/natural_language_inference/99/triples/ablation-analysis.txt
passage encoding,has,boost,ablation-analysis,/content/training-data/natural_language_inference/99/triples/ablation-analysis.txt
boost,has,result,ablation-analysis,/content/training-data/natural_language_inference/99/triples/ablation-analysis.txt
boost,has,up to 1.3 %,ablation-analysis,/content/training-data/natural_language_inference/99/triples/ablation-analysis.txt
up to 1.3 %,on,EM score,ablation-analysis,/content/training-data/natural_language_inference/99/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/99/triples/model.txt
Model,add,checking layer,model,/content/training-data/natural_language_inference/99/triples/model.txt
checking layer,to ensure,accuracy,model,/content/training-data/natural_language_inference/99/triples/model.txt
checking layer,on,answer refining,model,/content/training-data/natural_language_inference/99/triples/model.txt
Model,introduce,Smarnet framework,model,/content/training-data/natural_language_inference/99/triples/model.txt
Smarnet framework,exploits,fine - grained word understanding,model,/content/training-data/natural_language_inference/99/triples/model.txt
fine - grained word understanding,with,various attribution discriminations,model,/content/training-data/natural_language_inference/99/triples/model.txt
Model,develop,interactive attention,model,/content/training-data/natural_language_inference/99/triples/model.txt
interactive attention,with,memory network,model,/content/training-data/natural_language_inference/99/triples/model.txt
interactive attention,to mimic,human reading procedure,model,/content/training-data/natural_language_inference/99/triples/model.txt
Model,propose,novel framework,model,/content/training-data/natural_language_inference/99/triples/model.txt
novel framework,named,Smarnet,model,/content/training-data/natural_language_inference/99/triples/model.txt
Contribution,has research problem,Machine Comprehension ( MC ,research-problem,/content/training-data/natural_language_inference/99/triples/research-problem.txt
Contribution,has research problem,MC,research-problem,/content/training-data/natural_language_inference/99/triples/research-problem.txt
Contribution,has research problem,machine comprehension,research-problem,/content/training-data/natural_language_inference/99/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
Experimental setup,preprocess,each passage and question,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
each passage and question,using,library of nltk,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
Experimental setup,apply,dropout ( Srivastava et al. 2014 ,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
dropout ( Srivastava et al. 2014 ),with,dropout rate,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
dropout rate,of,0.2,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
dropout ( Srivastava et al. 2014 ),between,layers,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
Experimental setup,adopt,AdaDelta ( Zeiler 2012 ) optimizer,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
AdaDelta ( Zeiler 2012 ) optimizer,with,initial learning rate,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
initial learning rate,of,0.0005,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
AdaDelta ( Zeiler 2012 ) optimizer,for,training,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
Experimental setup,For,multi-hop reasoning,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
multi-hop reasoning,set,number of hops,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
number of hops,as,2,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
2,has,imitating,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
imitating,has,human reading procedure,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
imitating,on,skimming and scanning,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
Experimental setup,has,size,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
size,set as,100 - dimensional,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
size,of,char - level embedding,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
char - level embedding,obtained by,CNN filters,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
Experimental setup,has,batch size,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
batch size,set to be,48,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
48,for,SQuAD and TriviaQA datasets,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
Experimental setup,exploit,popular pretrained word embedding GloVe,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
popular pretrained word embedding GloVe,with,100 - dimensional vectors,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
popular pretrained word embedding GloVe,for,questions and passages,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
Experimental setup,During,training,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
training,set,moving averages,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
moving averages,as,exponential decay rate,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
exponential decay rate,of,0.999,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
moving averages,of,all weights,experimental-setup,/content/training-data/natural_language_inference/99/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/99/triples/results.txt
Results,can see,ensemble model,results,/content/training-data/natural_language_inference/99/triples/results.txt
ensemble model,has,improves,results,/content/training-data/natural_language_inference/99/triples/results.txt
improves,to,EM,results,/content/training-data/natural_language_inference/99/triples/results.txt
EM,has,75.989 %,results,/content/training-data/natural_language_inference/99/triples/results.txt
improves,to,F1,results,/content/training-data/natural_language_inference/99/triples/results.txt
F1,has,83. 475 %,results,/content/training-data/natural_language_inference/99/triples/results.txt
Results,can see,our single model,results,/content/training-data/natural_language_inference/99/triples/results.txt
our single model,achieves,EM score,results,/content/training-data/natural_language_inference/99/triples/results.txt
EM score,of,71.415 %,results,/content/training-data/natural_language_inference/99/triples/results.txt
our single model,achieves,F1 score,results,/content/training-data/natural_language_inference/99/triples/results.txt
F1 score,of,80.160 %,results,/content/training-data/natural_language_inference/99/triples/results.txt
Results,on,test set,results,/content/training-data/natural_language_inference/99/triples/results.txt
test set,of,Trivia QA,results,/content/training-data/natural_language_inference/99/triples/results.txt
Trivia QA,can see,our Smarnet model,results,/content/training-data/natural_language_inference/99/triples/results.txt
our Smarnet model,has,outperforms,results,/content/training-data/natural_language_inference/99/triples/results.txt
outperforms,has,other baselines,results,/content/training-data/natural_language_inference/99/triples/results.txt
other baselines,on,wikipedia domain,results,/content/training-data/natural_language_inference/99/triples/results.txt
other baselines,on,web domain,results,/content/training-data/natural_language_inference/99/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/42/triples/ablation-analysis.txt
Ablation analysis,confirms,effectiveness,ablation-analysis,/content/training-data/natural_language_inference/42/triples/ablation-analysis.txt
effectiveness,of,char- embeddings,ablation-analysis,/content/training-data/natural_language_inference/42/triples/ablation-analysis.txt
char- embeddings,addition,has,ablation-analysis,/content/training-data/natural_language_inference/42/triples/ablation-analysis.txt
has,increased,F1 and EM scores,ablation-analysis,/content/training-data/natural_language_inference/42/triples/ablation-analysis.txt
F1 and EM scores,by,2.7 % and 3.1 %,ablation-analysis,/content/training-data/natural_language_inference/42/triples/ablation-analysis.txt
Ablation analysis,indicate,reduction layer,ablation-analysis,/content/training-data/natural_language_inference/42/triples/ablation-analysis.txt
reduction layer,capable of producing,useful word representations,ablation-analysis,/content/training-data/natural_language_inference/42/triples/ablation-analysis.txt
useful word representations,when,compressing,ablation-analysis,/content/training-data/natural_language_inference/42/triples/ablation-analysis.txt
compressing,has,embeddings,ablation-analysis,/content/training-data/natural_language_inference/42/triples/ablation-analysis.txt
reduction layer,replaced,standard feedforward layer,ablation-analysis,/content/training-data/natural_language_inference/42/triples/ablation-analysis.txt
standard feedforward layer,with,same reduction ratio,ablation-analysis,/content/training-data/natural_language_inference/42/triples/ablation-analysis.txt
standard feedforward layer,has,drop,ablation-analysis,/content/training-data/natural_language_inference/42/triples/ablation-analysis.txt
drop,of,2.1 % and 2.5 %,ablation-analysis,/content/training-data/natural_language_inference/42/triples/ablation-analysis.txt
2.1 % and 2.5 %,in,F1 and EM scores,ablation-analysis,/content/training-data/natural_language_inference/42/triples/ablation-analysis.txt
Ablation analysis,when,convolutional attention,ablation-analysis,/content/training-data/natural_language_inference/42/triples/ablation-analysis.txt
convolutional attention,replaced by,standard attention mechanism,ablation-analysis,/content/training-data/natural_language_inference/42/triples/ablation-analysis.txt
standard attention mechanism,has,performance,ablation-analysis,/content/training-data/natural_language_inference/42/triples/ablation-analysis.txt
performance,has,dropped,ablation-analysis,/content/training-data/natural_language_inference/42/triples/ablation-analysis.txt
dropped,by,2.5 %,ablation-analysis,/content/training-data/natural_language_inference/42/triples/ablation-analysis.txt
2.5 %,in,EM,ablation-analysis,/content/training-data/natural_language_inference/42/triples/ablation-analysis.txt
dropped,by,2.4 %,ablation-analysis,/content/training-data/natural_language_inference/42/triples/ablation-analysis.txt
2.4 %,in,F1,ablation-analysis,/content/training-data/natural_language_inference/42/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/42/triples/model.txt
Model,named,Fully Attention - Based Information Retriever ( FABIR ,model,/content/training-data/natural_language_inference/42/triples/model.txt
Model,to verify,how much performance,model,/content/training-data/natural_language_inference/42/triples/model.txt
how much performance,get exclusively from,attention mechanism,model,/content/training-data/natural_language_inference/42/triples/model.txt
attention mechanism,without combining it with,several other techniques,model,/content/training-data/natural_language_inference/42/triples/model.txt
Model,has,Convolutional attention,model,/content/training-data/natural_language_inference/42/triples/model.txt
Convolutional attention,has,novel attention mechanism,model,/content/training-data/natural_language_inference/42/triples/model.txt
novel attention mechanism,encodes,many - to - many relationships,model,/content/training-data/natural_language_inference/42/triples/model.txt
many - to - many relationships,enabling,richer contextual representations,model,/content/training-data/natural_language_inference/42/triples/model.txt
many - to - many relationships,between,words,model,/content/training-data/natural_language_inference/42/triples/model.txt
Model,has,Reduction layer,model,/content/training-data/natural_language_inference/42/triples/model.txt
Reduction layer,has,new layer design,model,/content/training-data/natural_language_inference/42/triples/model.txt
new layer design,fits,pipeline,model,/content/training-data/natural_language_inference/42/triples/model.txt
pipeline,proposed by,Vaswani et al .,model,/content/training-data/natural_language_inference/42/triples/model.txt
Model,has,Column - wise cross - attention,model,/content/training-data/natural_language_inference/42/triples/model.txt
Column - wise cross - attention,modify,crossattention operation,model,/content/training-data/natural_language_inference/42/triples/model.txt
Column - wise cross - attention,propose,new technique,model,/content/training-data/natural_language_inference/42/triples/model.txt
new technique,better suited to,question - answering,model,/content/training-data/natural_language_inference/42/triples/model.txt
Contribution,has research problem,Information Retriever,research-problem,/content/training-data/natural_language_inference/42/triples/research-problem.txt
Contribution,has research problem,Question - answering ( QA ,research-problem,/content/training-data/natural_language_inference/42/triples/research-problem.txt
Contribution,has research problem,open - domain QA,research-problem,/content/training-data/natural_language_inference/42/triples/research-problem.txt
Contribution,has research problem,QA,research-problem,/content/training-data/natural_language_inference/42/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
Experimental setup,set,feed - forward hidden size,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
feed - forward hidden size,to,200,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
200,in,processing layers,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
feed - forward hidden size,to,400,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
400,in,reduction layer,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
Experimental setup,set,processing layers dimension,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
processing layers dimension,has,d model,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
d model,to,100,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
Experimental setup,set,number of heads,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
number of heads,has,n heads,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
n heads,in,each attention sublayer,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
each attention sublayer,to,4,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
Experimental setup,trained,FABIR model,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
FABIR model,with,batch size,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
batch size,of,75,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
FABIR model,in,GPU NVidia Titan X,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
GPU NVidia Titan X,with,12 GB,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
12 GB,of,RAM,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
FABIR model,during,54 epochs,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
Experimental setup,pre-processed,texts,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
texts,with,NLTK Tokenizer,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
Experimental setup,In,character - level embedding process,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
character - level embedding process,has,dropout,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
dropout,of,0.75,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
0.75,added before,convolution,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
Experimental setup,For,regularization,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
regularization,applied,residual and attention dropout,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
residual and attention dropout,of,0.8,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
0.8,in,reduction layer,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
residual and attention dropout,of,0.9,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
0.9,in,processing layers,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
Experimental setup,has,dropout,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
dropout,of,0.8,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
0.8,added before,each convolutional layer,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
each convolutional layer,in,answer selector,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
Experimental setup,developed,our model,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
our model,in,Tensorflow,experimental-setup,/content/training-data/natural_language_inference/42/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/42/triples/results.txt
Results,analyze,performance,results,/content/training-data/natural_language_inference/42/triples/results.txt
performance,of,FABIR and BiDAF,results,/content/training-data/natural_language_inference/42/triples/results.txt
FABIR and BiDAF,shows,shorter answers,results,/content/training-data/natural_language_inference/42/triples/results.txt
shorter answers,are,easier,results,/content/training-data/natural_language_inference/42/triples/results.txt
easier,for,both models,results,/content/training-data/natural_language_inference/42/triples/results.txt
FABIR and BiDAF,shows,best performance,results,/content/training-data/natural_language_inference/42/triples/results.txt
best performance,with,""" when "" questions",results,/content/training-data/natural_language_inference/42/triples/results.txt
FABIR and BiDAF,has,shorter passages,results,/content/training-data/natural_language_inference/42/triples/results.txt
shorter passages,showed,worst performance,results,/content/training-data/natural_language_inference/42/triples/results.txt
worst performance,for,both models,results,/content/training-data/natural_language_inference/42/triples/results.txt
FABIR and BiDAF,has,""" how "" and "" why "" questions",results,/content/training-data/natural_language_inference/42/triples/results.txt
""" how "" and "" why "" questions",resulted in,considerably lower F1 and EM scores,results,/content/training-data/natural_language_inference/42/triples/results.txt
FABIR and BiDAF,has,Questions,results,/content/training-data/natural_language_inference/42/triples/results.txt
Questions,expect,""" yes "" or a "" no """,results,/content/training-data/natural_language_inference/42/triples/results.txt
""" yes "" or a "" no """,as,answer,results,/content/training-data/natural_language_inference/42/triples/results.txt
""" yes "" or a "" no """,are,also difficult,results,/content/training-data/natural_language_inference/42/triples/results.txt
FABIR and BiDAF,has,""" how long "" and "" how many """,results,/content/training-data/natural_language_inference/42/triples/results.txt
""" how long "" and "" how many """,proved,easier,results,/content/training-data/natural_language_inference/42/triples/results.txt
easier,to,respond,results,/content/training-data/natural_language_inference/42/triples/results.txt
""" how long "" and "" how many """,possess,same property,results,/content/training-data/natural_language_inference/42/triples/results.txt
same property,of having,smaller universe,results,/content/training-data/natural_language_inference/42/triples/results.txt
smaller universe,of,possible answers,results,/content/training-data/natural_language_inference/42/triples/results.txt
Results,Regarding,EM and F 1 scores,results,/content/training-data/natural_language_inference/42/triples/results.txt
EM and F 1 scores,has,FABIR and BiDAF,results,/content/training-data/natural_language_inference/42/triples/results.txt
FABIR and BiDAF,showed,similar performances,results,/content/training-data/natural_language_inference/42/triples/results.txt
Contribution,has,Experiments,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
Experiments,has,Tasks,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
Tasks,has,Natural Language Inference,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
Natural Language Inference,has,Experimental setup,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
Experimental setup,initialize,word embedding matrix,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
word embedding matrix,with,GloVe 300D pretrained vectors,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
Experimental setup,has,dropout probability,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
dropout probability,set to,0.2,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
Experimental setup,has,size,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
size,of,mini-batches,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
mini-batches,set to,128,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
Experimental setup,has,temperature parameter,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
temperature parameter,of,Gumbel - Softmax,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
Gumbel - Softmax,set to,1.0,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
Experimental setup,For training,models,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
models,used,Adam optimizer,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
Experimental setup,on,machine,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
machine,with,NVIDIA Titan Xp GPU,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
Natural Language Inference,has,Results,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
Results,find that,our 100D and 300D model,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
our 100D and 300D model,outperform,all other models,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
all other models,of,similar numbers of parameters,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
Results,see that,LSTM - based leaf transformation,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
LSTM - based leaf transformation,has,clear advantage,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
clear advantage,over,affine - transformation - based one,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
Results,has,Our 600D model,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
Our 600D model,achieves,accuracy,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
accuracy,of,86.0 %,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
86.0 %,comparable to,state - of - the - art model,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
Tasks,has,Sentiment Analysis,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
Sentiment Analysis,has,Hyperparameters,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
Hyperparameters,trained,SST - 2 model,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
SST - 2 model,apply,dropout ( p = 0.5 ,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
dropout ( p = 0.5 ),on,output,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
output,of,word embedding layer,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
dropout ( p = 0.5 ),on,input and the output,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
input and the output,of,MLP layer,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
SST - 2 model,with hyperparameters,"D x = 300 , D h = 300 , D c = 300",experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
SST - 2 model,has,Adadelta optimizer,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
Adadelta optimizer,used for,optimization,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
SST - 2 model,has,size,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
size,of,mini-batches,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
mini-batches,set to,32,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
SST - 2 model,has,word vectors,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
word vectors,has,fine - tuned,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
fine - tuned,during,training,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
word vectors,has,initialized,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
initialized,with,GloVe 300D pretrained vectors,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
Hyperparameters,For,SST - 5 model,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
SST - 5 model,optimize,model,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
model,using,Adadelta optimizer,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
Adadelta optimizer,with,batch size 64,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
Adadelta optimizer,apply,dropout,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
dropout,with,p = 0.5,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
SST - 5 model,has,hyperparameters,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
hyperparameters,set to,"D x = 300 , D h = 300 , D c = 1024",experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
Hyperparameters,is,single - hidden layer MLP,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
single - hidden layer MLP,with,ReLU activation function,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
Sentiment Analysis,has,Results,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
Results,utilizing,pretraining and character n-gram embeddings,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
pretraining and character n-gram embeddings,improves,validation accuracy,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
validation accuracy,by,2.8 % ( SST - 2 ,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
validation accuracy,by,1.7 % ( SST - 5 ,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
Results,has,SST - 2 model,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
SST - 2 model,outperforms,all other models,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
all other models,except,byte - m LSTM,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
all other models,has,substantially,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
Results,see that,performance,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
performance,of,our SST - 5 model,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
our SST - 5 model,on par with,current state - of - the - art model,experiments,/content/training-data/natural_language_inference/10/triples/experiments.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/10/triples/model.txt
Model,use,Straight - Through ( ST ) Gumbel - Softmax estimator,model,/content/training-data/natural_language_inference/10/triples/model.txt
Straight - Through ( ST ) Gumbel - Softmax estimator,relaxes,discrete sampling operation,model,/content/training-data/natural_language_inference/10/triples/model.txt
discrete sampling operation,to be,continuous,model,/content/training-data/natural_language_inference/10/triples/model.txt
continuous,in,backward pass,model,/content/training-data/natural_language_inference/10/triples/model.txt
Straight - Through ( ST ) Gumbel - Softmax estimator,to sample,compositions,model,/content/training-data/natural_language_inference/10/triples/model.txt
compositions,in,training phase,model,/content/training-data/natural_language_inference/10/triples/model.txt
Model,Using,validity scores,model,/content/training-data/natural_language_inference/10/triples/model.txt
validity scores,computed by,composition query vector,model,/content/training-data/natural_language_inference/10/triples/model.txt
validity scores,has,our model,model,/content/training-data/natural_language_inference/10/triples/model.txt
our model,recursively selects,compositions,model,/content/training-data/natural_language_inference/10/triples/model.txt
compositions,until,only a single representation remains,model,/content/training-data/natural_language_inference/10/triples/model.txt
Model,has,Our Gumbel Tree - LSTM model,model,/content/training-data/natural_language_inference/10/triples/model.txt
Our Gumbel Tree - LSTM model,based on,tree - structured long short - term memory ( Tree - LSTM ) architecture,model,/content/training-data/natural_language_inference/10/triples/model.txt
Model,has,our model,model,/content/training-data/natural_language_inference/10/triples/model.txt
our model,introduces,composition query vector,model,/content/training-data/natural_language_inference/10/triples/model.txt
composition query vector,measures,validity,model,/content/training-data/natural_language_inference/10/triples/model.txt
validity,of,composition,model,/content/training-data/natural_language_inference/10/triples/model.txt
Model,propose,Gumbel Tree - LSTM,model,/content/training-data/natural_language_inference/10/triples/model.txt
Gumbel Tree - LSTM,is,novel RvNN architecture,model,/content/training-data/natural_language_inference/10/triples/model.txt
novel RvNN architecture,learns to compose,task - specific tree structures,model,/content/training-data/natural_language_inference/10/triples/model.txt
task - specific tree structures,without,explicit guidance,model,/content/training-data/natural_language_inference/10/triples/model.txt
novel RvNN architecture,not require,structured data,model,/content/training-data/natural_language_inference/10/triples/model.txt
Contribution,has research problem,Task - Specific Tree Structures,research-problem,/content/training-data/natural_language_inference/10/triples/research-problem.txt
Contribution,has research problem,task - specific tree structures only from plain text data,research-problem,/content/training-data/natural_language_inference/10/triples/research-problem.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
Ablation analysis,observe that,good sampling ratio,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
good sampling ratio,between,original and augmented data,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
original and augmented data,can further,boost,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
boost,has,model performance,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
original and augmented data,during,training,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
Ablation analysis,Making,training data,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
training data,has,twice as large,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
twice as large,by adding,En - Fr - En data only,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
twice as large,yields,increase,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
increase,in,F1,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
F1,by,0.5 percent,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
Ablation analysis,use of,convolutions,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
convolutions,in,encoders,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
convolutions,is,crucial,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
crucial,has,both F1 and EM,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
both F1 and EM,has,drop drastically,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
drop drastically,by,almost 3 percent,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
almost 3 percent,if,removed,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
Ablation analysis,has,ratio ( 3:1:1 ,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
ratio ( 3:1:1 ),yields,best performance,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
best performance,with,1.5/1.1 gain,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
1.5/1.1 gain,over,base model,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
base model,on,EM / F1,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
Ablation analysis,has,Self- attention,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
Self- attention,in,encoders,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
encoders,is,necessary component,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
necessary component,that contributes,1.4/1.3 gain of EM / F1,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
1.4/1.3 gain of EM / F1,to,ultimate performance,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
Ablation analysis,has,data augmentation,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
data augmentation,proves to be,helpful,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
helpful,in,further boosting performance,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
Ablation analysis,increase,sampling weight,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
sampling weight,of,augmented data,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
sampling weight,from,( 1:1:1 ,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
 1:1:1 ),to,( 1:2:1 ,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
sampling weight,has,EM / F1 performance,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
EM / F1 performance,has,drops,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
drops,by,0.5/0.3,ablation-analysis,/content/training-data/natural_language_inference/27/triples/ablation-analysis.txt
Contribution,has,Experiments,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
Experiments,ON,SQUAD,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
SQUAD,has,Experimental setup,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
Experimental setup,employ,two types,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
two types,of,standard regularizations,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
Experimental setup,use,learning rate warm - up scheme,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
learning rate warm - up scheme,with,inverse exponential,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
inverse exponential,has,increase,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
increase,in,first 1000 steps,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
increase,from,0.0,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
0.0,to,0.001,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
increase,then,maintain,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
maintain,has,constant learning rate,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
constant learning rate,for,remainder of training,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
Experimental setup,use,ADAM optimizer,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
ADAM optimizer,with,"? 1 = 0.8 , ? 2 = 0.999 , = 10 ?7",experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
Experimental setup,use,dropout,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
dropout,where,dropout rate,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
dropout rate,between,every two layers,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
every two layers,is,0.1,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
dropout,where,word and character dropout rates,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
word and character dropout rates,are,0.1 and 0.05,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
dropout,on,word,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
dropout,on,character embeddings,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
dropout,on,between layers,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
Experimental setup,use,L2 weight decay,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
L2 weight decay,with parameter,? = 3 10 ?7,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
L2 weight decay,on,all the trainable variables,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
Experimental setup,adopt,stochastic depth method ( layer dropout ,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
stochastic depth method ( layer dropout ),within,each embedding or model encoder layer,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
each embedding or model encoder layer,where,sublayer l,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
sublayer l,has,survival probability pl = 1 ? l L ( 1 ? p L ,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
survival probability pl = 1 ? l L ( 1 ? p L ),where,L,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
L,is,last layer,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
survival probability pl = 1 ? l L ( 1 ? p L ),where,p L,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
p L,=,0.9,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
Experimental setup,carry out,our experiments,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
our experiments,on,NVIDIA p 100 GPU,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
Experimental setup,implement,our model,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
our model,using,Tensorflow,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
our model,in,Python,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
Experimental setup,has,kernel sizes,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
kernel sizes,are,7 and 5,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
Experimental setup,has,Exponential moving average,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
Exponential moving average,applied on,all trainable variables,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
all trainable variables,with,decay rate 0.9999,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
Experimental setup,has,numbers of convolution layers,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
numbers of convolution layers,in,embedding and modeling encoder,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
embedding and modeling encoder,are,4 and 2,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
Experimental setup,has,batch size,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
batch size,is,32,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
Experimental setup,has,block numbers,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
block numbers,for,encoders,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
encoders,are,1 and 7,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
Experimental setup,has,hidden size and the convolution filter number,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
hidden size and the convolution filter number,are,128,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
Experimental setup,has,training steps,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
training steps,are,250 K,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
250 K,for,data augmentation 2,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
training steps,are,150 K,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
150 K,for,original data,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
training steps,are,340 K,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
340 K,for,data augmentation 3,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
SQUAD,has,Results,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
Results,has,accuracy ( EM / F1 ) performance,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
accuracy ( EM / F1 ) performance,of,our model,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
accuracy ( EM / F1 ) performance,is,on par,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
on par,with,state - of - the - art models,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
Results,has,our model,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
our model,trained on,original dataset,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
our model,has,outperforms,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
outperforms,has,all the documented results in the literature,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
outperforms,in terms of,EM and F1 scores,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
Results,trained with,augmented data,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
augmented data,with,proper sampling scheme,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
augmented data,has,our model,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
our model,can get,significant gain 1.5/1.1,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
significant gain 1.5/1.1,on,EM / F1,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
Results,on,official test set,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
official test set,is,76.2/84.6,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
76.2/84.6,which,significantly outperforms,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
significantly outperforms,has,best documented result 73.2/81.8,experiments,/content/training-data/natural_language_inference/27/triples/experiments.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/27/triples/model.txt
Model,call,architecture QANet,model,/content/training-data/natural_language_inference/27/triples/model.txt
Model,to make,machine comprehension,model,/content/training-data/natural_language_inference/27/triples/model.txt
machine comprehension,has,fast,model,/content/training-data/natural_language_inference/27/triples/model.txt
fast,remove,recurrent nature of these models,model,/content/training-data/natural_language_inference/27/triples/model.txt
Model,learn,interactions,model,/content/training-data/natural_language_inference/27/triples/model.txt
interactions,by,standard attentions,model,/content/training-data/natural_language_inference/27/triples/model.txt
interactions,between,context and question,model,/content/training-data/natural_language_inference/27/triples/model.txt
Model,use,convolutions and self - attentions,model,/content/training-data/natural_language_inference/27/triples/model.txt
convolutions and self - attentions,as,building blocks,model,/content/training-data/natural_language_inference/27/triples/model.txt
building blocks,separately encodes,query and context,model,/content/training-data/natural_language_inference/27/triples/model.txt
building blocks,of,encoders,model,/content/training-data/natural_language_inference/27/triples/model.txt
Model,has,resulting representation,model,/content/training-data/natural_language_inference/27/triples/model.txt
resulting representation,is,encoded again,model,/content/training-data/natural_language_inference/27/triples/model.txt
encoded again,with,our recurrency - free encoder,model,/content/training-data/natural_language_inference/27/triples/model.txt
encoded again,before,decoding,model,/content/training-data/natural_language_inference/27/triples/model.txt
decoding,to,probability,model,/content/training-data/natural_language_inference/27/triples/model.txt
probability,of,each position,model,/content/training-data/natural_language_inference/27/triples/model.txt
each position,being,start or end,model,/content/training-data/natural_language_inference/27/triples/model.txt
start or end,of,answer span,model,/content/training-data/natural_language_inference/27/triples/model.txt
Contribution,has research problem,READING COMPRE - HENSION,research-problem,/content/training-data/natural_language_inference/27/triples/research-problem.txt
Contribution,has research problem,machine reading and question answering ( Q&A ,research-problem,/content/training-data/natural_language_inference/27/triples/research-problem.txt
Contribution,has research problem,machine reading comprehension,research-problem,/content/training-data/natural_language_inference/27/triples/research-problem.txt
Contribution,has research problem,automated question answering,research-problem,/content/training-data/natural_language_inference/27/triples/research-problem.txt
Contribution,has research problem,machine comprehension,research-problem,/content/training-data/natural_language_inference/27/triples/research-problem.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/74/triples/ablation-analysis.txt
Ablation analysis,conduct,an ablation experiment,ablation-analysis,/content/training-data/natural_language_inference/74/triples/ablation-analysis.txt
an ablation experiment,on,SNLI development dataset,ablation-analysis,/content/training-data/natural_language_inference/74/triples/ablation-analysis.txt
SNLI development dataset,ablate,reinforcement learning part,ablation-analysis,/content/training-data/natural_language_inference/74/triples/ablation-analysis.txt
reinforcement learning part,has,result,ablation-analysis,/content/training-data/natural_language_inference/74/triples/ablation-analysis.txt
result,has,drops about 0.5 %,ablation-analysis,/content/training-data/natural_language_inference/74/triples/ablation-analysis.txt
SNLI development dataset,has,result,ablation-analysis,/content/training-data/natural_language_inference/74/triples/ablation-analysis.txt
result,only using,sentence embedding,ablation-analysis,/content/training-data/natural_language_inference/74/triples/ablation-analysis.txt
sentence embedding,from,discourse markers,ablation-analysis,/content/training-data/natural_language_inference/74/triples/ablation-analysis.txt
sentence embedding,is,not ideal,ablation-analysis,/content/training-data/natural_language_inference/74/triples/ablation-analysis.txt
not ideal,in,large - scale datasets,ablation-analysis,/content/training-data/natural_language_inference/74/triples/ablation-analysis.txt
sentence embedding,to predict,answer,ablation-analysis,/content/training-data/natural_language_inference/74/triples/ablation-analysis.txt
result,is,not satisfactory,ablation-analysis,/content/training-data/natural_language_inference/74/triples/ablation-analysis.txt
SNLI development dataset,has,performance,ablation-analysis,/content/training-data/natural_language_inference/74/triples/ablation-analysis.txt
performance,has,drops a lot,ablation-analysis,/content/training-data/natural_language_inference/74/triples/ablation-analysis.txt
drops a lot,remove,character - level embedding,ablation-analysis,/content/training-data/natural_language_inference/74/triples/ablation-analysis.txt
drops a lot,remove,POS and NER features,ablation-analysis,/content/training-data/natural_language_inference/74/triples/ablation-analysis.txt
SNLI development dataset,has,exact match feature,ablation-analysis,/content/training-data/natural_language_inference/74/triples/ablation-analysis.txt
exact match feature,demonstrates,effectiveness,ablation-analysis,/content/training-data/natural_language_inference/74/triples/ablation-analysis.txt
effectiveness,in,ablation result,ablation-analysis,/content/training-data/natural_language_inference/74/triples/ablation-analysis.txt
SNLI development dataset,remove,sentence encoder model,ablation-analysis,/content/training-data/natural_language_inference/74/triples/ablation-analysis.txt
sentence encoder model,observe that,performance,ablation-analysis,/content/training-data/natural_language_inference/74/triples/ablation-analysis.txt
performance,has,drops significantly,ablation-analysis,/content/training-data/natural_language_inference/74/triples/ablation-analysis.txt
drops significantly,to,87 . 24 %,ablation-analysis,/content/training-data/natural_language_inference/74/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/74/triples/model.txt
Model,employ,reinforcement learning,model,/content/training-data/natural_language_inference/74/triples/model.txt
reinforcement learning,with,reward,model,/content/training-data/natural_language_inference/74/triples/model.txt
reward,defined by,uniformity extent,model,/content/training-data/natural_language_inference/74/triples/model.txt
uniformity extent,of,original labels,model,/content/training-data/natural_language_inference/74/triples/model.txt
uniformity extent,to train,model,model,/content/training-data/natural_language_inference/74/triples/model.txt
Model,propose,Discourse Marker Augmented Network,model,/content/training-data/natural_language_inference/74/triples/model.txt
Discourse Marker Augmented Network,for,natural language inference,model,/content/training-data/natural_language_inference/74/triples/model.txt
Discourse Marker Augmented Network,where,transfer,model,/content/training-data/natural_language_inference/74/triples/model.txt
transfer,has,knowledge,model,/content/training-data/natural_language_inference/74/triples/model.txt
knowledge,from,existing supervised task,model,/content/training-data/natural_language_inference/74/triples/model.txt
existing supervised task,name,Discourse Marker Prediction ( DMP ,model,/content/training-data/natural_language_inference/74/triples/model.txt
existing supervised task,to,integrated NLI model,model,/content/training-data/natural_language_inference/74/triples/model.txt
Model,propose,sentence encoder model,model,/content/training-data/natural_language_inference/74/triples/model.txt
sentence encoder model,that learns,representations,model,/content/training-data/natural_language_inference/74/triples/model.txt
representations,of,sentences,model,/content/training-data/natural_language_inference/74/triples/model.txt
sentences,from,DMP task,model,/content/training-data/natural_language_inference/74/triples/model.txt
representations,inject,encoder,model,/content/training-data/natural_language_inference/74/triples/model.txt
encoder,to,NLI network,model,/content/training-data/natural_language_inference/74/triples/model.txt
Contribution,has research problem,Natural Language Inference,research-problem,/content/training-data/natural_language_inference/74/triples/research-problem.txt
Contribution,has research problem,Natural Language Inference ( NLI ,research-problem,/content/training-data/natural_language_inference/74/triples/research-problem.txt
Contribution,has research problem,Recognizing Textual Entailment ( RTE ,research-problem,/content/training-data/natural_language_inference/74/triples/research-problem.txt
Contribution,has research problem,NLI,research-problem,/content/training-data/natural_language_inference/74/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
Experimental setup,set,our batch size,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
our batch size,as,36,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
Experimental setup,set,initial learning rate,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
initial learning rate,as,0.6,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
Experimental setup,set,hidden size,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
hidden size,as,300,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
300,for,all the LSTM layers,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
Experimental setup,apply,Tensorflow r 1.3,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
Tensorflow r 1.3,as,our neural network framework,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
Experimental setup,apply,decay rate,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
decay rate,as,0.97,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
0.97,for,every 5000 step,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
Experimental setup,apply,dropout,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
dropout,with,initial ratio,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
initial ratio,of,0.9,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
dropout,between,layers,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
Experimental setup,use,Stanford CoreNLP toolkit,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
Stanford CoreNLP toolkit,to tokenize,words,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
Stanford CoreNLP toolkit,generate,POS and NER tags,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
Experimental setup,use,AdaDelta,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
AdaDelta,with,? as 0.95 and as 1 e - 8,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
AdaDelta,for,optimization,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
Experimental setup,For,DMP task,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
DMP task,use,stochastic gradient descent,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
stochastic gradient descent,with,initial learning rate,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
initial learning rate,as,0.1,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
DMP task,has,number of epochs,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
number of epochs,set to,10,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
DMP task,has,anneal,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
anneal,by,half,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
anneal,has,each time,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
each time,has,validation accuracy,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
validation accuracy,is,lower,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
lower,than,previous epoch,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
DMP task,has,feedforward dropout rate,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
feedforward dropout rate,is,0.2,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
Experimental setup,has,word embeddings,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
word embeddings,initialized by,300d Glove,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
Experimental setup,has,dimensions,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
dimensions,of,POS and NER embeddings,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
POS and NER embeddings,are,30 and 10,experimental-setup,/content/training-data/natural_language_inference/74/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/74/triples/results.txt
Results,has,performance,results,/content/training-data/natural_language_inference/74/triples/results.txt
performance,of,most of the integrated methods,results,/content/training-data/natural_language_inference/74/triples/results.txt
most of the integrated methods,are,better,results,/content/training-data/natural_language_inference/74/triples/results.txt
better,than,sentence encoding based models,results,/content/training-data/natural_language_inference/74/triples/results.txt
performance,of,our model,results,/content/training-data/natural_language_inference/74/triples/results.txt
our model,achieves,89.6 %,results,/content/training-data/natural_language_inference/74/triples/results.txt
89.6 %,on,SNLI,results,/content/training-data/natural_language_inference/74/triples/results.txt
our model,achieves,80.3 %,results,/content/training-data/natural_language_inference/74/triples/results.txt
80.3 %,on,matched MultiNLI,results,/content/training-data/natural_language_inference/74/triples/results.txt
our model,achieves,79.4 %,results,/content/training-data/natural_language_inference/74/triples/results.txt
79.4 %,on,mismatched MultiNLI,results,/content/training-data/natural_language_inference/74/triples/results.txt
our model,achieves,all state - of - the - art results,results,/content/training-data/natural_language_inference/74/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/97/triples/ablation-analysis.txt
Ablation analysis,Ablating,sentential component,ablation-analysis,/content/training-data/natural_language_inference/97/triples/ablation-analysis.txt
sentential component,made,most significant difference,ablation-analysis,/content/training-data/natural_language_inference/97/triples/ablation-analysis.txt
sentential component,reducing,performance,ablation-analysis,/content/training-data/natural_language_inference/97/triples/ablation-analysis.txt
performance,by,more than 5 %,ablation-analysis,/content/training-data/natural_language_inference/97/triples/ablation-analysis.txt
Ablation analysis,has,sequential sliding window,ablation-analysis,/content/training-data/natural_language_inference/97/triples/ablation-analysis.txt
sequential sliding window,makes,3 % contribution,ablation-analysis,/content/training-data/natural_language_inference/97/triples/ablation-analysis.txt
Ablation analysis,has,exogenous word weights,ablation-analysis,/content/training-data/natural_language_inference/97/triples/ablation-analysis.txt
exogenous word weights,make,significant contribution,ablation-analysis,/content/training-data/natural_language_inference/97/triples/ablation-analysis.txt
significant contribution,of,almost 5 %,ablation-analysis,/content/training-data/natural_language_inference/97/triples/ablation-analysis.txt
Ablation analysis,has,dependency - based sliding window,ablation-analysis,/content/training-data/natural_language_inference/97/triples/ablation-analysis.txt
dependency - based sliding window,makes,minor contribution,ablation-analysis,/content/training-data/natural_language_inference/97/triples/ablation-analysis.txt
Ablation analysis,has,Simple word - by - word matching,ablation-analysis,/content/training-data/natural_language_inference/97/triples/ablation-analysis.txt
Simple word - by - word matching,has,useful,ablation-analysis,/content/training-data/natural_language_inference/97/triples/ablation-analysis.txt
useful,on,MCTest,ablation-analysis,/content/training-data/natural_language_inference/97/triples/ablation-analysis.txt
Ablation analysis,has,n-gram functionality,ablation-analysis,/content/training-data/natural_language_inference/97/triples/ablation-analysis.txt
n-gram functionality,contributing,almost 5 % accuracy improvement,ablation-analysis,/content/training-data/natural_language_inference/97/triples/ablation-analysis.txt
n-gram functionality,is,important,ablation-analysis,/content/training-data/natural_language_inference/97/triples/ablation-analysis.txt
Ablation analysis,has,top N function,ablation-analysis,/content/training-data/natural_language_inference/97/triples/ablation-analysis.txt
top N function,contributes,very little,ablation-analysis,/content/training-data/natural_language_inference/97/triples/ablation-analysis.txt
very little,to,over all performance,ablation-analysis,/content/training-data/natural_language_inference/97/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/97/triples/model.txt
Model,learns to,comprehend,model,/content/training-data/natural_language_inference/97/triples/model.txt
comprehend,at,high level,model,/content/training-data/natural_language_inference/97/triples/model.txt
comprehend,even when,data,model,/content/training-data/natural_language_inference/97/triples/model.txt
data,is,sparse,model,/content/training-data/natural_language_inference/97/triples/model.txt
Model,has,word - by - word perspective,model,/content/training-data/natural_language_inference/97/triples/model.txt
word - by - word perspective,use,sliding window,model,/content/training-data/natural_language_inference/97/triples/model.txt
sliding window,acting on,subsentential scale,model,/content/training-data/natural_language_inference/97/triples/model.txt
word - by - word perspective,focuses on,similarity matches,model,/content/training-data/natural_language_inference/97/triples/model.txt
similarity matches,at,various scales,model,/content/training-data/natural_language_inference/97/triples/model.txt
similarity matches,between,individual words,model,/content/training-data/natural_language_inference/97/triples/model.txt
individual words,from,hypothesis and text,model,/content/training-data/natural_language_inference/97/triples/model.txt
word - by - word perspective,consider,matches,model,/content/training-data/natural_language_inference/97/triples/model.txt
matches,over,complete sentences,model,/content/training-data/natural_language_inference/97/triples/model.txt
Model,has,semantic perspective,model,/content/training-data/natural_language_inference/97/triples/model.txt
semantic perspective,compares,hypothesis,model,/content/training-data/natural_language_inference/97/triples/model.txt
hypothesis,to,sentences,model,/content/training-data/natural_language_inference/97/triples/model.txt
sentences,in,text,model,/content/training-data/natural_language_inference/97/triples/model.txt
sentences,viewed as,"single , self - contained thoughts",model,/content/training-data/natural_language_inference/97/triples/model.txt
semantic perspective,represented using,sum and transformation,model,/content/training-data/natural_language_inference/97/triples/model.txt
sum and transformation,of,word embedding vectors,model,/content/training-data/natural_language_inference/97/triples/model.txt
Model,present,parallel - hierarchical approach,model,/content/training-data/natural_language_inference/97/triples/model.txt
parallel - hierarchical approach,designed to,work well,model,/content/training-data/natural_language_inference/97/triples/model.txt
work well,in,data - limited setting,model,/content/training-data/natural_language_inference/97/triples/model.txt
parallel - hierarchical approach,to,machine comprehension,model,/content/training-data/natural_language_inference/97/triples/model.txt
Model,compares,question and answer candidates,model,/content/training-data/natural_language_inference/97/triples/model.txt
question and answer candidates,using,several distinct perspectives,model,/content/training-data/natural_language_inference/97/triples/model.txt
question and answer candidates,to,text,model,/content/training-data/natural_language_inference/97/triples/model.txt
Contribution,has research problem,Machine Comprehension,research-problem,/content/training-data/natural_language_inference/97/triples/research-problem.txt
Contribution,has research problem,Understanding unstructured text,research-problem,/content/training-data/natural_language_inference/97/triples/research-problem.txt
Contribution,has research problem,Machine comprehension ( MC ,research-problem,/content/training-data/natural_language_inference/97/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/97/triples/experimental-setup.txt
Experimental setup,For,word vectors,experimental-setup,/content/training-data/natural_language_inference/97/triples/experimental-setup.txt
word vectors,are,300 - dimensional,experimental-setup,/content/training-data/natural_language_inference/97/triples/experimental-setup.txt
word vectors,use,Google 's publicly available embeddings,experimental-setup,/content/training-data/natural_language_inference/97/triples/experimental-setup.txt
Google 's publicly available embeddings,trained with,word2vec,experimental-setup,/content/training-data/natural_language_inference/97/triples/experimental-setup.txt
word2vec,on,100 - billion - word News corpus,experimental-setup,/content/training-data/natural_language_inference/97/triples/experimental-setup.txt
word vectors,kept,fixed,experimental-setup,/content/training-data/natural_language_inference/97/triples/experimental-setup.txt
fixed,throughout,training,experimental-setup,/content/training-data/natural_language_inference/97/triples/experimental-setup.txt
Experimental setup,used,0.5,experimental-setup,/content/training-data/natural_language_inference/97/triples/experimental-setup.txt
0.5,as,dropout probability,experimental-setup,/content/training-data/natural_language_inference/97/triples/experimental-setup.txt
Experimental setup,used,learning rate,experimental-setup,/content/training-data/natural_language_inference/97/triples/experimental-setup.txt
learning rate,of,0.003,experimental-setup,/content/training-data/natural_language_inference/97/triples/experimental-setup.txt
Experimental setup,used,Adam optimizer,experimental-setup,/content/training-data/natural_language_inference/97/triples/experimental-setup.txt
Adam optimizer,with,standard settings,experimental-setup,/content/training-data/natural_language_inference/97/triples/experimental-setup.txt
Experimental setup,has,Dropout,experimental-setup,/content/training-data/natural_language_inference/97/triples/experimental-setup.txt
Dropout,occurs after,all neural - network transformations,experimental-setup,/content/training-data/natural_language_inference/97/triples/experimental-setup.txt
all neural - network transformations,allowed to,change,experimental-setup,/content/training-data/natural_language_inference/97/triples/experimental-setup.txt
change,with,training,experimental-setup,/content/training-data/natural_language_inference/97/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/97/triples/results.txt
Results,has,method,results,/content/training-data/natural_language_inference/97/triples/results.txt
method,achieves,best over all result,results,/content/training-data/natural_language_inference/97/triples/results.txt
best over all result,on,MCTest - 160,results,/content/training-data/natural_language_inference/97/triples/results.txt
Results,has,our model,results,/content/training-data/natural_language_inference/97/triples/results.txt
our model,has,outperforming,results,/content/training-data/natural_language_inference/97/triples/results.txt
outperforming,by,large margin,results,/content/training-data/natural_language_inference/97/triples/results.txt
large margin,across,board ( > 15 % ,results,/content/training-data/natural_language_inference/97/triples/results.txt
outperforming,has,alternatives,results,/content/training-data/natural_language_inference/97/triples/results.txt
Results,On,MCTest - 500,results,/content/training-data/natural_language_inference/97/triples/results.txt
MCTest - 500,has,Parallel Hierarchical model,results,/content/training-data/natural_language_inference/97/triples/results.txt
Parallel Hierarchical model,has,slightly outperforms,results,/content/training-data/natural_language_inference/97/triples/results.txt
slightly outperforms,has,latter two,results,/content/training-data/natural_language_inference/97/triples/results.txt
latter two,on,multi questions ( ? 0.3 % ,results,/content/training-data/natural_language_inference/97/triples/results.txt
Parallel Hierarchical model,has,significantly outperforms,results,/content/training-data/natural_language_inference/97/triples/results.txt
significantly outperforms,has,these methods,results,/content/training-data/natural_language_inference/97/triples/results.txt
these methods,on,single questions ( > 2 % ,results,/content/training-data/natural_language_inference/97/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/73/triples/ablation-analysis.txt
Ablation analysis,In terms of,prediction quality,ablation-analysis,/content/training-data/natural_language_inference/73/triples/ablation-analysis.txt
prediction quality,using,hard attention and soft self - attention modules,ablation-analysis,/content/training-data/natural_language_inference/73/triples/ablation-analysis.txt
hard attention and soft self - attention modules,improve,accuracy,ablation-analysis,/content/training-data/natural_language_inference/73/triples/ablation-analysis.txt
accuracy,by,0.3 % and 2.9 %,ablation-analysis,/content/training-data/natural_language_inference/73/triples/ablation-analysis.txt
prediction quality,using,separate RSS modules,ablation-analysis,/content/training-data/natural_language_inference/73/triples/ablation-analysis.txt
separate RSS modules,improves,accuracy,ablation-analysis,/content/training-data/natural_language_inference/73/triples/ablation-analysis.txt
accuracy,by,0.5 %,ablation-analysis,/content/training-data/natural_language_inference/73/triples/ablation-analysis.txt
separate RSS modules,to select,head and dependent tokens,ablation-analysis,/content/training-data/natural_language_inference/73/triples/ablation-analysis.txt
prediction quality,has,unselected head tokens,ablation-analysis,/content/training-data/natural_language_inference/73/triples/ablation-analysis.txt
unselected head tokens,contribute to,prediction,ablation-analysis,/content/training-data/natural_language_inference/73/triples/ablation-analysis.txt
unselected head tokens,bringing,0.2 % improvement,ablation-analysis,/content/training-data/natural_language_inference/73/triples/ablation-analysis.txt
Contribution,has,Experiments,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
Experiments,has,Tasks,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
Tasks,has,Natural Language Inference,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
Natural Language Inference,has,Results,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
Results,Compared to,convolutional models,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
convolutional models,has,ReSAN,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
ReSAN,has,significantly outperforms,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
significantly outperforms,by,3.1 % and 2.4 %,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
convolutional models,i.e.,Multiwindow CNN,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
convolutional models,i.e.,Hierarchical CNN,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
Results,Compared to,recurrent models,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
recurrent models,due to,parallelizable computations,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
parallelizable computations,has,ReSAN,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
ReSAN,shows,better prediction quality,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
ReSAN,shows,more compelling efficiency,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
recurrent models,e.g.,Bi - LSTM,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
recurrent models,e.g.,Bi - GRU,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
Results,Compared to,attention - based models,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
attention - based models,name,multi-head attention,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
attention - based models,name,DiSAN,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
attention - based models,has,ReSAN,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
ReSAN,with,better test performance,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
ReSAN,with,less time cost,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
ReSAN,uses,similar number of parameters,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
Results,has,ReSAN,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
ReSAN,has,outperforms,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
outperforms,has,300D SPINN - PI encoders,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
300D SPINN - PI encoders,by,3.1 %,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
Results,compared to,last best models,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
last best models,has,ReSAN,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
ReSAN,with,better performance,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
ReSAN,uses,far fewer parameters,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
last best models,i.e.,600D Gumbel TreeLSTM encoders,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
last best models,i.e.,600D Residual stacked encoders,experiments,/content/training-data/natural_language_inference/73/triples/experiments.txt
Contribution,Code,https://github.com/ taoshen58/DiSAN /tree/master/ReSAN,code,/content/training-data/natural_language_inference/73/triples/code.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/73/triples/model.txt
Model,In,ReSA,model,/content/training-data/natural_language_inference/73/triples/model.txt
ReSA,has,two parameter - untied RSS,model,/content/training-data/natural_language_inference/73/triples/model.txt
two parameter - untied RSS,applied to,two copies,model,/content/training-data/natural_language_inference/73/triples/model.txt
two copies,of,input sequence,model,/content/training-data/natural_language_inference/73/triples/model.txt
Model,build,sentence - encoding model,model,/content/training-data/natural_language_inference/73/triples/model.txt
sentence - encoding model,based on,ReSA,model,/content/training-data/natural_language_inference/73/triples/model.txt
ReSA,without,any CNN / RNN structure,model,/content/training-data/natural_language_inference/73/triples/model.txt
sentence - encoding model,name,reinforced self - attention network ( ReSAN ,model,/content/training-data/natural_language_inference/73/triples/model.txt
Model,develop,reinforced self - attention ( ReSA ,model,/content/training-data/natural_language_inference/73/triples/model.txt
reinforced self - attention ( ReSA ),combines,RSS,model,/content/training-data/natural_language_inference/73/triples/model.txt
RSS,with,soft self - attention,model,/content/training-data/natural_language_inference/73/triples/model.txt
Model,has,Re SA,model,/content/training-data/natural_language_inference/73/triples/model.txt
Re SA,models,sparse dependencies,model,/content/training-data/natural_language_inference/73/triples/model.txt
sparse dependencies,between,head and dependent tokens,model,/content/training-data/natural_language_inference/73/triples/model.txt
head and dependent tokens,selected by,two RSS modules,model,/content/training-data/natural_language_inference/73/triples/model.txt
Model,propose,novel hard attention mechanism,model,/content/training-data/natural_language_inference/73/triples/model.txt
novel hard attention mechanism,selects,tokens,model,/content/training-data/natural_language_inference/73/triples/model.txt
tokens,from,input sequence,model,/content/training-data/natural_language_inference/73/triples/model.txt
input sequence,in,parallel,model,/content/training-data/natural_language_inference/73/triples/model.txt
novel hard attention mechanism,called,reinforced sequence sampling ( RSS ,model,/content/training-data/natural_language_inference/73/triples/model.txt
novel hard attention mechanism,is,highly parallelizable,model,/content/training-data/natural_language_inference/73/triples/model.txt
highly parallelizable,without,any recurrent structure,model,/content/training-data/natural_language_inference/73/triples/model.txt
Contribution,has research problem,attention mechanisms,research-problem,/content/training-data/natural_language_inference/73/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/73/triples/experimental-setup.txt
Experimental setup,use,Adadelta,experimental-setup,/content/training-data/natural_language_inference/73/triples/experimental-setup.txt
Adadelta,performs,more stable,experimental-setup,/content/training-data/natural_language_inference/73/triples/experimental-setup.txt
more stable,than,Adam,experimental-setup,/content/training-data/natural_language_inference/73/triples/experimental-setup.txt
more stable,on,ReSAN,experimental-setup,/content/training-data/natural_language_inference/73/triples/experimental-setup.txt
Adadelta,as,optimizer,experimental-setup,/content/training-data/natural_language_inference/73/triples/experimental-setup.txt
Experimental setup,use,300D Glo Ve 6B pre-trained vectors,experimental-setup,/content/training-data/natural_language_inference/73/triples/experimental-setup.txt
Experimental setup,has,experiments,experimental-setup,/content/training-data/natural_language_inference/73/triples/experimental-setup.txt
experiments,conducted in,Python,experimental-setup,/content/training-data/natural_language_inference/73/triples/experimental-setup.txt
Python,with,Tensorflow,experimental-setup,/content/training-data/natural_language_inference/73/triples/experimental-setup.txt
experiments,run on,Nvidia GTX 1080 Ti,experimental-setup,/content/training-data/natural_language_inference/73/triples/experimental-setup.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/24/triples/model.txt
Model,call,stochastic answer network ( SAN ,model,/content/training-data/natural_language_inference/24/triples/model.txt
Model,derive,alternative multi-step reasoning neural network,model,/content/training-data/natural_language_inference/24/triples/model.txt
alternative multi-step reasoning neural network,for,MRC,model,/content/training-data/natural_language_inference/24/triples/model.txt
Model,During,training,model,/content/training-data/natural_language_inference/24/triples/model.txt
training,fix,number of reasoning steps,model,/content/training-data/natural_language_inference/24/triples/model.txt
training,perform,stochastic dropout,model,/content/training-data/natural_language_inference/24/triples/model.txt
stochastic dropout,on,answer module ( final layer predictions ,model,/content/training-data/natural_language_inference/24/triples/model.txt
Model,During,decoding,model,/content/training-data/natural_language_inference/24/triples/model.txt
decoding,generate,answers,model,/content/training-data/natural_language_inference/24/triples/model.txt
answers,based on,average,model,/content/training-data/natural_language_inference/24/triples/model.txt
average,of,predictions,model,/content/training-data/natural_language_inference/24/triples/model.txt
predictions,in,all steps,model,/content/training-data/natural_language_inference/24/triples/model.txt
all steps,rather than,final step,model,/content/training-data/natural_language_inference/24/triples/model.txt
Contribution,has research problem,Machine Reading Comprehension,research-problem,/content/training-data/natural_language_inference/24/triples/research-problem.txt
Contribution,has research problem,Machine reading comprehension ( MRC ,research-problem,/content/training-data/natural_language_inference/24/triples/research-problem.txt
Contribution,has research problem,MRC,research-problem,/content/training-data/natural_language_inference/24/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/24/triples/experimental-setup.txt
Experimental setup,To prevent,degenerate output,experimental-setup,/content/training-data/natural_language_inference/24/triples/experimental-setup.txt
degenerate output,ensure that,at least one step,experimental-setup,/content/training-data/natural_language_inference/24/triples/experimental-setup.txt
at least one step,in,answer module,experimental-setup,/content/training-data/natural_language_inference/24/triples/experimental-setup.txt
at least one step,is,active,experimental-setup,/content/training-data/natural_language_inference/24/triples/experimental-setup.txt
active,during,training,experimental-setup,/content/training-data/natural_language_inference/24/triples/experimental-setup.txt
Experimental setup,set,dropout rate,experimental-setup,/content/training-data/natural_language_inference/24/triples/experimental-setup.txt
dropout rate,to,0.4,experimental-setup,/content/training-data/natural_language_inference/24/triples/experimental-setup.txt
0.4,for,all the hidden units,experimental-setup,/content/training-data/natural_language_inference/24/triples/experimental-setup.txt
all the hidden units,of,LSTM,experimental-setup,/content/training-data/natural_language_inference/24/triples/experimental-setup.txt
all the hidden units,of,answer module output layer,experimental-setup,/content/training-data/natural_language_inference/24/triples/experimental-setup.txt
Experimental setup,use,2 - layer BiLSTM,experimental-setup,/content/training-data/natural_language_inference/24/triples/experimental-setup.txt
2 - layer BiLSTM,with,d = 128 hidden units,experimental-setup,/content/training-data/natural_language_inference/24/triples/experimental-setup.txt
d = 128 hidden units,for,both passage and question encoding,experimental-setup,/content/training-data/natural_language_inference/24/triples/experimental-setup.txt
Experimental setup,has,mini-batch size,experimental-setup,/content/training-data/natural_language_inference/24/triples/experimental-setup.txt
mini-batch size,set to,32,experimental-setup,/content/training-data/natural_language_inference/24/triples/experimental-setup.txt
Experimental setup,has,learning rate,experimental-setup,/content/training-data/natural_language_inference/24/triples/experimental-setup.txt
learning rate,set to,0.002,experimental-setup,/content/training-data/natural_language_inference/24/triples/experimental-setup.txt
0.002,at,first,experimental-setup,/content/training-data/natural_language_inference/24/triples/experimental-setup.txt
0.002,has,decreased,experimental-setup,/content/training-data/natural_language_inference/24/triples/experimental-setup.txt
decreased,by,half,experimental-setup,/content/training-data/natural_language_inference/24/triples/experimental-setup.txt
half,after,every 10 epochs,experimental-setup,/content/training-data/natural_language_inference/24/triples/experimental-setup.txt
Experimental setup,has,spaCy tool,experimental-setup,/content/training-data/natural_language_inference/24/triples/experimental-setup.txt
spaCy tool,used to,generate,experimental-setup,/content/training-data/natural_language_inference/24/triples/experimental-setup.txt
generate,has,lemma,experimental-setup,/content/training-data/natural_language_inference/24/triples/experimental-setup.txt
generate,has,part - of - speech,experimental-setup,/content/training-data/natural_language_inference/24/triples/experimental-setup.txt
generate,has,named entity tags,experimental-setup,/content/training-data/natural_language_inference/24/triples/experimental-setup.txt
spaCy tool,used to,tokenize,experimental-setup,/content/training-data/natural_language_inference/24/triples/experimental-setup.txt
tokenize,both,passages and questions,experimental-setup,/content/training-data/natural_language_inference/24/triples/experimental-setup.txt
Experimental setup,has,Adamax,experimental-setup,/content/training-data/natural_language_inference/24/triples/experimental-setup.txt
Adamax,used as,our optimizer,experimental-setup,/content/training-data/natural_language_inference/24/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/24/triples/results.txt
Results,has,SAN,results,/content/training-data/natural_language_inference/24/triples/results.txt
SAN,has,outperforms,results,/content/training-data/natural_language_inference/24/triples/results.txt
outperforms,other,models,results,/content/training-data/natural_language_inference/24/triples/results.txt
models,in terms of,K- best oracle scores,results,/content/training-data/natural_language_inference/24/triples/results.txt
outperforms,has,5 - step memory net,results,/content/training-data/natural_language_inference/24/triples/results.txt
5 - step memory net,with,averaging,results,/content/training-data/natural_language_inference/24/triples/results.txt
Results,has,Standard 1 - step model,results,/content/training-data/natural_language_inference/24/triples/results.txt
Standard 1 - step model,achieves,75.139 EM,results,/content/training-data/natural_language_inference/24/triples/results.txt
Results,has,dynamic steps ( via ReasoNet ,results,/content/training-data/natural_language_inference/24/triples/results.txt
dynamic steps ( via ReasoNet ),achieves,only 75.355 EM,results,/content/training-data/natural_language_inference/24/triples/results.txt
Results,see that,SAN,results,/content/training-data/natural_language_inference/24/triples/results.txt
SAN,is,very competitive,results,/content/training-data/natural_language_inference/24/triples/results.txt
very competitive,in,both single and ensemble settings,results,/content/training-data/natural_language_inference/24/triples/results.txt
very competitive,ranked,second,results,/content/training-data/natural_language_inference/24/triples/results.txt
Results,observe,SAN,results,/content/training-data/natural_language_inference/24/triples/results.txt
SAN,achieves,76.235 EM,results,/content/training-data/natural_language_inference/24/triples/results.txt
SAN,achieves,84.056 F1,results,/content/training-data/natural_language_inference/24/triples/results.txt
SAN,has,outperforming,results,/content/training-data/natural_language_inference/24/triples/results.txt
outperforming,has,all other models,results,/content/training-data/natural_language_inference/24/triples/results.txt
Contribution,has,Experiments,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
Experiments,has,Tasks,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
Tasks,has,QUESTION ANSWERING,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
QUESTION ANSWERING,has,Results,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
Results,find that,RUM,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
RUM,achieves,competitive result,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
competitive result,with,those of MemN2N,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
those of MemN2N,has,attention mechanism,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
RUM,has,outperforms significantly,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
outperforms significantly,has,LSTM and GORU,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
QUESTION ANSWERING,has,Baselines,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
Baselines,name,simple LSTM,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
Baselines,name,End - to - end Memory Network,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
Baselines,name,GORU,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
Tasks,has,COPYING MEMORY TASK,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
COPYING MEMORY TASK,has,RUM,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
RUM,solves,task,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
task,has,completely,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
RUM,utilizes,different representation of memory,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
different representation of memory,that,outperforms,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
outperforms,those of,LSTM and GRU,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
Tasks,has,CHARACTER LEVEL LANGUAGE MODELING,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
CHARACTER LEVEL LANGUAGE MODELING,has,PENN TREEBANK CORPUS DATA SET,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
PENN TREEBANK CORPUS DATA SET,has,Results,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
Results,has,FS - RUM - 2,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
FS - RUM - 2,has,generalizes better,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
generalizes better,than,other gated models,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
other gated models,such as,GRU and LSTM,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
Tasks,has,ASSOCIATIVE RECALL TASK,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
ASSOCIATIVE RECALL TASK,has,Hyperparameters,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
Hyperparameters,use,batch size,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
batch size,has,128,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
Hyperparameters,have,same hidden state N h = 50,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
same hidden state N h = 50,for,different lengths T,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
Hyperparameters,has,optimizer,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
optimizer,is,RMSProp,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
RMSProp,with,learning rate,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
learning rate,of,0.001,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
ASSOCIATIVE RECALL TASK,has,Results,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
Results,find that,LSTM,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
LSTM,has,fails,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
fails,to learn,task,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
Results,has,NTM and Fast - weight RNN,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
NTM and Fast - weight RNN,has,fail,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
fail,has,longer tasks,experiments,/content/training-data/natural_language_inference/28/triples/experiments.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/28/triples/model.txt
Model,has,Rotational Unit of Memory,model,/content/training-data/natural_language_inference/28/triples/model.txt
Rotational Unit of Memory,is,modified gated model,model,/content/training-data/natural_language_inference/28/triples/model.txt
modified gated model,whose,rotational operation,model,/content/training-data/natural_language_inference/28/triples/model.txt
rotational operation,acts as,associative memory,model,/content/training-data/natural_language_inference/28/triples/model.txt
Rotational Unit of Memory,is,orthogonal matrix,model,/content/training-data/natural_language_inference/28/triples/model.txt
Model,propose,novel RNN cell,model,/content/training-data/natural_language_inference/28/triples/model.txt
novel RNN cell,has,resolves simultaneously,model,/content/training-data/natural_language_inference/28/triples/model.txt
resolves simultaneously,has,weaknesses,model,/content/training-data/natural_language_inference/28/triples/model.txt
weaknesses,of,basic RNN,model,/content/training-data/natural_language_inference/28/triples/model.txt
Contribution,has research problem,Recurrent Neural Networks ( RNN ,research-problem,/content/training-data/natural_language_inference/28/triples/research-problem.txt
Contribution,has research problem,RNN,research-problem,/content/training-data/natural_language_inference/28/triples/research-problem.txt
Contribution,has,Experiments,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Experiments,has,Tasks,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Tasks,has,Algorithmic Task,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Algorithmic Task,has,Hyperparameters,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Hyperparameters,used,batch size,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
batch size,has,50,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Hyperparameters,used,hidden size,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
hidden size,has,128,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Hyperparameters,has,RNNs,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
RNNs,trained with,RMSProp optimizer,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
RMSProp optimizer,with,decay rate,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
decay rate,of,0.9,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
RMSProp optimizer,with,learning rate,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
learning rate,of,0.001,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Algorithmic Task,has,Results,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Results,found that,GORU,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
GORU,performs,averagely better,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
averagely better,than,GRU / LSTM and EURNN,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Tasks,has,Copying Memory Task,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Copying Memory Task,has,Hyperparameters,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Hyperparameters,use,RMSProp optimization,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
RMSProp optimization,with,decay rate,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
decay rate,of,0.9,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
RMSProp optimization,with,learning rate,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
learning rate,of,0.001,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Hyperparameters,has,batch size,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
batch size,set to,128,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Hyperparameters,has,Hidden state sizes,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Hidden state sizes,to match,total number,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
total number,of,hidden to hidden parameters,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Hidden state sizes,set to,"128 , 100 , 90 , 512",experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Copying Memory Task,has,Results,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Results,has,GORU,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
GORU,is,only gated - system,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
GORU,to,successfully solve,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
successfully solve,has,task,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Tasks,has,Parenthesis Task,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Parenthesis Task,has,Hyperparameters,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Hyperparameters,has,total input length,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
total input length,set to,200,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Hyperparameters,used,batch size,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
batch size,has,128,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Hyperparameters,used,RMSProp Optimizer,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
RMSProp Optimizer,with,decay rate,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
decay rate,has,0.9,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
RMSProp Optimizer,with,learning rate,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
learning rate,has,0.001,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Parenthesis Task,has,Results,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Results,analyzed,update gates,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
update gates,for,GORU and GRU,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Results,has,GORU,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
GORU,able to,successfully outperform,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
successfully outperform,has,GRU,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
successfully outperform,has,LSTM,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
successfully outperform,has,EURNN,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
successfully outperform,in terms of,learning speed,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
successfully outperform,in terms of,final performances,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Tasks,has,Denoise Task,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Denoise Task,has,Hyperparameters,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Hyperparameters,use,RM - SProp optimization algorithm,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
RM - SProp optimization algorithm,with,decay rate,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
decay rate,of,0.9,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
RM - SProp optimization algorithm,with,learning rate,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
learning rate,of,0.01,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Hyperparameters,has,batch size,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
batch size,set to,128,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Hyperparameters,has,Hidden state sizes,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Hidden state sizes,set to,"128 , 100 , 90 , 512",experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
"128 , 100 , 90 , 512",to match,total number of hidden to hidden parameters,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Denoise Task,has,Results,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Results,has,GORU and GRU,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
GORU and GRU,has,successfully solve,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
successfully solve,has,task,experiments,/content/training-data/natural_language_inference/71/triples/experiments.txt
Contribution,Code,https://github.com/jingli9111/GORU-tensorflow,code,/content/training-data/natural_language_inference/71/triples/code.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/71/triples/model.txt
Model,focus on,implementation,model,/content/training-data/natural_language_inference/71/triples/model.txt
implementation,of,orthogonal transition matrices,model,/content/training-data/natural_language_inference/71/triples/model.txt
orthogonal transition matrices,which is,subset,model,/content/training-data/natural_language_inference/71/triples/model.txt
subset,of,unitary matrices,model,/content/training-data/natural_language_inference/71/triples/model.txt
Model,demonstrate,GORU,model,/content/training-data/natural_language_inference/71/triples/model.txt
GORU,able to learn,long term dependencies,model,/content/training-data/natural_language_inference/71/triples/model.txt
long term dependencies,has,effectively,model,/content/training-data/natural_language_inference/71/triples/model.txt
Model,propose,new architecture,model,/content/training-data/natural_language_inference/71/triples/model.txt
new architecture,name,Gated Orthogonal Recurrent Unit ( GORU ,model,/content/training-data/natural_language_inference/71/triples/model.txt
new architecture,combines,ability,model,/content/training-data/natural_language_inference/71/triples/model.txt
ability,to capture,long term dependencies,model,/content/training-data/natural_language_inference/71/triples/model.txt
long term dependencies,by using,orthogonal matrices,model,/content/training-data/natural_language_inference/71/triples/model.txt
ability,to,forget,model,/content/training-data/natural_language_inference/71/triples/model.txt
forget,by using,GRU structure,model,/content/training-data/natural_language_inference/71/triples/model.txt
Contribution,has research problem,recurrent neural network ( RNN ,research-problem,/content/training-data/natural_language_inference/71/triples/research-problem.txt
Contribution,has research problem,Recurrent Neural Networks with gating units,research-problem,/content/training-data/natural_language_inference/71/triples/research-problem.txt
Contribution,has research problem,gating units for Recurrent Neural Networks,research-problem,/content/training-data/natural_language_inference/71/triples/research-problem.txt
Contribution,has research problem,RNNs,research-problem,/content/training-data/natural_language_inference/71/triples/research-problem.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/22/triples/ablation-analysis.txt
Ablation analysis,show,two ruminate layers,ablation-analysis,/content/training-data/natural_language_inference/22/triples/ablation-analysis.txt
two ruminate layers,are,important and helpful,ablation-analysis,/content/training-data/natural_language_inference/22/triples/ablation-analysis.txt
important and helpful,in contributing,performance,ablation-analysis,/content/training-data/natural_language_inference/22/triples/ablation-analysis.txt
Ablation analysis,worth noting,BiLSTM,ablation-analysis,/content/training-data/natural_language_inference/22/triples/ablation-analysis.txt
BiLSTM,in,context ruminate layer,ablation-analysis,/content/training-data/natural_language_inference/22/triples/ablation-analysis.txt
BiLSTM,contributes,substantially,ablation-analysis,/content/training-data/natural_language_inference/22/triples/ablation-analysis.txt
substantially,to,model performance,ablation-analysis,/content/training-data/natural_language_inference/22/triples/ablation-analysis.txt
Contribution,Code,https://rajpurkar.github.io/SQuAD -explorer/,code,/content/training-data/natural_language_inference/22/triples/code.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/22/triples/model.txt
Model,introduce,answer-question similarity loss,model,/content/training-data/natural_language_inference/22/triples/model.txt
answer-question similarity loss,to penalize,overlap,model,/content/training-data/natural_language_inference/22/triples/model.txt
overlap,between,question and predicted answer,model,/content/training-data/natural_language_inference/22/triples/model.txt
Model,introduce,two novel layer types,model,/content/training-data/natural_language_inference/22/triples/model.txt
two novel layer types,use,gating mechanisms,model,/content/training-data/natural_language_inference/22/triples/model.txt
gating mechanisms,to fuse,first and second passes,model,/content/training-data/natural_language_inference/22/triples/model.txt
two novel layer types,name,ruminate layers,model,/content/training-data/natural_language_inference/22/triples/model.txt
Model,propose,extension,model,/content/training-data/natural_language_inference/22/triples/model.txt
extension,called,Ruminating Reader,model,/content/training-data/natural_language_inference/22/triples/model.txt
extension,of,BIDAF,model,/content/training-data/natural_language_inference/22/triples/model.txt
extension,uses,second pass,model,/content/training-data/natural_language_inference/22/triples/model.txt
second pass,of,reading and reasoning,model,/content/training-data/natural_language_inference/22/triples/model.txt
reading and reasoning,to avoid,mistakes,model,/content/training-data/natural_language_inference/22/triples/model.txt
reading and reasoning,to ensure,effectively use,model,/content/training-data/natural_language_inference/22/triples/model.txt
effectively use,has,full context,model,/content/training-data/natural_language_inference/22/triples/model.txt
full context,when selecting,answer,model,/content/training-data/natural_language_inference/22/triples/model.txt
Contribution,has research problem,machine comprehension ( MC ,research-problem,/content/training-data/natural_language_inference/22/triples/research-problem.txt
Contribution,has research problem,question answering ( QA ,research-problem,/content/training-data/natural_language_inference/22/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
Experimental setup,using,Tensorflow,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
Experimental setup,using,single NVIDIA K80 GPU,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
Experimental setup,set,hidden layer dimension ( d ,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
hidden layer dimension ( d ),to,100,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
Experimental setup,In,character encoding layer,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
character encoding layer,use,100 filters,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
100 filters,of,width 5,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
Experimental setup,use,AdaDelta optimizer,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
AdaDelta optimizer,for,optimization,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
Experimental setup,use,pretrained 100D Glo Ve vectors ( 6B - token version ,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
pretrained 100D Glo Ve vectors ( 6B - token version ),as,word embeddings,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
Experimental setup,has,typical model run,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
typical model run,has,converges,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
converges,in about,40 k steps,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
Experimental setup,has,L2-regularization weight,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
L2-regularization weight,is,1 e - 4,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
Experimental setup,has,Out - of - vocobulary tokens,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
Out - of - vocobulary tokens,represented by,UNK symbol,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
UNK symbol,in,word embedding layer,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
Out - of - vocobulary tokens,treated,normally,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
normally,by,character embedding layer,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
Experimental setup,has,Learning rate,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
Learning rate,starts at,0.5,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
Learning rate,decreases to,0.2,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
0.2,has,stops improving,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
stops improving,has,model,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
Experimental setup,has,dropout,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
dropout,with,drop rate,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
drop rate,of,0.2,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
Experimental setup,has,Batch size,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
Batch size,is,30,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
Experimental setup,has,AQSL weight,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
AQSL weight,is,1,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
Experimental setup,selected,hyperparameter values,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
hyperparameter values,through,random search,experimental-setup,/content/training-data/natural_language_inference/22/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/22/triples/results.txt
Results,achieve,F 1 score,results,/content/training-data/natural_language_inference/22/triples/results.txt
F 1 score,of,79.5,results,/content/training-data/natural_language_inference/22/triples/results.txt
Results,achieve,EM score,results,/content/training-data/natural_language_inference/22/triples/results.txt
EM score,of,70.6,results,/content/training-data/natural_language_inference/22/triples/results.txt
Results,has,model,results,/content/training-data/natural_language_inference/22/triples/results.txt
model,is,tied,results,/content/training-data/natural_language_inference/22/triples/results.txt
tied,with,bestperforming published single model,results,/content/training-data/natural_language_inference/22/triples/results.txt
tied,in,accuracy,results,/content/training-data/natural_language_inference/22/triples/results.txt
tied,on,hidden test set,results,/content/training-data/natural_language_inference/22/triples/results.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/19/triples/model.txt
Model,In conjunction with,directional mask,model,/content/training-data/natural_language_inference/19/triples/model.txt
directional mask,has,distance mask,model,/content/training-data/natural_language_inference/19/triples/model.txt
distance mask,allows us to incorporate,complete positional information of words,model,/content/training-data/natural_language_inference/19/triples/model.txt
complete positional information of words,in,our model,model,/content/training-data/natural_language_inference/19/triples/model.txt
Model,propose,Distancebased Self - Attention Network,model,/content/training-data/natural_language_inference/19/triples/model.txt
Distancebased Self - Attention Network,introduces,distance mask,model,/content/training-data/natural_language_inference/19/triples/model.txt
distance mask,models,relative distance,model,/content/training-data/natural_language_inference/19/triples/model.txt
relative distance,between,words,model,/content/training-data/natural_language_inference/19/triples/model.txt
Contribution,has research problem,Natural Language Inference,research-problem,/content/training-data/natural_language_inference/19/triples/research-problem.txt
Contribution,has research problem,NLI,research-problem,/content/training-data/natural_language_inference/19/triples/research-problem.txt
Contribution,has research problem,Natural Language Inference ( NLI ,research-problem,/content/training-data/natural_language_inference/19/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/19/triples/experimental-setup.txt
Experimental setup,implemented via,Tensorflow,experimental-setup,/content/training-data/natural_language_inference/19/triples/experimental-setup.txt
Tensorflow,on,single Nvidia Geforce GTX 1080 Ti GPU,experimental-setup,/content/training-data/natural_language_inference/19/triples/experimental-setup.txt
Experimental setup,set,"h = 5 , ? = 1.5",experimental-setup,/content/training-data/natural_language_inference/19/triples/experimental-setup.txt
"h = 5 , ? = 1.5",in,masked multi-head attention,experimental-setup,/content/training-data/natural_language_inference/19/triples/experimental-setup.txt
Experimental setup,applied,residual dropout,experimental-setup,/content/training-data/natural_language_inference/19/triples/experimental-setup.txt
residual dropout,with,dropout,experimental-setup,/content/training-data/natural_language_inference/19/triples/experimental-setup.txt
dropout,to,output,experimental-setup,/content/training-data/natural_language_inference/19/triples/experimental-setup.txt
output,of,masked multi-head attention,experimental-setup,/content/training-data/natural_language_inference/19/triples/experimental-setup.txt
output,of,SF +H F +b F of fusion gate,experimental-setup,/content/training-data/natural_language_inference/19/triples/experimental-setup.txt
Experimental setup,used,Glove 840B 300D 1 ( d e = 300 ,experimental-setup,/content/training-data/natural_language_inference/19/triples/experimental-setup.txt
Glove 840B 300D 1 ( d e = 300 ),for,pre-trained word embedding,experimental-setup,/content/training-data/natural_language_inference/19/triples/experimental-setup.txt
pre-trained word embedding,without,finetuning,experimental-setup,/content/training-data/natural_language_inference/19/triples/experimental-setup.txt
Glove 840B 300D 1 ( d e = 300 ),train,more universally usable sentence encoder,experimental-setup,/content/training-data/natural_language_inference/19/triples/experimental-setup.txt
Experimental setup,has,dropout probability,experimental-setup,/content/training-data/natural_language_inference/19/triples/experimental-setup.txt
dropout probability,set to,0.1,experimental-setup,/content/training-data/natural_language_inference/19/triples/experimental-setup.txt
Experimental setup,has,Layer normalization,experimental-setup,/content/training-data/natural_language_inference/19/triples/experimental-setup.txt
Layer normalization,applied to,all linear projections,experimental-setup,/content/training-data/natural_language_inference/19/triples/experimental-setup.txt
all linear projections,of,masked multihead attention,experimental-setup,/content/training-data/natural_language_inference/19/triples/experimental-setup.txt
all linear projections,of,fusion gate,experimental-setup,/content/training-data/natural_language_inference/19/triples/experimental-setup.txt
all linear projections,of,multi-dimensional attention,experimental-setup,/content/training-data/natural_language_inference/19/triples/experimental-setup.txt
Experimental setup,has,Batch size,experimental-setup,/content/training-data/natural_language_inference/19/triples/experimental-setup.txt
Batch size,was,64,experimental-setup,/content/training-data/natural_language_inference/19/triples/experimental-setup.txt
Experimental setup,has,model,experimental-setup,/content/training-data/natural_language_inference/19/triples/experimental-setup.txt
model,trained with,Adam optimizer,experimental-setup,/content/training-data/natural_language_inference/19/triples/experimental-setup.txt
Adam optimizer,with,learning rate,experimental-setup,/content/training-data/natural_language_inference/19/triples/experimental-setup.txt
learning rate,of,0.001,experimental-setup,/content/training-data/natural_language_inference/19/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/19/triples/results.txt
Results,applying,SNLI best model,results,/content/training-data/natural_language_inference/19/triples/results.txt
SNLI best model,to,MultiNLI dataset,results,/content/training-data/natural_language_inference/19/triples/results.txt
MultiNLI dataset,has,our model,results,/content/training-data/natural_language_inference/19/triples/results.txt
our model,showed,similar average test accuracy,results,/content/training-data/natural_language_inference/19/triples/results.txt
similar average test accuracy,with,much lower number of parameters,results,/content/training-data/natural_language_inference/19/triples/results.txt
MultiNLI dataset,Compared with,result of RepEVAL 2017,results,/content/training-data/natural_language_inference/19/triples/results.txt
result of RepEVAL 2017,see that,Distance - based Self - Attention Network,results,/content/training-data/natural_language_inference/19/triples/results.txt
Distance - based Self - Attention Network,performs,well,results,/content/training-data/natural_language_inference/19/triples/results.txt
Results,of,SNLI data,results,/content/training-data/natural_language_inference/19/triples/results.txt
SNLI data,show,addition,results,/content/training-data/natural_language_inference/19/triples/results.txt
addition,of,distance mask,results,/content/training-data/natural_language_inference/19/triples/results.txt
distance mask,improved,performance,results,/content/training-data/natural_language_inference/19/triples/results.txt
performance,without,significantly affecting,results,/content/training-data/natural_language_inference/19/triples/results.txt
significantly affecting,has,training time,results,/content/training-data/natural_language_inference/19/triples/results.txt
performance,without,increasing,results,/content/training-data/natural_language_inference/19/triples/results.txt
increasing,has,parameters,results,/content/training-data/natural_language_inference/19/triples/results.txt
SNLI data,has,improvement,results,/content/training-data/natural_language_inference/19/triples/results.txt
improvement,by introducing,distance mask,results,/content/training-data/natural_language_inference/19/triples/results.txt
improvement,of,test accuracy,results,/content/training-data/natural_language_inference/19/triples/results.txt
test accuracy,is only by,0.3 % point,results,/content/training-data/natural_language_inference/19/triples/results.txt
SNLI data,Compared with,existing state - of - the - art model,results,/content/training-data/natural_language_inference/19/triples/results.txt
existing state - of - the - art model,has,our results,results,/content/training-data/natural_language_inference/19/triples/results.txt
our results,show,new state - of - theart record,results,/content/training-data/natural_language_inference/19/triples/results.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/natural_language_inference/61/triples/hyperparameters.txt
Hyperparameters,employ,non-linearity function f = selu,hyperparameters,/content/training-data/natural_language_inference/61/triples/hyperparameters.txt
non-linearity function f = selu,on account of,faster convergence rate,hyperparameters,/content/training-data/natural_language_inference/61/triples/hyperparameters.txt
non-linearity function f = selu,replacing,linear unit ReLU,hyperparameters,/content/training-data/natural_language_inference/61/triples/hyperparameters.txt
Hyperparameters,use,Adam method,hyperparameters,/content/training-data/natural_language_inference/61/triples/hyperparameters.txt
Adam method,for,optimization,hyperparameters,/content/training-data/natural_language_inference/61/triples/hyperparameters.txt
Hyperparameters,use,pre-trained 300 - D Glove 840B vectors,hyperparameters,/content/training-data/natural_language_inference/61/triples/hyperparameters.txt
pre-trained 300 - D Glove 840B vectors,to initialize,word embeddings,hyperparameters,/content/training-data/natural_language_inference/61/triples/hyperparameters.txt
Hyperparameters,has,initial learning rate,hyperparameters,/content/training-data/natural_language_inference/61/triples/hyperparameters.txt
initial learning rate,set to,0.0005,hyperparameters,/content/training-data/natural_language_inference/61/triples/hyperparameters.txt
Hyperparameters,has,Out - of - vocabulary ( OOV ) words,hyperparameters,/content/training-data/natural_language_inference/61/triples/hyperparameters.txt
Out - of - vocabulary ( OOV ) words,are,initialized randomly,hyperparameters,/content/training-data/natural_language_inference/61/triples/hyperparameters.txt
initialized randomly,with,Gaussian samples,hyperparameters,/content/training-data/natural_language_inference/61/triples/hyperparameters.txt
Hyperparameters,has,Dropout rate,hyperparameters,/content/training-data/natural_language_inference/61/triples/hyperparameters.txt
Dropout rate,set to,0.2,hyperparameters,/content/training-data/natural_language_inference/61/triples/hyperparameters.txt
0.2,during,training,hyperparameters,/content/training-data/natural_language_inference/61/triples/hyperparameters.txt
Hyperparameters,has,batch size,hyperparameters,/content/training-data/natural_language_inference/61/triples/hyperparameters.txt
batch size,is,128,hyperparameters,/content/training-data/natural_language_inference/61/triples/hyperparameters.txt
Hyperparameters,has,dimensions,hyperparameters,/content/training-data/natural_language_inference/61/triples/hyperparameters.txt
dimensions,of,hidden states,hyperparameters,/content/training-data/natural_language_inference/61/triples/hyperparameters.txt
hidden states,of,Bi - aLSTM and word embedding,hyperparameters,/content/training-data/natural_language_inference/61/triples/hyperparameters.txt
Bi - aLSTM and word embedding,are,300,hyperparameters,/content/training-data/natural_language_inference/61/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/61/triples/model.txt
Model,using,ESIM model,model,/content/training-data/natural_language_inference/61/triples/model.txt
ESIM model,add,a ention layer,model,/content/training-data/natural_language_inference/61/triples/model.txt
a ention layer,behind,each Bi - LSTM layer,model,/content/training-data/natural_language_inference/61/triples/model.txt
ESIM model,as,baseline,model,/content/training-data/natural_language_inference/61/triples/model.txt
ESIM model,use,adaptive orientation embedding layer,model,/content/training-data/natural_language_inference/61/triples/model.txt
adaptive orientation embedding layer,to jointly represent,forward and backward vectors,model,/content/training-data/natural_language_inference/61/triples/model.txt
Model,denote,modi ed ESIM,model,/content/training-data/natural_language_inference/61/triples/model.txt
modi ed ESIM,as,aESIM,model,/content/training-data/natural_language_inference/61/triples/model.txt
Model,name,a ention boosted Bi - LSTM,model,/content/training-data/natural_language_inference/61/triples/model.txt
a ention boosted Bi - LSTM,as,Bi - a LSTM,model,/content/training-data/natural_language_inference/61/triples/model.txt
Contribution,has research problem,natural language inference,research-problem,/content/training-data/natural_language_inference/61/triples/research-problem.txt
Contribution,has research problem,Natural language inference ( NLI ,research-problem,/content/training-data/natural_language_inference/61/triples/research-problem.txt
Contribution,has research problem,NLI,research-problem,/content/training-data/natural_language_inference/61/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/61/triples/results.txt
Results,has,ESIM model,results,/content/training-data/natural_language_inference/61/triples/results.txt
ESIM model,achieved,88.1 %,results,/content/training-data/natural_language_inference/61/triples/results.txt
88.1 %,has,elevating 0.8 percent,results,/content/training-data/natural_language_inference/61/triples/results.txt
elevating 0.8 percent,higher than,ESIM model,results,/content/training-data/natural_language_inference/61/triples/results.txt
88.1 %,on,SNLI corpus,results,/content/training-data/natural_language_inference/61/triples/results.txt
ESIM model,has,outperformed,results,/content/training-data/natural_language_inference/61/triples/results.txt
outperformed,has,baselines,results,/content/training-data/natural_language_inference/61/triples/results.txt
baselines,on,MultiNLI,results,/content/training-data/natural_language_inference/61/triples/results.txt
ESIM model,promoted,almost 0.5 percent accuracy,results,/content/training-data/natural_language_inference/61/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/94/triples/ablation-analysis.txt
Ablation analysis,has,self - attention,ablation-analysis,/content/training-data/natural_language_inference/94/triples/ablation-analysis.txt
self - attention,able to contribute,significantly,ablation-analysis,/content/training-data/natural_language_inference/94/triples/ablation-analysis.txt
significantly,to,performance,ablation-analysis,/content/training-data/natural_language_inference/94/triples/ablation-analysis.txt
performance,on top of,other components of the model,ablation-analysis,/content/training-data/natural_language_inference/94/triples/ablation-analysis.txt
Ablation analysis,see that,our commonsense selection and incorporation mechanism,ablation-analysis,/content/training-data/natural_language_inference/94/triples/ablation-analysis.txt
our commonsense selection and incorporation mechanism,has,improves,ablation-analysis,/content/training-data/natural_language_inference/94/triples/ablation-analysis.txt
improves,has,performance significantly,ablation-analysis,/content/training-data/natural_language_inference/94/triples/ablation-analysis.txt
performance significantly,across,all metrics,ablation-analysis,/content/training-data/natural_language_inference/94/triples/ablation-analysis.txt
Ablation analysis,see that,effectively introducing,ablation-analysis,/content/training-data/natural_language_inference/94/triples/ablation-analysis.txt
effectively introducing,has,external knowledge,ablation-analysis,/content/training-data/natural_language_inference/94/triples/ablation-analysis.txt
external knowledge,has,improve,ablation-analysis,/content/training-data/natural_language_inference/94/triples/ablation-analysis.txt
improve,has,performance,ablation-analysis,/content/training-data/natural_language_inference/94/triples/ablation-analysis.txt
performance,on top of,our strong baseline,ablation-analysis,/content/training-data/natural_language_inference/94/triples/ablation-analysis.txt
performance,via,our commonsense selection algorithm,ablation-analysis,/content/training-data/natural_language_inference/94/triples/ablation-analysis.txt
performance,via,NOIC,ablation-analysis,/content/training-data/natural_language_inference/94/triples/ablation-analysis.txt
Contribution,Code,https://github.com/yicheng-w/CommonSenseMultiHopQA,code,/content/training-data/natural_language_inference/94/triples/code.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/94/triples/model.txt
Model,present,novel method,model,/content/training-data/natural_language_inference/94/triples/model.txt
novel method,of,inserting,model,/content/training-data/natural_language_inference/94/triples/model.txt
inserting,has,selected commonsense paths,model,/content/training-data/natural_language_inference/94/triples/model.txt
selected commonsense paths,between,hops,model,/content/training-data/natural_language_inference/94/triples/model.txt
hops,of,document - context reasoning,model,/content/training-data/natural_language_inference/94/triples/model.txt
document - context reasoning,within,our model,model,/content/training-data/natural_language_inference/94/triples/model.txt
selected commonsense paths,via,Necessary and Optional Information Cell ( NOIC ,model,/content/training-data/natural_language_inference/94/triples/model.txt
Necessary and Optional Information Cell ( NOIC ),employs,selectivelygated attention mechanism,model,/content/training-data/natural_language_inference/94/triples/model.txt
selectivelygated attention mechanism,that utilizes,commonsense information,model,/content/training-data/natural_language_inference/94/triples/model.txt
commonsense information,to effectively fill in,gaps of inference,model,/content/training-data/natural_language_inference/94/triples/model.txt
Model,present,algorithm,model,/content/training-data/natural_language_inference/94/triples/model.txt
algorithm,for selecting,"useful , grounded multi-hop relational knowledge paths",model,/content/training-data/natural_language_inference/94/triples/model.txt
"useful , grounded multi-hop relational knowledge paths",from,ConceptNet,model,/content/training-data/natural_language_inference/94/triples/model.txt
"useful , grounded multi-hop relational knowledge paths",via,pointwise mutual information ( PMI ,model,/content/training-data/natural_language_inference/94/triples/model.txt
"useful , grounded multi-hop relational knowledge paths",via,term - frequency - based scoring function,model,/content/training-data/natural_language_inference/94/triples/model.txt
Model,first propose,Multi - Hop Pointer - Generator Model ( MHPGM ,model,/content/training-data/natural_language_inference/94/triples/model.txt
Multi - Hop Pointer - Generator Model ( MHPGM ),has,strong baseline model,model,/content/training-data/natural_language_inference/94/triples/model.txt
strong baseline model,uses,multiple hops,model,/content/training-data/natural_language_inference/94/triples/model.txt
multiple hops,of,bidirectional attention,model,/content/training-data/natural_language_inference/94/triples/model.txt
multiple hops,of,self - attention,model,/content/training-data/natural_language_inference/94/triples/model.txt
multiple hops,of,pointer - generator decoder,model,/content/training-data/natural_language_inference/94/triples/model.txt
multiple hops,to effectively,read and reason,model,/content/training-data/natural_language_inference/94/triples/model.txt
read and reason,within,along passage,model,/content/training-data/natural_language_inference/94/triples/model.txt
multiple hops,to effectively,synthesize,model,/content/training-data/natural_language_inference/94/triples/model.txt
synthesize,has,coherent response,model,/content/training-data/natural_language_inference/94/triples/model.txt
Contribution,has research problem,Generative Multi - Hop Question Answering,research-problem,/content/training-data/natural_language_inference/94/triples/research-problem.txt
Contribution,has research problem,Reading comprehension QA,research-problem,/content/training-data/natural_language_inference/94/triples/research-problem.txt
Contribution,has research problem,machine reading comprehension ( MRC ) based QA,research-problem,/content/training-data/natural_language_inference/94/triples/research-problem.txt
Contribution,has research problem,reasoning - based MRC - QA,research-problem,/content/training-data/natural_language_inference/94/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/94/triples/results.txt
Results,with,NOIC commonsense integration,results,/content/training-data/natural_language_inference/94/triples/results.txt
NOIC commonsense integration,able to,further improve,results,/content/training-data/natural_language_inference/94/triples/results.txt
further improve,has,performance,results,/content/training-data/natural_language_inference/94/triples/results.txt
performance,establishing,new state - of - the - art,results,/content/training-data/natural_language_inference/94/triples/results.txt
new state - of - the - art,for,task,results,/content/training-data/natural_language_inference/94/triples/results.txt
Results,see empirically that,our model,results,/content/training-data/natural_language_inference/94/triples/results.txt
our model,has,competitive,results,/content/training-data/natural_language_inference/94/triples/results.txt
competitive,with,top span prediction models,results,/content/training-data/natural_language_inference/94/triples/results.txt
our model,has,outperforms,results,/content/training-data/natural_language_inference/94/triples/results.txt
outperforms,has,all generative models,results,/content/training-data/natural_language_inference/94/triples/results.txt
all generative models,on,NarrativeQA,results,/content/training-data/natural_language_inference/94/triples/results.txt
Results,speculate,improvement,results,/content/training-data/natural_language_inference/94/triples/results.txt
improvement,has,smaller,results,/content/training-data/natural_language_inference/94/triples/results.txt
smaller,on,Wikihop,results,/content/training-data/natural_language_inference/94/triples/results.txt
Results,see that,our model,results,/content/training-data/natural_language_inference/94/triples/results.txt
our model,performs,reasonably well,results,/content/training-data/natural_language_inference/94/triples/results.txt
reasonably well,on,WikiHop,results,/content/training-data/natural_language_inference/94/triples/results.txt
our model,achieves,promising initial improvements,results,/content/training-data/natural_language_inference/94/triples/results.txt
promising initial improvements,via,addition,results,/content/training-data/natural_language_inference/94/triples/results.txt
addition,of,commonsense,results,/content/training-data/natural_language_inference/94/triples/results.txt
commonsense,hinting at,generalizability,results,/content/training-data/natural_language_inference/94/triples/results.txt
generalizability,of,our approaches,results,/content/training-data/natural_language_inference/94/triples/results.txt
Contribution,has,Dataset,datase,/content/training-data/natural_language_inference/46/triples/dataset.txt
Dataset,call,NarrativeQA,datase,/content/training-data/natural_language_inference/46/triples/dataset.txt
Dataset,consists of,stories,datase,/content/training-data/natural_language_inference/46/triples/dataset.txt
stories,which are,books and movie scripts,datase,/content/training-data/natural_language_inference/46/triples/dataset.txt
books and movie scripts,with,human written questions and answers,datase,/content/training-data/natural_language_inference/46/triples/dataset.txt
human written questions and answers,based solely on,human - generated abstractive summaries,datase,/content/training-data/natural_language_inference/46/triples/dataset.txt
Dataset,has,questions,datase,/content/training-data/natural_language_inference/46/triples/dataset.txt
questions,maybe,answered,datase,/content/training-data/natural_language_inference/46/triples/dataset.txt
answered,using,summaries,datase,/content/training-data/natural_language_inference/46/triples/dataset.txt
answered,using,full story text,datase,/content/training-data/natural_language_inference/46/triples/dataset.txt
Contribution,has,Experiments,experiments,/content/training-data/natural_language_inference/46/triples/experiments.txt
Experiments,has,Tasks,experiments,/content/training-data/natural_language_inference/46/triples/experiments.txt
Tasks,has,Reading Full Stories Only,experiments,/content/training-data/natural_language_inference/46/triples/experiments.txt
Reading Full Stories Only,has,Results,experiments,/content/training-data/natural_language_inference/46/triples/experiments.txt
Results,has,AS Reader,experiments,/content/training-data/natural_language_inference/46/triples/experiments.txt
AS Reader,has,underperforms,experiments,/content/training-data/natural_language_inference/46/triples/experiments.txt
underperforms,has,simple no -context Seq2Seq baseline,experiments,/content/training-data/natural_language_inference/46/triples/experiments.txt
underperforms,in terms of,MRR,experiments,/content/training-data/natural_language_inference/46/triples/experiments.txt
Results,observed,no significant differences,experiments,/content/training-data/natural_language_inference/46/triples/experiments.txt
no significant differences,for,varying,experiments,/content/training-data/natural_language_inference/46/triples/experiments.txt
varying,has,number of chunks,experiments,/content/training-data/natural_language_inference/46/triples/experiments.txt
Tasks,has,Reading Summaries Only,experiments,/content/training-data/natural_language_inference/46/triples/experiments.txt
Reading Summaries Only,has,Results,experiments,/content/training-data/natural_language_inference/46/triples/experiments.txt
Results,has,additional inductive bias,experiments,/content/training-data/natural_language_inference/46/triples/experiments.txt
additional inductive bias,results in,higher performance,experiments,/content/training-data/natural_language_inference/46/triples/experiments.txt
higher performance,for,span prediction model,experiments,/content/training-data/natural_language_inference/46/triples/experiments.txt
Results,has,Both the plain sequence to sequence model and the AS Reader,experiments,/content/training-data/natural_language_inference/46/triples/experiments.txt
Both the plain sequence to sequence model and the AS Reader,perform,well,experiments,/content/training-data/natural_language_inference/46/triples/experiments.txt
Results,has,neural span prediction model,experiments,/content/training-data/natural_language_inference/46/triples/experiments.txt
neural span prediction model,has,significantly outperforming,experiments,/content/training-data/natural_language_inference/46/triples/experiments.txt
significantly outperforming,has,all other proposed methods,experiments,/content/training-data/natural_language_inference/46/triples/experiments.txt
Results,on,full Narra - tive QA task,experiments,/content/training-data/natural_language_inference/46/triples/experiments.txt
full Narra - tive QA task,where,context documents,experiments,/content/training-data/natural_language_inference/46/triples/experiments.txt
context documents,are,full stories,experiments,/content/training-data/natural_language_inference/46/triples/experiments.txt
full Narra - tive QA task,observe,decline,experiments,/content/training-data/natural_language_inference/46/triples/experiments.txt
decline,in,performance,experiments,/content/training-data/natural_language_inference/46/triples/experiments.txt
performance,of,span- selection oracle IR model,experiments,/content/training-data/natural_language_inference/46/triples/experiments.txt
Contribution,has research problem,Reading Comprehension,research-problem,/content/training-data/natural_language_inference/46/triples/research-problem.txt
Contribution,has research problem,Reading comprehension ( RC ,research-problem,/content/training-data/natural_language_inference/46/triples/research-problem.txt
Contribution,has research problem,RC,research-problem,/content/training-data/natural_language_inference/46/triples/research-problem.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/43/triples/ablation-analysis.txt
Ablation analysis,has,TF - IDF,ablation-analysis,/content/training-data/natural_language_inference/43/triples/ablation-analysis.txt
TF - IDF,leading to,large drop,ablation-analysis,/content/training-data/natural_language_inference/43/triples/ablation-analysis.txt
large drop,of,12 points,ablation-analysis,/content/training-data/natural_language_inference/43/triples/ablation-analysis.txt
TF - IDF,is,most impactful feature,ablation-analysis,/content/training-data/natural_language_inference/43/triples/ablation-analysis.txt
Contribution,has,Experiments,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
Experiments,has,Results,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
Results,has,test F 1,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
test F 1,on,examples,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
examples,for which,candidate extraction succeeded ( WEBQA - SUBSET ,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
candidate extraction succeeded ( WEBQA - SUBSET ),is,"51.9 ( 53.4 p@1 , 67.5 MRR ",experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
Results,has,candidate extraction step,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
candidate extraction step,finds,correct answer,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
correct answer,in,top - K candidates,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
top - K candidates,in,62.7 %,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
62.7 %,of,test examples,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
top - K candidates,in,65.9 %,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
65.9 %,of,development examples,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
Results,has,COMPQ,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
COMPQ,obtained,42.2 F 1,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
42.2 F 1,compared to,40.9 F 1,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
40.9 F 1,when training on,COM - PLEXQUESTIONS only,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
42.2 F 1,on,test set,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
Results,has,WEBQA,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
WEBQA,obtained,32.6 F 1,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
32.6 F 1,compared to,40.9 F 1,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
40.9 F 1,of,COMPQ,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
Results,Restricting,predictions,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
predictions,to,subset,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
subset,for which,candidate extraction,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
candidate extraction,has,succeeded,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
predictions,has,F 1,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
F 1,of,COMPQ - SUBSET,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
COMPQ - SUBSET,is,48.5,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
48.5,is,3.4 F 1 points lower,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
3.4 F 1 points lower,than,WEBQA - SUBSET,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
Experiments,has,Baselines,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
Baselines,has,STAGG and COMPQ,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
STAGG and COMPQ,which are,highest performing semantic parsing models,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
highest performing semantic parsing models,on,COMPLEXQUESTIONS and WEBQUES - TIONS,experiments,/content/training-data/natural_language_inference/43/triples/experiments.txt
Contribution,Code,https://worksheets. codalab.org/worksheets/ 0x91d77db37e0a4bbbaeb37b8972f4784f/,code,/content/training-data/natural_language_inference/43/triples/code.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/43/triples/model.txt
Model,develop,simple log - linear model,model,/content/training-data/natural_language_inference/43/triples/model.txt
simple log - linear model,in the spirit of,traditional web - based QA systems,model,/content/training-data/natural_language_inference/43/triples/model.txt
simple log - linear model,that,answers questions,model,/content/training-data/natural_language_inference/43/triples/model.txt
answers questions,by,querying,model,/content/training-data/natural_language_inference/43/triples/model.txt
querying,has,web,model,/content/training-data/natural_language_inference/43/triples/model.txt
simple log - linear model,extracting,answer,model,/content/training-data/natural_language_inference/43/triples/model.txt
answer,from,returned web snippets,model,/content/training-data/natural_language_inference/43/triples/model.txt
Model,has,our evaluation scheme,model,/content/training-data/natural_language_inference/43/triples/model.txt
our evaluation scheme,suitable for,semantic parsing benchmarks,model,/content/training-data/natural_language_inference/43/triples/model.txt
semantic parsing benchmarks,in which,knowledge,model,/content/training-data/natural_language_inference/43/triples/model.txt
knowledge,required for,answering questions,model,/content/training-data/natural_language_inference/43/triples/model.txt
answering questions,covered by,web,model,/content/training-data/natural_language_inference/43/triples/model.txt
Contribution,has research problem,Semantic Parsing,research-problem,/content/training-data/natural_language_inference/43/triples/research-problem.txt
Contribution,has,Experiments,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
Experiments,on,Tasks,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
Tasks,for,answer sentence selection tasks,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
answer sentence selection tasks,experiment on,two datasets,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
two datasets,name,TREC - QA,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
two datasets,name,WikiQA,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
answer sentence selection tasks,see that,performance,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
performance,from,our model,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
performance,is,on par,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
on par,with,state - of - the - art models,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
Tasks,on,natural language inference task,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
natural language inference task,over,SNLI dataset,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
SNLI dataset,has,Results,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
Results,observe that,our ' BiMPM ( Ensemble ,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
our ' BiMPM ( Ensemble ),works,much better,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
much better,than,Ensemble,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
Results,observe that,"our single model "" BiMPM """,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
"our single model "" BiMPM """,on par with,state - of - the - art single models,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
Results,see that,Only P ? Q,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
Only P ? Q,has,works significantly better,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
works significantly better,tells,matching the hypothesis,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
matching the hypothesis,against,premise,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
matching the hypothesis,is,more effective,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
more effective,than,other way around,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
works significantly better,than,Only P ? Q,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
Results,has,"our "" BiMPM "" model",experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
"our "" BiMPM "" model",works,much better,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
much better,than,Only P ? Q,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
Results,has,our models,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
our models,achieve,state - of - the - art performance,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
state - of - the - art performance,in both,single and ensemble scenarios,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
state - of - the - art performance,for,natural language inference task,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
Tasks,on,paraphrase identification task,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
paraphrase identification task,has,Results,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
Results,experiment on,""" Quora Question Pairs "" dataset",experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
Results,see that,""" Multi - Perspective - CNN "" ( or "" Multi- Perspective - LSTM "" ",experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
""" Multi - Perspective - CNN "" ( or "" Multi- Perspective - LSTM "" )",works,much better,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
much better,than,""" Siamese - CNN "" ( or "" Siamese - LSTM "" ",experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
Results,has,"Our "" BiMPM "" model",experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
"Our "" BiMPM "" model",has,outperforms,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
outperforms,by,more than two percent,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
outperforms,has,""" L.D.C. "" model",experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
paraphrase identification task,has,Baselines,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
Baselines,re-implement,""" L.D.C. "" model",experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
""" L.D.C. "" model",is,model,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
model,under,""" matchingaggregation "" framework",experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
Baselines,implement,two more baseline models,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
two more baseline models,name,Multi - Perspective - CNN,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
two more baseline models,name,Multi - Perspective - LSTM,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
Baselines,under,Siamese framework,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
Siamese framework,implement,two baseline models,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
two baseline models,name,Siamese - CNN,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
two baseline models,name,Siamese - LSTM,experiments,/content/training-data/natural_language_inference/16/triples/experiments.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/natural_language_inference/16/triples/hyperparameters.txt
Hyperparameters,minimize,cross entropy,hyperparameters,/content/training-data/natural_language_inference/16/triples/hyperparameters.txt
cross entropy,of,training set,hyperparameters,/content/training-data/natural_language_inference/16/triples/hyperparameters.txt
Hyperparameters,set,learning rate,hyperparameters,/content/training-data/natural_language_inference/16/triples/hyperparameters.txt
learning rate,as,0.001,hyperparameters,/content/training-data/natural_language_inference/16/triples/hyperparameters.txt
Hyperparameters,set,hidden size,hyperparameters,/content/training-data/natural_language_inference/16/triples/hyperparameters.txt
hidden size,as,100,hyperparameters,/content/training-data/natural_language_inference/16/triples/hyperparameters.txt
hidden size,for,all BiLSTM layers,hyperparameters,/content/training-data/natural_language_inference/16/triples/hyperparameters.txt
Hyperparameters,apply,dropout,hyperparameters,/content/training-data/natural_language_inference/16/triples/hyperparameters.txt
dropout,set,dropout ratio,hyperparameters,/content/training-data/natural_language_inference/16/triples/hyperparameters.txt
dropout ratio,as,0.1,hyperparameters,/content/training-data/natural_language_inference/16/triples/hyperparameters.txt
dropout,to,every layers,hyperparameters,/content/training-data/natural_language_inference/16/triples/hyperparameters.txt
Hyperparameters,use,ADAM optimizer,hyperparameters,/content/training-data/natural_language_inference/16/triples/hyperparameters.txt
ADAM optimizer,to update,parameters,hyperparameters,/content/training-data/natural_language_inference/16/triples/hyperparameters.txt
Hyperparameters,For,out - of - vocabulary ( OOV ) words,hyperparameters,/content/training-data/natural_language_inference/16/triples/hyperparameters.txt
out - of - vocabulary ( OOV ) words,initialize,word embeddings,hyperparameters,/content/training-data/natural_language_inference/16/triples/hyperparameters.txt
word embeddings,has,randomly,hyperparameters,/content/training-data/natural_language_inference/16/triples/hyperparameters.txt
Hyperparameters,For,charactercomposed embeddings,hyperparameters,/content/training-data/natural_language_inference/16/triples/hyperparameters.txt
charactercomposed embeddings,compose,each word,hyperparameters,/content/training-data/natural_language_inference/16/triples/hyperparameters.txt
each word,into,50 dimensional vector,hyperparameters,/content/training-data/natural_language_inference/16/triples/hyperparameters.txt
50 dimensional vector,with,LSTM layer,hyperparameters,/content/training-data/natural_language_inference/16/triples/hyperparameters.txt
charactercomposed embeddings,initialize,each character,hyperparameters,/content/training-data/natural_language_inference/16/triples/hyperparameters.txt
each character,as,20 - dimensional vector,hyperparameters,/content/training-data/natural_language_inference/16/triples/hyperparameters.txt
Hyperparameters,initialize,word embeddings,hyperparameters,/content/training-data/natural_language_inference/16/triples/hyperparameters.txt
word embeddings,with,300 - dimensional GloVe word vectors,hyperparameters,/content/training-data/natural_language_inference/16/triples/hyperparameters.txt
300 - dimensional GloVe word vectors,pretrained from,840B Common Crawl corpus,hyperparameters,/content/training-data/natural_language_inference/16/triples/hyperparameters.txt
word embeddings,in,word representation layer,hyperparameters,/content/training-data/natural_language_inference/16/triples/hyperparameters.txt
Hyperparameters,During,training,hyperparameters,/content/training-data/natural_language_inference/16/triples/hyperparameters.txt
training,do not update,pre-trained word embeddings,hyperparameters,/content/training-data/natural_language_inference/16/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/16/triples/model.txt
Model,belongs to,""" matching aggregation "" framework",model,/content/training-data/natural_language_inference/16/triples/model.txt
Model,based on,matching vector,model,/content/training-data/natural_language_inference/16/triples/model.txt
matching vector,has,decision,model,/content/training-data/natural_language_inference/16/triples/model.txt
decision,made through,fully connected layer,model,/content/training-data/natural_language_inference/16/triples/model.txt
Model,has,BiLSTM layer,model,/content/training-data/natural_language_inference/16/triples/model.txt
BiLSTM layer,utilized to,aggregate,model,/content/training-data/natural_language_inference/16/triples/model.txt
aggregate,has,matching results,model,/content/training-data/natural_language_inference/16/triples/model.txt
matching results,into,fixed - length matching vector,model,/content/training-data/natural_language_inference/16/triples/model.txt
Model,propose,bilateral multi-perspective matching ( BiMPM ) model,model,/content/training-data/natural_language_inference/16/triples/model.txt
bilateral multi-perspective matching ( BiMPM ) model,for,NLSM tasks,model,/content/training-data/natural_language_inference/16/triples/model.txt
Contribution,has research problem,Natural language sentence matching,research-problem,/content/training-data/natural_language_inference/16/triples/research-problem.txt
Contribution,has research problem,Natural language sentence matching ( NLSM ,research-problem,/content/training-data/natural_language_inference/16/triples/research-problem.txt
Contribution,has research problem,NLSM,research-problem,/content/training-data/natural_language_inference/16/triples/research-problem.txt
Contribution,has,Experiments,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
Experiments,has,Tasks,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
Tasks,has,HYPERNYM PREDICTION,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
HYPERNYM PREDICTION,has,Hyperparameters,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
Hyperparameters,set,dimension,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
dimension,of,margin,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
margin,to,0.05,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
dimension,of,embedding space and the GRU hidden state N,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
embedding space and the GRU hidden state N,to,1024,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
dimension,of,learned word embeddings,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
learned word embeddings,to,300,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
Hyperparameters,use,standard pairwise ranking objective,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
Hyperparameters,draw,contrastive terms,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
contrastive terms,from,minibatch,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
contrastive terms,giving,127 contrastive images,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
127 contrastive images,for,each caption,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
contrastive terms,giving,captions,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
captions,for,each image,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
Hyperparameters,constrain,caption and image embeddings,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
caption and image embeddings,to have,unit L2 norm,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
Hyperparameters,sample,minibatches,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
minibatches,of,128 random image - caption pairs,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
Hyperparameters,train for,15 - 30 epochs,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
15 - 30 epochs,with,learning rate 0.001,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
15 - 30 epochs,with,early stopping,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
early stopping,on,validation set,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
15 - 30 epochs,using,Adam optimizer,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
HYPERNYM PREDICTION,has,Results,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
Results,see that,order- embeddings,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
order- embeddings,has,outperform,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
outperform,not using,external text corpora,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
outperform,has,skipthought baseline,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
Tasks,has,TEXTUAL ENTAILMENT / NATURAL LANGUAGE INFERENCE,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
TEXTUAL ENTAILMENT / NATURAL LANGUAGE INFERENCE,has,Hyperparameters,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
Hyperparameters,set,dimension,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
dimension,of,word embeddings,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
word embeddings,to be,300,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
Hyperparameters,set,dimensions,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
dimensions,of,embedding space and GRU hidden state,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
embedding space and GRU hidden state,to be,1024,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
Hyperparameters,use,Adam optimizer,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
Adam optimizer,with,learning rate 0.001,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
Adam optimizer,with,early stopping,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
early stopping,on,validation set,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
Hyperparameters,constrain,embeddings,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
embeddings,to have,unit L2 norm,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
Hyperparameters,train for,10 epochs,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
10 epochs,with,batches,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
batches,of,128 sentence pairs,experiments,/content/training-data/natural_language_inference/57/triples/experiments.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/57/triples/model.txt
Model,exploit,partial order structure,model,/content/training-data/natural_language_inference/57/triples/model.txt
partial order structure,by learning,mapping,model,/content/training-data/natural_language_inference/57/triples/model.txt
mapping,call,embeddings,model,/content/training-data/natural_language_inference/57/triples/model.txt
embeddings,learned,order- embeddings,model,/content/training-data/natural_language_inference/57/triples/model.txt
mapping,not,distance - preserving,model,/content/training-data/natural_language_inference/57/triples/model.txt
distance - preserving,but,order - preserving,model,/content/training-data/natural_language_inference/57/triples/model.txt
mapping,between,visualsemantic hierarchy,model,/content/training-data/natural_language_inference/57/triples/model.txt
mapping,between,partial order,model,/content/training-data/natural_language_inference/57/triples/model.txt
partial order,over,embedding space,model,/content/training-data/natural_language_inference/57/triples/model.txt
partial order structure,of,visual - semantic hierarchy,model,/content/training-data/natural_language_inference/57/triples/model.txt
Contribution,has research problem,ORDER - EMBEDDINGS OF IMAGES AND LANGUAGE,research-problem,/content/training-data/natural_language_inference/57/triples/research-problem.txt
Contribution,has research problem,"single visual - semantic hierarchy over words , sentences , and images",research-problem,/content/training-data/natural_language_inference/57/triples/research-problem.txt
Contribution,has research problem,visualsemantic hierarchy,research-problem,/content/training-data/natural_language_inference/57/triples/research-problem.txt
Contribution,has,Experiments,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Experiments,has,Tasks,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Tasks,has,Graph Reachability,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Graph Reachability,has,Experimental setup,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Experimental setup,use,AdaDelta optimizer,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
AdaDelta optimizer,with,initial learning rate,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
initial learning rate,of,0.5,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
AdaDelta optimizer,with,batch size,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
batch size,of,32,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
AdaDelta optimizer,for,parameter optimization,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Experimental setup,has,maximum reasoning step T max,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
maximum reasoning step T max,set to,15 and 25,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
15 and 25,for,small graph and large graph dataset,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Experimental setup,has,Embedding Layer,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Embedding Layer,use,100 - dimensional embedding vector,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
100 - dimensional embedding vector,for,each symbol,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
each symbol,in,query and graph description,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Graph Reachability,has,Results,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Results,has,Deep LSTM Reader,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Deep LSTM Reader,achieves,90.92 % and 71.55 % accuracy,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
90.92 % and 71.55 % accuracy,in,small and large graph dataset,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Tasks,has,CNN and Daily Mail Datasets,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
CNN and Daily Mail Datasets,has,Experimental setup,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Experimental setup,use,ADAM optimizer,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
ADAM optimizer,with,initial learning rate,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
initial learning rate,of,"0.0005 , ? 1 = 0.9 and ? 2 = 0.999",experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
ADAM optimizer,for,parameter optimization,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Experimental setup,has,Vocab Size,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Vocab Size,keep,most frequent | V | = 101 k words,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
most frequent | V | = 101 k words,in,CNN dataset,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Vocab Size,keep,| V | = 151 k words,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
| V | = 151 k words,in,Daily Mail dataset,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Experimental setup,has,absolute value,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
absolute value,of,gradient,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
gradient,is,clipped,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
clipped,within,0.001,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
gradient,on,each parameter,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Experimental setup,has,batch size,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
batch size,is,64,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
64,for,both CNN and Daily Mail datasets,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Experimental setup,has,Embedding Layer,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Embedding Layer,apply,dropout,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
dropout,with,probability 0.2,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
probability 0.2,to,embedding layer,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Embedding Layer,use,300 - dimensional pretrained Glove word embeddings,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
300 - dimensional pretrained Glove word embeddings,for,initialization,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Embedding Layer,choose,300 - dimensional word embeddings,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Experimental setup,trained on,GTX TitanX 12 GB,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
CNN and Daily Mail Datasets,has,Results,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Results,Comparing with,AS Reader,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
AS Reader,has,ReasoNet,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
ReasoNet,shows,signi cant improvement,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
signi cant improvement,by capturing,reasoning,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
reasoning,in,paragraph,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Results,has,ReasoNet,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
ReasoNet,obtains,comparable results,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
comparable results,with,AoA Reader,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
comparable results,on,CNN test set,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Results,has,"Iterative Attention Reader , EpiReader and GA Reader",experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
"Iterative Attention Reader , EpiReader and GA Reader",are,three multi-turn reasoning models,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
three multi-turn reasoning models,with,xed reasoning steps,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
"Iterative Attention Reader , EpiReader and GA Reader",has,outperforms,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
outperforms,by integrating,termination gate,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
termination gate,allows,di erent reasoning steps,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
di erent reasoning steps,for,di erent test cases,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
termination gate,in,model,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
outperforms,has,ReasoNet,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Tasks,has,SQuAD Dataset,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
SQuAD Dataset,has,Experimental setup,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Experimental setup,use,AdaDelta optimizer,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
AdaDelta optimizer,for,parameter optimization,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
parameter optimization,with,initial learning rate,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
initial learning rate,of,0.5,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Experimental setup,has,maximum reasoning step T max,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
maximum reasoning step T max,set to,10,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Experimental setup,has,Vocab Size,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Vocab Size,use,python NLTK tokenizer,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
python NLTK tokenizer,to preprocess,passages and questions,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
passages and questions,obtain about,100K words,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
100K words,in,vocabulary,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Experimental setup,has,Embedding Layer,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Embedding Layer,use,100 - dimensional pretrained Glove vectors,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
100 - dimensional pretrained Glove vectors,as,word embeddings,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
SQuAD Dataset,has,Results,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Results,compare,ReasoNet,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
ReasoNet,exceeds,BiDAF,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
ReasoNet,both in,single model and ensemble model cases,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Results,demonstrate,ReasoNet,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
ReasoNet,has,outperforms,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
outperforms,has,all existing published approaches,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Results,has,ReasoNet,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
ReasoNet,holds,second position,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
second position,in,all the competing approaches,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
all the competing approaches,in,SQuAD leaderboard,experiments,/content/training-data/natural_language_inference/58/triples/experiments.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/58/triples/model.txt
Model,propose,novel neural network architecture,model,/content/training-data/natural_language_inference/58/triples/model.txt
novel neural network architecture,called,Reasoning Network ( ReasoNet ,model,/content/training-data/natural_language_inference/58/triples/model.txt
Model,With,question,model,/content/training-data/natural_language_inference/58/triples/model.txt
question,in,mind,model,/content/training-data/natural_language_inference/58/triples/model.txt
mind,has,ReasoNets,model,/content/training-data/natural_language_inference/58/triples/model.txt
ReasoNets,proposing,reinforcement learning approach,model,/content/training-data/natural_language_inference/58/triples/model.txt
reinforcement learning approach,utilizes,instance - dependent reward baseline,model,/content/training-data/natural_language_inference/58/triples/model.txt
instance - dependent reward baseline,to successfully train,ReasoNets,model,/content/training-data/natural_language_inference/58/triples/model.txt
ReasoNets,read,document,model,/content/training-data/natural_language_inference/58/triples/model.txt
document,has,repeatedly,model,/content/training-data/natural_language_inference/58/triples/model.txt
document,focusing on,di erent parts,model,/content/training-data/natural_language_inference/58/triples/model.txt
di erent parts,of,document,model,/content/training-data/natural_language_inference/58/triples/model.txt
di erent parts,until,satisfying answer,model,/content/training-data/natural_language_inference/58/triples/model.txt
satisfying answer,is,found or formed,model,/content/training-data/natural_language_inference/58/triples/model.txt
ReasoNets,introduce,termination state,model,/content/training-data/natural_language_inference/58/triples/model.txt
termination state,in,inference,model,/content/training-data/natural_language_inference/58/triples/model.txt
Contribution,has research problem,Machine Comprehension,research-problem,/content/training-data/natural_language_inference/58/triples/research-problem.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/63/triples/ablation-analysis.txt
Ablation analysis,add,ELMo,ablation-analysis,/content/training-data/natural_language_inference/63/triples/ablation-analysis.txt
ELMo,helped,our model ( MAMCN + ELMo ,ablation-analysis,/content/training-data/natural_language_inference/63/triples/ablation-analysis.txt
our model ( MAMCN + ELMo ),to improve,EM,ablation-analysis,/content/training-data/natural_language_inference/63/triples/ablation-analysis.txt
EM,to,77.44,ablation-analysis,/content/training-data/natural_language_inference/63/triples/ablation-analysis.txt
our model ( MAMCN + ELMo ),to improve,F1,ablation-analysis,/content/training-data/natural_language_inference/63/triples/ablation-analysis.txt
F1,to,85.13,ablation-analysis,/content/training-data/natural_language_inference/63/triples/ablation-analysis.txt
our model ( MAMCN + ELMo ),is,best,ablation-analysis,/content/training-data/natural_language_inference/63/triples/ablation-analysis.txt
best,among,models,ablation-analysis,/content/training-data/natural_language_inference/63/triples/ablation-analysis.txt
models,only with,additional feature augmentation,ablation-analysis,/content/training-data/natural_language_inference/63/triples/ablation-analysis.txt
Ablation analysis,replace,BiGRU units,ablation-analysis,/content/training-data/natural_language_inference/63/triples/ablation-analysis.txt
BiGRU units,with,embedding block,ablation-analysis,/content/training-data/natural_language_inference/63/triples/ablation-analysis.txt
embedding block,in,our model ( MAMCN + ELMo + DC ,ablation-analysis,/content/training-data/natural_language_inference/63/triples/ablation-analysis.txt
our model ( MAMCN + ELMo + DC ),achieve,state of the art performance,ablation-analysis,/content/training-data/natural_language_inference/63/triples/ablation-analysis.txt
state of the art performance,has,86.73 F1 and 79.69 EM,ablation-analysis,/content/training-data/natural_language_inference/63/triples/ablation-analysis.txt
embedding block,except,controller layer,ablation-analysis,/content/training-data/natural_language_inference/63/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/63/triples/model.txt
Model,build,QA model,model,/content/training-data/natural_language_inference/63/triples/model.txt
QA model,understand,long documents,model,/content/training-data/natural_language_inference/63/triples/model.txt
long documents,by utilizing,Memory Augmented Neural Networks ( MANNs ,model,/content/training-data/natural_language_inference/63/triples/model.txt
Model,decouples,memory capacity,model,/content/training-data/natural_language_inference/63/triples/model.txt
memory capacity,from,number of model parameters,model,/content/training-data/natural_language_inference/63/triples/model.txt
Contribution,has research problem,Machine Reading Comprehension,research-problem,/content/training-data/natural_language_inference/63/triples/research-problem.txt
Contribution,has research problem,Reading Comprehension ( RC ,research-problem,/content/training-data/natural_language_inference/63/triples/research-problem.txt
Contribution,has research problem,RC,research-problem,/content/training-data/natural_language_inference/63/triples/research-problem.txt
Contribution,has research problem,answer span prediction style Question Answering ( QA ,research-problem,/content/training-data/natural_language_inference/63/triples/research-problem.txt
Contribution,has research problem,QA,research-problem,/content/training-data/natural_language_inference/63/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/63/triples/experimental-setup.txt
Experimental setup,In,memory controller,experimental-setup,/content/training-data/natural_language_inference/63/triples/experimental-setup.txt
memory controller,use,1,experimental-setup,/content/training-data/natural_language_inference/63/triples/experimental-setup.txt
1,has,write head,experimental-setup,/content/training-data/natural_language_inference/63/triples/experimental-setup.txt
memory controller,use,4,experimental-setup,/content/training-data/natural_language_inference/63/triples/experimental-setup.txt
4,has,read heads,experimental-setup,/content/training-data/natural_language_inference/63/triples/experimental-setup.txt
memory controller,use,100 x 36 size memory,experimental-setup,/content/training-data/natural_language_inference/63/triples/experimental-setup.txt
100 x 36 size memory,initialized with,zeros,experimental-setup,/content/training-data/natural_language_inference/63/triples/experimental-setup.txt
Experimental setup,For,word - level embedding,experimental-setup,/content/training-data/natural_language_inference/63/triples/experimental-setup.txt
word - level embedding,using,NLTK toolkit,experimental-setup,/content/training-data/natural_language_inference/63/triples/experimental-setup.txt
word - level embedding,tokenize,documents,experimental-setup,/content/training-data/natural_language_inference/63/triples/experimental-setup.txt
word - level embedding,substitute,words,experimental-setup,/content/training-data/natural_language_inference/63/triples/experimental-setup.txt
words,with,GloVe 6B,experimental-setup,/content/training-data/natural_language_inference/63/triples/experimental-setup.txt
Experimental setup,develop,MAMCN,experimental-setup,/content/training-data/natural_language_inference/63/triples/experimental-setup.txt
MAMCN,using,Tensorflow 1 deep learning framework,experimental-setup,/content/training-data/natural_language_inference/63/triples/experimental-setup.txt
MAMCN,using,Sonnet 2 library,experimental-setup,/content/training-data/natural_language_inference/63/triples/experimental-setup.txt
Experimental setup,has,optimizer,experimental-setup,/content/training-data/natural_language_inference/63/triples/experimental-setup.txt
optimizer,is,"AdaDelta ( Zeiler , 2012 ",experimental-setup,/content/training-data/natural_language_inference/63/triples/experimental-setup.txt
"AdaDelta ( Zeiler , 2012 )",with,initial learning rate,experimental-setup,/content/training-data/natural_language_inference/63/triples/experimental-setup.txt
initial learning rate,of,0.5,experimental-setup,/content/training-data/natural_language_inference/63/triples/experimental-setup.txt
Experimental setup,has,batch size,experimental-setup,/content/training-data/natural_language_inference/63/triples/experimental-setup.txt
batch size,set to,30,experimental-setup,/content/training-data/natural_language_inference/63/triples/experimental-setup.txt
Experimental setup,During,training,experimental-setup,/content/training-data/natural_language_inference/63/triples/experimental-setup.txt
training,keep,exponential moving average,experimental-setup,/content/training-data/natural_language_inference/63/triples/experimental-setup.txt
exponential moving average,with,0.001 decay,experimental-setup,/content/training-data/natural_language_inference/63/triples/experimental-setup.txt
exponential moving average,use,averages,experimental-setup,/content/training-data/natural_language_inference/63/triples/experimental-setup.txt
averages,at,test time,experimental-setup,/content/training-data/natural_language_inference/63/triples/experimental-setup.txt
exponential moving average,of,weights,experimental-setup,/content/training-data/natural_language_inference/63/triples/experimental-setup.txt
Experimental setup,train,model,experimental-setup,/content/training-data/natural_language_inference/63/triples/experimental-setup.txt
model,for,12 epochs,experimental-setup,/content/training-data/natural_language_inference/63/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/63/triples/results.txt
Results,has,TriviaQA,results,/content/training-data/natural_language_inference/63/triples/results.txt
TriviaQA,has,Our model,results,/content/training-data/natural_language_inference/63/triples/results.txt
Our model,achieves,state of the art performance,results,/content/training-data/natural_language_inference/63/triples/results.txt
Results,has,QUASAR - T,results,/content/training-data/natural_language_inference/63/triples/results.txt
QUASAR - T,has,our proposed memory controller,results,/content/training-data/natural_language_inference/63/triples/results.txt
our proposed memory controller,gives,more performance improvement,results,/content/training-data/natural_language_inference/63/triples/results.txt
more performance improvement,achieve,68.13 EM and 70.32 F1,results,/content/training-data/natural_language_inference/63/triples/results.txt
68.13 EM and 70.32 F1,for,short documents,results,/content/training-data/natural_language_inference/63/triples/results.txt
more performance improvement,achieve,63.44 and 65.19,results,/content/training-data/natural_language_inference/63/triples/results.txt
63.44 and 65.19,for,long documents,results,/content/training-data/natural_language_inference/63/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/natural_language_inference/3/triples/baselines.txt
Baselines,has,Parallel LSTMs,baselines,/content/training-data/natural_language_inference/3/triples/baselines.txt
Parallel LSTMs,has,Two sequences,baselines,/content/training-data/natural_language_inference/3/triples/baselines.txt
Two sequences,encoded by,two LSTMs separately,baselines,/content/training-data/natural_language_inference/3/triples/baselines.txt
Two sequences,are,concatenated and fed,baselines,/content/training-data/natural_language_inference/3/triples/baselines.txt
concatenated and fed,to,MLP,baselines,/content/training-data/natural_language_inference/3/triples/baselines.txt
Baselines,has,Neural bag - of - words ( NBOW ,baselines,/content/training-data/natural_language_inference/3/triples/baselines.txt
Neural bag - of - words ( NBOW ),has,Each sequence,baselines,/content/training-data/natural_language_inference/3/triples/baselines.txt
Each sequence,has,concatenated and fed,baselines,/content/training-data/natural_language_inference/3/triples/baselines.txt
concatenated and fed,to,MLP,baselines,/content/training-data/natural_language_inference/3/triples/baselines.txt
Each sequence,sum of,embeddings,baselines,/content/training-data/natural_language_inference/3/triples/baselines.txt
embeddings,of,words it contains,baselines,/content/training-data/natural_language_inference/3/triples/baselines.txt
Baselines,has,Attention LSTMs,baselines,/content/training-data/natural_language_inference/3/triples/baselines.txt
Attention LSTMs,has,attentive LSTM,baselines,/content/training-data/natural_language_inference/3/triples/baselines.txt
attentive LSTM,to encode,two sentences,baselines,/content/training-data/natural_language_inference/3/triples/baselines.txt
two sentences,into,semantic space,baselines,/content/training-data/natural_language_inference/3/triples/baselines.txt
Baselines,has,Single LSTM,baselines,/content/training-data/natural_language_inference/3/triples/baselines.txt
Single LSTM,encode,two sequences,baselines,/content/training-data/natural_language_inference/3/triples/baselines.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/natural_language_inference/3/triples/hyperparameters.txt
Hyperparameters,For,each task,hyperparameters,/content/training-data/natural_language_inference/3/triples/hyperparameters.txt
each task,take,hyperparameters,hyperparameters,/content/training-data/natural_language_inference/3/triples/hyperparameters.txt
hyperparameters,achieve,best performance,hyperparameters,/content/training-data/natural_language_inference/3/triples/hyperparameters.txt
best performance,on,development set,hyperparameters,/content/training-data/natural_language_inference/3/triples/hyperparameters.txt
best performance,via,small grid search,hyperparameters,/content/training-data/natural_language_inference/3/triples/hyperparameters.txt
small grid search,over,combinations,hyperparameters,/content/training-data/natural_language_inference/3/triples/hyperparameters.txt
combinations,of,"initial learning rate [ 0.05 , 0.0005 , 0.0001 ]",hyperparameters,/content/training-data/natural_language_inference/3/triples/hyperparameters.txt
combinations,of,"l 2 regularization [ 0.0 , 5 E? 5 , 1E? 5 , 1E? 6 ]",hyperparameters,/content/training-data/natural_language_inference/3/triples/hyperparameters.txt
combinations,of,threshold value,hyperparameters,/content/training-data/natural_language_inference/3/triples/hyperparameters.txt
Hyperparameters,has,other parameters,hyperparameters,/content/training-data/natural_language_inference/3/triples/hyperparameters.txt
other parameters,initialized by,randomly sampling,hyperparameters,/content/training-data/natural_language_inference/3/triples/hyperparameters.txt
randomly sampling,from,uniform distribution,hyperparameters,/content/training-data/natural_language_inference/3/triples/hyperparameters.txt
uniform distribution,in,"[ ? 0.1 , 0.1 ]",hyperparameters,/content/training-data/natural_language_inference/3/triples/hyperparameters.txt
Hyperparameters,has,word embeddings,hyperparameters,/content/training-data/natural_language_inference/3/triples/hyperparameters.txt
word embeddings,for,all of the models,hyperparameters,/content/training-data/natural_language_inference/3/triples/hyperparameters.txt
word embeddings,fine - tuned during,training,hyperparameters,/content/training-data/natural_language_inference/3/triples/hyperparameters.txt
training,to improve,performance,hyperparameters,/content/training-data/natural_language_inference/3/triples/hyperparameters.txt
word embeddings,initialized with,100d GloVe vectors,hyperparameters,/content/training-data/natural_language_inference/3/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/3/triples/model.txt
Model,utilize,two interdependent LSTMs,model,/content/training-data/natural_language_inference/3/triples/model.txt
two interdependent LSTMs,called,coupled - LSTMs,model,/content/training-data/natural_language_inference/3/triples/model.txt
two interdependent LSTMs,to fully affect,each other,model,/content/training-data/natural_language_inference/3/triples/model.txt
each other,at,different time steps,model,/content/training-data/natural_language_inference/3/triples/model.txt
Model,utilize,all the information,model,/content/training-data/natural_language_inference/3/triples/model.txt
all the information,feed them into,fully connected layer,model,/content/training-data/natural_language_inference/3/triples/model.txt
fully connected layer,followed by,output layer,model,/content/training-data/natural_language_inference/3/triples/model.txt
output layer,to compute,matching score,model,/content/training-data/natural_language_inference/3/triples/model.txt
all the information,adopt,dynamic pooling strategy,model,/content/training-data/natural_language_inference/3/triples/model.txt
dynamic pooling strategy,to automatically select,most informative interaction signals,model,/content/training-data/natural_language_inference/3/triples/model.txt
all the information,of,four directions,model,/content/training-data/natural_language_inference/3/triples/model.txt
four directions,of,coupled - LSTMs,model,/content/training-data/natural_language_inference/3/triples/model.txt
all the information,has,aggregate,model,/content/training-data/natural_language_inference/3/triples/model.txt
Model,has,output,model,/content/training-data/natural_language_inference/3/triples/model.txt
output,of,coupled - LSTMs,model,/content/training-data/natural_language_inference/3/triples/model.txt
coupled - LSTMs,at,each step,model,/content/training-data/natural_language_inference/3/triples/model.txt
coupled - LSTMs,depends on,both sentences,model,/content/training-data/natural_language_inference/3/triples/model.txt
Model,propose,new deep neural network architecture,model,/content/training-data/natural_language_inference/3/triples/model.txt
new deep neural network architecture,to model,strong interactions,model,/content/training-data/natural_language_inference/3/triples/model.txt
strong interactions,of,two sentences,model,/content/training-data/natural_language_inference/3/triples/model.txt
Model,propose,two interdependent ways,model,/content/training-data/natural_language_inference/3/triples/model.txt
two interdependent ways,for,coupled - LSTMs,model,/content/training-data/natural_language_inference/3/triples/model.txt
coupled - LSTMs,name,loosely coupled model ( LC - LSTMs ,model,/content/training-data/natural_language_inference/3/triples/model.txt
coupled - LSTMs,name,tightly coupled model ( TC - LSTMs ,model,/content/training-data/natural_language_inference/3/triples/model.txt
Contribution,has research problem,Modelling Interaction of Sentence Pair,research-problem,/content/training-data/natural_language_inference/3/triples/research-problem.txt
Contribution,has research problem,modelling the interactions of two sentences,research-problem,/content/training-data/natural_language_inference/3/triples/research-problem.txt
Contribution,has research problem,modelling the relevance / similarity of the sentence pair,research-problem,/content/training-data/natural_language_inference/3/triples/research-problem.txt
Contribution,has research problem,text semantic matching,research-problem,/content/training-data/natural_language_inference/3/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/3/triples/results.txt
Results,has,Experiment - I : Recognizing Textual Entailment,results,/content/training-data/natural_language_inference/3/triples/results.txt
Experiment - I : Recognizing Textual Entailment,has,proposed two C - LSTMs models with four stacked blocks,results,/content/training-data/natural_language_inference/3/triples/results.txt
proposed two C - LSTMs models with four stacked blocks,outperform,all the competitor models,results,/content/training-data/natural_language_inference/3/triples/results.txt
proposed two C - LSTMs models with four stacked blocks,indicates that,our thinner and deeper network,results,/content/training-data/natural_language_inference/3/triples/results.txt
our thinner and deeper network,has,does work effectively,results,/content/training-data/natural_language_inference/3/triples/results.txt
Experiment - I : Recognizing Textual Entailment,Compared with,attention LSTMs,results,/content/training-data/natural_language_inference/3/triples/results.txt
attention LSTMs,has,our two models,results,/content/training-data/natural_language_inference/3/triples/results.txt
our two models,using,much fewer parameters ( nearly 1 / 5 ,results,/content/training-data/natural_language_inference/3/triples/results.txt
our two models,achieve,comparable results,results,/content/training-data/natural_language_inference/3/triples/results.txt
Experiment - I : Recognizing Textual Entailment,By stacking,C - LSTMs,results,/content/training-data/natural_language_inference/3/triples/results.txt
C - LSTMs,has,performance,results,/content/training-data/natural_language_inference/3/triples/results.txt
performance,improved,significantly,results,/content/training-data/natural_language_inference/3/triples/results.txt
Contribution,has,Approach,approach,/content/training-data/natural_language_inference/39/triples/approach.txt
Approach,based on,pairwise word interactions,approach,/content/training-data/natural_language_inference/39/triples/approach.txt
pairwise word interactions,describe,novel similarity focus layer,approach,/content/training-data/natural_language_inference/39/triples/approach.txt
novel similarity focus layer,which,helps,approach,/content/training-data/natural_language_inference/39/triples/approach.txt
helps,has,model,approach,/content/training-data/natural_language_inference/39/triples/approach.txt
model,selectively identify,important word interactions,approach,/content/training-data/natural_language_inference/39/triples/approach.txt
important word interactions,for,similarity measurement,approach,/content/training-data/natural_language_inference/39/triples/approach.txt
Approach,focus on,capturing,approach,/content/training-data/natural_language_inference/39/triples/approach.txt
capturing,has,fine - grained word - level information,approach,/content/training-data/natural_language_inference/39/triples/approach.txt
Approach,instead of using,sentence modeling,approach,/content/training-data/natural_language_inference/39/triples/approach.txt
sentence modeling,propose,pairwise word interaction modeling,approach,/content/training-data/natural_language_inference/39/triples/approach.txt
pairwise word interaction modeling,that encourages,explicit word context interactions,approach,/content/training-data/natural_language_inference/39/triples/approach.txt
explicit word context interactions,across,sentences,approach,/content/training-data/natural_language_inference/39/triples/approach.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/39/triples/ablation-analysis.txt
Ablation analysis,found,large drops,ablation-analysis,/content/training-data/natural_language_inference/39/triples/ablation-analysis.txt
large drops,removing,context modeling component,ablation-analysis,/content/training-data/natural_language_inference/39/triples/ablation-analysis.txt
Ablation analysis,use,similarity focus layer,ablation-analysis,/content/training-data/natural_language_inference/39/triples/ablation-analysis.txt
similarity focus layer,is,beneficial,ablation-analysis,/content/training-data/natural_language_inference/39/triples/ablation-analysis.txt
beneficial,on,WikiQA data,ablation-analysis,/content/training-data/natural_language_inference/39/triples/ablation-analysis.txt
Ablation analysis,replaced,entire similarity focus layer,ablation-analysis,/content/training-data/natural_language_inference/39/triples/ablation-analysis.txt
entire similarity focus layer,with,random dropout layer ( p = 0.3 ,ablation-analysis,/content/training-data/natural_language_inference/39/triples/ablation-analysis.txt
random dropout layer ( p = 0.3 ),has,hurts,ablation-analysis,/content/training-data/natural_language_inference/39/triples/ablation-analysis.txt
hurts,has,accuracy,ablation-analysis,/content/training-data/natural_language_inference/39/triples/ablation-analysis.txt
Contribution,has research problem,Semantic Similarity Measurement,research-problem,/content/training-data/natural_language_inference/39/triples/research-problem.txt
Contribution,has research problem,Textual similarity measurement,research-problem,/content/training-data/natural_language_inference/39/triples/research-problem.txt
Contribution,has research problem,semantic textual similarity ( STS ,research-problem,/content/training-data/natural_language_inference/39/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/39/triples/experimental-setup.txt
Experimental setup,For,SICK and MSRVID experiments,experimental-setup,/content/training-data/natural_language_inference/39/triples/experimental-setup.txt
SICK and MSRVID experiments,used,300 - dimension,experimental-setup,/content/training-data/natural_language_inference/39/triples/experimental-setup.txt
300 - dimension,has,Glo Ve word embeddings,experimental-setup,/content/training-data/natural_language_inference/39/triples/experimental-setup.txt
Experimental setup,For,"STS2014 , WikiQA , and TrecQA experiments",experimental-setup,/content/training-data/natural_language_inference/39/triples/experimental-setup.txt
"STS2014 , WikiQA , and TrecQA experiments",trained on,word pairs,experimental-setup,/content/training-data/natural_language_inference/39/triples/experimental-setup.txt
word pairs,from,Paraphrase Database ( PPDB ,experimental-setup,/content/training-data/natural_language_inference/39/triples/experimental-setup.txt
Paraphrase Database ( PPDB ),used,300 dimension,experimental-setup,/content/training-data/natural_language_inference/39/triples/experimental-setup.txt
300 dimension,has,PARAGRAM - SL999 embeddings,experimental-setup,/content/training-data/natural_language_inference/39/triples/experimental-setup.txt
300 dimension,has,PARAGRAM - PHRASE embeddings,experimental-setup,/content/training-data/natural_language_inference/39/triples/experimental-setup.txt
Experimental setup,has,Our timing experiments,experimental-setup,/content/training-data/natural_language_inference/39/triples/experimental-setup.txt
Our timing experiments,conducted on,Intel Xeon E5 - 2680 CPU,experimental-setup,/content/training-data/natural_language_inference/39/triples/experimental-setup.txt
Experimental setup,Due to,sentence length variations,experimental-setup,/content/training-data/natural_language_inference/39/triples/experimental-setup.txt
sentence length variations,for,SICK and MSRVID data,experimental-setup,/content/training-data/natural_language_inference/39/triples/experimental-setup.txt
SICK and MSRVID data,padded,sentences,experimental-setup,/content/training-data/natural_language_inference/39/triples/experimental-setup.txt
sentences,to,32 words,experimental-setup,/content/training-data/natural_language_inference/39/triples/experimental-setup.txt
sentence length variations,for,"STS2014 , WikiQA , and TrecQA data",experimental-setup,/content/training-data/natural_language_inference/39/triples/experimental-setup.txt
"STS2014 , WikiQA , and TrecQA data",padded,sentences,experimental-setup,/content/training-data/natural_language_inference/39/triples/experimental-setup.txt
sentences,to,48 words,experimental-setup,/content/training-data/natural_language_inference/39/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/39/triples/results.txt
Results,has,Wiki QA,results,/content/training-data/natural_language_inference/39/triples/results.txt
Wiki QA,has,"paragraph vector ( PV ) , CNN , and PV - Cnt / CNN - Cnt with word matching features",results,/content/training-data/natural_language_inference/39/triples/results.txt
"paragraph vector ( PV ) , CNN , and PV - Cnt / CNN - Cnt with word matching features",has,Our model,results,/content/training-data/natural_language_inference/39/triples/results.txt
Our model,has,outperforms,results,/content/training-data/natural_language_inference/39/triples/results.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/77/triples/model.txt
Model,namely,FastQA,model,/content/training-data/natural_language_inference/77/triples/model.txt
FastQA,develop,"neural , bag - of - words ( BoW ",model,/content/training-data/natural_language_inference/77/triples/model.txt
FastQA,develop,recurrent neural network ( RNN ,model,/content/training-data/natural_language_inference/77/triples/model.txt
Model,model,interaction,model,/content/training-data/natural_language_inference/77/triples/model.txt
interaction,between,question and context,model,/content/training-data/natural_language_inference/77/triples/model.txt
question and context,through,computable features,model,/content/training-data/natural_language_inference/77/triples/model.txt
computable features,on,word level,model,/content/training-data/natural_language_inference/77/triples/model.txt
Contribution,has research problem,Neural QA,research-problem,/content/training-data/natural_language_inference/77/triples/research-problem.txt
Contribution,has research problem,question answering ( QA ,research-problem,/content/training-data/natural_language_inference/77/triples/research-problem.txt
Contribution,has research problem,QA,research-problem,/content/training-data/natural_language_inference/77/triples/research-problem.txt
Contribution,has research problem,Question answering,research-problem,/content/training-data/natural_language_inference/77/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
Experimental setup,has,BoW Model,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
BoW Model,As,pre-processing steps,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
pre-processing steps,has,lowercase,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
lowercase,has,all inputs,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
pre-processing steps,has,tokenize,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
tokenize,using,spacy,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
BoW Model,use,hidden dimensionality,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
hidden dimensionality,of,fixed word - embeddings,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
fixed word - embeddings,has,300 - dimensional,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
300 - dimensional,from,Glove,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
hidden dimensionality,of,dropout,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
dropout,with,same mask,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
same mask,for,all words,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
dropout,at,input embeddings,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
dropout,has,rate,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
rate,of,0.2,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
hidden dimensionality,of,n,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
n,=,150,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
BoW Model,trained on,spans,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
spans,up to length,10,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
BoW Model,has,binary word,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
binary word,in,question feature,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
binary word,computed on,lemmas,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
lemmas,provided by,spacy,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
lemmas,restricted to,alphanumeric words,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
alphanumeric words,are not,stopwords,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
BoW Model,used,mini-batches,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
mini-batches,of,size,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
size,has,32,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
BoW Model,employed,ADAM,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
ADAM,with,initial learning - rate,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
initial learning - rate,of,10 ?3,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
ADAM,for,optimization,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
Experimental setup,has,FastQA,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
FastQA,use,hidden dimensionality,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
hidden dimensionality,of,n,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
n,=,300,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
FastQA,use,variational dropout,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
variational dropout,with,same mask,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
same mask,for,all words,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
variational dropout,at,input embeddings,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
variational dropout,has,rate,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
rate,of,0.5,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
variational dropout,has,fixed word - embeddings,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
fixed word - embeddings,has,300 dimensional,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
300 dimensional,from,Glove,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
FastQA,has,binary word,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
binary word,in,question feature,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
binary word,computed on,words,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
words,appear in,context,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
FastQA,tokenize,input,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
input,on,whitespaces,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
input,on,non-alphanumeric characters,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
FastQA,employed,ADAM,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
ADAM,with,initial learning - rate,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
initial learning - rate,of,10 ?3,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
ADAM,for,optimization,experimental-setup,/content/training-data/natural_language_inference/77/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/77/triples/results.txt
Results,has,nearly reaches,results,/content/training-data/natural_language_inference/77/triples/results.txt
nearly reaches,has,BiLSTM baseline system,results,/content/training-data/natural_language_inference/77/triples/results.txt
Results,has,neural BoW baseline,results,/content/training-data/natural_language_inference/77/triples/results.txt
neural BoW baseline,achieves,good results,results,/content/training-data/natural_language_inference/77/triples/results.txt
Results,has,improves,results,/content/training-data/natural_language_inference/77/triples/results.txt
improves,for,News QA,results,/content/training-data/natural_language_inference/77/triples/results.txt
Results,has,very competitive,results,/content/training-data/natural_language_inference/77/triples/results.txt
very competitive,to,previously established stateof - the - art results,results,/content/training-data/natural_language_inference/77/triples/results.txt
Results,has,outperforms,results,/content/training-data/natural_language_inference/77/triples/results.txt
outperforms,has,feature rich logistic - regression baseline,results,/content/training-data/natural_language_inference/77/triples/results.txt
feature rich logistic - regression baseline,on,SQuAD development set,results,/content/training-data/natural_language_inference/77/triples/results.txt
Contribution,has,Dataset,datase,/content/training-data/natural_language_inference/96/triples/dataset.txt
Dataset,start with,pairs,datase,/content/training-data/natural_language_inference/96/triples/dataset.txt
pairs,of,temporally adjacent video captions,datase,/content/training-data/natural_language_inference/96/triples/dataset.txt
temporally adjacent video captions,each with,context,datase,/content/training-data/natural_language_inference/96/triples/dataset.txt
temporally adjacent video captions,each with,follow - up event,datase,/content/training-data/natural_language_inference/96/triples/dataset.txt
follow - up event,know,physically possible,datase,/content/training-data/natural_language_inference/96/triples/dataset.txt
Dataset,use,state - of - theart language model,datase,/content/training-data/natural_language_inference/96/triples/dataset.txt
state - of - theart language model,to massively oversample,diverse set,datase,/content/training-data/natural_language_inference/96/triples/dataset.txt
diverse set,of,possible negative sentence endings ( or counterfactuals ,datase,/content/training-data/natural_language_inference/96/triples/dataset.txt
possible negative sentence endings ( or counterfactuals ),filter,aggressively and adversarially,datase,/content/training-data/natural_language_inference/96/triples/dataset.txt
aggressively and adversarially,using,committee of trained models,datase,/content/training-data/natural_language_inference/96/triples/dataset.txt
committee of trained models,to obtain,population,datase,/content/training-data/natural_language_inference/96/triples/dataset.txt
population,of,de-biased endings,datase,/content/training-data/natural_language_inference/96/triples/dataset.txt
de-biased endings,with,similar stylistic features,datase,/content/training-data/natural_language_inference/96/triples/dataset.txt
similar stylistic features,to,real ones,datase,/content/training-data/natural_language_inference/96/triples/dataset.txt
state - of - theart language model,fine - tuned on,data,datase,/content/training-data/natural_language_inference/96/triples/dataset.txt
Dataset,construct,Swag,datase,/content/training-data/natural_language_inference/96/triples/dataset.txt
Swag,has,adversarial dataset,datase,/content/training-data/natural_language_inference/96/triples/dataset.txt
adversarial dataset,with,113 k multiple - choice questions,datase,/content/training-data/natural_language_inference/96/triples/dataset.txt
Dataset,has,filtered counterfactuals,datase,/content/training-data/natural_language_inference/96/triples/dataset.txt
filtered counterfactuals,validated by,crowd workers,datase,/content/training-data/natural_language_inference/96/triples/dataset.txt
crowd workers,to further ensure,data quality,datase,/content/training-data/natural_language_inference/96/triples/dataset.txt
Contribution,has research problem,Grounded Commonsense Inference,research-problem,/content/training-data/natural_language_inference/96/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/96/triples/results.txt
Results,has,Further improvement,results,/content/training-data/natural_language_inference/96/triples/results.txt
Further improvement,gained from,models,results,/content/training-data/natural_language_inference/96/triples/results.txt
models,that compute,pairwise representations,results,/content/training-data/natural_language_inference/96/triples/results.txt
pairwise representations,of,inputs,results,/content/training-data/natural_language_inference/96/triples/results.txt
Results,has,best results,results,/content/training-data/natural_language_inference/96/triples/results.txt
best results,come from,pairwise NLI models,results,/content/training-data/natural_language_inference/96/triples/results.txt
pairwise NLI models,fully trained on,"Swag , ESIM + ELMo",results,/content/training-data/natural_language_inference/96/triples/results.txt
"Swag , ESIM + ELMo",obtains,59.2 % accuracy,results,/content/training-data/natural_language_inference/96/triples/results.txt
Results,has,best model,results,/content/training-data/natural_language_inference/96/triples/results.txt
best model,only uses,ending,results,/content/training-data/natural_language_inference/96/triples/results.txt
ending,is,LSTM sequence model,results,/content/training-data/natural_language_inference/96/triples/results.txt
LSTM sequence model,obtains,43.6 %,results,/content/training-data/natural_language_inference/96/triples/results.txt
LSTM sequence model,with,ELMo embeddings,results,/content/training-data/natural_language_inference/96/triples/results.txt
LSTM sequence model,has,greatly improves,results,/content/training-data/natural_language_inference/96/triples/results.txt
greatly improves,with,more context,results,/content/training-data/natural_language_inference/96/triples/results.txt
greatly improves,by,ad-ditional 4 %,results,/content/training-data/natural_language_inference/96/triples/results.txt
ad-ditional 4 %,given,first sentence,results,/content/training-data/natural_language_inference/96/triples/results.txt
greatly improves,by,3.1 %,results,/content/training-data/natural_language_inference/96/triples/results.txt
3.1 %,given,initial noun phrase,results,/content/training-data/natural_language_inference/96/triples/results.txt
Results,has,simplest such model,results,/content/training-data/natural_language_inference/96/triples/results.txt
simplest such model,obtains,only 35.1 % accuracy,results,/content/training-data/natural_language_inference/96/triples/results.txt
simplest such model,name,Dual - BoW,results,/content/training-data/natural_language_inference/96/triples/results.txt
simplest such model,combining,In - fer Sent sentence representations,results,/content/training-data/natural_language_inference/96/triples/results.txt
In - fer Sent sentence representations,name,InferSent - Bilinear,results,/content/training-data/natural_language_inference/96/triples/results.txt
In - fer Sent sentence representations,gives,40.5 % accuracy,results,/content/training-data/natural_language_inference/96/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/93/triples/ablation-analysis.txt
Ablation analysis,Removing,self - matching,ablation-analysis,/content/training-data/natural_language_inference/93/triples/ablation-analysis.txt
self - matching,results in,3.5 point EM drop,ablation-analysis,/content/training-data/natural_language_inference/93/triples/ablation-analysis.txt
Ablation analysis,has,positively contribute,ablation-analysis,/content/training-data/natural_language_inference/93/triples/ablation-analysis.txt
positively contribute,to,final results,ablation-analysis,/content/training-data/natural_language_inference/93/triples/ablation-analysis.txt
final results,of,gated self - matching networks,ablation-analysis,/content/training-data/natural_language_inference/93/triples/ablation-analysis.txt
positively contribute,has,attention - based recurrent network ( GARNN ,ablation-analysis,/content/training-data/natural_language_inference/93/triples/ablation-analysis.txt
positively contribute,has,self - matching attention mechanism,ablation-analysis,/content/training-data/natural_language_inference/93/triples/ablation-analysis.txt
Ablation analysis,has,gate,ablation-analysis,/content/training-data/natural_language_inference/93/triples/ablation-analysis.txt
gate,introduced in,question and passage matching layer,ablation-analysis,/content/training-data/natural_language_inference/93/triples/ablation-analysis.txt
question and passage matching layer,has,helpful,ablation-analysis,/content/training-data/natural_language_inference/93/triples/ablation-analysis.txt
helpful,for,GRU and LSTM,ablation-analysis,/content/training-data/natural_language_inference/93/triples/ablation-analysis.txt
Ablation analysis,has,Characterlevel embeddings,ablation-analysis,/content/training-data/natural_language_inference/93/triples/ablation-analysis.txt
Characterlevel embeddings,can,better handle,ablation-analysis,/content/training-data/natural_language_inference/93/triples/ablation-analysis.txt
better handle,has,out - ofvocab or rare words,ablation-analysis,/content/training-data/natural_language_inference/93/triples/ablation-analysis.txt
Characterlevel embeddings,contribute towards,model 's performance,ablation-analysis,/content/training-data/natural_language_inference/93/triples/ablation-analysis.txt
Ablation analysis,has,Character - level embeddings,ablation-analysis,/content/training-data/natural_language_inference/93/triples/ablation-analysis.txt
Character - level embeddings,not,utilized,ablation-analysis,/content/training-data/natural_language_inference/93/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/93/triples/model.txt
Model,introduce,gated self - matching network,model,/content/training-data/natural_language_inference/93/triples/model.txt
gated self - matching network,has,end - to - end neural network model,model,/content/training-data/natural_language_inference/93/triples/model.txt
end - to - end neural network model,for,reading comprehension and question answering,model,/content/training-data/natural_language_inference/93/triples/model.txt
Model,consists of,four parts,model,/content/training-data/natural_language_inference/93/triples/model.txt
four parts,has,recurrent network encoder,model,/content/training-data/natural_language_inference/93/triples/model.txt
recurrent network encoder,to build,representation,model,/content/training-data/natural_language_inference/93/triples/model.txt
representation,for,questions and passages,model,/content/training-data/natural_language_inference/93/triples/model.txt
four parts,has,gated matching layer,model,/content/training-data/natural_language_inference/93/triples/model.txt
gated matching layer,to match,question and passage,model,/content/training-data/natural_language_inference/93/triples/model.txt
four parts,has,self - matching layer,model,/content/training-data/natural_language_inference/93/triples/model.txt
self - matching layer,to aggregate,information,model,/content/training-data/natural_language_inference/93/triples/model.txt
information,from,whole passage,model,/content/training-data/natural_language_inference/93/triples/model.txt
four parts,has,pointernetwork based answer boundary prediction layer,model,/content/training-data/natural_language_inference/93/triples/model.txt
Contribution,has research problem,Reading Comprehension and Question Answering,research-problem,/content/training-data/natural_language_inference/93/triples/research-problem.txt
Contribution,has research problem,reading comprehension style question answering,research-problem,/content/training-data/natural_language_inference/93/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
Experimental setup,apply,dropout,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
dropout,with,dropout rate,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
dropout rate,of,0.2,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
dropout,between,layers,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
Experimental setup,use,hidden size,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
hidden size,to compute,attention scores,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
attention scores,is,75,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
Experimental setup,use,hidden vector length,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
hidden vector length,set to,75,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
75,for,all layers,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
Experimental setup,use,tokenizer,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
tokenizer,from,Stanford CoreNLP,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
tokenizer,to preprocess,each passage and question,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
Experimental setup,utilize,3 layers of bi-directional GRU,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
3 layers of bi-directional GRU,to encode,questions and passages,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
Experimental setup,utilize,gated attention - based recurrent network,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
gated attention - based recurrent network,for,question and passage matching,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
gated attention - based recurrent network,encoded,bidirectionally,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
Experimental setup,utilize,1 layer of bi-directional GRU,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
1 layer of bi-directional GRU,to compute,character - level embeddings,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
Experimental setup,For,word embedding,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
word embedding,use,zero vectors,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
zero vectors,to represent,all out - of - vocab words,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
word embedding,use,pretrained case - sensitive GloVe embeddings,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
pretrained case - sensitive GloVe embeddings,for,questions and passages,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
pretrained case - sensitive GloVe embeddings,has,fixed,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
fixed,during,training,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
Experimental setup,has,Gated Recurrent Unit variant,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
Gated Recurrent Unit variant,of,LSTM,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
Gated Recurrent Unit variant,used throughout,our model,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
Experimental setup,has,model,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
model,optimized with,"AdaDelta ( Zeiler , 2012 ",experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
"AdaDelta ( Zeiler , 2012 )",with,initial learning rate,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
initial learning rate,of,1,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
Experimental setup,used in,AdaDelta,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
AdaDelta,are,0.95 and 1e ? 6,experimental-setup,/content/training-data/natural_language_inference/93/triples/experimental-setup.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/natural_language_inference/11/triples/hyperparameters.txt
Hyperparameters,For,DQA baseline system,hyperparameters,/content/training-data/natural_language_inference/11/triples/hyperparameters.txt
DQA baseline system,add,lemma,hyperparameters,/content/training-data/natural_language_inference/11/triples/hyperparameters.txt
lemma,in,question feature ( liq ,hyperparameters,/content/training-data/natural_language_inference/11/triples/hyperparameters.txt
Hyperparameters,has,All baselines,hyperparameters,/content/training-data/natural_language_inference/11/triples/hyperparameters.txt
All baselines,operate on,unrefined word embeddings,hyperparameters,/content/training-data/natural_language_inference/11/triples/hyperparameters.txt
Hyperparameters,has,All models,hyperparameters,/content/training-data/natural_language_inference/11/triples/hyperparameters.txt
All models,trained,end - to - end,hyperparameters,/content/training-data/natural_language_inference/11/triples/hyperparameters.txt
All models,trained,jointly,hyperparameters,/content/training-data/natural_language_inference/11/triples/hyperparameters.txt
jointly,with,refinement module,hyperparameters,/content/training-data/natural_language_inference/11/triples/hyperparameters.txt
All models,trained,using a dimensionality,hyperparameters,/content/training-data/natural_language_inference/11/triples/hyperparameters.txt
using a dimensionality,of,n = 300,hyperparameters,/content/training-data/natural_language_inference/11/triples/hyperparameters.txt
n = 300,for,all but the TriviaQA experiments,hyperparameters,/content/training-data/natural_language_inference/11/triples/hyperparameters.txt
all but the TriviaQA experiments,had to reduce,n to 150,hyperparameters,/content/training-data/natural_language_inference/11/triples/hyperparameters.txt
n to 150,due to,memory constraints,hyperparameters,/content/training-data/natural_language_inference/11/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/11/triples/model.txt
Model,develop,new architecture,model,/content/training-data/natural_language_inference/11/triples/model.txt
new architecture,for dynamically incorporating,external background knowledge,model,/content/training-data/natural_language_inference/11/triples/model.txt
external background knowledge,in,NLU models,model,/content/training-data/natural_language_inference/11/triples/model.txt
Model,has,supplementary knowledge,model,/content/training-data/natural_language_inference/11/triples/model.txt
supplementary knowledge,retrieved from,external knowledge sources,model,/content/training-data/natural_language_inference/11/triples/model.txt
external knowledge sources,assist with understanding,text inputs,model,/content/training-data/natural_language_inference/11/triples/model.txt
external knowledge sources,name,ConceptNet,model,/content/training-data/natural_language_inference/11/triples/model.txt
external knowledge sources,name,Wikipedia,model,/content/training-data/natural_language_inference/11/triples/model.txt
Model,has,initial reading module and the task module,model,/content/training-data/natural_language_inference/11/triples/model.txt
initial reading module and the task module,are learnt,"jointly , end - to - end",model,/content/training-data/natural_language_inference/11/triples/model.txt
Model,has,retrieved supplementary texts,model,/content/training-data/natural_language_inference/11/triples/model.txt
retrieved supplementary texts,read together with,task inputs,model,/content/training-data/natural_language_inference/11/triples/model.txt
retrieved supplementary texts,by,initial reading module,model,/content/training-data/natural_language_inference/11/triples/model.txt
initial reading module,whose,outputs,model,/content/training-data/natural_language_inference/11/triples/model.txt
outputs,are,contextually refined word embeddings,model,/content/training-data/natural_language_inference/11/triples/model.txt
contextually refined word embeddings,used as,input,model,/content/training-data/natural_language_inference/11/triples/model.txt
input,to,task - specific NLU architecture,model,/content/training-data/natural_language_inference/11/triples/model.txt
Contribution,has research problem,Neural NLU,research-problem,/content/training-data/natural_language_inference/11/triples/research-problem.txt
Contribution,has research problem,neural natural language understanding ( NLU ,research-problem,/content/training-data/natural_language_inference/11/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/11/triples/results.txt
Results,has,RTE experiments,results,/content/training-data/natural_language_inference/11/triples/results.txt
RTE experiments,find that,little impact,results,/content/training-data/natural_language_inference/11/triples/results.txt
little impact,of using,external knowledge,results,/content/training-data/natural_language_inference/11/triples/results.txt
external knowledge,on,RTE task,results,/content/training-data/natural_language_inference/11/triples/results.txt
RTE task,with,ESIM,results,/content/training-data/natural_language_inference/11/triples/results.txt
RTE experiments,introduction of,our refinement strategy,results,/content/training-data/natural_language_inference/11/triples/results.txt
our refinement strategy,has,almost always helps,results,/content/training-data/natural_language_inference/11/triples/results.txt
almost always helps,with and without,external knowledge,results,/content/training-data/natural_language_inference/11/triples/results.txt
RTE experiments,increasing,coverage,results,/content/training-data/natural_language_inference/11/triples/results.txt
coverage,of,assertions,results,/content/training-data/natural_language_inference/11/triples/results.txt
assertions,in,ConceptNet,results,/content/training-data/natural_language_inference/11/triples/results.txt
assertions,most likely yield,improved performance,results,/content/training-data/natural_language_inference/11/triples/results.txt
improved performance,without retraining,our models,results,/content/training-data/natural_language_inference/11/triples/results.txt
RTE experiments,When providing,additional background knowledge,results,/content/training-data/natural_language_inference/11/triples/results.txt
additional background knowledge,from,ConceptNet,results,/content/training-data/natural_language_inference/11/triples/results.txt
additional background knowledge,has,ESIM - based models,results,/content/training-data/natural_language_inference/11/triples/results.txt
ESIM - based models,has,improve,results,/content/training-data/natural_language_inference/11/triples/results.txt
improve,only on,more difficult MultiNLI dataset,results,/content/training-data/natural_language_inference/11/triples/results.txt
additional background knowledge,has,our BiLSTM based models,results,/content/training-data/natural_language_inference/11/triples/results.txt
our BiLSTM based models,has,improve substantially,results,/content/training-data/natural_language_inference/11/triples/results.txt
RTE experiments,has,both ESIM and our BiL - STM models,results,/content/training-data/natural_language_inference/11/triples/results.txt
both ESIM and our BiL - STM models,trained with,knowledge,results,/content/training-data/natural_language_inference/11/triples/results.txt
knowledge,sensitive to,semantics,results,/content/training-data/natural_language_inference/11/triples/results.txt
semantics,of,provided assertions,results,/content/training-data/natural_language_inference/11/triples/results.txt
knowledge,from,ConceptNet,results,/content/training-data/natural_language_inference/11/triples/results.txt
RTE experiments,has,our models,results,/content/training-data/natural_language_inference/11/triples/results.txt
our models,acquit,competitively,results,/content/training-data/natural_language_inference/11/triples/results.txt
competitively,on,SNLI benchmark,results,/content/training-data/natural_language_inference/11/triples/results.txt
our models,acquit,quite well,results,/content/training-data/natural_language_inference/11/triples/results.txt
quite well,on,MultiNLI benchmark,results,/content/training-data/natural_language_inference/11/triples/results.txt
Results,has,Wikipedia ( W ,results,/content/training-data/natural_language_inference/11/triples/results.txt
Wikipedia ( W ),yields,"further , significant improvements",results,/content/training-data/natural_language_inference/11/triples/results.txt
"further , significant improvements",has,slightly outperforming,results,/content/training-data/natural_language_inference/11/triples/results.txt
slightly outperforming,has,current state of the art model,results,/content/training-data/natural_language_inference/11/triples/results.txt
"further , significant improvements",on,TriviaQA,results,/content/training-data/natural_language_inference/11/triples/results.txt
Contribution,has,Experiments,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Experiments,has,Tasks,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Tasks,has,Natural Language Inference,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Natural Language Inference,has,Experimental setup,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Experimental setup,used,mini- batch size,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
mini- batch size,set to,16 or 32,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Experimental setup,used,Out - of - vocabulary ( OOV ) words,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Out - of - vocabulary ( OOV ) words,initialized,randomly,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
randomly,with,Gaussian samples,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Experimental setup,used,"Adam ( Kingma and Ba , 2015 ",experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
"Adam ( Kingma and Ba , 2015 )",with,initial learning rate,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
initial learning rate,set to,1E - 3,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
"Adam ( Kingma and Ba , 2015 )",with,two momentum parameters,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
two momentum parameters,set to,0.9 and 0.999,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
"Adam ( Kingma and Ba , 2015 )",for,optimization,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Experimental setup,used,pre-trained 300 - D Glove 840B vectors,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
pre-trained 300 - D Glove 840B vectors,to initialize,word embeddings,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Experimental setup,has,dropout rate,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
dropout rate,selected from,"[ 0.1 , 0.2 , 0.3 , 0.4 ]",experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Experimental setup,updated,OOV vectors,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
OOV vectors,in,first epoch,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
first epoch,after,all word embeddings,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
all word embeddings,updated,normally,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Natural Language Inference,has,Results,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Results,has,LSTMNs,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
LSTMNs,achieve,better performance,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Results,observe,fusion,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
fusion,is,generally beneficial,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Results,observe,deep fusion,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
deep fusion,has,slightly improves,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
slightly improves,over,shallow fusion,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Results,With,standard training,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
standard training,has,deep fusion,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
deep fusion,yields,state - of - the - art performance,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Natural Language Inference,has,Baselines,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Baselines,has,"shared LSTM ( Rocktschel et al. , 2016 ",experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Baselines,has,word - by - word attention model,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Baselines,has,matching LSTM ( m LSTM ; ,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Baselines,compared,bag - of - words baseline,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Tasks,has,Language Modeling,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Language Modeling,has,Hyperparameters,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Hyperparameters,used,stochastic gradient descent,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
stochastic gradient descent,with,initial learning rate,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
initial learning rate,of,0.65,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
stochastic gradient descent,for,optimization,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
stochastic gradient descent,has,decays,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
decays,by,factor,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
factor,of,0.85,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
factor,per,epoch,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Hyperparameters,has,renormalize,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
renormalize,has,gradient,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
gradient,if,norm,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
norm,greater than,5,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Hyperparameters,has,mini - batch size,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
mini - batch size,set to,40,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Hyperparameters,has,dimensions,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
dimensions,of,word embeddings,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
dimensions,set to,150,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
150,for,all models,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Language Modeling,has,Results,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Results,Amongst,all deep architectures,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
all deep architectures,has,three - layer LSTMN,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
three - layer LSTMN,performs,best,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Language Modeling,has,Baselines,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Baselines,has,gated - feedback LSTM,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
gated - feedback LSTM,has,feedback gates,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
feedback gates,connecting,hidden states,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
hidden states,across,multiple time steps,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
hidden states,as,adaptive control,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
adaptive control,of,information flow,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Baselines,has,Kneser - Ney 5 - gram language model ( KN5 ,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Kneser - Ney 5 - gram language model ( KN5 ),serves as,non-neural baseline,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
non-neural baseline,for,language modeling task,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Baselines,has,depth - gated LSTM,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
depth - gated LSTM,uses,depth gate,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
depth gate,to connect,memory cells,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
memory cells,of,vertically adjacent layers,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Tasks,has,Sentiment Analysis,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Sentiment Analysis,has,Hyperparameters,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Hyperparameters,used,pretrained 300 - D Glove 840B vectors,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
pretrained 300 - D Glove 840B vectors,to initialize,word embeddings,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Hyperparameters,used,"Adam ( Kingma and Ba , 2015 ",experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
"Adam ( Kingma and Ba , 2015 )",for,optimization,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
optimization,with,two momentum parameters,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
two momentum parameters,set to,0.9 and 0.999,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Hyperparameters,has,regularization constant,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
regularization constant,was,1E - 4,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Hyperparameters,has,mini-batch size,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
mini-batch size,was,5,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Hyperparameters,has,initial learning rate,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
initial learning rate,set to,2E - 3,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Hyperparameters,has,gradient,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
gradient,for,words,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
words,with,Glove embeddings,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
gradient,scaled by,0.35,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
0.35,in,first epoch,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
first epoch,after,all word embeddings,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
all word embeddings,updated,normally,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Hyperparameters,has,dropout rate,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
dropout rate,of,0.5,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
dropout rate,applied to,neural network classifier,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Sentiment Analysis,has,Results,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Results,show that,both 1 - and 2 - layer LSTMNs,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
both 1 - and 2 - layer LSTMNs,has,outperform,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
outperform,has,LSTM baselines,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Results,On,fine - grained and binary classification tasks,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
fine - grained and binary classification tasks,has,our 2 - layer LSTMN,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
our 2 - layer LSTMN,performs,close,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
close,to,best system,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Sentiment Analysis,has,Baselines,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Baselines,are,LSTM variants,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Baselines,report,performance,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
performance,of,paragraph vector model,experiments,/content/training-data/natural_language_inference/88/triples/experiments.txt
Contribution,Code,https://github.com/cheng6076/,code,/content/training-data/natural_language_inference/88/triples/code.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/88/triples/model.txt
Model,realized by,inserting,model,/content/training-data/natural_language_inference/88/triples/model.txt
inserting,has,memory network module,model,/content/training-data/natural_language_inference/88/triples/model.txt
memory network module,together with,attention,model,/content/training-data/natural_language_inference/88/triples/model.txt
attention,for,memory addressing,model,/content/training-data/natural_language_inference/88/triples/model.txt
memory network module,in,update,model,/content/training-data/natural_language_inference/88/triples/model.txt
update,of,recurrent network,model,/content/training-data/natural_language_inference/88/triples/model.txt
Model,induces,undirected relations,model,/content/training-data/natural_language_inference/88/triples/model.txt
undirected relations,as an,intermediate step,model,/content/training-data/natural_language_inference/88/triples/model.txt
intermediate step,of learning,representations,model,/content/training-data/natural_language_inference/88/triples/model.txt
undirected relations,among,tokens,model,/content/training-data/natural_language_inference/88/triples/model.txt
Model,leverage,memory and attention,model,/content/training-data/natural_language_inference/88/triples/model.txt
memory and attention,to empower,recurrent network,model,/content/training-data/natural_language_inference/88/triples/model.txt
recurrent network,with,stronger memorization capability,model,/content/training-data/natural_language_inference/88/triples/model.txt
recurrent network,with,ability,model,/content/training-data/natural_language_inference/88/triples/model.txt
ability,to discover,relations,model,/content/training-data/natural_language_inference/88/triples/model.txt
relations,among,tokens,model,/content/training-data/natural_language_inference/88/triples/model.txt
Model,processes,text,model,/content/training-data/natural_language_inference/88/triples/model.txt
text,has,incrementally,model,/content/training-data/natural_language_inference/88/triples/model.txt
text,while,learning,model,/content/training-data/natural_language_inference/88/triples/model.txt
learning,past tokens,in the memory,model,/content/training-data/natural_language_inference/88/triples/model.txt
learning,past tokens,to what extent,model,/content/training-data/natural_language_inference/88/triples/model.txt
to what extent,relate to,current token,model,/content/training-data/natural_language_inference/88/triples/model.txt
current token,being,processed,model,/content/training-data/natural_language_inference/88/triples/model.txt
Model,use,multiple memory slots,model,/content/training-data/natural_language_inference/88/triples/model.txt
multiple memory slots,outside,recurrence,model,/content/training-data/natural_language_inference/88/triples/model.txt
recurrence,to,piece - wise store representations,model,/content/training-data/natural_language_inference/88/triples/model.txt
piece - wise store representations,of,input,model,/content/training-data/natural_language_inference/88/triples/model.txt
Model,use,read and write operations,model,/content/training-data/natural_language_inference/88/triples/model.txt
read and write operations,for,each slot,model,/content/training-data/natural_language_inference/88/triples/model.txt
each slot,modeled as,attention mechanism,model,/content/training-data/natural_language_inference/88/triples/model.txt
attention mechanism,with,recurrent controller,model,/content/training-data/natural_language_inference/88/triples/model.txt
Model,term,Long Short - Term Memory - Network ( LSTMN ,model,/content/training-data/natural_language_inference/88/triples/model.txt
Long Short - Term Memory - Network ( LSTMN ),is,reading simulator,model,/content/training-data/natural_language_inference/88/triples/model.txt
reading simulator,used for,sequence processing tasks,model,/content/training-data/natural_language_inference/88/triples/model.txt
Contribution,has research problem,Machine Reading,research-problem,/content/training-data/natural_language_inference/88/triples/research-problem.txt
Contribution,has,Experiments,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
Experiments,has,Tasks,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
Tasks,has,LANGUAGE MODELLING,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
LANGUAGE MODELLING,has,Results,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
Results,using,external information,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
external information,to compute,embeddings,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
embeddings,of,unknown words,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
external information,has,helps,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
helps,in,all cases,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
Results,Using,Glo Ve embeddings,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
Glo Ve embeddings,results in,best perplexity,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
Results,Using,dictionary and spelling,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
dictionary and spelling,has,consistently slightly better,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
consistently slightly better,than,just spelling,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
Results,note that,lemma + lowercase,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
lemma + lowercase,performs,worse,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
worse,than,any model,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
any model,with,dictionary,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
Results,Adding,spelling,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
spelling,has,consistently helps more,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
consistently helps more,than,adding,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
adding,has,dictionary definitions,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
Tasks,has,QUESTION ANSWERING,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
QUESTION ANSWERING,has,Results,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
Results,adding,any external information,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
any external information,results in,significant improvement,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
significant improvement,over,baseline model,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
significant improvement,has,3.7 - 10.5 points,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
Results,adding,spelling,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
spelling,has,helps more,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
helps more,than,adding,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
adding,has,dictionary,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
helps more,has,3 points difference,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
Results,uses,SD,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
SD,has,1.1 point advantage,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
1.1 point advantage,over,model,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
model,uses,just the spelling,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
Results,has,model with GLoVe embeddings ( G ,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
model with GLoVe embeddings ( G ),is,still ahead,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
still ahead,with,1.1 point margin,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
Results,has,dictionary alone,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
dictionary alone,has,mean pooling,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
mean pooling,performs,similarly,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
similarly,to,LSTM,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
Tasks,has,ENTAILMENT PREDICTION,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
ENTAILMENT PREDICTION,has,Results,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
Results,using,fixed random embeddings,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
fixed random embeddings,for,OOV words,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
fixed random embeddings,has,did not bring a significant advantage,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
did not bring a significant advantage,over,baseline,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
Results,has,spelling,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
spelling,was,not as useful,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
not as useful,on,SNLI and MultiNLI,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
Results,has,dictionary - enabled models,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
dictionary - enabled models,has,significantly outperform,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
significantly outperform,for,sentences,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
sentences,containing,rare words,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
significantly outperform,has,baseline models,experiments,/content/training-data/natural_language_inference/84/triples/experiments.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/84/triples/model.txt
Model,has,auxiliary data encoders,model,/content/training-data/natural_language_inference/84/triples/model.txt
auxiliary data encoders,ensuring,preservation,model,/content/training-data/natural_language_inference/84/triples/model.txt
preservation,of,semantic alignment,model,/content/training-data/natural_language_inference/84/triples/model.txt
semantic alignment,with,representations,model,/content/training-data/natural_language_inference/84/triples/model.txt
representations,of,within - vocabulary words,model,/content/training-data/natural_language_inference/84/triples/model.txt
auxiliary data encoders,trained jointly with,objective,model,/content/training-data/natural_language_inference/84/triples/model.txt
Model,has,Several sources of auxiliary data,model,/content/training-data/natural_language_inference/84/triples/model.txt
Several sources of auxiliary data,as,input,model,/content/training-data/natural_language_inference/84/triples/model.txt
input,to,neural network,model,/content/training-data/natural_language_inference/84/triples/model.txt
neural network,compute,combined representation,model,/content/training-data/natural_language_inference/84/triples/model.txt
combined representation,used for,out - of - vocabulary words,model,/content/training-data/natural_language_inference/84/triples/model.txt
combined representation,combined with,withinvocabulary word embeddings,model,/content/training-data/natural_language_inference/84/triples/model.txt
withinvocabulary word embeddings,directly trained on,task of interest,model,/content/training-data/natural_language_inference/84/triples/model.txt
withinvocabulary word embeddings,pretrained from,external data source,model,/content/training-data/natural_language_inference/84/triples/model.txt
Several sources of auxiliary data,used,simultaneously,model,/content/training-data/natural_language_inference/84/triples/model.txt
Model,propose,new method,model,/content/training-data/natural_language_inference/84/triples/model.txt
new method,for computing,"embeddings "" on the fly """,model,/content/training-data/natural_language_inference/84/triples/model.txt
"embeddings "" on the fly """,jointly addresses,large vocabulary problem,model,/content/training-data/natural_language_inference/84/triples/model.txt
"embeddings "" on the fly """,jointly addresses,paucity of data,model,/content/training-data/natural_language_inference/84/triples/model.txt
paucity of data,for learning,representations,model,/content/training-data/natural_language_inference/84/triples/model.txt
representations,in,long tail of the Zipfian distribution,model,/content/training-data/natural_language_inference/84/triples/model.txt
Model,train,network,model,/content/training-data/natural_language_inference/84/triples/model.txt
network,to predict,representations,model,/content/training-data/natural_language_inference/84/triples/model.txt
representations,based on,auxiliary data,model,/content/training-data/natural_language_inference/84/triples/model.txt
representations,of,words,model,/content/training-data/natural_language_inference/84/triples/model.txt
Contribution,has research problem,Learning representations for rare words,research-problem,/content/training-data/natural_language_inference/84/triples/research-problem.txt
Contribution,has research problem,COMPUTE WORD EMBEDDINGS ON THE FLY,research-problem,/content/training-data/natural_language_inference/84/triples/research-problem.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/natural_language_inference/87/triples/hyperparameters.txt
Hyperparameters,adopt,Whole Word Masking BERT,hyperparameters,/content/training-data/natural_language_inference/87/triples/hyperparameters.txt
Hyperparameters,has,maximum number of epochs,hyperparameters,/content/training-data/natural_language_inference/87/triples/hyperparameters.txt
maximum number of epochs,set to,3 or 10,hyperparameters,/content/training-data/natural_language_inference/87/triples/hyperparameters.txt
Hyperparameters,has,maximum input length,hyperparameters,/content/training-data/natural_language_inference/87/triples/hyperparameters.txt
maximum input length,set to,384,hyperparameters,/content/training-data/natural_language_inference/87/triples/hyperparameters.txt
384,for,SQuAD and RACE,hyperparameters,/content/training-data/natural_language_inference/87/triples/hyperparameters.txt
Hyperparameters,has,texts,hyperparameters,/content/training-data/natural_language_inference/87/triples/hyperparameters.txt
texts,are,tokenized,hyperparameters,/content/training-data/natural_language_inference/87/triples/hyperparameters.txt
tokenized,using,wordpieces,hyperparameters,/content/training-data/natural_language_inference/87/triples/hyperparameters.txt
Hyperparameters,has,initial learning rate,hyperparameters,/content/training-data/natural_language_inference/87/triples/hyperparameters.txt
initial learning rate,with,warm - up rate,hyperparameters,/content/training-data/natural_language_inference/87/triples/hyperparameters.txt
warm - up rate,of,0.1,hyperparameters,/content/training-data/natural_language_inference/87/triples/hyperparameters.txt
initial learning rate,with,L2 weight decay,hyperparameters,/content/training-data/natural_language_inference/87/triples/hyperparameters.txt
L2 weight decay,of,0.01,hyperparameters,/content/training-data/natural_language_inference/87/triples/hyperparameters.txt
initial learning rate,set in,"{ 8e -6 , 1 e - 5 , 2 e - 5 , 3 e - 5 }",hyperparameters,/content/training-data/natural_language_inference/87/triples/hyperparameters.txt
Hyperparameters,has,weight,hyperparameters,/content/training-data/natural_language_inference/87/triples/hyperparameters.txt
weight,in,dual context aggregation,hyperparameters,/content/training-data/natural_language_inference/87/triples/hyperparameters.txt
weight,is,0.5,hyperparameters,/content/training-data/natural_language_inference/87/triples/hyperparameters.txt
Hyperparameters,has,batch size,hyperparameters,/content/training-data/natural_language_inference/87/triples/hyperparameters.txt
batch size,selected in,"{ 16 , 20 , 32 }",hyperparameters,/content/training-data/natural_language_inference/87/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/87/triples/model.txt
Model,extend,self - attention mechanism,model,/content/training-data/natural_language_inference/87/triples/model.txt
self - attention mechanism,with,syntax - guided constraint,model,/content/training-data/natural_language_inference/87/triples/model.txt
self - attention mechanism,to capture,syntax related parts,model,/content/training-data/natural_language_inference/87/triples/model.txt
syntax related parts,with,each concerned word,model,/content/training-data/natural_language_inference/87/triples/model.txt
Model,accommodate,SDOI information,model,/content/training-data/natural_language_inference/87/triples/model.txt
SDOI information,propose,novel syntax - guided network ( SG - Net ,model,/content/training-data/natural_language_inference/87/triples/model.txt
novel syntax - guided network ( SG - Net ),to provide,more linguistically inspired representation,model,/content/training-data/natural_language_inference/87/triples/model.txt
more linguistically inspired representation,for,reading comprehension tasks,model,/content/training-data/natural_language_inference/87/triples/model.txt
novel syntax - guided network ( SG - Net ),fuses,original SAN and SDOI - SAN,model,/content/training-data/natural_language_inference/87/triples/model.txt
Model,adopt,pre-trained dependency syntactic parse tree structure,model,/content/training-data/natural_language_inference/87/triples/model.txt
pre-trained dependency syntactic parse tree structure,namely,syntactic dependency of interest ( SDOI ,model,/content/training-data/natural_language_inference/87/triples/model.txt
pre-trained dependency syntactic parse tree structure,to produce,related nodes,model,/content/training-data/natural_language_inference/87/triples/model.txt
related nodes,for,each word,model,/content/training-data/natural_language_inference/87/triples/model.txt
each word,in,sentence,model,/content/training-data/natural_language_inference/87/triples/model.txt
each word,regarding,each word,model,/content/training-data/natural_language_inference/87/triples/model.txt
each word,as,child node,model,/content/training-data/natural_language_inference/87/triples/model.txt
Contribution,has research problem,Machine Reading Comprehension,research-problem,/content/training-data/natural_language_inference/87/triples/research-problem.txt
Contribution,has research problem,machine reading comprehension ( MRC ,research-problem,/content/training-data/natural_language_inference/87/triples/research-problem.txt
Contribution,has research problem,MRC,research-problem,/content/training-data/natural_language_inference/87/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/87/triples/results.txt
Results,adding,extra answer verifier module,results,/content/training-data/natural_language_inference/87/triples/results.txt
extra answer verifier module,yield,better result,results,/content/training-data/natural_language_inference/87/triples/results.txt
Results,has,outperforms,results,/content/training-data/natural_language_inference/87/triples/results.txt
outperforms,achieves,2nd place,results,/content/training-data/natural_language_inference/87/triples/results.txt
2nd place,when submitting,SG - NET,results,/content/training-data/natural_language_inference/87/triples/results.txt
2nd place,on,leaderboard,results,/content/training-data/natural_language_inference/87/triples/results.txt
outperforms,has,all the published works,results,/content/training-data/natural_language_inference/87/triples/results.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/21/triples/model.txt
Model,compute,feature - wise attention,model,/content/training-data/natural_language_inference/21/triples/model.txt
feature - wise attention,since,each element,model,/content/training-data/natural_language_inference/21/triples/model.txt
each element,represented by,vector,model,/content/training-data/natural_language_inference/21/triples/model.txt
each element,in,sequence,model,/content/training-data/natural_language_inference/21/triples/model.txt
Model,apply,positional masks,model,/content/training-data/natural_language_inference/21/triples/model.txt
positional masks,easily encode,prior structure knowledge,model,/content/training-data/natural_language_inference/21/triples/model.txt
prior structure knowledge,such as,temporal order,model,/content/training-data/natural_language_inference/21/triples/model.txt
prior structure knowledge,such as,dependency parsing,model,/content/training-data/natural_language_inference/21/triples/model.txt
positional masks,to,attention distribution,model,/content/training-data/natural_language_inference/21/triples/model.txt
Model,build,light - weight and RNN / CNN - free neural network,model,/content/training-data/natural_language_inference/21/triples/model.txt
light - weight and RNN / CNN - free neural network,name,Directional Self - Attention Network ( DiSAN ,model,/content/training-data/natural_language_inference/21/triples/model.txt
light - weight and RNN / CNN - free neural network,for,sentence encoding,model,/content/training-data/natural_language_inference/21/triples/model.txt
Model,In,DiSAN,model,/content/training-data/natural_language_inference/21/triples/model.txt
DiSAN,has,multi-dimensional attention,model,/content/training-data/natural_language_inference/21/triples/model.txt
multi-dimensional attention,computes,vector representation,model,/content/training-data/natural_language_inference/21/triples/model.txt
vector representation,passed into,classification / regression module,model,/content/training-data/natural_language_inference/21/triples/model.txt
classification / regression module,to compute,final prediction for a particular task,model,/content/training-data/natural_language_inference/21/triples/model.txt
vector representation,of,entire sequence,model,/content/training-data/natural_language_inference/21/triples/model.txt
DiSAN,has,input sequence,model,/content/training-data/natural_language_inference/21/triples/model.txt
input sequence,processed by,directional ( forward and backward ) self - attentions,model,/content/training-data/natural_language_inference/21/triples/model.txt
directional ( forward and backward ) self - attentions,produce,context - aware representations,model,/content/training-data/natural_language_inference/21/triples/model.txt
context - aware representations,for,all tokens,model,/content/training-data/natural_language_inference/21/triples/model.txt
directional ( forward and backward ) self - attentions,to model,context dependency,model,/content/training-data/natural_language_inference/21/triples/model.txt
Model,propose,novel attention mechanism,model,/content/training-data/natural_language_inference/21/triples/model.txt
novel attention mechanism,differs from,previous ones,model,/content/training-data/natural_language_inference/21/triples/model.txt
previous ones,in,multi-dimensional,model,/content/training-data/natural_language_inference/21/triples/model.txt
previous ones,in,directional,model,/content/training-data/natural_language_inference/21/triples/model.txt
Contribution,has research problem,RNN / CNN - Free Language Understanding,research-problem,/content/training-data/natural_language_inference/21/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
Experimental setup,as,optimization objective,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
optimization objective,use,cross-entropy loss,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
cross-entropy loss,has,minimize,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
minimize,with,batch size,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
batch size,of,64,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
minimize,by,Adadelta,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
cross-entropy loss,plus,L2 regularization penalty,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
Experimental setup,use,Dropout,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
Dropout,with,keep probability,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
keep probability,has,0.75,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
0.75,for,language inference,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
keep probability,has,0.8,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
0.8,for,sentiment analysis,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
Experimental setup,implemented with,TensorFlow 2,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
Experimental setup,has,Initial learning rate,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
Initial learning rate,set to,0.5,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
Experimental setup,has,Hidden units number,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
Hidden units number,has,d h,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
d h,set to,300,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
Experimental setup,has,biases,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
biases,initialized with,0,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
Experimental setup,has,Activation functions,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
Activation functions,are,ELU ( exponential linear unit ,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
Experimental setup,has,weight matrices,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
weight matrices,initialized by,Glorot Initialization,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
Experimental setup,has,L2 regularization decay factors,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
L2 regularization decay factors,are,5 10 ?5 and 10 ? 4,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
5 10 ?5 and 10 ? 4,for,language inference and sentiment analysis,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
Experimental setup,has,Out - of - Vocabulary words,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
Out - of - Vocabulary words,in,training set,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
Out - of - Vocabulary words,has,randomly initialized,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
randomly initialized,by,uniform distribution,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
uniform distribution,between,"( ? 0.05 , 0.05 ",experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
Experimental setup,initialize,word embedding,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
word embedding,in,x,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
word embedding,by,300D Glo Ve 6B pre-trained vectors,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
Experimental setup,run on,Nvidia GTX 1080 Ti graphic card,experimental-setup,/content/training-data/natural_language_inference/21/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/21/triples/results.txt
Results,has,Natural Language Inference,results,/content/training-data/natural_language_inference/21/triples/results.txt
Natural Language Inference,shows,changing,results,/content/training-data/natural_language_inference/21/triples/results.txt
changing,has,token - wise attention,results,/content/training-data/natural_language_inference/21/triples/results.txt
token - wise attention,leads to,3.31 % improvement,results,/content/training-data/natural_language_inference/21/triples/results.txt
3.31 % improvement,on,word embedding based model,results,/content/training-data/natural_language_inference/21/triples/results.txt
token - wise attention,to,multi-dimensional / feature - wise attention,results,/content/training-data/natural_language_inference/21/triples/results.txt
Natural Language Inference,Compared to,results,results,/content/training-data/natural_language_inference/21/triples/results.txt
results,from,official leaderboard of SNLI,results,/content/training-data/natural_language_inference/21/triples/results.txt
official leaderboard of SNLI,has,DiSAN,results,/content/training-data/natural_language_inference/21/triples/results.txt
DiSAN,improves,best latest test accuracy,results,/content/training-data/natural_language_inference/21/triples/results.txt
best latest test accuracy,achieved by,memory - based NSE encoder network,results,/content/training-data/natural_language_inference/21/triples/results.txt
best latest test accuracy,by,remarkable margin,results,/content/training-data/natural_language_inference/21/triples/results.txt
remarkable margin,of,1.02 %,results,/content/training-data/natural_language_inference/21/triples/results.txt
DiSAN,outperforms,previous works,results,/content/training-data/natural_language_inference/21/triples/results.txt
Natural Language Inference,has,comparison,results,/content/training-data/natural_language_inference/21/triples/results.txt
comparison,between,forth baseline and DiSAN,results,/content/training-data/natural_language_inference/21/triples/results.txt
forth baseline and DiSAN,shows,DiSA block,results,/content/training-data/natural_language_inference/21/triples/results.txt
DiSA block,has,outperform,results,/content/training-data/natural_language_inference/21/triples/results.txt
outperform,improving,test accuracy,results,/content/training-data/natural_language_inference/21/triples/results.txt
test accuracy,by,0.64 %,results,/content/training-data/natural_language_inference/21/triples/results.txt
outperform,has,Bi - LSTM layer,results,/content/training-data/natural_language_inference/21/triples/results.txt
Bi - LSTM layer,in,context encoding,results,/content/training-data/natural_language_inference/21/triples/results.txt
comparison,between,third baseline and DiSAN,results,/content/training-data/natural_language_inference/21/triples/results.txt
third baseline and DiSAN,shows,DiSAN,results,/content/training-data/natural_language_inference/21/triples/results.txt
DiSAN,has,substantially outperform,results,/content/training-data/natural_language_inference/21/triples/results.txt
substantially outperform,has,multi-head attention,results,/content/training-data/natural_language_inference/21/triples/results.txt
multi-head attention,by,1.45 %,results,/content/training-data/natural_language_inference/21/triples/results.txt
comparison,between,fifth baseline and DiSAN,results,/content/training-data/natural_language_inference/21/triples/results.txt
fifth baseline and DiSAN,shows,directional self - attention,results,/content/training-data/natural_language_inference/21/triples/results.txt
directional self - attention,with,forward and backward masks ( with temporal order encoded ,results,/content/training-data/natural_language_inference/21/triples/results.txt
directional self - attention,bring,0.96 % improvement,results,/content/training-data/natural_language_inference/21/triples/results.txt
Natural Language Inference,has,DiSAN,results,/content/training-data/natural_language_inference/21/triples/results.txt
DiSAN,surpasses,RNN / CNN based models,results,/content/training-data/natural_language_inference/21/triples/results.txt
RNN / CNN based models,with,more complicated architecture,results,/content/training-data/natural_language_inference/21/triples/results.txt
RNN / CNN based models,with,more parameters,results,/content/training-data/natural_language_inference/21/triples/results.txt
RNN / CNN based models,by,large margins,results,/content/training-data/natural_language_inference/21/triples/results.txt
DiSAN,outperforms,models,results,/content/training-data/natural_language_inference/21/triples/results.txt
models,with,assistance,results,/content/training-data/natural_language_inference/21/triples/results.txt
assistance,of,semantic parsing tree,results,/content/training-data/natural_language_inference/21/triples/results.txt
Results,has,Sentiment Analysis,results,/content/training-data/natural_language_inference/21/triples/results.txt
Sentiment Analysis,Compared to,tree - based models,results,/content/training-data/natural_language_inference/21/triples/results.txt
tree - based models,with,heavy use,results,/content/training-data/natural_language_inference/21/triples/results.txt
heavy use,of,prior structure,results,/content/training-data/natural_language_inference/21/triples/results.txt
tree - based models,has,DiSAN,results,/content/training-data/natural_language_inference/21/triples/results.txt
DiSAN,has,outperforms,results,/content/training-data/natural_language_inference/21/triples/results.txt
outperforms,by,"7.32 % , 6.02 % and 0.72 %",results,/content/training-data/natural_language_inference/21/triples/results.txt
tree - based models,e.g.,MV - RNN,results,/content/training-data/natural_language_inference/21/triples/results.txt
tree - based models,e.g.,RNTN,results,/content/training-data/natural_language_inference/21/triples/results.txt
tree - based models,e.g.,Tree - LSTM,results,/content/training-data/natural_language_inference/21/triples/results.txt
Sentiment Analysis,has,DiSAN,results,/content/training-data/natural_language_inference/21/triples/results.txt
DiSAN,improves,last best accuracy,results,/content/training-data/natural_language_inference/21/triples/results.txt
last best accuracy,by,0.52 %,results,/content/training-data/natural_language_inference/21/triples/results.txt
last best accuracy,given by,CNN - Tensor,results,/content/training-data/natural_language_inference/21/triples/results.txt
DiSAN,achieves,better performance,results,/content/training-data/natural_language_inference/21/triples/results.txt
better performance,than,CNN - based models,results,/content/training-data/natural_language_inference/21/triples/results.txt
DiSAN,has,outperforms,results,/content/training-data/natural_language_inference/21/triples/results.txt
outperforms,such as,LR- Bi- LSTM,results,/content/training-data/natural_language_inference/21/triples/results.txt
LR- Bi- LSTM,has,+ 1.12 %,results,/content/training-data/natural_language_inference/21/triples/results.txt
outperforms,such as,NCSL,results,/content/training-data/natural_language_inference/21/triples/results.txt
NCSL,has,+ 0.62 %,results,/content/training-data/natural_language_inference/21/triples/results.txt
Contribution,has,Approach,approach,/content/training-data/natural_language_inference/53/triples/approach.txt
Approach,study,task,approach,/content/training-data/natural_language_inference/53/triples/approach.txt
task,of,learning,approach,/content/training-data/natural_language_inference/53/triples/approach.txt
learning,has,universal representations of sentences,approach,/content/training-data/natural_language_inference/53/triples/approach.txt
Approach,has,sentence encoder model,approach,/content/training-data/natural_language_inference/53/triples/approach.txt
sentence encoder model,subsequently transferred to,other tasks,approach,/content/training-data/natural_language_inference/53/triples/approach.txt
sentence encoder model,trained on,large corpus,approach,/content/training-data/natural_language_inference/53/triples/approach.txt
Approach,investigate,supervised learning,approach,/content/training-data/natural_language_inference/53/triples/approach.txt
supervised learning,can be,leveraged,approach,/content/training-data/natural_language_inference/53/triples/approach.txt
Approach,investigate,impact,approach,/content/training-data/natural_language_inference/53/triples/approach.txt
impact,of,sentence encoding architecture,approach,/content/training-data/natural_language_inference/53/triples/approach.txt
sentence encoding architecture,compare,"convolutional , recurrent and even simpler word composition schemes",approach,/content/training-data/natural_language_inference/53/triples/approach.txt
sentence encoding architecture,on,representational transferability,approach,/content/training-data/natural_language_inference/53/triples/approach.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/natural_language_inference/53/triples/hyperparameters.txt
Hyperparameters,At,each epoch,hyperparameters,/content/training-data/natural_language_inference/53/triples/hyperparameters.txt
each epoch,divide,learning rate,hyperparameters,/content/training-data/natural_language_inference/53/triples/hyperparameters.txt
learning rate,by,5,hyperparameters,/content/training-data/natural_language_inference/53/triples/hyperparameters.txt
learning rate,if,dev accuracy,hyperparameters,/content/training-data/natural_language_inference/53/triples/hyperparameters.txt
dev accuracy,has,decreases,hyperparameters,/content/training-data/natural_language_inference/53/triples/hyperparameters.txt
Hyperparameters,use,opensource GloVe vectors,hyperparameters,/content/training-data/natural_language_inference/53/triples/hyperparameters.txt
opensource GloVe vectors,trained on,Common Crawl 840B,hyperparameters,/content/training-data/natural_language_inference/53/triples/hyperparameters.txt
Common Crawl 840B,with,300 dimensions,hyperparameters,/content/training-data/natural_language_inference/53/triples/hyperparameters.txt
Common Crawl 840B,as,fixed word embeddings,hyperparameters,/content/training-data/natural_language_inference/53/triples/hyperparameters.txt
Hyperparameters,use,minibatches,hyperparameters,/content/training-data/natural_language_inference/53/triples/hyperparameters.txt
minibatches,of size,64,hyperparameters,/content/training-data/natural_language_inference/53/triples/hyperparameters.txt
minibatches,has,training,hyperparameters,/content/training-data/natural_language_inference/53/triples/hyperparameters.txt
training,is,stopped,hyperparameters,/content/training-data/natural_language_inference/53/triples/hyperparameters.txt
stopped,when,learning rate,hyperparameters,/content/training-data/natural_language_inference/53/triples/hyperparameters.txt
learning rate,goes under,threshold of 10 ?5,hyperparameters,/content/training-data/natural_language_inference/53/triples/hyperparameters.txt
Hyperparameters,For,models,hyperparameters,/content/training-data/natural_language_inference/53/triples/hyperparameters.txt
models,use,SGD,hyperparameters,/content/training-data/natural_language_inference/53/triples/hyperparameters.txt
SGD,with,learning rate,hyperparameters,/content/training-data/natural_language_inference/53/triples/hyperparameters.txt
learning rate,of,0.1,hyperparameters,/content/training-data/natural_language_inference/53/triples/hyperparameters.txt
SGD,with,weight decay,hyperparameters,/content/training-data/natural_language_inference/53/triples/hyperparameters.txt
weight decay,of,0.99,hyperparameters,/content/training-data/natural_language_inference/53/triples/hyperparameters.txt
models,trained on,SNLI,hyperparameters,/content/training-data/natural_language_inference/53/triples/hyperparameters.txt
Hyperparameters,For,classifier,hyperparameters,/content/training-data/natural_language_inference/53/triples/hyperparameters.txt
classifier,use,multi - layer perceptron,hyperparameters,/content/training-data/natural_language_inference/53/triples/hyperparameters.txt
multi - layer perceptron,with,1 hidden - layer,hyperparameters,/content/training-data/natural_language_inference/53/triples/hyperparameters.txt
1 hidden - layer,of,512 hidden units,hyperparameters,/content/training-data/natural_language_inference/53/triples/hyperparameters.txt
Contribution,has research problem,Supervised Learning of Universal Sentence Representations,research-problem,/content/training-data/natural_language_inference/53/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/53/triples/results.txt
Results,has,Comparison with SkipThought,results,/content/training-data/natural_language_inference/53/triples/results.txt
Comparison with SkipThought,performs,better,results,/content/training-data/natural_language_inference/53/triples/results.txt
better,than,SkipThought,results,/content/training-data/natural_language_inference/53/triples/results.txt
SkipThought,Except for,SUBJ dataset,results,/content/training-data/natural_language_inference/53/triples/results.txt
SkipThought,on,MR,results,/content/training-data/natural_language_inference/53/triples/results.txt
SkipThought,on,CR,results,/content/training-data/natural_language_inference/53/triples/results.txt
SkipThought,on,MPQA,results,/content/training-data/natural_language_inference/53/triples/results.txt
Comparison with SkipThought,has,Our BiLSTM - max,results,/content/training-data/natural_language_inference/53/triples/results.txt
Our BiLSTM - max,performs,much better,results,/content/training-data/natural_language_inference/53/triples/results.txt
much better,than,released SkipThought vectors,results,/content/training-data/natural_language_inference/53/triples/results.txt
released SkipThought vectors,on,MR,results,/content/training-data/natural_language_inference/53/triples/results.txt
released SkipThought vectors,on,CR,results,/content/training-data/natural_language_inference/53/triples/results.txt
released SkipThought vectors,on,MPQA,results,/content/training-data/natural_language_inference/53/triples/results.txt
released SkipThought vectors,on,SST,results,/content/training-data/natural_language_inference/53/triples/results.txt
released SkipThought vectors,on,MRPC - accuracy,results,/content/training-data/natural_language_inference/53/triples/results.txt
released SkipThought vectors,on,SICK - R,results,/content/training-data/natural_language_inference/53/triples/results.txt
released SkipThought vectors,on,SICK - E,results,/content/training-data/natural_language_inference/53/triples/results.txt
released SkipThought vectors,on,STS,results,/content/training-data/natural_language_inference/53/triples/results.txt
Our BiLSTM - max,trained on,SNLI,results,/content/training-data/natural_language_inference/53/triples/results.txt
Comparison with SkipThought,With,much less data ( 570 k compared to 64M sentences ,results,/content/training-data/natural_language_inference/53/triples/results.txt
much less data ( 570 k compared to 64M sentences ),with,high - quality supervision,results,/content/training-data/natural_language_inference/53/triples/results.txt
high - quality supervision,from,SNLI dataset,results,/content/training-data/natural_language_inference/53/triples/results.txt
much less data ( 570 k compared to 64M sentences ),able to,consistently outperform,results,/content/training-data/natural_language_inference/53/triples/results.txt
consistently outperform,has,results,results,/content/training-data/natural_language_inference/53/triples/results.txt
results,obtained by,SkipThought vectors,results,/content/training-data/natural_language_inference/53/triples/results.txt
Comparison with SkipThought,looking at,STS14 results,results,/content/training-data/natural_language_inference/53/triples/results.txt
STS14 results,observe,cosine metrics,results,/content/training-data/natural_language_inference/53/triples/results.txt
cosine metrics,in,our embedding space,results,/content/training-data/natural_language_inference/53/triples/results.txt
cosine metrics,is,more semantically informative,results,/content/training-data/natural_language_inference/53/triples/results.txt
more semantically informative,than,SkipThought embedding space,results,/content/training-data/natural_language_inference/53/triples/results.txt
Results,has,Image - caption retrieval results,results,/content/training-data/natural_language_inference/53/triples/results.txt
Image - caption retrieval results,has,Our approach,results,/content/training-data/natural_language_inference/53/triples/results.txt
Our approach,pushes,results,results,/content/training-data/natural_language_inference/53/triples/results.txt
results,from,30.6,results,/content/training-data/natural_language_inference/53/triples/results.txt
30.6,to,33.2,results,/content/training-data/natural_language_inference/53/triples/results.txt
33.2,on,image retrieval,results,/content/training-data/natural_language_inference/53/triples/results.txt
results,from,37.9,results,/content/training-data/natural_language_inference/53/triples/results.txt
37.9,to,42.4,results,/content/training-data/natural_language_inference/53/triples/results.txt
42.4,on,cap-tion retrieval,results,/content/training-data/natural_language_inference/53/triples/results.txt
results,has,even further,results,/content/training-data/natural_language_inference/53/triples/results.txt
Image - caption retrieval results,trained with,ResNet features and 30 k more training data,results,/content/training-data/natural_language_inference/53/triples/results.txt
ResNet features and 30 k more training data,has,SkipThought vectors,results,/content/training-data/natural_language_inference/53/triples/results.txt
SkipThought vectors,going from,33.8 to 37.9,results,/content/training-data/natural_language_inference/53/triples/results.txt
33.8 to 37.9,for,caption retrieval R@1,results,/content/training-data/natural_language_inference/53/triples/results.txt
SkipThought vectors,perform,significantly better,results,/content/training-data/natural_language_inference/53/triples/results.txt
significantly better,than,original setting,results,/content/training-data/natural_language_inference/53/triples/results.txt
SkipThought vectors,from,25.9 to 30.6,results,/content/training-data/natural_language_inference/53/triples/results.txt
25.9 to 30.6,on,image retrieval R@1,results,/content/training-data/natural_language_inference/53/triples/results.txt
Results,has,MultiGenre NLI,results,/content/training-data/natural_language_inference/53/triples/results.txt
MultiGenre NLI,has,Our model,results,/content/training-data/natural_language_inference/53/triples/results.txt
Our model,reaches,AdaSent performance,results,/content/training-data/natural_language_inference/53/triples/results.txt
AdaSent performance,on,CR,results,/content/training-data/natural_language_inference/53/triples/results.txt
MultiGenre NLI,observe,significant boost,results,/content/training-data/natural_language_inference/53/triples/results.txt
significant boost,in,performance over all,results,/content/training-data/natural_language_inference/53/triples/results.txt
significant boost,compared to,model,results,/content/training-data/natural_language_inference/53/triples/results.txt
model,trained only on,SLNI,results,/content/training-data/natural_language_inference/53/triples/results.txt
Results,has,Embedding size,results,/content/training-data/natural_language_inference/53/triples/results.txt
Embedding size,has,increased embedding sizes,results,/content/training-data/natural_language_inference/53/triples/results.txt
increased embedding sizes,lead to,increased performance,results,/content/training-data/natural_language_inference/53/triples/results.txt
increased performance,for,almost all models,results,/content/training-data/natural_language_inference/53/triples/results.txt
Results,has,Domain adaptation on SICK tasks,results,/content/training-data/natural_language_inference/53/triples/results.txt
Domain adaptation on SICK tasks,obtain,86.3 % test accuracy,results,/content/training-data/natural_language_inference/53/triples/results.txt
86.3 % test accuracy,while,previous best handengineered models,results,/content/training-data/natural_language_inference/53/triples/results.txt
previous best handengineered models,obtained,84.5 %,results,/content/training-data/natural_language_inference/53/triples/results.txt
86.3 % test accuracy,on,SICK - E,results,/content/training-data/natural_language_inference/53/triples/results.txt
Domain adaptation on SICK tasks,obtain,pearson score,results,/content/training-data/natural_language_inference/53/triples/results.txt
pearson score,of,0.885,results,/content/training-data/natural_language_inference/53/triples/results.txt
0.885,on,SICK - R,results,/content/training-data/natural_language_inference/53/triples/results.txt
Domain adaptation on SICK tasks,has,Our transfer learning approach,results,/content/training-data/natural_language_inference/53/triples/results.txt
Our transfer learning approach,obtains,better results,results,/content/training-data/natural_language_inference/53/triples/results.txt
better results,than,previous state - of - the - art,results,/content/training-data/natural_language_inference/53/triples/results.txt
Domain adaptation on SICK tasks,has,significantly outperformed,results,/content/training-data/natural_language_inference/53/triples/results.txt
significantly outperformed,has,previous transfer learning approaches,results,/content/training-data/natural_language_inference/53/triples/results.txt
previous transfer learning approaches,that used,parameters,results,/content/training-data/natural_language_inference/53/triples/results.txt
parameters,to fine - tune on,SICK,results,/content/training-data/natural_language_inference/53/triples/results.txt
parameters,of,LSTM model,results,/content/training-data/natural_language_inference/53/triples/results.txt
parameters,trained on,SNLI,results,/content/training-data/natural_language_inference/53/triples/results.txt
previous transfer learning approaches,on,SICK - E,results,/content/training-data/natural_language_inference/53/triples/results.txt
Results,has,Architecture impact,results,/content/training-data/natural_language_inference/53/triples/results.txt
Architecture impact,has,BiLSTM - 4096,results,/content/training-data/natural_language_inference/53/triples/results.txt
BiLSTM - 4096,with,max - pooling operation,results,/content/training-data/natural_language_inference/53/triples/results.txt
BiLSTM - 4096,performs,best,results,/content/training-data/natural_language_inference/53/triples/results.txt
best,on,SNLI and transfer tasks,results,/content/training-data/natural_language_inference/53/triples/results.txt
BiLSTM - 4096,Looking at,micro and macro averages,results,/content/training-data/natural_language_inference/53/triples/results.txt
micro and macro averages,performs,significantly better,results,/content/training-data/natural_language_inference/53/triples/results.txt
significantly better,than,other models,results,/content/training-data/natural_language_inference/53/triples/results.txt
other models,name,LSTM,results,/content/training-data/natural_language_inference/53/triples/results.txt
other models,name,GRU,results,/content/training-data/natural_language_inference/53/triples/results.txt
other models,name,BiGRU - last,results,/content/training-data/natural_language_inference/53/triples/results.txt
other models,name,BiLSTM - Mean,results,/content/training-data/natural_language_inference/53/triples/results.txt
other models,name,inner-attention,results,/content/training-data/natural_language_inference/53/triples/results.txt
other models,name,hierarchical - ConvNet,results,/content/training-data/natural_language_inference/53/triples/results.txt
Architecture impact,has,transfer quality,results,/content/training-data/natural_language_inference/53/triples/results.txt
transfer quality,sensitive to,optimization algorithm,results,/content/training-data/natural_language_inference/53/triples/results.txt
transfer quality,when training with,Adam,results,/content/training-data/natural_language_inference/53/triples/results.txt
Adam,instead of,SGD,results,/content/training-data/natural_language_inference/53/triples/results.txt
Adam,observed,BiLSTM - max,results,/content/training-data/natural_language_inference/53/triples/results.txt
BiLSTM - max,converged,faster,results,/content/training-data/natural_language_inference/53/triples/results.txt
faster,on,SNLI,results,/content/training-data/natural_language_inference/53/triples/results.txt
BiLSTM - max,obtained,worse results,results,/content/training-data/natural_language_inference/53/triples/results.txt
worse results,on,transfer tasks,results,/content/training-data/natural_language_inference/53/triples/results.txt
Results,has,NLI as a supervised training set,results,/content/training-data/natural_language_inference/53/triples/results.txt
NLI as a supervised training set,indicate,our model,results,/content/training-data/natural_language_inference/53/triples/results.txt
our model,obtains,much better over all results,results,/content/training-data/natural_language_inference/53/triples/results.txt
much better over all results,than,models,results,/content/training-data/natural_language_inference/53/triples/results.txt
models,trained on,other supervised tasks,results,/content/training-data/natural_language_inference/53/triples/results.txt
other supervised tasks,such as,COCO,results,/content/training-data/natural_language_inference/53/triples/results.txt
other supervised tasks,such as,dictionary definitions,results,/content/training-data/natural_language_inference/53/triples/results.txt
other supervised tasks,such as,NMT,results,/content/training-data/natural_language_inference/53/triples/results.txt
other supervised tasks,such as,PPDB,results,/content/training-data/natural_language_inference/53/triples/results.txt
other supervised tasks,such as,SST,results,/content/training-data/natural_language_inference/53/triples/results.txt
our model,trained on,SNLI,results,/content/training-data/natural_language_inference/53/triples/results.txt
Contribution,Code,https://github.com/kimiyoung/fg-gating,code,/content/training-data/natural_language_inference/45/triples/code.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/45/triples/model.txt
Model,compute,vector gate,model,/content/training-data/natural_language_inference/45/triples/model.txt
vector gate,as,linear projection,model,/content/training-data/natural_language_inference/45/triples/model.txt
linear projection,of,token features,model,/content/training-data/natural_language_inference/45/triples/model.txt
Model,for,token properties,model,/content/training-data/natural_language_inference/45/triples/model.txt
token properties,determine,gate,model,/content/training-data/natural_language_inference/45/triples/model.txt
token properties,use,named entity tags,model,/content/training-data/natural_language_inference/45/triples/model.txt
token properties,use,part - ofspeech tags,model,/content/training-data/natural_language_inference/45/triples/model.txt
token properties,use,document frequencies,model,/content/training-data/natural_language_inference/45/triples/model.txt
token properties,use,word - level representations,model,/content/training-data/natural_language_inference/45/triples/model.txt
Model,has,our fine - grained gating mechanism,model,/content/training-data/natural_language_inference/45/triples/model.txt
our fine - grained gating mechanism,used to,model,model,/content/training-data/natural_language_inference/45/triples/model.txt
model,multiple levels,structure,model,/content/training-data/natural_language_inference/45/triples/model.txt
structure,including,words,model,/content/training-data/natural_language_inference/45/triples/model.txt
structure,including,characters,model,/content/training-data/natural_language_inference/45/triples/model.txt
structure,including,phrases,model,/content/training-data/natural_language_inference/45/triples/model.txt
structure,including,sentences,model,/content/training-data/natural_language_inference/45/triples/model.txt
structure,including,paragraphs,model,/content/training-data/natural_language_inference/45/triples/model.txt
structure,in,language,model,/content/training-data/natural_language_inference/45/triples/model.txt
Model,has,Each dimension,model,/content/training-data/natural_language_inference/45/triples/model.txt
Each dimension,controls,how much information,model,/content/training-data/natural_language_inference/45/triples/model.txt
how much information,flowed from,word - level and character - level representations,model,/content/training-data/natural_language_inference/45/triples/model.txt
Each dimension,of,gate,model,/content/training-data/natural_language_inference/45/triples/model.txt
Model,has,multiplicatively,model,/content/training-data/natural_language_inference/45/triples/model.txt
multiplicatively,apply,gate,model,/content/training-data/natural_language_inference/45/triples/model.txt
gate,to,character - level and wordlevel representations,model,/content/training-data/natural_language_inference/45/triples/model.txt
Model,present,fine - grained gating mechanism,model,/content/training-data/natural_language_inference/45/triples/model.txt
fine - grained gating mechanism,to combine,word - level and characterlevel representations,model,/content/training-data/natural_language_inference/45/triples/model.txt
Contribution,has research problem,READING COMPREHENSION,research-problem,/content/training-data/natural_language_inference/45/triples/research-problem.txt
Contribution,has,Experiments,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
Experiments,has,Tasks,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
Tasks,has,Window Size Analysis,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
Window Size Analysis,has,Results,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
Results,illustrate,performances,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
performances,of,models,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
performances,has,increase,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
increase,with,length,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
length,of,window,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
Results,limiting,window size,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
window size,has,benefits,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
benefits,has,performance,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
performance,of,our models,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
Results,observed,larger window size,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
larger window size,does not generate,predictive results,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
predictive results,as good as,one with window size,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
one with window size,set to,10,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
Tasks,has,Predictive Performance,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
Predictive Performance,has,Results,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
Results,found that,SECT - LSTM and SEDT - LSTM,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
SECT - LSTM and SEDT - LSTM,have,better performance,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
better performance,than,CNN counterparts,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
Results,has,our propose models,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
our propose models,achieve,higher relative improvements,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
higher relative improvements,in,EM scores,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
EM scores,over,baseline methods,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
EM scores,than,F 1 scores,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
Predictive Performance,has,Baselines,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
Baselines,compared,performance,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
performance,has,baseline approach BiDAF,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
performance,has,proposed SEST approaches,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
proposed SEST approaches,including,SE - POS,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
proposed SEST approaches,including,SECT - LSTM,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
proposed SEST approaches,including,SECT - CNN,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
proposed SEST approaches,including,SEDT - LSTM,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
proposed SEST approaches,including,SEDT - CNN,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
performance,on,development dataset,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
development dataset,of,SQuAD,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
Tasks,has,Contribution of Syntactic Sequence,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
Contribution of Syntactic Sequence,has,Results,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
Results,are,important,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
important,for,models,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
models,to work,properly,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
important,both,ordering,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
important,both,contents,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
contents,of,syntactic tree,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
Results,has,constituency and dependency trees,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
constituency and dependency trees,achieved,over 20 % boost,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
over 20 % boost,has,our proposed ordering,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
our proposed ordering,out - performed,random ordering,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
over 20 % boost,compared to,randomly generated ones,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
over 20 % boost,on,performance,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
Results,has,ordering,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
ordering,of,dependency trees,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
dependency trees,seems to have,less impact,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
less impact,compared to,constituency trees,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
less impact,on,performance,experiments,/content/training-data/natural_language_inference/54/triples/experiments.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/54/triples/model.txt
Model,propose,Structural Embedding of Syntactic Trees ( SEST ,model,/content/training-data/natural_language_inference/54/triples/model.txt
Structural Embedding of Syntactic Trees ( SEST ),encode,syntactic information,model,/content/training-data/natural_language_inference/54/triples/model.txt
syntactic information,into,neural attention models,model,/content/training-data/natural_language_inference/54/triples/model.txt
neural attention models,for,question answering task,model,/content/training-data/natural_language_inference/54/triples/model.txt
syntactic information,structured by,constituency tree and dependency tree,model,/content/training-data/natural_language_inference/54/triples/model.txt
Contribution,has research problem,Machine Comprehension,research-problem,/content/training-data/natural_language_inference/54/triples/research-problem.txt
Contribution,has research problem,Reading comprehension,research-problem,/content/training-data/natural_language_inference/54/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
Experimental setup,use,variable character embedding,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
variable character embedding,with,fixed pre-trained word embedding,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
variable character embedding,to serve,part,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
part,of,input,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
input,into,model,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
Experimental setup,For,SECT and SEDT,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
SECT and SEDT,has,input,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
input,with,30 units,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
30 units,to be,output,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
input,of,model,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
input,has,size,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
size,of,8,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
SECT and SEDT,has,maximum length size,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
maximum length size,set to be,10 and 20,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
Experimental setup,run,our experiments,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
our experiments,on,machine,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
machine,contains,single GTX 1080 GPU,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
single GTX 1080 GPU,with,8 GB VRAM,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
Experimental setup,has,max length,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
max length,of,SQuAD,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
SQuAD,is,16,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
Experimental setup,has,character embedding,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
character embedding,with,one -dimensional layer,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
one -dimensional layer,consists of,100 units,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
100 units,with,channel size,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
channel size,of,5,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
character embedding,implemented using,CNN,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
Experimental setup,has,input depth,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
input depth,of,8,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
Experimental setup,has,fixed word embedding,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
fixed word embedding,has,dimension,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
dimension,of,100,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
fixed word embedding,provided by,GloVe data set,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
Experimental setup,has,POS model,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
POS model,contains,syntactic information,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
syntactic information,with,39 different POS tags,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
syntactic information,serve,both input and output,experimental-setup,/content/training-data/natural_language_inference/54/triples/experimental-setup.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
Ablation analysis,relace,default attention function,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
default attention function,with,"dot product : f ( u , v ) = u v ( 5 ",ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
default attention function,has,both metrics,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
both metrics,suffer from,degradations,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
Ablation analysis,using,2 blocks,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
2 blocks,causes,slight performance drop,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
Ablation analysis,Removing,any of the two heuristics,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
any of the two heuristics,leads to,some performance declines,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
any of the two heuristics,has,heuristic subtraction,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
heuristic subtraction,is,more effective,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
more effective,than,multiplication,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
Ablation analysis,has,very deep alignment,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
very deep alignment,with,5 blocks,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
5 blocks,results in,significant performance decline,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
Ablation analysis,has,highway - like function,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
highway - like function,has,outperformed,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
outperformed,has,simpler variants,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
Ablation analysis,Replacing,DCRL,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
DCRL,with,SCST,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
DCRL,causes,marginal decline,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
marginal decline,of,performance,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
performance,on,both metrics,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
Ablation analysis,increasing to,4 blocks,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
4 blocks,barely affects,SoTA result,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
Ablation analysis,notice,reattention,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
reattention,has,more influences,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
more influences,on,EM score,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
Ablation analysis,notice,DCRL,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
DCRL,contributes more to,F1 metric,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
Ablation analysis,notice,removing both,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
removing both,results in,huge drops,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
huge drops,on,both metrics,ablation-analysis,/content/training-data/natural_language_inference/17/triples/ablation-analysis.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/natural_language_inference/17/triples/hyperparameters.txt
Hyperparameters,to prevent,overfitting,hyperparameters,/content/training-data/natural_language_inference/17/triples/hyperparameters.txt
overfitting,has,batch size,hyperparameters,/content/training-data/natural_language_inference/17/triples/hyperparameters.txt
batch size,is,48,hyperparameters,/content/training-data/natural_language_inference/17/triples/hyperparameters.txt
overfitting,has,dropout rate,hyperparameters,/content/training-data/natural_language_inference/17/triples/hyperparameters.txt
dropout rate,of,0.3,hyperparameters,/content/training-data/natural_language_inference/17/triples/hyperparameters.txt
Hyperparameters,use,Adam optimizer,hyperparameters,/content/training-data/natural_language_inference/17/triples/hyperparameters.txt
Adam optimizer,for,ML and DCRL training,hyperparameters,/content/training-data/natural_language_inference/17/triples/hyperparameters.txt
Hyperparameters,For,out of vocabulary words,hyperparameters,/content/training-data/natural_language_inference/17/triples/hyperparameters.txt
out of vocabulary words,set,embeddings,hyperparameters,/content/training-data/natural_language_inference/17/triples/hyperparameters.txt
embeddings,keep them,trainable,hyperparameters,/content/training-data/natural_language_inference/17/triples/hyperparameters.txt
embeddings,from,Gaussian distributions,hyperparameters,/content/training-data/natural_language_inference/17/triples/hyperparameters.txt
Hyperparameters,has,hyperparameter,hyperparameters,/content/training-data/natural_language_inference/17/triples/hyperparameters.txt
hyperparameter,is,3,hyperparameters,/content/training-data/natural_language_inference/17/triples/hyperparameters.txt
Hyperparameters,has,initial learning rates,hyperparameters,/content/training-data/natural_language_inference/17/triples/hyperparameters.txt
initial learning rates,are,0.0008 and 0.0001,hyperparameters,/content/training-data/natural_language_inference/17/triples/hyperparameters.txt
initial learning rates,are,halved,hyperparameters,/content/training-data/natural_language_inference/17/triples/hyperparameters.txt
halved,whenever meeting,bad iteration,hyperparameters,/content/training-data/natural_language_inference/17/triples/hyperparameters.txt
Hyperparameters,has,size,hyperparameters,/content/training-data/natural_language_inference/17/triples/hyperparameters.txt
size,of,character embedding and corresponding LSTMs,hyperparameters,/content/training-data/natural_language_inference/17/triples/hyperparameters.txt
character embedding and corresponding LSTMs,is,50,hyperparameters,/content/training-data/natural_language_inference/17/triples/hyperparameters.txt
Hyperparameters,has,main hidden size,hyperparameters,/content/training-data/natural_language_inference/17/triples/hyperparameters.txt
main hidden size,is,100,hyperparameters,/content/training-data/natural_language_inference/17/triples/hyperparameters.txt
Hyperparameters,has,Word embeddings,hyperparameters,/content/training-data/natural_language_inference/17/triples/hyperparameters.txt
Word embeddings,remain,fixed,hyperparameters,/content/training-data/natural_language_inference/17/triples/hyperparameters.txt
fixed,during,training,hyperparameters,/content/training-data/natural_language_inference/17/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/17/triples/model.txt
Model,extend,traditional training method,model,/content/training-data/natural_language_inference/17/triples/model.txt
traditional training method,with,novel approach,model,/content/training-data/natural_language_inference/17/triples/model.txt
novel approach,called,dynamic - critical reinforcement learning,model,/content/training-data/natural_language_inference/17/triples/model.txt
dynamic - critical reinforcement learning,dynamically decides,reward and the baseline,model,/content/training-data/natural_language_inference/17/triples/model.txt
reward and the baseline,according to,two sampling strategies,model,/content/training-data/natural_language_inference/17/triples/model.txt
Model,has,reattention,model,/content/training-data/natural_language_inference/17/triples/model.txt
reattention,be,more concentrated,model,/content/training-data/natural_language_inference/17/triples/model.txt
more concentrated,if,past attentions,model,/content/training-data/natural_language_inference/17/triples/model.txt
past attentions,focus on,same parts,model,/content/training-data/natural_language_inference/17/triples/model.txt
same parts,of,input,model,/content/training-data/natural_language_inference/17/triples/model.txt
reattention,be,relatively more distracted,model,/content/training-data/natural_language_inference/17/triples/model.txt
relatively more distracted,to focus on,new regions,model,/content/training-data/natural_language_inference/17/triples/model.txt
new regions,if,past attentions,model,/content/training-data/natural_language_inference/17/triples/model.txt
past attentions,are,not overlapped at all,model,/content/training-data/natural_language_inference/17/triples/model.txt
Model,has,computation,model,/content/training-data/natural_language_inference/17/triples/model.txt
computation,based on,two words,model,/content/training-data/natural_language_inference/17/triples/model.txt
two words,share,similar semantics,model,/content/training-data/natural_language_inference/17/triples/model.txt
two words,if,attentions,model,/content/training-data/natural_language_inference/17/triples/model.txt
attentions,about,same texts,model,/content/training-data/natural_language_inference/17/triples/model.txt
same texts,be,less similar,model,/content/training-data/natural_language_inference/17/triples/model.txt
same texts,are,highly overlapped,model,/content/training-data/natural_language_inference/17/triples/model.txt
Model,present,reattention mechanism,model,/content/training-data/natural_language_inference/17/triples/model.txt
reattention mechanism,that,temporally memorizes,model,/content/training-data/natural_language_inference/17/triples/model.txt
temporally memorizes,has,past attentions,model,/content/training-data/natural_language_inference/17/triples/model.txt
past attentions,to refine,current attentions,model,/content/training-data/natural_language_inference/17/triples/model.txt
current attentions,in,multi-round alignment architecture,model,/content/training-data/natural_language_inference/17/triples/model.txt
Contribution,has research problem,Machine Reading Comprehension,research-problem,/content/training-data/natural_language_inference/17/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/17/triples/results.txt
Results,on,hidden test set,results,/content/training-data/natural_language_inference/17/triples/results.txt
hidden test set,of,SQuAD,results,/content/training-data/natural_language_inference/17/triples/results.txt
SQuAD,has,R.M - Reader,results,/content/training-data/natural_language_inference/17/triples/results.txt
R.M - Reader,achieves,EM score,results,/content/training-data/natural_language_inference/17/triples/results.txt
EM score,of,79.5 %,results,/content/training-data/natural_language_inference/17/triples/results.txt
R.M - Reader,achieves,F1 score,results,/content/training-data/natural_language_inference/17/triples/results.txt
F1 score,of,86.6 %,results,/content/training-data/natural_language_inference/17/triples/results.txt
R.M - Reader,has,comfortably outperforms,results,/content/training-data/natural_language_inference/17/triples/results.txt
comfortably outperforms,has,previous models,results,/content/training-data/natural_language_inference/17/triples/results.txt
previous models,by,more than 6 %,results,/content/training-data/natural_language_inference/17/triples/results.txt
more than 6 %,in,EM and F 1 scores,results,/content/training-data/natural_language_inference/17/triples/results.txt
SQuAD,has,Our ensemble model,results,/content/training-data/natural_language_inference/17/triples/results.txt
Our ensemble model,improves,metrics,results,/content/training-data/natural_language_inference/17/triples/results.txt
metrics,to,82.3 % and 88.5 %,results,/content/training-data/natural_language_inference/17/triples/results.txt
Contribution,Code,https://github.com/shuohangwang/,code,/content/training-data/natural_language_inference/66/triples/code.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/natural_language_inference/66/triples/hyperparameters.txt
Hyperparameters,experiment with,d = 150 and d = 300,hyperparameters,/content/training-data/natural_language_inference/66/triples/hyperparameters.txt
Hyperparameters,use,Adam method,hyperparameters,/content/training-data/natural_language_inference/66/triples/hyperparameters.txt
Adam method,for,optimization,hyperparameters,/content/training-data/natural_language_inference/66/triples/hyperparameters.txt
optimization,with,hyperparameters,hyperparameters,/content/training-data/natural_language_inference/66/triples/hyperparameters.txt
hyperparameters,set to,0.9,hyperparameters,/content/training-data/natural_language_inference/66/triples/hyperparameters.txt
hyperparameters,set to,0.999,hyperparameters,/content/training-data/natural_language_inference/66/triples/hyperparameters.txt
Hyperparameters,has,initial learning rate,hyperparameters,/content/training-data/natural_language_inference/66/triples/hyperparameters.txt
initial learning rate,set to,0.001,hyperparameters,/content/training-data/natural_language_inference/66/triples/hyperparameters.txt
Hyperparameters,has,batch size,hyperparameters,/content/training-data/natural_language_inference/66/triples/hyperparameters.txt
batch size,set to,30,hyperparameters,/content/training-data/natural_language_inference/66/triples/hyperparameters.txt
Hyperparameters,has,decay ratio,hyperparameters,/content/training-data/natural_language_inference/66/triples/hyperparameters.txt
decay ratio,of,0.95,hyperparameters,/content/training-data/natural_language_inference/66/triples/hyperparameters.txt
0.95,for,each iteration,hyperparameters,/content/training-data/natural_language_inference/66/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/66/triples/model.txt
Model,use,LSTM,model,/content/training-data/natural_language_inference/66/triples/model.txt
LSTM,to perform,word - by - word matching,model,/content/training-data/natural_language_inference/66/triples/model.txt
word - by - word matching,of,hypothesis,model,/content/training-data/natural_language_inference/66/triples/model.txt
hypothesis,with,premise,model,/content/training-data/natural_language_inference/66/triples/model.txt
Model,refer to,match - LSTM,model,/content/training-data/natural_language_inference/66/triples/model.txt
Model,refer to,m LSTM,model,/content/training-data/natural_language_inference/66/triples/model.txt
m LSTM,for,short,model,/content/training-data/natural_language_inference/66/triples/model.txt
Model,has,Our LSTM,model,/content/training-data/natural_language_inference/66/triples/model.txt
Our LSTM,at each position,tries to match,model,/content/training-data/natural_language_inference/66/triples/model.txt
tries to match,has,current word,model,/content/training-data/natural_language_inference/66/triples/model.txt
current word,with,attention - weighted representation,model,/content/training-data/natural_language_inference/66/triples/model.txt
attention - weighted representation,of,premise,model,/content/training-data/natural_language_inference/66/triples/model.txt
current word,in,hypothesis,model,/content/training-data/natural_language_inference/66/triples/model.txt
Our LSTM,sequentially processes,hypothesis,model,/content/training-data/natural_language_inference/66/triples/model.txt
Model,propose,new LSTM - based architecture,model,/content/training-data/natural_language_inference/66/triples/model.txt
new LSTM - based architecture,for learning,natural language inference,model,/content/training-data/natural_language_inference/66/triples/model.txt
Contribution,has research problem,Natural Language Inference,research-problem,/content/training-data/natural_language_inference/66/triples/research-problem.txt
Contribution,has research problem,Natural language inference ( NLI ,research-problem,/content/training-data/natural_language_inference/66/triples/research-problem.txt
Contribution,has research problem,NLI,research-problem,/content/training-data/natural_language_inference/66/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/66/triples/results.txt
Results,compare,our m LSTM model,results,/content/training-data/natural_language_inference/66/triples/results.txt
our m LSTM model,with,word - by - word attention model,results,/content/training-data/natural_language_inference/66/triples/results.txt
our m LSTM model,see that,our performance,results,/content/training-data/natural_language_inference/66/triples/results.txt
our performance,on,test data ( 85.7 % ,results,/content/training-data/natural_language_inference/66/triples/results.txt
test data ( 85.7 % ),is,higher,results,/content/training-data/natural_language_inference/66/triples/results.txt
higher,than,their model ( 82.6 % ,results,/content/training-data/natural_language_inference/66/triples/results.txt
our m LSTM model,under,same setting,results,/content/training-data/natural_language_inference/66/triples/results.txt
same setting,with,d,results,/content/training-data/natural_language_inference/66/triples/results.txt
d,=,150,results,/content/training-data/natural_language_inference/66/triples/results.txt
Results,see that,set d,results,/content/training-data/natural_language_inference/66/triples/results.txt
set d,to,300,results,/content/training-data/natural_language_inference/66/triples/results.txt
300,has,our model,results,/content/training-data/natural_language_inference/66/triples/results.txt
our model,achieves,accuracy,results,/content/training-data/natural_language_inference/66/triples/results.txt
accuracy,of,86.1 %,results,/content/training-data/natural_language_inference/66/triples/results.txt
86.1 %,on,test data,results,/content/training-data/natural_language_inference/66/triples/results.txt
Results,has,performance,results,/content/training-data/natural_language_inference/66/triples/results.txt
performance,of,mLSTM,results,/content/training-data/natural_language_inference/66/triples/results.txt
mLSTM,with,bi - LSTM sentence modeling,results,/content/training-data/natural_language_inference/66/triples/results.txt
mLSTM,compared with,model,results,/content/training-data/natural_language_inference/66/triples/results.txt
model,with,standard LSTM sentence modeling,results,/content/training-data/natural_language_inference/66/triples/results.txt
model,shows that,bi - LSTM,results,/content/training-data/natural_language_inference/66/triples/results.txt
bi - LSTM,has,helps,results,/content/training-data/natural_language_inference/66/triples/results.txt
helps,has,86.0 % vs. 85.7 %,results,/content/training-data/natural_language_inference/66/triples/results.txt
86.0 % vs. 85.7 %,on,test data,results,/content/training-data/natural_language_inference/66/triples/results.txt
bi - LSTM,to process,original sentences,results,/content/training-data/natural_language_inference/66/triples/results.txt
model,when,d,results,/content/training-data/natural_language_inference/66/triples/results.txt
d,set to,150,results,/content/training-data/natural_language_inference/66/triples/results.txt
Results,experimented with,m LSTM model,results,/content/training-data/natural_language_inference/66/triples/results.txt
m LSTM model,using,pre-trained word embeddings,results,/content/training-data/natural_language_inference/66/triples/results.txt
pre-trained word embeddings,as,initial representations,results,/content/training-data/natural_language_inference/66/triples/results.txt
initial representations,of,premise and the hypothesis,results,/content/training-data/natural_language_inference/66/triples/results.txt
initial representations,able to achieve,accuracy,results,/content/training-data/natural_language_inference/66/triples/results.txt
accuracy,of,85.3 %,results,/content/training-data/natural_language_inference/66/triples/results.txt
85.3 %,is,better,results,/content/training-data/natural_language_inference/66/triples/results.txt
better,than,previously reported state of the art,results,/content/training-data/natural_language_inference/66/triples/results.txt
85.3 %,on,test data,results,/content/training-data/natural_language_inference/66/triples/results.txt
pre-trained word embeddings,instead of,LSTMgenerated hidden states,results,/content/training-data/natural_language_inference/66/triples/results.txt
Contribution,has,Approach,approach,/content/training-data/natural_language_inference/60/triples/approach.txt
Approach,explore,supervised learning,approach,/content/training-data/natural_language_inference/60/triples/approach.txt
supervised learning,of,"task - specific , dynamic meta-embeddings",approach,/content/training-data/natural_language_inference/60/triples/approach.txt
"task - specific , dynamic meta-embeddings",apply,sentence representations,approach,/content/training-data/natural_language_inference/60/triples/approach.txt
Approach,is,embedding - agnostic,approach,/content/training-data/natural_language_inference/60/triples/approach.txt
Contribution,has research problem,Improved Sentence Representations,research-problem,/content/training-data/natural_language_inference/60/triples/research-problem.txt
Contribution,has research problem,sentence representations,research-problem,/content/training-data/natural_language_inference/60/triples/research-problem.txt
Contribution,has,Approach,approach,/content/training-data/natural_language_inference/40/triples/approach.txt
Approach,introduce,Memory as Acyclic Graph Encoding RNN ( MAGE - RNN ) framework,approach,/content/training-data/natural_language_inference/40/triples/approach.txt
Memory as Acyclic Graph Encoding RNN ( MAGE - RNN ) framework,to compute,representation,approach,/content/training-data/natural_language_inference/40/triples/approach.txt
representation,of,graphs,approach,/content/training-data/natural_language_inference/40/triples/approach.txt
representation,while,touching,approach,/content/training-data/natural_language_inference/40/triples/approach.txt
touching,has,only once,approach,/content/training-data/natural_language_inference/40/triples/approach.txt
touching,every,node,approach,/content/training-data/natural_language_inference/40/triples/approach.txt
Memory as Acyclic Graph Encoding RNN ( MAGE - RNN ) framework,implement,GRU version,approach,/content/training-data/natural_language_inference/40/triples/approach.txt
GRU version,called,MAGE - GRU,approach,/content/training-data/natural_language_inference/40/triples/approach.txt
Approach,use,MAGE - RNN,approach,/content/training-data/natural_language_inference/40/triples/approach.txt
MAGE - RNN,to model,coreference relations,approach,/content/training-data/natural_language_inference/40/triples/approach.txt
coreference relations,for,text comprehension tasks,approach,/content/training-data/natural_language_inference/40/triples/approach.txt
text comprehension tasks,where,answers to a query,approach,/content/training-data/natural_language_inference/40/triples/approach.txt
answers to a query,have to be extracted from,context document,approach,/content/training-data/natural_language_inference/40/triples/approach.txt
coreference relations,has,Tokens,approach,/content/training-data/natural_language_inference/40/triples/approach.txt
Tokens,in,document,approach,/content/training-data/natural_language_inference/40/triples/approach.txt
Tokens,connected by,coreference relation,approach,/content/training-data/natural_language_inference/40/triples/approach.txt
coreference relation,if,refer,approach,/content/training-data/natural_language_inference/40/triples/approach.txt
refer,to,same underlying entity,approach,/content/training-data/natural_language_inference/40/triples/approach.txt
Approach,utilize,order inherent,approach,/content/training-data/natural_language_inference/40/triples/approach.txt
order inherent,in,unaugmented sequence,approach,/content/training-data/natural_language_inference/40/triples/approach.txt
order inherent,to decompose,graph,approach,/content/training-data/natural_language_inference/40/triples/approach.txt
graph,into,two Directed Acyclic Graphs ( DAGs ,approach,/content/training-data/natural_language_inference/40/triples/approach.txt
two Directed Acyclic Graphs ( DAGs ),with,topological ordering,approach,/content/training-data/natural_language_inference/40/triples/approach.txt
Approach,has,MAGE - RNN,approach,/content/training-data/natural_language_inference/40/triples/approach.txt
MAGE - RNN,learns,separate representations,approach,/content/training-data/natural_language_inference/40/triples/approach.txt
separate representations,leads to,superior performance empirically,approach,/content/training-data/natural_language_inference/40/triples/approach.txt
separate representations,for,propagation,approach,/content/training-data/natural_language_inference/40/triples/approach.txt
propagation,along,each edge type,approach,/content/training-data/natural_language_inference/40/triples/approach.txt
Contribution,has research problem,Recurrent Neural Networks,research-problem,/content/training-data/natural_language_inference/40/triples/research-problem.txt
Contribution,has research problem,Training recurrent neural networks to model long term dependencies,research-problem,/content/training-data/natural_language_inference/40/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/40/triples/results.txt
Results,has,Cloze - style QA,results,/content/training-data/natural_language_inference/40/triples/results.txt
Cloze - style QA,on,CNN dataset,results,/content/training-data/natural_language_inference/40/triples/results.txt
CNN dataset,Augmenting,bi - GRU model,results,/content/training-data/natural_language_inference/40/triples/results.txt
bi - GRU model,with,MAGE,results,/content/training-data/natural_language_inference/40/triples/results.txt
MAGE,leads to,improvement,results,/content/training-data/natural_language_inference/40/triples/results.txt
improvement,of,2.5 %,results,/content/training-data/natural_language_inference/40/triples/results.txt
improvement,on,test set,results,/content/training-data/natural_language_inference/40/triples/results.txt
CNN dataset,has,previous best results,results,/content/training-data/natural_language_inference/40/triples/results.txt
previous best results,achieved by,GA Reader,results,/content/training-data/natural_language_inference/40/triples/results.txt
GA Reader,adding,MAGE,results,/content/training-data/natural_language_inference/40/triples/results.txt
MAGE,leads to,further improvement,results,/content/training-data/natural_language_inference/40/triples/results.txt
further improvement,of,0.7 %,results,/content/training-data/natural_language_inference/40/triples/results.txt
further improvement,setting,new state of the art,results,/content/training-data/natural_language_inference/40/triples/results.txt
Results,has,Story Based,results,/content/training-data/natural_language_inference/40/triples/results.txt
Story Based,Adding,same information,results,/content/training-data/natural_language_inference/40/triples/results.txt
same information,as,one - hot features,results,/content/training-data/natural_language_inference/40/triples/results.txt
one - hot features,fails to,improve,results,/content/training-data/natural_language_inference/40/triples/results.txt
improve,has,performance,results,/content/training-data/natural_language_inference/40/triples/results.txt
Story Based,has,Both variants of MAGE,results,/content/training-data/natural_language_inference/40/triples/results.txt
Both variants of MAGE,has,substantially outperform,results,/content/training-data/natural_language_inference/40/triples/results.txt
substantially outperform,has,QRNs,results,/content/training-data/natural_language_inference/40/triples/results.txt
QRNs,are,current state - of - the - art models,results,/content/training-data/natural_language_inference/40/triples/results.txt
current state - of - the - art models,on,bAbi dataset,results,/content/training-data/natural_language_inference/40/triples/results.txt
Story Based,has,Our model,results,/content/training-data/natural_language_inference/40/triples/results.txt
Our model,achieves,new state - of - the - art results,results,/content/training-data/natural_language_inference/40/triples/results.txt
Our model,has,outperforming,results,/content/training-data/natural_language_inference/40/triples/results.txt
outperforming,has,strong baselines,results,/content/training-data/natural_language_inference/40/triples/results.txt
strong baselines,such as,QRNs,results,/content/training-data/natural_language_inference/40/triples/results.txt
Story Based,showing,our proposed architecture,results,/content/training-data/natural_language_inference/40/triples/results.txt
our proposed architecture,perform,worse,results,/content/training-data/natural_language_inference/40/triples/results.txt
worse,has,DAG - RNN baseline,results,/content/training-data/natural_language_inference/40/triples/results.txt
worse,has,shared version of MAGE,results,/content/training-data/natural_language_inference/40/triples/results.txt
our proposed architecture,is,superior,results,/content/training-data/natural_language_inference/40/triples/results.txt
Story Based,observe,proposed MAGE architecture,results,/content/training-data/natural_language_inference/40/triples/results.txt
proposed MAGE architecture,has,substantially improve,results,/content/training-data/natural_language_inference/40/triples/results.txt
substantially improve,has,performance,results,/content/training-data/natural_language_inference/40/triples/results.txt
performance,for,both bi - GRUs and GAs,results,/content/training-data/natural_language_inference/40/triples/results.txt
Results,has,Broad Context Language Modeling,results,/content/training-data/natural_language_inference/40/triples/results.txt
Broad Context Language Modeling,pick,LAMBADA dataset,results,/content/training-data/natural_language_inference/40/triples/results.txt
LAMBADA dataset,has,Our implementation of GA,results,/content/training-data/natural_language_inference/40/triples/results.txt
Our implementation of GA,gave,higher performance,results,/content/training-data/natural_language_inference/40/triples/results.txt
LAMBADA dataset,On,multi - layer GA architecture,results,/content/training-data/natural_language_inference/40/triples/results.txt
multi - layer GA architecture,has,coreference edges,results,/content/training-data/natural_language_inference/40/triples/results.txt
coreference edges,lead to,improvement,results,/content/training-data/natural_language_inference/40/triples/results.txt
improvement,of,2 %,results,/content/training-data/natural_language_inference/40/triples/results.txt
2 %,setting,new state - of - theart,results,/content/training-data/natural_language_inference/40/triples/results.txt
LAMBADA dataset,On,simple bi - GRU architecture,results,/content/training-data/natural_language_inference/40/triples/results.txt
simple bi - GRU architecture,see,improvement,results,/content/training-data/natural_language_inference/40/triples/results.txt
improvement,of,1.7 %,results,/content/training-data/natural_language_inference/40/triples/results.txt
1.7 %,by incorporating,coreference edges,results,/content/training-data/natural_language_inference/40/triples/results.txt
coreference edges,in,graph,results,/content/training-data/natural_language_inference/40/triples/results.txt
Contribution,has,Dataset,datase,/content/training-data/natural_language_inference/14/triples/dataset.txt
Dataset,introduce,COmmonsense Dataset Adversarially - authored by Humans ( CODAH ,datase,/content/training-data/natural_language_inference/14/triples/dataset.txt
COmmonsense Dataset Adversarially - authored by Humans ( CODAH ),for,commonsense question answering,datase,/content/training-data/natural_language_inference/14/triples/dataset.txt
commonsense question answering,in the style of,SWAG multiple choice sentence completion,datase,/content/training-data/natural_language_inference/14/triples/dataset.txt
Contribution,Code,https://github.com/Websail-NU /CODAH,code,/content/training-data/natural_language_inference/14/triples/code.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/natural_language_inference/14/triples/hyperparameters.txt
Hyperparameters,added,6 - epoch setting,hyperparameters,/content/training-data/natural_language_inference/14/triples/hyperparameters.txt
Hyperparameters,replaced,5e - 5 learning rate,hyperparameters,/content/training-data/natural_language_inference/14/triples/hyperparameters.txt
5e - 5 learning rate,with,1 e - 5,hyperparameters,/content/training-data/natural_language_inference/14/triples/hyperparameters.txt
5e - 5 learning rate,in,original grid search,hyperparameters,/content/training-data/natural_language_inference/14/triples/hyperparameters.txt
Hyperparameters,when training,initial SWAG model,hyperparameters,/content/training-data/natural_language_inference/14/triples/hyperparameters.txt
initial SWAG model,use,hyperparameters,hyperparameters,/content/training-data/natural_language_inference/14/triples/hyperparameters.txt
hyperparameters,namely,learning rate,hyperparameters,/content/training-data/natural_language_inference/14/triples/hyperparameters.txt
learning rate,of,2 e - 5,hyperparameters,/content/training-data/natural_language_inference/14/triples/hyperparameters.txt
hyperparameters,namely,batch size,hyperparameters,/content/training-data/natural_language_inference/14/triples/hyperparameters.txt
batch size,of,16,hyperparameters,/content/training-data/natural_language_inference/14/triples/hyperparameters.txt
hyperparameters,namely,epochs,hyperparameters,/content/training-data/natural_language_inference/14/triples/hyperparameters.txt
epochs,has,3,hyperparameters,/content/training-data/natural_language_inference/14/triples/hyperparameters.txt
hyperparameters,recommended in,BERT paper,hyperparameters,/content/training-data/natural_language_inference/14/triples/hyperparameters.txt
Hyperparameters,has,final hyperparameter grid,hyperparameters,/content/training-data/natural_language_inference/14/triples/hyperparameters.txt
final hyperparameter grid,has,Number of epochs,hyperparameters,/content/training-data/natural_language_inference/14/triples/hyperparameters.txt
Number of epochs,has,"3 , 4 , 6",hyperparameters,/content/training-data/natural_language_inference/14/triples/hyperparameters.txt
final hyperparameter grid,has,Learning rate,hyperparameters,/content/training-data/natural_language_inference/14/triples/hyperparameters.txt
Learning rate,has,"1 e - 5 , 2 e - 5 , 3 e - 5",hyperparameters,/content/training-data/natural_language_inference/14/triples/hyperparameters.txt
final hyperparameter grid,has,Batch size,hyperparameters,/content/training-data/natural_language_inference/14/triples/hyperparameters.txt
Batch size,has,"16 , 32",hyperparameters,/content/training-data/natural_language_inference/14/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/14/triples/model.txt
Model,has,Annotators,model,/content/training-data/natural_language_inference/14/triples/model.txt
Annotators,rewarded for,submissions,model,/content/training-data/natural_language_inference/14/triples/model.txt
submissions,in which,model,model,/content/training-data/natural_language_inference/14/triples/model.txt
model,has,fails,model,/content/training-data/natural_language_inference/14/triples/model.txt
fails,to identify,correct sentence completion,model,/content/training-data/natural_language_inference/14/triples/model.txt
correct sentence completion,before and after,fine - tuning,model,/content/training-data/natural_language_inference/14/triples/model.txt
fine - tuning,on,sample of the submitted questions,model,/content/training-data/natural_language_inference/14/triples/model.txt
Model,propose,novel method,model,/content/training-data/natural_language_inference/14/triples/model.txt
novel method,in which,human annotators,model,/content/training-data/natural_language_inference/14/triples/model.txt
human annotators,educated on,workings,model,/content/training-data/natural_language_inference/14/triples/model.txt
workings,of,state - of - the - art question answering model,model,/content/training-data/natural_language_inference/14/triples/model.txt
human annotators,asked to,submit,model,/content/training-data/natural_language_inference/14/triples/model.txt
submit,has,questions,model,/content/training-data/natural_language_inference/14/triples/model.txt
questions,adversarially target,weaknesses,model,/content/training-data/natural_language_inference/14/triples/model.txt
novel method,for,question generation,model,/content/training-data/natural_language_inference/14/triples/model.txt
Contribution,has research problem,Question Answering,research-problem,/content/training-data/natural_language_inference/14/triples/research-problem.txt
Contribution,has research problem,Commonsense reasoning,research-problem,/content/training-data/natural_language_inference/14/triples/research-problem.txt
Contribution,has research problem,commonsense reasoning over text,research-problem,/content/training-data/natural_language_inference/14/triples/research-problem.txt
Contribution,has research problem,commonsense question answering,research-problem,/content/training-data/natural_language_inference/14/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/14/triples/results.txt
Results,As,baseline,results,/content/training-data/natural_language_inference/14/triples/results.txt
baseline,evaluate,both models,results,/content/training-data/natural_language_inference/14/triples/results.txt
both models,providing,accuracy,results,/content/training-data/natural_language_inference/14/triples/results.txt
accuracy,of,84.2 %,results,/content/training-data/natural_language_inference/14/triples/results.txt
84.2 %,on,BERT,results,/content/training-data/natural_language_inference/14/triples/results.txt
accuracy,of,80.2 %,results,/content/training-data/natural_language_inference/14/triples/results.txt
80.2 %,on,GPT,results,/content/training-data/natural_language_inference/14/triples/results.txt
both models,on,full SWAG training and validation sets,results,/content/training-data/natural_language_inference/14/triples/results.txt
Results,train,models,results,/content/training-data/natural_language_inference/14/triples/results.txt
models,produces,accuracy,results,/content/training-data/natural_language_inference/14/triples/results.txt
accuracy,of,63.6 %,results,/content/training-data/natural_language_inference/14/triples/results.txt
63.6 %,for,GPT,results,/content/training-data/natural_language_inference/14/triples/results.txt
accuracy,of,75.2 %,results,/content/training-data/natural_language_inference/14/triples/results.txt
75.2 %,for,BERT,results,/content/training-data/natural_language_inference/14/triples/results.txt
models,has,evaluate,results,/content/training-data/natural_language_inference/14/triples/results.txt
evaluate,on,full SWAG validation set,results,/content/training-data/natural_language_inference/14/triples/results.txt
models,on,sample,results,/content/training-data/natural_language_inference/14/triples/results.txt
sample,of,"2,241 SWAG questions",results,/content/training-data/natural_language_inference/14/triples/results.txt
Contribution,Code,https://github.com/Helsinki-NLP/HBMP,code,/content/training-data/natural_language_inference/20/triples/code.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/20/triples/model.txt
Model,extend,InferSent architecture,model,/content/training-data/natural_language_inference/20/triples/model.txt
InferSent architecture,with,hierarchylike structure,model,/content/training-data/natural_language_inference/20/triples/model.txt
hierarchylike structure,of,bidirectional LSTM ( BiLSTM ) layers,model,/content/training-data/natural_language_inference/20/triples/model.txt
bidirectional LSTM ( BiLSTM ) layers,with,max pooling,model,/content/training-data/natural_language_inference/20/triples/model.txt
Model,opt for,sentence encoding approach,model,/content/training-data/natural_language_inference/20/triples/model.txt
Contribution,has research problem,Sentence Embeddings,research-problem,/content/training-data/natural_language_inference/20/triples/research-problem.txt
Contribution,has research problem,Sentence - level representations,research-problem,/content/training-data/natural_language_inference/20/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/20/triples/experimental-setup.txt
Experimental setup,use,dropout,experimental-setup,/content/training-data/natural_language_inference/20/triples/experimental-setup.txt
dropout,of,0.1,experimental-setup,/content/training-data/natural_language_inference/20/triples/experimental-setup.txt
0.1,between,MLP layers,experimental-setup,/content/training-data/natural_language_inference/20/triples/experimental-setup.txt
Experimental setup,use,pre-trained Glo Ve word embeddings,experimental-setup,/content/training-data/natural_language_inference/20/triples/experimental-setup.txt
pre-trained Glo Ve word embeddings,of,size,experimental-setup,/content/training-data/natural_language_inference/20/triples/experimental-setup.txt
size,has,300 dimensions,experimental-setup,/content/training-data/natural_language_inference/20/triples/experimental-setup.txt
Experimental setup,has,3 - layer multilayer perceptron ( MLP ,experimental-setup,/content/training-data/natural_language_inference/20/triples/experimental-setup.txt
3 - layer multilayer perceptron ( MLP ),have,size,experimental-setup,/content/training-data/natural_language_inference/20/triples/experimental-setup.txt
size,of,600 dimensions,experimental-setup,/content/training-data/natural_language_inference/20/triples/experimental-setup.txt
Experimental setup,has,sentence embeddings,experimental-setup,/content/training-data/natural_language_inference/20/triples/experimental-setup.txt
sentence embeddings,have,hidden size,experimental-setup,/content/training-data/natural_language_inference/20/triples/experimental-setup.txt
hidden size,of,600,experimental-setup,/content/training-data/natural_language_inference/20/triples/experimental-setup.txt
600,for,both direction,experimental-setup,/content/training-data/natural_language_inference/20/triples/experimental-setup.txt
Experimental setup,has,architecture,experimental-setup,/content/training-data/natural_language_inference/20/triples/experimental-setup.txt
architecture,implemented using,PyTorch,experimental-setup,/content/training-data/natural_language_inference/20/triples/experimental-setup.txt
Experimental setup,used,gradient descent optimization algorithm,experimental-setup,/content/training-data/natural_language_inference/20/triples/experimental-setup.txt
gradient descent optimization algorithm,based on,Adam update rule,experimental-setup,/content/training-data/natural_language_inference/20/triples/experimental-setup.txt
gradient descent optimization algorithm,pre-implemented in,PyTorch,experimental-setup,/content/training-data/natural_language_inference/20/triples/experimental-setup.txt
Experimental setup,used,learning rate,experimental-setup,/content/training-data/natural_language_inference/20/triples/experimental-setup.txt
learning rate,of,5e - 4,experimental-setup,/content/training-data/natural_language_inference/20/triples/experimental-setup.txt
learning rate,decreased by,factor,experimental-setup,/content/training-data/natural_language_inference/20/triples/experimental-setup.txt
factor,of,0.2,experimental-setup,/content/training-data/natural_language_inference/20/triples/experimental-setup.txt
factor,after,each epoch,experimental-setup,/content/training-data/natural_language_inference/20/triples/experimental-setup.txt
each epoch,if,model,experimental-setup,/content/training-data/natural_language_inference/20/triples/experimental-setup.txt
model,not,improve,experimental-setup,/content/training-data/natural_language_inference/20/triples/experimental-setup.txt
Experimental setup,used,batch size,experimental-setup,/content/training-data/natural_language_inference/20/triples/experimental-setup.txt
batch size,of,64,experimental-setup,/content/training-data/natural_language_inference/20/triples/experimental-setup.txt
Experimental setup,trained using,one NVIDIA Tesla P100 GPU,experimental-setup,/content/training-data/natural_language_inference/20/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/20/triples/results.txt
Results,close to,current state of the art,results,/content/training-data/natural_language_inference/20/triples/results.txt
current state of the art,on,SNLI,results,/content/training-data/natural_language_inference/20/triples/results.txt
Results,For,MultiNLI matched test set ( MultiNLI - m ,results,/content/training-data/natural_language_inference/20/triples/results.txt
MultiNLI matched test set ( MultiNLI - m ),has,our model,results,/content/training-data/natural_language_inference/20/triples/results.txt
our model,achieves,test accuracy,results,/content/training-data/natural_language_inference/20/triples/results.txt
test accuracy,of,73.7 %,results,/content/training-data/natural_language_inference/20/triples/results.txt
73.7 %,after,3 epochs,results,/content/training-data/natural_language_inference/20/triples/results.txt
3 epochs,of,training,results,/content/training-data/natural_language_inference/20/triples/results.txt
73.7 %,which is,0.8 % points lower,results,/content/training-data/natural_language_inference/20/triples/results.txt
0.8 % points lower,than,state of the art 74.5 %,results,/content/training-data/natural_language_inference/20/triples/results.txt
Results,For,SNLI dataset,results,/content/training-data/natural_language_inference/20/triples/results.txt
SNLI dataset,has,our model,results,/content/training-data/natural_language_inference/20/triples/results.txt
our model,provides,test accuracy,results,/content/training-data/natural_language_inference/20/triples/results.txt
test accuracy,of,86.6 %,results,/content/training-data/natural_language_inference/20/triples/results.txt
test accuracy,after,4 epochs,results,/content/training-data/natural_language_inference/20/triples/results.txt
4 epochs,of,training,results,/content/training-data/natural_language_inference/20/triples/results.txt
Results,For,mismatched test set ( MultiNLI - mm ,results,/content/training-data/natural_language_inference/20/triples/results.txt
mismatched test set ( MultiNLI - mm ),has,our model,results,/content/training-data/natural_language_inference/20/triples/results.txt
our model,achieves,test accuracy,results,/content/training-data/natural_language_inference/20/triples/results.txt
test accuracy,of,73.0 %,results,/content/training-data/natural_language_inference/20/triples/results.txt
73.0 %,after,3 epochs,results,/content/training-data/natural_language_inference/20/triples/results.txt
3 epochs,of,training,results,/content/training-data/natural_language_inference/20/triples/results.txt
73.0 %,which is,0.6 % points lower,results,/content/training-data/natural_language_inference/20/triples/results.txt
0.6 % points lower,than,state of the art 73.6 %,results,/content/training-data/natural_language_inference/20/triples/results.txt
Results,has,clearly outperforms,results,/content/training-data/natural_language_inference/20/triples/results.txt
clearly outperforms,has,non-hierarchical BiLSTM models,results,/content/training-data/natural_language_inference/20/triples/results.txt
Results,strong on,matched and mismatched test sets,results,/content/training-data/natural_language_inference/20/triples/results.txt
matched and mismatched test sets,of,MultiNLI,results,/content/training-data/natural_language_inference/20/triples/results.txt
Results,fares,well,results,/content/training-data/natural_language_inference/20/triples/results.txt
well,in comparison to,other state of the art architectures,results,/content/training-data/natural_language_inference/20/triples/results.txt
other state of the art architectures,in,sentence encoding category,results,/content/training-data/natural_language_inference/20/triples/results.txt
Results,on,SciTail,results,/content/training-data/natural_language_inference/20/triples/results.txt
SciTail,achieve,new state of the art,results,/content/training-data/natural_language_inference/20/triples/results.txt
new state of the art,with,accuracy,results,/content/training-data/natural_language_inference/20/triples/results.txt
accuracy,of,86.0 %,results,/content/training-data/natural_language_inference/20/triples/results.txt
Results,On,SciTail dataset,results,/content/training-data/natural_language_inference/20/triples/results.txt
SciTail dataset,obtain,score,results,/content/training-data/natural_language_inference/20/triples/results.txt
score,of,86.0 %,results,/content/training-data/natural_language_inference/20/triples/results.txt
86.0 %,is,2.7 % points absolute improvement,results,/content/training-data/natural_language_inference/20/triples/results.txt
2.7 % points absolute improvement,on,previous published state of the art,results,/content/training-data/natural_language_inference/20/triples/results.txt
86.0 %,after,4 epochs,results,/content/training-data/natural_language_inference/20/triples/results.txt
4 epochs,of,training,results,/content/training-data/natural_language_inference/20/triples/results.txt
SciTail dataset,has,Our model,results,/content/training-data/natural_language_inference/20/triples/results.txt
Our model,outperforms,In - fer Sent,results,/content/training-data/natural_language_inference/20/triples/results.txt
In - fer Sent,which,achieves,results,/content/training-data/natural_language_inference/20/triples/results.txt
achieves,accuracy,85.1 %,results,/content/training-data/natural_language_inference/20/triples/results.txt
SciTail dataset,has,results,results,/content/training-data/natural_language_inference/20/triples/results.txt
results,achieved by,proposed model,results,/content/training-data/natural_language_inference/20/triples/results.txt
proposed model,are,significantly higher,results,/content/training-data/natural_language_inference/20/triples/results.txt
significantly higher,than,previously published results,results,/content/training-data/natural_language_inference/20/triples/results.txt
SciTail dataset,compared,our model,results,/content/training-data/natural_language_inference/20/triples/results.txt
our model,against,non-sentence embedding - based models,results,/content/training-data/natural_language_inference/20/triples/results.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/55/triples/model.txt
Model,providing,ability,model,/content/training-data/natural_language_inference/55/triples/model.txt
ability,to answer,more complicated questions,model,/content/training-data/natural_language_inference/55/triples/model.txt
Model,has,richer representation,model,/content/training-data/natural_language_inference/55/triples/model.txt
richer representation,of,answers,model,/content/training-data/natural_language_inference/55/triples/model.txt
richer representation,encodes,question - answer path,model,/content/training-data/natural_language_inference/55/triples/model.txt
richer representation,encodes,surrounding subgraph,model,/content/training-data/natural_language_inference/55/triples/model.txt
surrounding subgraph,of,KB,model,/content/training-data/natural_language_inference/55/triples/model.txt
Model,has,more sophisticated inference procedure,model,/content/training-data/natural_language_inference/55/triples/model.txt
more sophisticated inference procedure,is,efficient,model,/content/training-data/natural_language_inference/55/triples/model.txt
more sophisticated inference procedure,consider,longer paths,model,/content/training-data/natural_language_inference/55/triples/model.txt
Contribution,has research problem,Question Answering,research-problem,/content/training-data/natural_language_inference/55/triples/research-problem.txt
Contribution,has research problem,answer questions on a broad range of topics,research-problem,/content/training-data/natural_language_inference/55/triples/research-problem.txt
Contribution,has research problem,automatically answer questions asked in natural language on any topic or in any domain,research-problem,/content/training-data/natural_language_inference/55/triples/research-problem.txt
Contribution,has research problem,open - domain question answering ( or open QA ,research-problem,/content/training-data/natural_language_inference/55/triples/research-problem.txt
Contribution,has research problem,open QA,research-problem,/content/training-data/natural_language_inference/55/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/55/triples/results.txt
Results,using,all 2 - hops connections,results,/content/training-data/natural_language_inference/55/triples/results.txt
all 2 - hops connections,as,candidate set,results,/content/training-data/natural_language_inference/55/triples/results.txt
all 2 - hops connections,is,detrimental,results,/content/training-data/natural_language_inference/55/triples/results.txt
Results,verify,hypothesis,results,/content/training-data/natural_language_inference/55/triples/results.txt
hypothesis,that,richer representation,results,/content/training-data/natural_language_inference/55/triples/results.txt
richer representation,for,answers,results,/content/training-data/natural_language_inference/55/triples/results.txt
richer representation,store,more pertinent information,results,/content/training-data/natural_language_inference/55/triples/results.txt
Results,demonstrate,greatly improve,results,/content/training-data/natural_language_inference/55/triples/results.txt
greatly improve,corresponds to,setting,results,/content/training-data/natural_language_inference/55/triples/results.txt
setting,with,Path representation,results,/content/training-data/natural_language_inference/55/triples/results.txt
setting,with,C 1,results,/content/training-data/natural_language_inference/55/triples/results.txt
C 1,as,candidate set,results,/content/training-data/natural_language_inference/55/triples/results.txt
greatly improve,upon,model,results,/content/training-data/natural_language_inference/55/triples/results.txt
Results,has,ensemble,results,/content/training-data/natural_language_inference/55/triples/results.txt
ensemble,has,improves,results,/content/training-data/natural_language_inference/55/triples/results.txt
improves,has,state - of - the - art,results,/content/training-data/natural_language_inference/55/triples/results.txt
Results,Replacing,C 2,results,/content/training-data/natural_language_inference/55/triples/results.txt
C 2,induces,large drop,results,/content/training-data/natural_language_inference/55/triples/results.txt
large drop,in,performance,results,/content/training-data/natural_language_inference/55/triples/results.txt
C 2,by,C 1,results,/content/training-data/natural_language_inference/55/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/12/triples/ablation-analysis.txt
Ablation analysis,based on,Multi - NLI development sets,ablation-analysis,/content/training-data/natural_language_inference/12/triples/ablation-analysis.txt
Ablation analysis,show,shortcut connections,ablation-analysis,/content/training-data/natural_language_inference/12/triples/ablation-analysis.txt
shortcut connections,among,biLSTM layers,ablation-analysis,/content/training-data/natural_language_inference/12/triples/ablation-analysis.txt
shortcut connections,is,important contributor,ablation-analysis,/content/training-data/natural_language_inference/12/triples/ablation-analysis.txt
important contributor,to,accuracy improvement,ablation-analysis,/content/training-data/natural_language_inference/12/triples/ablation-analysis.txt
accuracy improvement,around,1.5 %,ablation-analysis,/content/training-data/natural_language_inference/12/triples/ablation-analysis.txt
1.5 %,on top of,full 3 - layered stacked - RNN model,ablation-analysis,/content/training-data/natural_language_inference/12/triples/ablation-analysis.txt
Ablation analysis,has,last ablation,ablation-analysis,/content/training-data/natural_language_inference/12/triples/ablation-analysis.txt
last ablation,shows that,classifier,ablation-analysis,/content/training-data/natural_language_inference/12/triples/ablation-analysis.txt
classifier,with,two layers of relu,ablation-analysis,/content/training-data/natural_language_inference/12/triples/ablation-analysis.txt
two layers of relu,is,preferable,ablation-analysis,/content/training-data/natural_language_inference/12/triples/ablation-analysis.txt
Ablation analysis,has,each added layer model,ablation-analysis,/content/training-data/natural_language_inference/12/triples/ablation-analysis.txt
each added layer model,achieve,substantial improvement,ablation-analysis,/content/training-data/natural_language_inference/12/triples/ablation-analysis.txt
substantial improvement,in,accuracy ( around 2 % ,ablation-analysis,/content/training-data/natural_language_inference/12/triples/ablation-analysis.txt
substantial improvement,compared to,single - layer biLSTM,ablation-analysis,/content/training-data/natural_language_inference/12/triples/ablation-analysis.txt
substantial improvement,on,matched and mismatched settings,ablation-analysis,/content/training-data/natural_language_inference/12/triples/ablation-analysis.txt
each added layer model,improves,accuracy,ablation-analysis,/content/training-data/natural_language_inference/12/triples/ablation-analysis.txt
Ablation analysis,show that,fine - tuning,ablation-analysis,/content/training-data/natural_language_inference/12/triples/ablation-analysis.txt
fine - tuning,improves,results,ablation-analysis,/content/training-data/natural_language_inference/12/triples/ablation-analysis.txt
results,for both,in - domain task and cross - domain tasks,ablation-analysis,/content/training-data/natural_language_inference/12/triples/ablation-analysis.txt
fine - tuning,has,word embeddings,ablation-analysis,/content/training-data/natural_language_inference/12/triples/ablation-analysis.txt
Contribution,Code,https://github.com/ easonnie/multiNLI_encoder,code,/content/training-data/natural_language_inference/12/triples/code.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/natural_language_inference/12/triples/hyperparameters.txt
Hyperparameters,use,cross - entropy loss,hyperparameters,/content/training-data/natural_language_inference/12/triples/hyperparameters.txt
cross - entropy loss,as,training objective,hyperparameters,/content/training-data/natural_language_inference/12/triples/hyperparameters.txt
training objective,with,Adam,hyperparameters,/content/training-data/natural_language_inference/12/triples/hyperparameters.txt
Adam,with,32 batch size,hyperparameters,/content/training-data/natural_language_inference/12/triples/hyperparameters.txt
Hyperparameters,has,number of hidden units,hyperparameters,/content/training-data/natural_language_inference/12/triples/hyperparameters.txt
number of hidden units,in,classifier,hyperparameters,/content/training-data/natural_language_inference/12/triples/hyperparameters.txt
number of hidden units,for,MLP,hyperparameters,/content/training-data/natural_language_inference/12/triples/hyperparameters.txt
number of hidden units,is,1600,hyperparameters,/content/training-data/natural_language_inference/12/triples/hyperparameters.txt
Hyperparameters,has,Dropout layer,hyperparameters,/content/training-data/natural_language_inference/12/triples/hyperparameters.txt
Dropout layer,applied on,output,hyperparameters,/content/training-data/natural_language_inference/12/triples/hyperparameters.txt
output,with,dropout rate,hyperparameters,/content/training-data/natural_language_inference/12/triples/hyperparameters.txt
dropout rate,set to,0.1,hyperparameters,/content/training-data/natural_language_inference/12/triples/hyperparameters.txt
output,of,each layer of MLP,hyperparameters,/content/training-data/natural_language_inference/12/triples/hyperparameters.txt
Hyperparameters,has,starting learning rate,hyperparameters,/content/training-data/natural_language_inference/12/triples/hyperparameters.txt
starting learning rate,is,0.0002,hyperparameters,/content/training-data/natural_language_inference/12/triples/hyperparameters.txt
0.0002,with,half decay,hyperparameters,/content/training-data/natural_language_inference/12/triples/hyperparameters.txt
half decay,every,two epochs,hyperparameters,/content/training-data/natural_language_inference/12/triples/hyperparameters.txt
Hyperparameters,used,pre-trained 300D Glove 840B vectors,hyperparameters,/content/training-data/natural_language_inference/12/triples/hyperparameters.txt
pre-trained 300D Glove 840B vectors,to initialize,word embeddings,hyperparameters,/content/training-data/natural_language_inference/12/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/12/triples/model.txt
Model,has,over all supervised model,model,/content/training-data/natural_language_inference/12/triples/model.txt
over all supervised model,use,classifier,model,/content/training-data/natural_language_inference/12/triples/model.txt
classifier,over,vector combination,model,/content/training-data/natural_language_inference/12/triples/model.txt
vector combination,to label,relationship,model,/content/training-data/natural_language_inference/12/triples/model.txt
relationship,as,entailment,model,/content/training-data/natural_language_inference/12/triples/model.txt
relationship,as,contradiction,model,/content/training-data/natural_language_inference/12/triples/model.txt
relationship,as,neural,model,/content/training-data/natural_language_inference/12/triples/model.txt
relationship,between,two sentences,model,/content/training-data/natural_language_inference/12/triples/model.txt
over all supervised model,uses,shortcutstacked encoders,model,/content/training-data/natural_language_inference/12/triples/model.txt
shortcutstacked encoders,to encode,two input sentences,model,/content/training-data/natural_language_inference/12/triples/model.txt
two input sentences,into,two vectors,model,/content/training-data/natural_language_inference/12/triples/model.txt
Model,has,stacked ( multi-layered ) bidirectional LSTM - RNN,model,/content/training-data/natural_language_inference/12/triples/model.txt
stacked ( multi-layered ) bidirectional LSTM - RNN,with,shortcut connections,model,/content/training-data/natural_language_inference/12/triples/model.txt
stacked ( multi-layered ) bidirectional LSTM - RNN,with,word embedding fine - tuning,model,/content/training-data/natural_language_inference/12/triples/model.txt
Model,follow,former approach,model,/content/training-data/natural_language_inference/12/triples/model.txt
former approach,of,encoding - based models,model,/content/training-data/natural_language_inference/12/triples/model.txt
Model,propose,novel yet simple sequential sentence encoder,model,/content/training-data/natural_language_inference/12/triples/model.txt
novel yet simple sequential sentence encoder,for,Multi - NLI problem,model,/content/training-data/natural_language_inference/12/triples/model.txt
Contribution,has research problem,Multi- Domain Inference,research-problem,/content/training-data/natural_language_inference/12/triples/research-problem.txt
Contribution,has research problem,multi-domain natural language inference,research-problem,/content/training-data/natural_language_inference/12/triples/research-problem.txt
Contribution,has research problem,Natural language inference ( NLI ,research-problem,/content/training-data/natural_language_inference/12/triples/research-problem.txt
Contribution,has research problem,recognizing textual entailment ( RTE ,research-problem,/content/training-data/natural_language_inference/12/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/12/triples/results.txt
Results,for,Multi - NLI,results,/content/training-data/natural_language_inference/12/triples/results.txt
Multi - NLI,improve,substantially,results,/content/training-data/natural_language_inference/12/triples/results.txt
substantially,over,CBOW and biL - STM Encoder baselines,results,/content/training-data/natural_language_inference/12/triples/results.txt
Multi - NLI,has,shortcut - encoder,results,/content/training-data/natural_language_inference/12/triples/results.txt
shortcut - encoder,was,top singe - model ( non-ensemble ) result,results,/content/training-data/natural_language_inference/12/triples/results.txt
top singe - model ( non-ensemble ) result,on,EMNLP RepEval Shared Task leaderboard,results,/content/training-data/natural_language_inference/12/triples/results.txt
Multi - NLI,show that,our final shortcut - based stacked encoder,results,/content/training-data/natural_language_inference/12/triples/results.txt
our final shortcut - based stacked encoder,achieves,around 3 % improvement,results,/content/training-data/natural_language_inference/12/triples/results.txt
around 3 % improvement,compared to,1 layer biLSTM - Max Encoder,results,/content/training-data/natural_language_inference/12/triples/results.txt
Results,for,SNLI,results,/content/training-data/natural_language_inference/12/triples/results.txt
SNLI,compare to,recent biLSTM - Max Encoder,results,/content/training-data/natural_language_inference/12/triples/results.txt
recent biLSTM - Max Encoder,has,results,results,/content/training-data/natural_language_inference/12/triples/results.txt
results,indicate,Our Shortcut - Stacked Encoder,results,/content/training-data/natural_language_inference/12/triples/results.txt
Our Shortcut - Stacked Encoder,sur-passes,all the previous state - of - the - art encoders,results,/content/training-data/natural_language_inference/12/triples/results.txt
Our Shortcut - Stacked Encoder,achieves,new best encoding - based result,results,/content/training-data/natural_language_inference/12/triples/results.txt
new best encoding - based result,on,SNLI,results,/content/training-data/natural_language_inference/12/triples/results.txt
SNLI,compare,shortcutstacked encoder,results,/content/training-data/natural_language_inference/12/triples/results.txt
shortcutstacked encoder,with,current state - of - the - art encoders,results,/content/training-data/natural_language_inference/12/triples/results.txt
current state - of - the - art encoders,from,SNLI leaderboard,results,/content/training-data/natural_language_inference/12/triples/results.txt
Contribution,Code,https://github.com / stanfordnlp/spinn.,code,/content/training-data/natural_language_inference/91/triples/code.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/91/triples/model.txt
Model,introduces,Stack - augmented Parser - Interpreter Neural Network,model,/content/training-data/natural_language_inference/91/triples/model.txt
Stack - augmented Parser - Interpreter Neural Network,name,SPINN,model,/content/training-data/natural_language_inference/91/triples/model.txt
Model,improves upon,TreeRNN architecture,model,/content/training-data/natural_language_inference/91/triples/model.txt
TreeRNN architecture,At,test time,model,/content/training-data/natural_language_inference/91/triples/model.txt
test time,can simultaneously,parse,model,/content/training-data/natural_language_inference/91/triples/model.txt
test time,can simultaneously,interpret,model,/content/training-data/natural_language_inference/91/triples/model.txt
interpret,has,unparsed sentences,model,/content/training-data/natural_language_inference/91/triples/model.txt
test time,removing,dependence,model,/content/training-data/natural_language_inference/91/triples/model.txt
dependence,on,external parser,model,/content/training-data/natural_language_inference/91/triples/model.txt
TreeRNN architecture,supports,batched computation,model,/content/training-data/natural_language_inference/91/triples/model.txt
batched computation,yielding,dramatic speedups,model,/content/training-data/natural_language_inference/91/triples/model.txt
dramatic speedups,over,standard TreeRNNs,model,/content/training-data/natural_language_inference/91/triples/model.txt
batched computation,for,parsed and unparsed sentences,model,/content/training-data/natural_language_inference/91/triples/model.txt
TreeRNN architecture,supports,novel tree - sequence hybrid architecture,model,/content/training-data/natural_language_inference/91/triples/model.txt
novel tree - sequence hybrid architecture,for handling,local linear context,model,/content/training-data/natural_language_inference/91/triples/model.txt
local linear context,in,sentence interpretation,model,/content/training-data/natural_language_inference/91/triples/model.txt
Model,has,SPINN,model,/content/training-data/natural_language_inference/91/triples/model.txt
SPINN,executes,computations,model,/content/training-data/natural_language_inference/91/triples/model.txt
computations,in,linearized sequence,model,/content/training-data/natural_language_inference/91/triples/model.txt
computations,of,tree - structured model,model,/content/training-data/natural_language_inference/91/triples/model.txt
SPINN,incorporate,neural network parser,model,/content/training-data/natural_language_inference/91/triples/model.txt
neural network parser,that produces,required parse structure,model,/content/training-data/natural_language_inference/91/triples/model.txt
Contribution,has research problem,Parsing,research-problem,/content/training-data/natural_language_inference/91/triples/research-problem.txt
Contribution,has research problem,Sentence Understanding,research-problem,/content/training-data/natural_language_inference/91/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/91/triples/experimental-setup.txt
Experimental setup,has,full SPINN,experimental-setup,/content/training-data/natural_language_inference/91/triples/experimental-setup.txt
full SPINN,has,Our optimized C ++/ CUDA models,experimental-setup,/content/training-data/natural_language_inference/91/triples/experimental-setup.txt
full SPINN,has,Theano source code,experimental-setup,/content/training-data/natural_language_inference/91/triples/experimental-setup.txt
Experimental setup,has,fix,experimental-setup,/content/training-data/natural_language_inference/91/triples/experimental-setup.txt
fix,at,300,experimental-setup,/content/training-data/natural_language_inference/91/triples/experimental-setup.txt
300,has,model dimension D,experimental-setup,/content/training-data/natural_language_inference/91/triples/experimental-setup.txt
300,has,word embedding dimension,experimental-setup,/content/training-data/natural_language_inference/91/triples/experimental-setup.txt
Experimental setup,has,CPU performance test,experimental-setup,/content/training-data/natural_language_inference/91/triples/experimental-setup.txt
CPU performance test,on,2.20 GHz 16 core Intel Xeon E5-2660 processor,experimental-setup,/content/training-data/natural_language_inference/91/triples/experimental-setup.txt
2.20 GHz 16 core Intel Xeon E5-2660 processor,with,hyperthreading,experimental-setup,/content/training-data/natural_language_inference/91/triples/experimental-setup.txt
Experimental setup,on,NVIDIA Titan X GPU,experimental-setup,/content/training-data/natural_language_inference/91/triples/experimental-setup.txt
NVIDIA Titan X GPU,test,thin - stack implementation,experimental-setup,/content/training-data/natural_language_inference/91/triples/experimental-setup.txt
NVIDIA Titan X GPU,test,RNN model,experimental-setup,/content/training-data/natural_language_inference/91/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/91/triples/results.txt
Results,find that,SPINN - PI,results,/content/training-data/natural_language_inference/91/triples/results.txt
SPINN - PI,with,added tracking LSTM,results,/content/training-data/natural_language_inference/91/triples/results.txt
SPINN - PI,performs,well,results,/content/training-data/natural_language_inference/91/triples/results.txt
Results,find that,bare SPINN - PI - NT model,results,/content/training-data/natural_language_inference/91/triples/results.txt
bare SPINN - PI - NT model,performs,little better,results,/content/training-data/natural_language_inference/91/triples/results.txt
little better,than,RNN baseline,results,/content/training-data/natural_language_inference/91/triples/results.txt
Results,has,full SPINN,results,/content/training-data/natural_language_inference/91/triples/results.txt
full SPINN,performed,moderately well,results,/content/training-data/natural_language_inference/91/triples/results.txt
moderately well,at reproducing,Stanford Parser 's parses,results,/content/training-data/natural_language_inference/91/triples/results.txt
Stanford Parser 's parses,at,transition - by - transition level,results,/content/training-data/natural_language_inference/91/triples/results.txt
Stanford Parser 's parses,of,SNLI data,results,/content/training-data/natural_language_inference/91/triples/results.txt
moderately well,with,92.4 % accuracy,results,/content/training-data/natural_language_inference/91/triples/results.txt
92.4 % accuracy,at,test time,results,/content/training-data/natural_language_inference/91/triples/results.txt
Results,has,full SPINN model,results,/content/training-data/natural_language_inference/91/triples/results.txt
full SPINN model,with,relatively weak internal parser,results,/content/training-data/natural_language_inference/91/triples/results.txt
full SPINN model,performs,slightly less well,results,/content/training-data/natural_language_inference/91/triples/results.txt
full SPINN model,has,robustly exceeds,results,/content/training-data/natural_language_inference/91/triples/results.txt
robustly exceeds,has,performance,results,/content/training-data/natural_language_inference/91/triples/results.txt
performance,of,RNN baseline,results,/content/training-data/natural_language_inference/91/triples/results.txt
Results,has,SPINN - PI and the full SPINN,results,/content/training-data/natural_language_inference/91/triples/results.txt
SPINN - PI and the full SPINN,has,outperform,results,/content/training-data/natural_language_inference/91/triples/results.txt
outperform,has,tree - based CNN,results,/content/training-data/natural_language_inference/91/triples/results.txt
tree - based CNN,uses,tree - structured composition,results,/content/training-data/natural_language_inference/91/triples/results.txt
tree - structured composition,for,local feature extraction,results,/content/training-data/natural_language_inference/91/triples/results.txt
tree - based CNN,uses,simpler pooling techniques,results,/content/training-data/natural_language_inference/91/triples/results.txt
simpler pooling techniques,to build,sentence features,results,/content/training-data/natural_language_inference/91/triples/results.txt
SPINN - PI and the full SPINN,has,significantly outperform,results,/content/training-data/natural_language_inference/91/triples/results.txt
significantly outperform,has,previous sentence - encoding models,results,/content/training-data/natural_language_inference/91/triples/results.txt
Results,show that,model,results,/content/training-data/natural_language_inference/91/triples/results.txt
model,that uses,tree - structured composition fully ( SPINN ,results,/content/training-data/natural_language_inference/91/triples/results.txt
tree - structured composition fully ( SPINN ),has,outper - forms,results,/content/training-data/natural_language_inference/91/triples/results.txt
outper - forms,has,only partially ( tree - based CNN ,results,/content/training-data/natural_language_inference/91/triples/results.txt
only partially ( tree - based CNN ),has,outperforms,results,/content/training-data/natural_language_inference/91/triples/results.txt
outperforms,has,one,results,/content/training-data/natural_language_inference/91/triples/results.txt
one,which,not use it at all ( RNN ,results,/content/training-data/natural_language_inference/91/triples/results.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/75/triples/model.txt
Model,encode,prior knowledge,model,/content/training-data/natural_language_inference/75/triples/model.txt
Model,to leverage,complex transforms,model,/content/training-data/natural_language_inference/75/triples/model.txt
complex transforms,between,keys and values,model,/content/training-data/natural_language_inference/75/triples/model.txt
Model,has,memory,model,/content/training-data/natural_language_inference/75/triples/model.txt
memory,is,designed,model,/content/training-data/natural_language_inference/75/triples/model.txt
designed,to use,keys,model,/content/training-data/natural_language_inference/75/triples/model.txt
keys,to address,relevant memories,model,/content/training-data/natural_language_inference/75/triples/model.txt
relevant memories,with respect to,question,model,/content/training-data/natural_language_inference/75/triples/model.txt
Model,has,KV - MemNN,model,/content/training-data/natural_language_inference/75/triples/model.txt
KV - MemNN,performs,QA,model,/content/training-data/natural_language_inference/75/triples/model.txt
QA,storing,facts,model,/content/training-data/natural_language_inference/75/triples/model.txt
facts,in,value structured memory,model,/content/training-data/natural_language_inference/75/triples/model.txt
Model,propose,Key - Value Memory Network ( KV - MemNN ,model,/content/training-data/natural_language_inference/75/triples/model.txt
Key - Value Memory Network ( KV - MemNN ),has,new neural network architecture,model,/content/training-data/natural_language_inference/75/triples/model.txt
new neural network architecture,generalizes,original Memory Network,model,/content/training-data/natural_language_inference/75/triples/model.txt
Contribution,has research problem,Directly reading documents,research-problem,/content/training-data/natural_language_inference/75/triples/research-problem.txt
Contribution,has research problem,question answering ( QA ,research-problem,/content/training-data/natural_language_inference/75/triples/research-problem.txt
Contribution,has research problem,QA,research-problem,/content/training-data/natural_language_inference/75/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/75/triples/results.txt
Results,has,WikiMovies,results,/content/training-data/natural_language_inference/75/triples/results.txt
WikiMovies,has,Reading,results,/content/training-data/natural_language_inference/75/triples/results.txt
Reading,from,Wikipedia documents,results,/content/training-data/natural_language_inference/75/triples/results.txt
Wikipedia documents,has,outperforms,results,/content/training-data/natural_language_inference/75/triples/results.txt
outperforms,has,IE - based KB ( IE ,results,/content/training-data/natural_language_inference/75/triples/results.txt
WikiMovies,has,Key - Value Memory Networks,results,/content/training-data/natural_language_inference/75/triples/results.txt
Key - Value Memory Networks,has,outperform,results,/content/training-data/natural_language_inference/75/triples/results.txt
outperform,has,all other methods,results,/content/training-data/natural_language_inference/75/triples/results.txt
Results,has,WikiQA,results,/content/training-data/natural_language_inference/75/triples/results.txt
WikiQA,has,Key - Value Memory Networks,results,/content/training-data/natural_language_inference/75/triples/results.txt
Key - Value Memory Networks,has,outperform,results,/content/training-data/natural_language_inference/75/triples/results.txt
outperform,has,large set of other methods,results,/content/training-data/natural_language_inference/75/triples/results.txt
Contribution,has,Experiments,experiments,/content/training-data/natural_language_inference/70/triples/experiments.txt
Experiments,has,Tasks,experiments,/content/training-data/natural_language_inference/70/triples/experiments.txt
Tasks,has,Subtask C,experiments,/content/training-data/natural_language_inference/70/triples/experiments.txt
Subtask C,has,Results,experiments,/content/training-data/natural_language_inference/70/triples/experiments.txt
Results,noted that,F 1 our system,experiments,/content/training-data/natural_language_inference/70/triples/experiments.txt
F 1 our system,is,best,experiments,/content/training-data/natural_language_inference/70/triples/experiments.txt
best,among,10 primary submissions,experiments,/content/training-data/natural_language_inference/70/triples/experiments.txt
Results,has,primary submission,experiments,/content/training-data/natural_language_inference/70/triples/experiments.txt
primary submission,achieved,second highest MAP,experiments,/content/training-data/natural_language_inference/70/triples/experiments.txt
Tasks,has,Subtask B,experiments,/content/training-data/natural_language_inference/70/triples/experiments.txt
Subtask B,has,Results,experiments,/content/training-data/natural_language_inference/70/triples/experiments.txt
Results,has,primary system,experiments,/content/training-data/natural_language_inference/70/triples/experiments.txt
primary system,achieves,highest F 1 and accuracy,experiments,/content/training-data/natural_language_inference/70/triples/experiments.txt
highest F 1 and accuracy,on,both tuning and test stages,experiments,/content/training-data/natural_language_inference/70/triples/experiments.txt
Results,has,our primary submission,experiments,/content/training-data/natural_language_inference/70/triples/experiments.txt
our primary submission,achieved,third position,experiments,/content/training-data/natural_language_inference/70/triples/experiments.txt
third position,w.r.t.,MAP,experiments,/content/training-data/natural_language_inference/70/triples/experiments.txt
third position,among,11 systems,experiments,/content/training-data/natural_language_inference/70/triples/experiments.txt
Tasks,has,Subtask A,experiments,/content/training-data/natural_language_inference/70/triples/experiments.txt
Subtask A,has,Results,experiments,/content/training-data/natural_language_inference/70/triples/experiments.txt
Results,achieved,first position,experiments,/content/training-data/natural_language_inference/70/triples/experiments.txt
first position,with,best MAP,experiments,/content/training-data/natural_language_inference/70/triples/experiments.txt
first position,among,12 systems,experiments,/content/training-data/natural_language_inference/70/triples/experiments.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/70/triples/model.txt
Model,applies,tree kernels,model,/content/training-data/natural_language_inference/70/triples/model.txt
tree kernels,directly to,question and answer texts,model,/content/training-data/natural_language_inference/70/triples/model.txt
question and answer texts,modeled as,pairs,model,/content/training-data/natural_language_inference/70/triples/model.txt
pairs,of,linked syntactic trees,model,/content/training-data/natural_language_inference/70/triples/model.txt
Model,has,classifiers and kernels,model,/content/training-data/natural_language_inference/70/triples/model.txt
classifiers and kernels,implemented within,Kernel - based Learning Platform 2 ( KeLP ,model,/content/training-data/natural_language_inference/70/triples/model.txt
Model,propose,stacking schema,model,/content/training-data/natural_language_inference/70/triples/model.txt
stacking schema,has,classifiers,model,/content/training-data/natural_language_inference/70/triples/model.txt
classifiers,for,Subtask B and C,model,/content/training-data/natural_language_inference/70/triples/model.txt
classifiers,exploit,inferences,model,/content/training-data/natural_language_inference/70/triples/model.txt
inferences,obtained in,previous subtasks,model,/content/training-data/natural_language_inference/70/triples/model.txt
Model,modeled as,binary classification problems,model,/content/training-data/natural_language_inference/70/triples/model.txt
binary classification problems,has,kernel - based classifiers,model,/content/training-data/natural_language_inference/70/triples/model.txt
kernel - based classifiers,are,trained,model,/content/training-data/natural_language_inference/70/triples/model.txt
kernel - based classifiers,has,classification score,model,/content/training-data/natural_language_inference/70/triples/model.txt
classification score,to sort,instances,model,/content/training-data/natural_language_inference/70/triples/model.txt
classification score,produce,final ranking,model,/content/training-data/natural_language_inference/70/triples/model.txt
Contribution,has research problem,Community Question Answering ( c QA ,research-problem,/content/training-data/natural_language_inference/70/triples/research-problem.txt
Contribution,has research problem,c QA,research-problem,/content/training-data/natural_language_inference/70/triples/research-problem.txt
Contribution,has,Experiments,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
Experiments,has,Tasks,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
Tasks,has,SQuAD - Adversarial,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
SQuAD - Adversarial,has,Results,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
Results,shows,MINIMAL,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
MINIMAL,has,outperforms,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
outperforms,achieving,new state - of - the - art,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
new state - of - the - art,by,large margin,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
large margin,has,+ 11.1 and + 11.5 F1,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
+ 11.1 and + 11.5 F1,on,AddSent and Add OneSent,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
outperforms,has,FULL,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
Tasks,has,Trivia QA and SQuAD - Open,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
Trivia QA and SQuAD - Open,has,Results,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
Results,has,model,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
model,with,our sentence selector,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
our sentence selector,with,Dyn,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
our sentence selector,achieves,higher F1 and EM,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
higher F1 and EM,over,model,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
model,with,TF - IDF selector,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
Results,has,MINI - MAL,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
MINI - MAL,obtains,higher F1 and EM,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
higher F1 and EM,over,FULL,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
Results,has,outperforms,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
outperforms,has,state - of - the - art,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
state - of - the - art,on,both dataset,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
Trivia QA and SQuAD - Open,has,Baselines,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
Baselines,compare with,TF - IDF method,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
Baselines,compare with,our selector ( Dyn ,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
Baselines,compare with,published Rank1 - 3 models,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
Tasks,has,SQuAD and New s QA,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
SQuAD and New s QA,has,Results,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
Results,has,Dyn method,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
Dyn method,achieves,higher accuracy,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
higher accuracy,with,less sentences,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
less sentences,than,Top k method,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
Results,has,three training techniques,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
three training techniques,improve,performance,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
performance,by,up to 5.6 % MAP,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
three training techniques,has,weight transfer,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
three training techniques,has,data modification,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
three training techniques,has,score normalization,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
Results,has,selector,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
selector,has,outperforms,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
outperforms,has,TF - IDF method and the previous state - of - the - art,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
TF - IDF method and the previous state - of - the - art,by,large margin,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
large margin,up to,2.9 % MAP,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
Results,On,SQuAD,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
SQuAD,has,S - Reader,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
S - Reader,on,SQuAD,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
SQuAD,achieves,speedup,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
speedup,has,6.7 training,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
speedup,has,3.6 inference,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
S - Reader,on,News QA,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
News QA,has,speedup,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
speedup,has,15.0 training,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
speedup,has,6.9 inference,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
Results,On,News QA,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
News QA,has,Dyn,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
Dyn,achieves,94.6 accuracy,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
94.6 accuracy,with,3.9 sentences per example,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
News QA,has,Top 4,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
Top 4,achieves,92.5 accuracy,experiments,/content/training-data/natural_language_inference/44/triples/experiments.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/44/triples/model.txt
Model,study,context,model,/content/training-data/natural_language_inference/44/triples/model.txt
context,by sampling,examples,model,/content/training-data/natural_language_inference/44/triples/model.txt
examples,in,dataset,model,/content/training-data/natural_language_inference/44/triples/model.txt
context,required to,answer the question,model,/content/training-data/natural_language_inference/44/triples/model.txt
Model,develop,QA system,model,/content/training-data/natural_language_inference/44/triples/model.txt
QA system,scalable to,large documents,model,/content/training-data/natural_language_inference/44/triples/model.txt
QA system,robust to,adversarial inputs,model,/content/training-data/natural_language_inference/44/triples/model.txt
Model,propose,sentence selector,model,/content/training-data/natural_language_inference/44/triples/model.txt
sentence selector,to select,minimal set of sentences,model,/content/training-data/natural_language_inference/44/triples/model.txt
minimal set of sentences,to give,QA model,model,/content/training-data/natural_language_inference/44/triples/model.txt
QA model,to answer,question,model,/content/training-data/natural_language_inference/44/triples/model.txt
sentence selector,leverages,three simple techniques,model,/content/training-data/natural_language_inference/44/triples/model.txt
three simple techniques,has,weight transfer,model,/content/training-data/natural_language_inference/44/triples/model.txt
three simple techniques,has,data modification,model,/content/training-data/natural_language_inference/44/triples/model.txt
three simple techniques,has,score normalization,model,/content/training-data/natural_language_inference/44/triples/model.txt
sentence selector,chooses,different number of sentences,model,/content/training-data/natural_language_inference/44/triples/model.txt
different number of sentences,for,each question,model,/content/training-data/natural_language_inference/44/triples/model.txt
Contribution,has research problem,Question Answering,research-problem,/content/training-data/natural_language_inference/44/triples/research-problem.txt
Contribution,has research problem,question answering ( QA ,research-problem,/content/training-data/natural_language_inference/44/triples/research-problem.txt
Contribution,has research problem,QA,research-problem,/content/training-data/natural_language_inference/44/triples/research-problem.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/98/triples/ablation-analysis.txt
Ablation analysis,remove,word - level embedding,ablation-analysis,/content/training-data/natural_language_inference/98/triples/ablation-analysis.txt
word - level embedding,has,accuracies,ablation-analysis,/content/training-data/natural_language_inference/98/triples/ablation-analysis.txt
accuracies,has,drop,ablation-analysis,/content/training-data/natural_language_inference/98/triples/ablation-analysis.txt
drop,to,65.6 % and 66.0 %,ablation-analysis,/content/training-data/natural_language_inference/98/triples/ablation-analysis.txt
Ablation analysis,remove,gated - attention,ablation-analysis,/content/training-data/natural_language_inference/98/triples/ablation-analysis.txt
gated - attention,has,accuracies,ablation-analysis,/content/training-data/natural_language_inference/98/triples/ablation-analysis.txt
accuracies,has,drop,ablation-analysis,/content/training-data/natural_language_inference/98/triples/ablation-analysis.txt
drop,to,72.8 % and 73.6 %,ablation-analysis,/content/training-data/natural_language_inference/98/triples/ablation-analysis.txt
Ablation analysis,remove,charactercomposition vector,ablation-analysis,/content/training-data/natural_language_inference/98/triples/ablation-analysis.txt
charactercomposition vector,has,accuracies,ablation-analysis,/content/training-data/natural_language_inference/98/triples/ablation-analysis.txt
accuracies,has,drop,ablation-analysis,/content/training-data/natural_language_inference/98/triples/ablation-analysis.txt
drop,to,72.9 % and 73.5 %,ablation-analysis,/content/training-data/natural_language_inference/98/triples/ablation-analysis.txt
Contribution,Code,https : //github.com/lukecq1231/enc_nli,code,/content/training-data/natural_language_inference/98/triples/code.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/98/triples/model.txt
Model,proposed,natural language inference networks,model,/content/training-data/natural_language_inference/98/triples/model.txt
natural language inference networks,composed of,word embedding,model,/content/training-data/natural_language_inference/98/triples/model.txt
natural language inference networks,composed of,sequence encoder,model,/content/training-data/natural_language_inference/98/triples/model.txt
natural language inference networks,composed of,composition layer,model,/content/training-data/natural_language_inference/98/triples/model.txt
natural language inference networks,composed of,toplayer classifier,model,/content/training-data/natural_language_inference/98/triples/model.txt
Model,has,Top - layer Classifier,model,/content/training-data/natural_language_inference/98/triples/model.txt
Top - layer Classifier,has,Our inference model,model,/content/training-data/natural_language_inference/98/triples/model.txt
Our inference model,feeds,resulting vectors,model,/content/training-data/natural_language_inference/98/triples/model.txt
resulting vectors,to determine,over all inference relationship,model,/content/training-data/natural_language_inference/98/triples/model.txt
resulting vectors,to,final classifier,model,/content/training-data/natural_language_inference/98/triples/model.txt
Model,has,Sequence Encoder,model,/content/training-data/natural_language_inference/98/triples/model.txt
Sequence Encoder,use,stacked bidirectional LSTMs ( BiL - STM ,model,/content/training-data/natural_language_inference/98/triples/model.txt
stacked bidirectional LSTMs ( BiL - STM ),as,encoders,model,/content/training-data/natural_language_inference/98/triples/model.txt
Sequence Encoder,has,sentence pairs,model,/content/training-data/natural_language_inference/98/triples/model.txt
sentence pairs,fed into,sentence encoders,model,/content/training-data/natural_language_inference/98/triples/model.txt
sentence encoders,to obtain,hidden vectors ( h p and h h ,model,/content/training-data/natural_language_inference/98/triples/model.txt
Model,has,Composition Layer,model,/content/training-data/natural_language_inference/98/triples/model.txt
Composition Layer,compose,hidden vectors,model,/content/training-data/natural_language_inference/98/triples/model.txt
hidden vectors,obtained by,sequence encoder layer ( h p and h h ,model,/content/training-data/natural_language_inference/98/triples/model.txt
sequence encoder layer ( h p and h h ),To transform,sentences,model,/content/training-data/natural_language_inference/98/triples/model.txt
sentences,into,fixed - length vector representations,model,/content/training-data/natural_language_inference/98/triples/model.txt
Composition Layer,propose,intra-sentence gated - attention,model,/content/training-data/natural_language_inference/98/triples/model.txt
intra-sentence gated - attention,to obtain,fixed - length vector,model,/content/training-data/natural_language_inference/98/triples/model.txt
Model,has,Word Embedding,model,/content/training-data/natural_language_inference/98/triples/model.txt
Word Embedding,concatenate,embeddings,model,/content/training-data/natural_language_inference/98/triples/model.txt
embeddings,learned at,two different levels,model,/content/training-data/natural_language_inference/98/triples/model.txt
two different levels,name,character composition,model,/content/training-data/natural_language_inference/98/triples/model.txt
two different levels,name,holistic word - level embedding,model,/content/training-data/natural_language_inference/98/triples/model.txt
embeddings,to represent,each word,model,/content/training-data/natural_language_inference/98/triples/model.txt
each word,in,sentence,model,/content/training-data/natural_language_inference/98/triples/model.txt
Contribution,has research problem,Natural Language Inference,research-problem,/content/training-data/natural_language_inference/98/triples/research-problem.txt
Contribution,has research problem,natural language inference ( NLI ,research-problem,/content/training-data/natural_language_inference/98/triples/research-problem.txt
Contribution,has research problem,NLI,research-problem,/content/training-data/natural_language_inference/98/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/98/triples/experimental-setup.txt
Experimental setup,use,pretrained GloVe - 840B - 300D vectors,experimental-setup,/content/training-data/natural_language_inference/98/triples/experimental-setup.txt
pretrained GloVe - 840B - 300D vectors,as,our word - level embeddings,experimental-setup,/content/training-data/natural_language_inference/98/triples/experimental-setup.txt
pretrained GloVe - 840B - 300D vectors,has,fix,experimental-setup,/content/training-data/natural_language_inference/98/triples/experimental-setup.txt
fix,during,training process,experimental-setup,/content/training-data/natural_language_inference/98/triples/experimental-setup.txt
Experimental setup,use,"Adam ( Kingma and Ba , 2014 ",experimental-setup,/content/training-data/natural_language_inference/98/triples/experimental-setup.txt
"Adam ( Kingma and Ba , 2014 )",for,optimization,experimental-setup,/content/training-data/natural_language_inference/98/triples/experimental-setup.txt
Experimental setup,has,100 dimensions,experimental-setup,/content/training-data/natural_language_inference/98/triples/experimental-setup.txt
100 dimensions,has,CNN filters length,experimental-setup,/content/training-data/natural_language_inference/98/triples/experimental-setup.txt
CNN filters length,is,"[ 1 , 3 , 5 ]",experimental-setup,/content/training-data/natural_language_inference/98/triples/experimental-setup.txt
100 dimensions,has,character embedding,experimental-setup,/content/training-data/natural_language_inference/98/triples/experimental-setup.txt
character embedding,has,15 dimensions,experimental-setup,/content/training-data/natural_language_inference/98/triples/experimental-setup.txt
Experimental setup,has,Stacked BiLSTM,experimental-setup,/content/training-data/natural_language_inference/98/triples/experimental-setup.txt
Stacked BiLSTM,has,3 layers,experimental-setup,/content/training-data/natural_language_inference/98/triples/experimental-setup.txt
Experimental setup,has,Out - of - vocabulary ( OOV ) words,experimental-setup,/content/training-data/natural_language_inference/98/triples/experimental-setup.txt
Out - of - vocabulary ( OOV ) words,has,initialized randomly,experimental-setup,/content/training-data/natural_language_inference/98/triples/experimental-setup.txt
initialized randomly,with,Gaussian samples,experimental-setup,/content/training-data/natural_language_inference/98/triples/experimental-setup.txt
Experimental setup,has,all hidden states,experimental-setup,/content/training-data/natural_language_inference/98/triples/experimental-setup.txt
all hidden states,of,BiLSTMs and MLP,experimental-setup,/content/training-data/natural_language_inference/98/triples/experimental-setup.txt
BiLSTMs and MLP,have,600 dimensions,experimental-setup,/content/training-data/natural_language_inference/98/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/98/triples/results.txt
Results,When,ensembling,results,/content/training-data/natural_language_inference/98/triples/results.txt
ensembling,our models,obtain,results,/content/training-data/natural_language_inference/98/triples/results.txt
obtain,accuracies,has,results,/content/training-data/natural_language_inference/98/triples/results.txt
has,74.9 % and 74.9 %,ranks,results,/content/training-data/natural_language_inference/98/triples/results.txt
ranks,first in,both test sets,results,/content/training-data/natural_language_inference/98/triples/results.txt
Results,achieve,accuracies,results,/content/training-data/natural_language_inference/98/triples/results.txt
accuracies,of,73.5 % and 73.6 %,results,/content/training-data/natural_language_inference/98/triples/results.txt
73.5 % and 73.6 %,among,single models,results,/content/training-data/natural_language_inference/98/triples/results.txt
single models,ranks,first,results,/content/training-data/natural_language_inference/98/triples/results.txt
first,in,cross - domain test set,results,/content/training-data/natural_language_inference/98/triples/results.txt
single models,ranks,second,results,/content/training-data/natural_language_inference/98/triples/results.txt
second,in,in - domain test set,results,/content/training-data/natural_language_inference/98/triples/results.txt
73.5 % and 73.6 %,adding,our gated - attention model,results,/content/training-data/natural_language_inference/98/triples/results.txt
73.5 % and 73.6 %,After removing,cross - sentence attention,results,/content/training-data/natural_language_inference/98/triples/results.txt
Results,has,our implementation of ESIM,results,/content/training-data/natural_language_inference/98/triples/results.txt
our implementation of ESIM,achieves,accuracy,results,/content/training-data/natural_language_inference/98/triples/results.txt
accuracy,of,75.8 %,results,/content/training-data/natural_language_inference/98/triples/results.txt
75.8 %,in,cross - domain test set,results,/content/training-data/natural_language_inference/98/triples/results.txt
accuracy,of,76.8 %,results,/content/training-data/natural_language_inference/98/triples/results.txt
76.8 %,in,in - domain test set,results,/content/training-data/natural_language_inference/98/triples/results.txt
our implementation of ESIM,presents,state - of - the - art results,results,/content/training-data/natural_language_inference/98/triples/results.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/79/triples/model.txt
Model,concatenate,paragraph vector,model,/content/training-data/natural_language_inference/79/triples/model.txt
paragraph vector,with,several word vectors,model,/content/training-data/natural_language_inference/79/triples/model.txt
several word vectors,from,paragraph,model,/content/training-data/natural_language_inference/79/triples/model.txt
paragraph vector,predict,following word,model,/content/training-data/natural_language_inference/79/triples/model.txt
following word,in,given context,model,/content/training-data/natural_language_inference/79/triples/model.txt
Model,At,prediction time,model,/content/training-data/natural_language_inference/79/triples/model.txt
prediction time,training,new paragraph vector,model,/content/training-data/natural_language_inference/79/triples/model.txt
new paragraph vector,until,convergence,model,/content/training-data/natural_language_inference/79/triples/model.txt
prediction time,has,paragraph vectors,model,/content/training-data/natural_language_inference/79/triples/model.txt
paragraph vectors,inferred by,fixing,model,/content/training-data/natural_language_inference/79/triples/model.txt
fixing,has,word vectors,model,/content/training-data/natural_language_inference/79/triples/model.txt
Model,name,Paragraph Vector,model,/content/training-data/natural_language_inference/79/triples/model.txt
Paragraph Vector,can be applied to,variable - length pieces of texts,model,/content/training-data/natural_language_inference/79/triples/model.txt
Model,has,vector representation,model,/content/training-data/natural_language_inference/79/triples/model.txt
vector representation,trained to be,useful,model,/content/training-data/natural_language_inference/79/triples/model.txt
useful,for predicting,words,model,/content/training-data/natural_language_inference/79/triples/model.txt
words,in,paragraph,model,/content/training-data/natural_language_inference/79/triples/model.txt
Model,has,paragraph vectors,model,/content/training-data/natural_language_inference/79/triples/model.txt
paragraph vectors,are,unique,model,/content/training-data/natural_language_inference/79/triples/model.txt
unique,among,paragraphs,model,/content/training-data/natural_language_inference/79/triples/model.txt
Model,has,Both word vectors and paragraph vectors,model,/content/training-data/natural_language_inference/79/triples/model.txt
Both word vectors and paragraph vectors,trained by,stochastic gradient descent,model,/content/training-data/natural_language_inference/79/triples/model.txt
Both word vectors and paragraph vectors,trained by,backpropagation,model,/content/training-data/natural_language_inference/79/triples/model.txt
Model,has,word vectors,model,/content/training-data/natural_language_inference/79/triples/model.txt
word vectors,are,shared,model,/content/training-data/natural_language_inference/79/triples/model.txt
Model,propose,Paragraph Vector,model,/content/training-data/natural_language_inference/79/triples/model.txt
Paragraph Vector,has,unsupervised framework,model,/content/training-data/natural_language_inference/79/triples/model.txt
unsupervised framework,that learns,continuous distributed vector representations,model,/content/training-data/natural_language_inference/79/triples/model.txt
continuous distributed vector representations,for,pieces of texts,model,/content/training-data/natural_language_inference/79/triples/model.txt
Contribution,has research problem,Distributed Representations of Sentences and Documents,research-problem,/content/training-data/natural_language_inference/79/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/79/triples/experimental-setup.txt
Experimental setup,learn,word vectors and paragraph vectors,experimental-setup,/content/training-data/natural_language_inference/79/triples/experimental-setup.txt
word vectors and paragraph vectors,using,"75,000 training documents",experimental-setup,/content/training-data/natural_language_inference/79/triples/experimental-setup.txt
"75,000 training documents",has,"25,000 labeled",experimental-setup,/content/training-data/natural_language_inference/79/triples/experimental-setup.txt
"75,000 training documents",has,"50,000 unlabeled instances",experimental-setup,/content/training-data/natural_language_inference/79/triples/experimental-setup.txt
Experimental setup,has,Special characters,experimental-setup,/content/training-data/natural_language_inference/79/triples/experimental-setup.txt
Special characters,such as,", .!?",experimental-setup,/content/training-data/natural_language_inference/79/triples/experimental-setup.txt
", .!?",treated as,normal word,experimental-setup,/content/training-data/natural_language_inference/79/triples/experimental-setup.txt
Experimental setup,has,paragraph vectors,experimental-setup,/content/training-data/natural_language_inference/79/triples/experimental-setup.txt
paragraph vectors,fed through,neural network,experimental-setup,/content/training-data/natural_language_inference/79/triples/experimental-setup.txt
neural network,with,one hidden layer,experimental-setup,/content/training-data/natural_language_inference/79/triples/experimental-setup.txt
one hidden layer,with,50 units,experimental-setup,/content/training-data/natural_language_inference/79/triples/experimental-setup.txt
paragraph vectors,fed through,logistic classifier,experimental-setup,/content/training-data/natural_language_inference/79/triples/experimental-setup.txt
logistic classifier,to predict,sentiment,experimental-setup,/content/training-data/natural_language_inference/79/triples/experimental-setup.txt
paragraph vectors,for,"25,000 labeled instances",experimental-setup,/content/training-data/natural_language_inference/79/triples/experimental-setup.txt
Experimental setup,has,optimal window size,experimental-setup,/content/training-data/natural_language_inference/79/triples/experimental-setup.txt
optimal window size,is,10 words,experimental-setup,/content/training-data/natural_language_inference/79/triples/experimental-setup.txt
Experimental setup,has,vector,experimental-setup,/content/training-data/natural_language_inference/79/triples/experimental-setup.txt
vector,presented to,classifier,experimental-setup,/content/training-data/natural_language_inference/79/triples/experimental-setup.txt
classifier,concatenation of,PV - DBOW,experimental-setup,/content/training-data/natural_language_inference/79/triples/experimental-setup.txt
PV - DBOW,has,learned vector representations,experimental-setup,/content/training-data/natural_language_inference/79/triples/experimental-setup.txt
learned vector representations,have,400 dimensions,experimental-setup,/content/training-data/natural_language_inference/79/triples/experimental-setup.txt
classifier,concatenation of,PV - DM,experimental-setup,/content/training-data/natural_language_inference/79/triples/experimental-setup.txt
PV - DM,has,learned vector representations,experimental-setup,/content/training-data/natural_language_inference/79/triples/experimental-setup.txt
learned vector representations,have,400 dimensions,experimental-setup,/content/training-data/natural_language_inference/79/triples/experimental-setup.txt
400 dimensions,for,words and documents,experimental-setup,/content/training-data/natural_language_inference/79/triples/experimental-setup.txt
Experimental setup,To predict,10 - th word,experimental-setup,/content/training-data/natural_language_inference/79/triples/experimental-setup.txt
10 - th word,concatenate,paragraph vectors,experimental-setup,/content/training-data/natural_language_inference/79/triples/experimental-setup.txt
10 - th word,concatenate,word vectors,experimental-setup,/content/training-data/natural_language_inference/79/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/79/triples/results.txt
Results,for,long documents,results,/content/training-data/natural_language_inference/79/triples/results.txt
long documents,has,bag - of - words models,results,/content/training-data/natural_language_inference/79/triples/results.txt
bag - of - words models,perform,quite well,results,/content/training-data/natural_language_inference/79/triples/results.txt
bag - of - words models,has,difficult to improve,results,/content/training-data/natural_language_inference/79/triples/results.txt
difficult to improve,using,word vectors,results,/content/training-data/natural_language_inference/79/triples/results.txt
Results,has,combination of two models,results,/content/training-data/natural_language_inference/79/triples/results.txt
combination of two models,yields,improvement,results,/content/training-data/natural_language_inference/79/triples/results.txt
improvement,has,approximately 1.5 %,results,/content/training-data/natural_language_inference/79/triples/results.txt
approximately 1.5 %,in terms of,error rates,results,/content/training-data/natural_language_inference/79/triples/results.txt
Results,has,method described,results,/content/training-data/natural_language_inference/79/triples/results.txt
method described,achieves,7.42 %,results,/content/training-data/natural_language_inference/79/triples/results.txt
7.42 %,over,best previous result,results,/content/training-data/natural_language_inference/79/triples/results.txt
7.42 %,which is,another 1.3 % absolute improvement,results,/content/training-data/natural_language_inference/79/triples/results.txt
7.42 %,which is,15 % relative improvement,results,/content/training-data/natural_language_inference/79/triples/results.txt
method described,is,only approach,results,/content/training-data/natural_language_inference/79/triples/results.txt
only approach,goes,significantly beyond the barrier,results,/content/training-data/natural_language_inference/79/triples/results.txt
significantly beyond the barrier,of,10 % error rate,results,/content/training-data/natural_language_inference/79/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/86/triples/ablation-analysis.txt
Ablation analysis,Among,all the layers,ablation-analysis,/content/training-data/natural_language_inference/86/triples/ablation-analysis.txt
all the layers,has,Aggregation Layer,ablation-analysis,/content/training-data/natural_language_inference/86/triples/ablation-analysis.txt
Aggregation Layer,is,most crucial layer,ablation-analysis,/content/training-data/natural_language_inference/86/triples/ablation-analysis.txt
Ablation analysis,Among,all the matching strategies,ablation-analysis,/content/training-data/natural_language_inference/86/triples/ablation-analysis.txt
all the matching strategies,has,Maxpooling - Matching,ablation-analysis,/content/training-data/natural_language_inference/86/triples/ablation-analysis.txt
Maxpooling - Matching,has,biggest effect,ablation-analysis,/content/training-data/natural_language_inference/86/triples/ablation-analysis.txt
Ablation analysis,removing,any components,ablation-analysis,/content/training-data/natural_language_inference/86/triples/ablation-analysis.txt
any components,from,MPCM model,ablation-analysis,/content/training-data/natural_language_inference/86/triples/ablation-analysis.txt
any components,has,decreases,ablation-analysis,/content/training-data/natural_language_inference/86/triples/ablation-analysis.txt
decreases,has,performance,ablation-analysis,/content/training-data/natural_language_inference/86/triples/ablation-analysis.txt
performance,has,significantly,ablation-analysis,/content/training-data/natural_language_inference/86/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/86/triples/model.txt
Model,identifies,answer span,model,/content/training-data/natural_language_inference/86/triples/model.txt
answer span,by predicting,beginning and ending points,model,/content/training-data/natural_language_inference/86/triples/model.txt
beginning and ending points,with,globally normalized probability distributions,model,/content/training-data/natural_language_inference/86/triples/model.txt
globally normalized probability distributions,across,whole passage,model,/content/training-data/natural_language_inference/86/triples/model.txt
beginning and ending points,has,individually,model,/content/training-data/natural_language_inference/86/triples/model.txt
Model,design,Multi - Perspective Context Matching ( MPCM ) model,model,/content/training-data/natural_language_inference/86/triples/model.txt
Multi - Perspective Context Matching ( MPCM ) model,to identify,answer span,model,/content/training-data/natural_language_inference/86/triples/model.txt
answer span,by matching,context,model,/content/training-data/natural_language_inference/86/triples/model.txt
context,with,question,model,/content/training-data/natural_language_inference/86/triples/model.txt
question,from,multiple perspectives,model,/content/training-data/natural_language_inference/86/triples/model.txt
context,of,each point,model,/content/training-data/natural_language_inference/86/triples/model.txt
each point,in,passage,model,/content/training-data/natural_language_inference/86/triples/model.txt
Model,propose,end - to - end deep neural network model,model,/content/training-data/natural_language_inference/86/triples/model.txt
end - to - end deep neural network model,for,machine comprehension,model,/content/training-data/natural_language_inference/86/triples/model.txt
Contribution,has research problem,Machine Comprehension,research-problem,/content/training-data/natural_language_inference/86/triples/research-problem.txt
Contribution,has research problem,machine comprehension ( MC ,research-problem,/content/training-data/natural_language_inference/86/triples/research-problem.txt
Contribution,has research problem,MC,research-problem,/content/training-data/natural_language_inference/86/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/86/triples/experimental-setup.txt
Experimental setup,minimize,cross entropy,experimental-setup,/content/training-data/natural_language_inference/86/triples/experimental-setup.txt
cross entropy,of,be - ginning and end points,experimental-setup,/content/training-data/natural_language_inference/86/triples/experimental-setup.txt
Experimental setup,process,corpus,experimental-setup,/content/training-data/natural_language_inference/86/triples/experimental-setup.txt
corpus,with,tokenizer,experimental-setup,/content/training-data/natural_language_inference/86/triples/experimental-setup.txt
tokenizer,from,Stanford CorNLP,experimental-setup,/content/training-data/natural_language_inference/86/triples/experimental-setup.txt
Experimental setup,set,number of perspectives l,experimental-setup,/content/training-data/natural_language_inference/86/triples/experimental-setup.txt
number of perspectives l,as,50,experimental-setup,/content/training-data/natural_language_inference/86/triples/experimental-setup.txt
number of perspectives l,of,our multiperspective matching function,experimental-setup,/content/training-data/natural_language_inference/86/triples/experimental-setup.txt
Experimental setup,set,learning rate,experimental-setup,/content/training-data/natural_language_inference/86/triples/experimental-setup.txt
learning rate,as,0.0001,experimental-setup,/content/training-data/natural_language_inference/86/triples/experimental-setup.txt
Experimental setup,set,dropout ratio,experimental-setup,/content/training-data/natural_language_inference/86/triples/experimental-setup.txt
dropout ratio,as,0.2,experimental-setup,/content/training-data/natural_language_inference/86/triples/experimental-setup.txt
Experimental setup,set,hidden size,experimental-setup,/content/training-data/natural_language_inference/86/triples/experimental-setup.txt
hidden size,as,100,experimental-setup,/content/training-data/natural_language_inference/86/triples/experimental-setup.txt
hidden size,for,all the LSTM layers,experimental-setup,/content/training-data/natural_language_inference/86/triples/experimental-setup.txt
Experimental setup,apply,dropout,experimental-setup,/content/training-data/natural_language_inference/86/triples/experimental-setup.txt
dropout,to,every layers,experimental-setup,/content/training-data/natural_language_inference/86/triples/experimental-setup.txt
Experimental setup,use,ADAM optimizer,experimental-setup,/content/training-data/natural_language_inference/86/triples/experimental-setup.txt
ADAM optimizer,to update,parameters,experimental-setup,/content/training-data/natural_language_inference/86/triples/experimental-setup.txt
Experimental setup,For,out - of - vocabulary ( OOV ) words,experimental-setup,/content/training-data/natural_language_inference/86/triples/experimental-setup.txt
out - of - vocabulary ( OOV ) words,initialize,word embeddings,experimental-setup,/content/training-data/natural_language_inference/86/triples/experimental-setup.txt
word embeddings,has,randomly,experimental-setup,/content/training-data/natural_language_inference/86/triples/experimental-setup.txt
Experimental setup,initialize,word embeddings,experimental-setup,/content/training-data/natural_language_inference/86/triples/experimental-setup.txt
word embeddings,in,word representation layer,experimental-setup,/content/training-data/natural_language_inference/86/triples/experimental-setup.txt
word embeddings,use,300 - dimensional GloVe word vectors,experimental-setup,/content/training-data/natural_language_inference/86/triples/experimental-setup.txt
300 - dimensional GloVe word vectors,pre-trained from,840B Common Crawl corpus,experimental-setup,/content/training-data/natural_language_inference/86/triples/experimental-setup.txt
Contribution,has,Approach,approach,/content/training-data/natural_language_inference/5/triples/approach.txt
Approach,select,ELMo language model,approach,/content/training-data/natural_language_inference/5/triples/approach.txt
Approach,explore,effect,approach,/content/training-data/natural_language_inference/5/triples/approach.txt
effect,of,additional information,approach,/content/training-data/natural_language_inference/5/triples/approach.txt
additional information,by adopting,pretrained language model ( LM ,approach,/content/training-data/natural_language_inference/5/triples/approach.txt
pretrained language model ( LM ),to compute,vector representation,approach,/content/training-data/natural_language_inference/5/triples/approach.txt
vector representation,of,input text,approach,/content/training-data/natural_language_inference/5/triples/approach.txt
effect,of,different objective functions,approach,/content/training-data/natural_language_inference/5/triples/approach.txt
different objective functions,has,listwise and pointwise learning,approach,/content/training-data/natural_language_inference/5/triples/approach.txt
Approach,investigate,applicability,approach,/content/training-data/natural_language_inference/5/triples/approach.txt
applicability,of,transfer learning ( TL ,approach,/content/training-data/natural_language_inference/5/triples/approach.txt
transfer learning ( TL ),using,large - scale corpus,approach,/content/training-data/natural_language_inference/5/triples/approach.txt
large - scale corpus,created for,relevant - sentence - selection task,approach,/content/training-data/natural_language_inference/5/triples/approach.txt
relevant - sentence - selection task,i.e.,question - answering NLI ( QNLI ) dataset,approach,/content/training-data/natural_language_inference/5/triples/approach.txt
Approach,has,LC method,approach,/content/training-data/natural_language_inference/5/triples/approach.txt
LC method,computes,latent cluster information,approach,/content/training-data/natural_language_inference/5/triples/approach.txt
latent cluster information,by creating,latent memory space,approach,/content/training-data/natural_language_inference/5/triples/approach.txt
latent cluster information,for,target samples,approach,/content/training-data/natural_language_inference/5/triples/approach.txt
latent cluster information,calculating,similarity,approach,/content/training-data/natural_language_inference/5/triples/approach.txt
similarity,between,sample,approach,/content/training-data/natural_language_inference/5/triples/approach.txt
similarity,between,memory,approach,/content/training-data/natural_language_inference/5/triples/approach.txt
LC method,assigns,true - label question - answer pairs,approach,/content/training-data/natural_language_inference/5/triples/approach.txt
true - label question - answer pairs,to,similar clusters,approach,/content/training-data/natural_language_inference/5/triples/approach.txt
similar clusters,By,endto - end learning process,approach,/content/training-data/natural_language_inference/5/triples/approach.txt
Approach,enhance,one of the baseline models,approach,/content/training-data/natural_language_inference/5/triples/approach.txt
one of the baseline models,name,Comp - Clip,approach,/content/training-data/natural_language_inference/5/triples/approach.txt
one of the baseline models,for,target QA task,approach,/content/training-data/natural_language_inference/5/triples/approach.txt
target QA task,by proposing,novel latent clustering ( LC ) method,approach,/content/training-data/natural_language_inference/5/triples/approach.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/natural_language_inference/5/triples/hyperparameters.txt
Hyperparameters,applying,TL,hyperparameters,/content/training-data/natural_language_inference/5/triples/hyperparameters.txt
TL,has,vocabulary size,hyperparameters,/content/training-data/natural_language_inference/5/triples/hyperparameters.txt
vocabulary size,set to,"154,442",hyperparameters,/content/training-data/natural_language_inference/5/triples/hyperparameters.txt
TL,has,dimension,hyperparameters,/content/training-data/natural_language_inference/5/triples/hyperparameters.txt
dimension,of,context projection weight matrix,hyperparameters,/content/training-data/natural_language_inference/5/triples/hyperparameters.txt
context projection weight matrix,set to,300,hyperparameters,/content/training-data/natural_language_inference/5/triples/hyperparameters.txt
Hyperparameters,select,k,hyperparameters,/content/training-data/natural_language_inference/5/triples/hyperparameters.txt
k,as,6 and 4,hyperparameters,/content/training-data/natural_language_inference/5/triples/hyperparameters.txt
6 and 4,for,WikiQA and TREC - QA case,hyperparameters,/content/training-data/natural_language_inference/5/triples/hyperparameters.txt
WikiQA and TREC - QA case,apply,8 latent clusters,hyperparameters,/content/training-data/natural_language_inference/5/triples/hyperparameters.txt
Hyperparameters,In,aggregation part,hyperparameters,/content/training-data/natural_language_inference/5/triples/hyperparameters.txt
aggregation part,use,1 - D CNN,hyperparameters,/content/training-data/natural_language_inference/5/triples/hyperparameters.txt
1 - D CNN,with,total of 500 filters,hyperparameters,/content/training-data/natural_language_inference/5/triples/hyperparameters.txt
Hyperparameters,use,Adam optimizer,hyperparameters,/content/training-data/natural_language_inference/5/triples/hyperparameters.txt
Adam optimizer,including,gradient clipping,hyperparameters,/content/training-data/natural_language_inference/5/triples/hyperparameters.txt
gradient clipping,by,norm,hyperparameters,/content/training-data/natural_language_inference/5/triples/hyperparameters.txt
norm,at,threshold,hyperparameters,/content/training-data/natural_language_inference/5/triples/hyperparameters.txt
threshold,of,5,hyperparameters,/content/training-data/natural_language_inference/5/triples/hyperparameters.txt
Hyperparameters,For,regularization,hyperparameters,/content/training-data/natural_language_inference/5/triples/hyperparameters.txt
regularization,applied,dropout,hyperparameters,/content/training-data/natural_language_inference/5/triples/hyperparameters.txt
dropout,with,ratio,hyperparameters,/content/training-data/natural_language_inference/5/triples/hyperparameters.txt
ratio,of,0.5,hyperparameters,/content/training-data/natural_language_inference/5/triples/hyperparameters.txt
Hyperparameters,implement,Comp - Clip model,hyperparameters,/content/training-data/natural_language_inference/5/triples/hyperparameters.txt
Comp - Clip model,apply,context projection weight matrix,hyperparameters,/content/training-data/natural_language_inference/5/triples/hyperparameters.txt
context projection weight matrix,with,100 dimensions,hyperparameters,/content/training-data/natural_language_inference/5/triples/hyperparameters.txt
context projection weight matrix,shared between,question part,hyperparameters,/content/training-data/natural_language_inference/5/triples/hyperparameters.txt
context projection weight matrix,shared between,answer part,hyperparameters,/content/training-data/natural_language_inference/5/triples/hyperparameters.txt
Hyperparameters,has,vocabulary size,hyperparameters,/content/training-data/natural_language_inference/5/triples/hyperparameters.txt
vocabulary size,in,"WiKiQA , TREC - QA and QNLI dataset",hyperparameters,/content/training-data/natural_language_inference/5/triples/hyperparameters.txt
"WiKiQA , TREC - QA and QNLI dataset",are,"30,104 , 56,908 and 154,442",hyperparameters,/content/training-data/natural_language_inference/5/triples/hyperparameters.txt
Contribution,has research problem,Answer Selection,research-problem,/content/training-data/natural_language_inference/5/triples/research-problem.txt
Contribution,has research problem,sentence - level answer- selection,research-problem,/content/training-data/natural_language_inference/5/triples/research-problem.txt
Contribution,has research problem,Automatic question answering ( QA ,research-problem,/content/training-data/natural_language_inference/5/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/5/triples/results.txt
Results,with,TREC - QA dataset,results,/content/training-data/natural_language_inference/5/triples/results.txt
TREC - QA dataset,achieve,additional performance gains,results,/content/training-data/natural_language_inference/5/triples/results.txt
additional performance gains,in terms of,MAP,results,/content/training-data/natural_language_inference/5/triples/results.txt
MAP,apply,"LM , LC , and TL",results,/content/training-data/natural_language_inference/5/triples/results.txt
"LM , LC , and TL",has,"0.850 , 0.868 and 0.875",results,/content/training-data/natural_language_inference/5/triples/results.txt
TREC - QA dataset,has,pointwise learning approach,results,/content/training-data/natural_language_inference/5/triples/results.txt
pointwise learning approach,shows,excellent performance,results,/content/training-data/natural_language_inference/5/triples/results.txt
TREC - QA dataset,has,our model,results,/content/training-data/natural_language_inference/5/triples/results.txt
our model,outperforms,best previous result,results,/content/training-data/natural_language_inference/5/triples/results.txt
best previous result,add,"LC method , ( Comp - Clip + LM + LC ",results,/content/training-data/natural_language_inference/5/triples/results.txt
best previous result,in terms of,MAP,results,/content/training-data/natural_language_inference/5/triples/results.txt
MAP,has,0.865,results,/content/training-data/natural_language_inference/5/triples/results.txt
0.865,to,0.868,results,/content/training-data/natural_language_inference/5/triples/results.txt
Results,For,WikiQA dataset,results,/content/training-data/natural_language_inference/5/triples/results.txt
WikiQA dataset,add,LC method ( Comp - Clip + LM + LC ,results,/content/training-data/natural_language_inference/5/triples/results.txt
LC method ( Comp - Clip + LM + LC ),has,best previous results,results,/content/training-data/natural_language_inference/5/triples/results.txt
best previous results,are,surpassed,results,/content/training-data/natural_language_inference/5/triples/results.txt
surpassed,in terms of,MAP ( 0.718 to 0.764 absolute ,results,/content/training-data/natural_language_inference/5/triples/results.txt
WikiQA dataset,has,pointwise learning approach,results,/content/training-data/natural_language_inference/5/triples/results.txt
pointwise learning approach,shows,better performance,results,/content/training-data/natural_language_inference/5/triples/results.txt
better performance,than,listwise learning approach,results,/content/training-data/natural_language_inference/5/triples/results.txt
WikiQA dataset,combine,LM,results,/content/training-data/natural_language_inference/5/triples/results.txt
LM,with,base model ( Comp - Clip + LM ,results,/content/training-data/natural_language_inference/5/triples/results.txt
LM,observe,significant improvement,results,/content/training-data/natural_language_inference/5/triples/results.txt
significant improvement,in,performance,results,/content/training-data/natural_language_inference/5/triples/results.txt
performance,in terms of,MAP ( 0.714 to 0.746 absolute ,results,/content/training-data/natural_language_inference/5/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
Ablation analysis,with,three additional baselines,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
three additional baselines,name,NUTM,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
NUTM,using,direct attention ( DA ,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
three additional baselines,name,NUTM,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
NUTM,using,key - value without regularization ( KV ,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
three additional baselines,name,NUTM,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
NUTM,using,"fixed , uniform program distribution ( UP ",ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
three additional baselines,name,vanilla NTM,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
vanilla NTM,with,2 memory heads ( h = 2 ,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
Ablation analysis,demonstrate,DA,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
DA,fall into,local minima,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
DA,exhibits,fast yet shallow convergence,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
DA,has,fails,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
fails,to reach,zero loss,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
Ablation analysis,has,NTM,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
NTM,with,2 heads,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
NTM,shows,underperforms,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
underperforms,has,NUTM ( p = 2 ,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
NUTM ( p = 2 ),with,1 head and fewer parameters,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
NTM,shows,slightly better convergence,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
slightly better convergence,compared to,NTM,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
Ablation analysis,has,UP,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
UP,has,underperforms,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
underperforms,as,lacks,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
lacks,has,dynamic programs,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
underperforms,has,NUTM,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
Ablation analysis,has,Key- value attention,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
Key- value attention,helps,NUTM converge,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
NUTM converge,with,fewer iterations,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
Key- value attention,has,performance,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
performance,is,further improved,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
further improved,with,proposed regularization loss,ablation-analysis,/content/training-data/natural_language_inference/51/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/51/triples/model.txt
Model,step further towards,UTM,model,/content/training-data/natural_language_inference/51/triples/model.txt
UTM,by coupling,MANN,model,/content/training-data/natural_language_inference/51/triples/model.txt
MANN,with,external program memory,model,/content/training-data/natural_language_inference/51/triples/model.txt
Model,referred to,Neural Stored - program Memory ( NSM ,model,/content/training-data/natural_language_inference/51/triples/model.txt
Neural Stored - program Memory ( NSM ),learn to,switch,model,/content/training-data/natural_language_inference/51/triples/model.txt
switch,has,programs / weights,model,/content/training-data/natural_language_inference/51/triples/model.txt
programs / weights,in,controller network,model,/content/training-data/natural_language_inference/51/triples/model.txt
controller network,has,appropriately,model,/content/training-data/natural_language_inference/51/triples/model.txt
programs / weights,adapting to,different functionalities,model,/content/training-data/natural_language_inference/51/triples/model.txt
different functionalities,aligning with,different parts,model,/content/training-data/natural_language_inference/51/triples/model.txt
different parts,of,sequential task,model,/content/training-data/natural_language_inference/51/triples/model.txt
different functionalities,aligning with,different tasks,model,/content/training-data/natural_language_inference/51/triples/model.txt
different tasks,in,continual and few - shot learning,model,/content/training-data/natural_language_inference/51/triples/model.txt
Model,has,program memory,model,/content/training-data/natural_language_inference/51/triples/model.txt
program memory,co-exists with,data memory,model,/content/training-data/natural_language_inference/51/triples/model.txt
data memory,in,MANN,model,/content/training-data/natural_language_inference/51/triples/model.txt
data memory,in,learning,model,/content/training-data/natural_language_inference/51/triples/model.txt
learning,providing,flexibility,model,/content/training-data/natural_language_inference/51/triples/model.txt
learning,providing,reuseability,model,/content/training-data/natural_language_inference/51/triples/model.txt
learning,providing,modularity,model,/content/training-data/natural_language_inference/51/triples/model.txt
learning,has,complicated tasks,model,/content/training-data/natural_language_inference/51/triples/model.txt
program memory,stores,weights,model,/content/training-data/natural_language_inference/51/triples/model.txt
weights,of,MANN 's controller network,model,/content/training-data/natural_language_inference/51/triples/model.txt
weights,retrieved,quickly,model,/content/training-data/natural_language_inference/51/triples/model.txt
quickly,via,key - value attention mechanism,model,/content/training-data/natural_language_inference/51/triples/model.txt
key - value attention mechanism,across,timesteps,model,/content/training-data/natural_language_inference/51/triples/model.txt
weights,updated,slowly,model,/content/training-data/natural_language_inference/51/triples/model.txt
slowly,via,backpropagation,model,/content/training-data/natural_language_inference/51/triples/model.txt
Contribution,has research problem,Neural Stored - program Memory,research-problem,/content/training-data/natural_language_inference/51/triples/research-problem.txt
Contribution,has research problem,stored - program memory,research-problem,/content/training-data/natural_language_inference/51/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/51/triples/results.txt
Results,has,NUTM,results,/content/training-data/natural_language_inference/51/triples/results.txt
NUTM,generalizes,better,results,/content/training-data/natural_language_inference/51/triples/results.txt
better,to,unseen sequences,results,/content/training-data/natural_language_inference/51/triples/results.txt
unseen sequences,that are,longer,results,/content/training-data/natural_language_inference/51/triples/results.txt
longer,than,training sequences,results,/content/training-data/natural_language_inference/51/triples/results.txt
NUTM,requires,fewer training samples,results,/content/training-data/natural_language_inference/51/triples/results.txt
fewer training samples,to,converge,results,/content/training-data/natural_language_inference/51/triples/results.txt
Results,has,other tasks,results,/content/training-data/natural_language_inference/51/triples/results.txt
other tasks,observe,convergence speed improvement,results,/content/training-data/natural_language_inference/51/triples/results.txt
convergence speed improvement,validating,benefit,results,/content/training-data/natural_language_inference/51/triples/results.txt
benefit,of using,two programs,results,/content/training-data/natural_language_inference/51/triples/results.txt
two programs,across,timesteps,results,/content/training-data/natural_language_inference/51/triples/results.txt
two programs,even for,single task setting,results,/content/training-data/natural_language_inference/51/triples/results.txt
convergence speed improvement,of,NUTM,results,/content/training-data/natural_language_inference/51/triples/results.txt
NUTM,over,NTM,results,/content/training-data/natural_language_inference/51/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/0/triples/ablation-analysis.txt
Ablation analysis,observe,substantial drop,ablation-analysis,/content/training-data/natural_language_inference/0/triples/ablation-analysis.txt
substantial drop,when removing,tokenspecific attentions,ablation-analysis,/content/training-data/natural_language_inference/0/triples/ablation-analysis.txt
tokenspecific attentions,over,query in the GA module,ablation-analysis,/content/training-data/natural_language_inference/0/triples/ablation-analysis.txt
tokenspecific attentions,allow gating,individual tokens,ablation-analysis,/content/training-data/natural_language_inference/0/triples/ablation-analysis.txt
individual tokens,in,document,ablation-analysis,/content/training-data/natural_language_inference/0/triples/ablation-analysis.txt
individual tokens,by,parts of the query,ablation-analysis,/content/training-data/natural_language_inference/0/triples/ablation-analysis.txt
parts of the query,relevant to,token,ablation-analysis,/content/training-data/natural_language_inference/0/triples/ablation-analysis.txt
token,rather than,over all query representation,ablation-analysis,/content/training-data/natural_language_inference/0/triples/ablation-analysis.txt
Ablation analysis,removing,character embeddings,ablation-analysis,/content/training-data/natural_language_inference/0/triples/ablation-analysis.txt
character embeddings,leads to,reduction,ablation-analysis,/content/training-data/natural_language_inference/0/triples/ablation-analysis.txt
reduction,of,about 1 % in the performance,ablation-analysis,/content/training-data/natural_language_inference/0/triples/ablation-analysis.txt
character embeddings,used for,WDW and CBT,ablation-analysis,/content/training-data/natural_language_inference/0/triples/ablation-analysis.txt
Contribution,Code,https:// github.com/bdhingra/ga-reader,code,/content/training-data/natural_language_inference/0/triples/code.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/0/triples/model.txt
Model,has,Gated - Attention ( GA ) module,model,/content/training-data/natural_language_inference/0/triples/model.txt
Gated - Attention ( GA ) module,allows,query,model,/content/training-data/natural_language_inference/0/triples/model.txt
query,to directly interact with,each dimension,model,/content/training-data/natural_language_inference/0/triples/model.txt
each dimension,of,token embeddings,model,/content/training-data/natural_language_inference/0/triples/model.txt
token embeddings,at,semantic - level,model,/content/training-data/natural_language_inference/0/triples/model.txt
Gated - Attention ( GA ) module,applied,layer - wise,model,/content/training-data/natural_language_inference/0/triples/model.txt
layer - wise,as,information filters,model,/content/training-data/natural_language_inference/0/triples/model.txt
information filters,during,multi-hop representation learning process,model,/content/training-data/natural_language_inference/0/triples/model.txt
Model,has,fine - grained attention,model,/content/training-data/natural_language_inference/0/triples/model.txt
fine - grained attention,enables,model,model,/content/training-data/natural_language_inference/0/triples/model.txt
model,to learn,conditional token representations,model,/content/training-data/natural_language_inference/0/triples/model.txt
conditional token representations,w.r.t.,given question,model,/content/training-data/natural_language_inference/0/triples/model.txt
Contribution,has research problem,Text Comprehension,research-problem,/content/training-data/natural_language_inference/0/triples/research-problem.txt
Contribution,has research problem,machine reading,research-problem,/content/training-data/natural_language_inference/0/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/0/triples/results.txt
Results,observe that,feature engineering,results,/content/training-data/natural_language_inference/0/triples/results.txt
feature engineering,leads to,significant improvements,results,/content/training-data/natural_language_inference/0/triples/results.txt
significant improvements,not for,CNN and Daily Mail datasets,results,/content/training-data/natural_language_inference/0/triples/results.txt
significant improvements,for,WDW and CBT datasets,results,/content/training-data/natural_language_inference/0/triples/results.txt
Results,fixing,word embeddings,results,/content/training-data/natural_language_inference/0/triples/results.txt
word embeddings,provides,improvement,results,/content/training-data/natural_language_inference/0/triples/results.txt
improvement,not for,CNN and Daily Mail,results,/content/training-data/natural_language_inference/0/triples/results.txt
improvement,for,WDW and CBT,results,/content/training-data/natural_language_inference/0/triples/results.txt
Results,adding,qecomm feature,results,/content/training-data/natural_language_inference/0/triples/results.txt
qecomm feature,has,performance,results,/content/training-data/natural_language_inference/0/triples/results.txt
performance,has,increases,results,/content/training-data/natural_language_inference/0/triples/results.txt
increases,by,3.2 % and 3.5 %,results,/content/training-data/natural_language_inference/0/triples/results.txt
increases,on,Strict and Relaxed settings,results,/content/training-data/natural_language_inference/0/triples/results.txt
Results,For,CBT - NE,results,/content/training-data/natural_language_inference/0/triples/results.txt
CBT - NE,has,GA Reader,results,/content/training-data/natural_language_inference/0/triples/results.txt
GA Reader,with,qecomm feature,results,/content/training-data/natural_language_inference/0/triples/results.txt
GA Reader,outperforms,all previous single and ensemble models,results,/content/training-data/natural_language_inference/0/triples/results.txt
all previous single and ensemble models,except,AS Reader,results,/content/training-data/natural_language_inference/0/triples/results.txt
AS Reader,trained on,much larger BookTest Corpus,results,/content/training-data/natural_language_inference/0/triples/results.txt
Results,on,CBT - CN,results,/content/training-data/natural_language_inference/0/triples/results.txt
CBT - CN,has,GA Reader,results,/content/training-data/natural_language_inference/0/triples/results.txt
GA Reader,with,qe-comm feature,results,/content/training-data/natural_language_inference/0/triples/results.txt
GA Reader,outperforms,all previously published single models,results,/content/training-data/natural_language_inference/0/triples/results.txt
all previously published single models,except,NSE,results,/content/training-data/natural_language_inference/0/triples/results.txt
all previously published single models,except,AS Reader,results,/content/training-data/natural_language_inference/0/triples/results.txt
AS Reader,trained on,larger corpus,results,/content/training-data/natural_language_inference/0/triples/results.txt
Results,on,WDW dataset,results,/content/training-data/natural_language_inference/0/triples/results.txt
WDW dataset,has,basic version of the GA Reader,results,/content/training-data/natural_language_inference/0/triples/results.txt
basic version of the GA Reader,when trained on,Strict setting,results,/content/training-data/natural_language_inference/0/triples/results.txt
basic version of the GA Reader,outperforms,all previously published models,results,/content/training-data/natural_language_inference/0/triples/results.txt
Results,On,CNN and Daily Mail datasets,results,/content/training-data/natural_language_inference/0/triples/results.txt
CNN and Daily Mail datasets,has,GA Reader,results,/content/training-data/natural_language_inference/0/triples/results.txt
GA Reader,leads to,improvement,results,/content/training-data/natural_language_inference/0/triples/results.txt
improvement,of,3.2 % and 4.3 %,results,/content/training-data/natural_language_inference/0/triples/results.txt
3.2 % and 4.3 %,over,best previous single models,results,/content/training-data/natural_language_inference/0/triples/results.txt
Contribution,has,Approach,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
Approach,take,approach,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
approach,of,converting questions,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
converting questions,to,( uninterpretable ) vectorial representations,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
 uninterpretable ) vectorial representations,can,query,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
query,any KB,independent,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
independent,of,schema,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
 uninterpretable ) vectorial representations,require no,pre-defined grammars or lexicons,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
Approach,make use of,weak supervision,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
Approach,end up learning,meaningful vectorial representations,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
meaningful vectorial representations,for,triples,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
triples,of,mostly automatically created KB,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
mostly automatically created KB,with,2.4 M entities,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
mostly automatically created KB,with,600 k relationships,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
meaningful vectorial representations,for,questions,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
questions,involving up to,800 k words,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
Approach,based on,learning,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
learning,has,low - dimensional vector embeddings,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
low - dimensional vector embeddings,so that,representations,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
representations,end up,similar,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
similar,in,embedding space,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
representations,of,questions and corresponding answers,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
low - dimensional vector embeddings,of,words,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
low - dimensional vector embeddings,of,KB triples,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
Approach,focus on,answering,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
answering,has,simple factual questions,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
simple factual questions,on,broad range of topics,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
Approach,has,model,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
model,able to,take advantage,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
take advantage,of,noisy and indirect supervision,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
noisy and indirect supervision,by,automatically generating,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
automatically generating,has,questions,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
questions,treating this as,training data,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
questions,from,KB triples,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
noisy and indirect supervision,by,supplementing,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
supplementing,with,data set of questions,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
data set of questions,collaboratively marked as,paraphrases,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
data set of questions,with no,associated answers,approach,/content/training-data/natural_language_inference/30/triples/approach.txt
Contribution,has research problem,Open Question Answering,research-problem,/content/training-data/natural_language_inference/30/triples/research-problem.txt
Contribution,has research problem,open - domain question answering,research-problem,/content/training-data/natural_language_inference/30/triples/research-problem.txt
Contribution,has research problem,Question answering,research-problem,/content/training-data/natural_language_inference/30/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/30/triples/results.txt
Results,see that,multitasking,results,/content/training-data/natural_language_inference/30/triples/results.txt
multitasking,with,paraphrase data,results,/content/training-data/natural_language_inference/30/triples/results.txt
paraphrase data,improves,F1,results,/content/training-data/natural_language_inference/30/triples/results.txt
F1,from,0.60,results,/content/training-data/natural_language_inference/30/triples/results.txt
0.60,to,0.68,results,/content/training-data/natural_language_inference/30/triples/results.txt
paraphrase data,is,essential,results,/content/training-data/natural_language_inference/30/triples/results.txt
Results,has,Fine - tuning,results,/content/training-data/natural_language_inference/30/triples/results.txt
Fine - tuning,has,embedding model,results,/content/training-data/natural_language_inference/30/triples/results.txt
embedding model,grants,bump,results,/content/training-data/natural_language_inference/30/triples/results.txt
bump,of,5 points,results,/content/training-data/natural_language_inference/30/triples/results.txt
5 points,of,F1,results,/content/training-data/natural_language_inference/30/triples/results.txt
embedding model,is,very beneficial,results,/content/training-data/natural_language_inference/30/triples/results.txt
very beneficial,to optimize,top of the list,results,/content/training-data/natural_language_inference/30/triples/results.txt
Results,has,final F1,results,/content/training-data/natural_language_inference/30/triples/results.txt
final F1,obtained by,our fine - tuned model,results,/content/training-data/natural_language_inference/30/triples/results.txt
our fine - tuned model,is,even better,results,/content/training-data/natural_language_inference/30/triples/results.txt
even better,in,reranking,results,/content/training-data/natural_language_inference/30/triples/results.txt
even better,then,result,results,/content/training-data/natural_language_inference/30/triples/results.txt
result,of,paralex,results,/content/training-data/natural_language_inference/30/triples/results.txt
Results,has,All versions of our system,results,/content/training-data/natural_language_inference/30/triples/results.txt
All versions of our system,has,greatly outperform,results,/content/training-data/natural_language_inference/30/triples/results.txt
greatly outperform,has,paralex,results,/content/training-data/natural_language_inference/30/triples/results.txt
All versions of our system,has,fine - tuned model,results,/content/training-data/natural_language_inference/30/triples/results.txt
fine - tuned model,improves,F1 - score,results,/content/training-data/natural_language_inference/30/triples/results.txt
F1 - score,by,almost 20 points,results,/content/training-data/natural_language_inference/30/triples/results.txt
Results,has,string matching,results,/content/training-data/natural_language_inference/30/triples/results.txt
string matching,has,significantly reduces,results,/content/training-data/natural_language_inference/30/triples/results.txt
significantly reduces,has,evaluation time,results,/content/training-data/natural_language_inference/30/triples/results.txt
string matching,has,greatly improves,results,/content/training-data/natural_language_inference/30/triples/results.txt
greatly improves,has,results,results,/content/training-data/natural_language_inference/30/triples/results.txt
greatly improves,both,precision and recall,results,/content/training-data/natural_language_inference/30/triples/results.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/2/triples/model.txt
Model,has,AMANDA,model,/content/training-data/natural_language_inference/2/triples/model.txt
AMANDA,extracts,answer,model,/content/training-data/natural_language_inference/2/triples/model.txt
answer,by implicitly determining,suitable answer type,model,/content/training-data/natural_language_inference/2/triples/model.txt
suitable answer type,during,prediction,model,/content/training-data/natural_language_inference/2/triples/model.txt
answer,by synthesizing,relevant facts,model,/content/training-data/natural_language_inference/2/triples/model.txt
relevant facts,from,passage,model,/content/training-data/natural_language_inference/2/triples/model.txt
Model,propose,end - to - end question - focused multi-factor attention network,model,/content/training-data/natural_language_inference/2/triples/model.txt
end - to - end question - focused multi-factor attention network,learns to,aggregate,model,/content/training-data/natural_language_inference/2/triples/model.txt
aggregate,has,evidence,model,/content/training-data/natural_language_inference/2/triples/model.txt
evidence,distributed across,multiple sentences,model,/content/training-data/natural_language_inference/2/triples/model.txt
end - to - end question - focused multi-factor attention network,identifies,important question words,model,/content/training-data/natural_language_inference/2/triples/model.txt
important question words,to help,extract,model,/content/training-data/natural_language_inference/2/triples/model.txt
extract,has,answer,model,/content/training-data/natural_language_inference/2/triples/model.txt
end - to - end question - focused multi-factor attention network,for,document - based question answering,model,/content/training-data/natural_language_inference/2/triples/model.txt
end - to - end question - focused multi-factor attention network,name,AMANDA,model,/content/training-data/natural_language_inference/2/triples/model.txt
Contribution,has research problem,Question Answering,research-problem,/content/training-data/natural_language_inference/2/triples/research-problem.txt
Contribution,has research problem,question answering ( QA ,research-problem,/content/training-data/natural_language_inference/2/triples/research-problem.txt
Contribution,has research problem,QA,research-problem,/content/training-data/natural_language_inference/2/triples/research-problem.txt
Contribution,has research problem,machine comprehension - based ( MC ) question answering ( QA ,research-problem,/content/training-data/natural_language_inference/2/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/2/triples/experimental-setup.txt
Experimental setup,use,50 - dimension,experimental-setup,/content/training-data/natural_language_inference/2/triples/experimental-setup.txt
50 - dimension,has,character - level embedding vectors,experimental-setup,/content/training-data/natural_language_inference/2/triples/experimental-setup.txt
Experimental setup,use,dropout,experimental-setup,/content/training-data/natural_language_inference/2/triples/experimental-setup.txt
dropout,with,probability 0.3,experimental-setup,/content/training-data/natural_language_inference/2/triples/experimental-setup.txt
probability 0.3,for,every learnable layer,experimental-setup,/content/training-data/natural_language_inference/2/triples/experimental-setup.txt
Experimental setup,use,Adam optimizer,experimental-setup,/content/training-data/natural_language_inference/2/triples/experimental-setup.txt
Adam optimizer,with,learning rate,experimental-setup,/content/training-data/natural_language_inference/2/triples/experimental-setup.txt
learning rate,of,0.001,experimental-setup,/content/training-data/natural_language_inference/2/triples/experimental-setup.txt
Adam optimizer,with,clipnorm,experimental-setup,/content/training-data/natural_language_inference/2/triples/experimental-setup.txt
clipnorm,of,5,experimental-setup,/content/training-data/natural_language_inference/2/triples/experimental-setup.txt
Experimental setup,use,300 dimension,experimental-setup,/content/training-data/natural_language_inference/2/triples/experimental-setup.txt
300 dimension,has,pre-trained word vectors,experimental-setup,/content/training-data/natural_language_inference/2/triples/experimental-setup.txt
pre-trained word vectors,from,GloVe,experimental-setup,/content/training-data/natural_language_inference/2/triples/experimental-setup.txt
Experimental setup,For,multi-factor attentive encoding,experimental-setup,/content/training-data/natural_language_inference/2/triples/experimental-setup.txt
multi-factor attentive encoding,choose,4 factors,experimental-setup,/content/training-data/natural_language_inference/2/triples/experimental-setup.txt
Experimental setup,has,out - of - vocabulary words,experimental-setup,/content/training-data/natural_language_inference/2/triples/experimental-setup.txt
out - of - vocabulary words,initialized with,zero vectors,experimental-setup,/content/training-data/natural_language_inference/2/triples/experimental-setup.txt
Experimental setup,has,number of hidden units,experimental-setup,/content/training-data/natural_language_inference/2/triples/experimental-setup.txt
number of hidden units,in,all the LSTMs,experimental-setup,/content/training-data/natural_language_inference/2/triples/experimental-setup.txt
number of hidden units,is,150,experimental-setup,/content/training-data/natural_language_inference/2/triples/experimental-setup.txt
Experimental setup,tokenize,corpora,experimental-setup,/content/training-data/natural_language_inference/2/triples/experimental-setup.txt
corpora,with,NLTK,experimental-setup,/content/training-data/natural_language_inference/2/triples/experimental-setup.txt
Experimental setup,During,training,experimental-setup,/content/training-data/natural_language_inference/2/triples/experimental-setup.txt
training,has,minibatch size,experimental-setup,/content/training-data/natural_language_inference/2/triples/experimental-setup.txt
minibatch size,fixed at,60,experimental-setup,/content/training-data/natural_language_inference/2/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/2/triples/results.txt
Results,shows,results,results,/content/training-data/natural_language_inference/2/triples/results.txt
results,on,TriviaQA dataset,results,/content/training-data/natural_language_inference/2/triples/results.txt
TriviaQA dataset,shows,AMANDA,results,/content/training-data/natural_language_inference/2/triples/results.txt
AMANDA,achieves,state - of the - art results,results,/content/training-data/natural_language_inference/2/triples/results.txt
state - of the - art results,in,Wikipedia and Web domain,results,/content/training-data/natural_language_inference/2/triples/results.txt
Wikipedia and Web domain,on,distantly supervised and verified data,results,/content/training-data/natural_language_inference/2/triples/results.txt
Results,shows that,AMANDA,results,/content/training-data/natural_language_inference/2/triples/results.txt
AMANDA,outperforms,all the stateof - the - art models,results,/content/training-data/natural_language_inference/2/triples/results.txt
all the stateof - the - art models,by,significant margin,results,/content/training-data/natural_language_inference/2/triples/results.txt
all the stateof - the - art models,on,New s QA dataset,results,/content/training-data/natural_language_inference/2/triples/results.txt
Results,on,Search QA dataset,results,/content/training-data/natural_language_inference/2/triples/results.txt
Search QA dataset,has,AMANDA,results,/content/training-data/natural_language_inference/2/triples/results.txt
AMANDA,performs,better,results,/content/training-data/natural_language_inference/2/triples/results.txt
better,than,any of the ablated models,results,/content/training-data/natural_language_inference/2/triples/results.txt
any of the ablated models,include,ablation,results,/content/training-data/natural_language_inference/2/triples/results.txt
ablation,of,multifactor attentive encoding,results,/content/training-data/natural_language_inference/2/triples/results.txt
ablation,of,max - attentional question aggregation ( q ma ,results,/content/training-data/natural_language_inference/2/triples/results.txt
ablation,of,question type representation ( q f ,results,/content/training-data/natural_language_inference/2/triples/results.txt
AMANDA,outperforms,both systems,results,/content/training-data/natural_language_inference/2/triples/results.txt
both systems,especially for,multi-word - answer questions,results,/content/training-data/natural_language_inference/2/triples/results.txt
multi-word - answer questions,by,huge margin,results,/content/training-data/natural_language_inference/2/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/natural_language_inference/83/triples/baselines.txt
Baselines,has,Msap,baselines,/content/training-data/natural_language_inference/83/triples/baselines.txt
Msap,trains,logistic regression,baselines,/content/training-data/natural_language_inference/83/triples/baselines.txt
logistic regression,based on,stylistic and languagemodel based features,baselines,/content/training-data/natural_language_inference/83/triples/baselines.txt
Baselines,has,Majority Vote,baselines,/content/training-data/natural_language_inference/83/triples/baselines.txt
Majority Vote,has,ensemble method,baselines,/content/training-data/natural_language_inference/83/triples/baselines.txt
ensemble method,uses,features,baselines,/content/training-data/natural_language_inference/83/triples/baselines.txt
features,extracted for,K = 3 aspects,baselines,/content/training-data/natural_language_inference/83/triples/baselines.txt
features,to train,K separate logistic regression models,baselines,/content/training-data/natural_language_inference/83/triples/baselines.txt
Baselines,has,LR,baselines,/content/training-data/natural_language_inference/83/triples/baselines.txt
LR,is,simple logistic regression model,baselines,/content/training-data/natural_language_inference/83/triples/baselines.txt
simple logistic regression model,agnostic to,multiple types of aspects,baselines,/content/training-data/natural_language_inference/83/triples/baselines.txt
Baselines,has,Soft Voting,baselines,/content/training-data/natural_language_inference/83/triples/baselines.txt
Soft Voting,learns,K different aspect - specific classifiers,baselines,/content/training-data/natural_language_inference/83/triples/baselines.txt
Baselines,has,DSSM,baselines,/content/training-data/natural_language_inference/83/triples/baselines.txt
DSSM,trains,two deep neural networks,baselines,/content/training-data/natural_language_inference/83/triples/baselines.txt
two deep neural networks,to project,context and the ending - options,baselines,/content/training-data/natural_language_inference/83/triples/baselines.txt
context and the ending - options,into,same vector space,baselines,/content/training-data/natural_language_inference/83/triples/baselines.txt
Baselines,has,Aspect - aware Ensemble,baselines,/content/training-data/natural_language_inference/83/triples/baselines.txt
Aspect - aware Ensemble,trains,K different aspectspecific classifiers,baselines,/content/training-data/natural_language_inference/83/triples/baselines.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/83/triples/ablation-analysis.txt
Ablation analysis,shows,performance,ablation-analysis,/content/training-data/natural_language_inference/83/triples/ablation-analysis.txt
performance,of,logistic regression model,ablation-analysis,/content/training-data/natural_language_inference/83/triples/ablation-analysis.txt
logistic regression model,has,trained,ablation-analysis,/content/training-data/natural_language_inference/83/triples/ablation-analysis.txt
trained,using,all the features ( All ,ablation-analysis,/content/training-data/natural_language_inference/83/triples/ablation-analysis.txt
trained,using,individual feature - groups,ablation-analysis,/content/training-data/natural_language_inference/83/triples/ablation-analysis.txt
Ablation analysis,see that,features,ablation-analysis,/content/training-data/natural_language_inference/83/triples/ablation-analysis.txt
features,extracted from,aspect,ablation-analysis,/content/training-data/natural_language_inference/83/triples/ablation-analysis.txt
aspect,analyzing,event - sequence,ablation-analysis,/content/training-data/natural_language_inference/83/triples/ablation-analysis.txt
event - sequence,have,strongest predictive power,ablation-analysis,/content/training-data/natural_language_inference/83/triples/ablation-analysis.txt
strongest predictive power,followed by,characterizing,ablation-analysis,/content/training-data/natural_language_inference/83/triples/ablation-analysis.txt
characterizing,has,Sentiment - trajectory,ablation-analysis,/content/training-data/natural_language_inference/83/triples/ablation-analysis.txt
Ablation analysis,has,features,ablation-analysis,/content/training-data/natural_language_inference/83/triples/ablation-analysis.txt
features,measuring,top - ical consistency,ablation-analysis,/content/training-data/natural_language_inference/83/triples/ablation-analysis.txt
top - ical consistency,result in,lowest accuracy,ablation-analysis,/content/training-data/natural_language_inference/83/triples/ablation-analysis.txt
top - ical consistency,has,perform better,ablation-analysis,/content/training-data/natural_language_inference/83/triples/ablation-analysis.txt
perform better,than,random,ablation-analysis,/content/training-data/natural_language_inference/83/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/83/triples/model.txt
Model,analyzing if,topic,model,/content/training-data/natural_language_inference/83/triples/model.txt
topic,of,ending option,model,/content/training-data/natural_language_inference/83/triples/model.txt
topic,consistent with,preceding context,model,/content/training-data/natural_language_inference/83/triples/model.txt
Model,explore,three semantic aspects,model,/content/training-data/natural_language_inference/83/triples/model.txt
three semantic aspects,of,story understanding,model,/content/training-data/natural_language_inference/83/triples/model.txt
three semantic aspects,has,sequence of events,model,/content/training-data/natural_language_inference/83/triples/model.txt
sequence of events,described in,story,model,/content/training-data/natural_language_inference/83/triples/model.txt
three semantic aspects,has,evolution,model,/content/training-data/natural_language_inference/83/triples/model.txt
evolution,of,sentiment and emotional trajectories,model,/content/training-data/natural_language_inference/83/triples/model.txt
three semantic aspects,has,topical consistency,model,/content/training-data/natural_language_inference/83/triples/model.txt
Model,evaluating if,sentiment,model,/content/training-data/natural_language_inference/83/triples/model.txt
sentiment,has,makes sense,model,/content/training-data/natural_language_inference/83/triples/model.txt
makes sense,considering,context,model,/content/training-data/natural_language_inference/83/triples/model.txt
context,of,story,model,/content/training-data/natural_language_inference/83/triples/model.txt
sentiment,described in,ending option,model,/content/training-data/natural_language_inference/83/triples/model.txt
Model,has,first aspect,model,/content/training-data/natural_language_inference/83/triples/model.txt
first aspect,motivated from,semantic script induction,model,/content/training-data/natural_language_inference/83/triples/model.txt
first aspect,evaluates if,events,model,/content/training-data/natural_language_inference/83/triples/model.txt
events,likely to,occur,model,/content/training-data/natural_language_inference/83/triples/model.txt
occur,within,sequence of events,model,/content/training-data/natural_language_inference/83/triples/model.txt
sequence of events,described in,preceding context,model,/content/training-data/natural_language_inference/83/triples/model.txt
events,described in,ending - alternative,model,/content/training-data/natural_language_inference/83/triples/model.txt
Model,present,log - linear model,model,/content/training-data/natural_language_inference/83/triples/model.txt
log - linear model,used to,weigh,model,/content/training-data/natural_language_inference/83/triples/model.txt
weigh,using,hidden variable,model,/content/training-data/natural_language_inference/83/triples/model.txt
weigh,has,various aspects,model,/content/training-data/natural_language_inference/83/triples/model.txt
various aspects,of,story,model,/content/training-data/natural_language_inference/83/triples/model.txt
Contribution,has research problem,Story Comprehension,research-problem,/content/training-data/natural_language_inference/83/triples/research-problem.txt
Contribution,has research problem,Automatic story comprehension,research-problem,/content/training-data/natural_language_inference/83/triples/research-problem.txt
Contribution,has research problem,automatically understanding stories,research-problem,/content/training-data/natural_language_inference/83/triples/research-problem.txt
Contribution,has research problem,story - cloze,research-problem,/content/training-data/natural_language_inference/83/triples/research-problem.txt
Contribution,has,Experiments,experiments,/content/training-data/natural_language_inference/36/triples/experiments.txt
Experiments,has,Tasks,experiments,/content/training-data/natural_language_inference/36/triples/experiments.txt
Tasks,has,Textual Entailment,experiments,/content/training-data/natural_language_inference/36/triples/experiments.txt
Textual Entailment,has,Results,experiments,/content/training-data/natural_language_inference/36/triples/experiments.txt
Results,show,SRL embedding,experiments,/content/training-data/natural_language_inference/36/triples/experiments.txt
SRL embedding,has,boost,experiments,/content/training-data/natural_language_inference/36/triples/experiments.txt
boost,has,ESIM + ELMo model,experiments,/content/training-data/natural_language_inference/36/triples/experiments.txt
ESIM + ELMo model,by,+ 0.7 % improvement,experiments,/content/training-data/natural_language_inference/36/triples/experiments.txt
Results,With,semantic cues,experiments,/content/training-data/natural_language_inference/36/triples/experiments.txt
semantic cues,has,simple sequential encoding model,experiments,/content/training-data/natural_language_inference/36/triples/experiments.txt
simple sequential encoding model,yields,substantial gains,experiments,/content/training-data/natural_language_inference/36/triples/experiments.txt
semantic cues,has,our single BERT LARGE model,experiments,/content/training-data/natural_language_inference/36/triples/experiments.txt
our single BERT LARGE model,achieves,new stateof - the - art,experiments,/content/training-data/natural_language_inference/36/triples/experiments.txt
our single BERT LARGE model,has,outperforms,experiments,/content/training-data/natural_language_inference/36/triples/experiments.txt
outperforms,has,all the ensemble models,experiments,/content/training-data/natural_language_inference/36/triples/experiments.txt
all the ensemble models,in,leaderboard,experiments,/content/training-data/natural_language_inference/36/triples/experiments.txt
Tasks,has,Machine Reading Comprehension,experiments,/content/training-data/natural_language_inference/36/triples/experiments.txt
Machine Reading Comprehension,has,Results,experiments,/content/training-data/natural_language_inference/36/triples/experiments.txt
Results,has,SRL embeddings,experiments,/content/training-data/natural_language_inference/36/triples/experiments.txt
SRL embeddings,give,substantial performance gains,experiments,/content/training-data/natural_language_inference/36/triples/experiments.txt
substantial performance gains,over,all the strong baselines,experiments,/content/training-data/natural_language_inference/36/triples/experiments.txt
SRL embeddings,showing,quite effective,experiments,/content/training-data/natural_language_inference/36/triples/experiments.txt
quite effective,for,more complex document and question encoding,experiments,/content/training-data/natural_language_inference/36/triples/experiments.txt
Machine Reading Comprehension,has,Baselines,experiments,/content/training-data/natural_language_inference/36/triples/experiments.txt
Baselines,includes,MQAN,experiments,/content/training-data/natural_language_inference/36/triples/experiments.txt
MQAN,for,single task and multi-task,experiments,/content/training-data/natural_language_inference/36/triples/experiments.txt
single task and multi-task,with,SRL,experiments,/content/training-data/natural_language_inference/36/triples/experiments.txt
single task and multi-task,with,BiDAF + ELMo,experiments,/content/training-data/natural_language_inference/36/triples/experiments.txt
single task and multi-task,with,R.M. Reader,experiments,/content/training-data/natural_language_inference/36/triples/experiments.txt
single task and multi-task,with,BERT,experiments,/content/training-data/natural_language_inference/36/triples/experiments.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/36/triples/model.txt
Model,explore,integrative models,model,/content/training-data/natural_language_inference/36/triples/model.txt
integrative models,for,finer - grained text comprehension and inference,model,/content/training-data/natural_language_inference/36/triples/model.txt
Model,implement,easy and feasible scheme,model,/content/training-data/natural_language_inference/36/triples/model.txt
easy and feasible scheme,to integrate,semantic signals,model,/content/training-data/natural_language_inference/36/triples/model.txt
semantic signals,in,downstream neural models,model,/content/training-data/natural_language_inference/36/triples/model.txt
downstream neural models,in,end - to - end manner,model,/content/training-data/natural_language_inference/36/triples/model.txt
downstream neural models,to boost,effectively,model,/content/training-data/natural_language_inference/36/triples/model.txt
effectively,has,strong baselines,model,/content/training-data/natural_language_inference/36/triples/model.txt
Model,propose,semantics enhancement framework,model,/content/training-data/natural_language_inference/36/triples/model.txt
semantics enhancement framework,for,TC tasks,model,/content/training-data/natural_language_inference/36/triples/model.txt
Contribution,has research problem,Text Comprehension,research-problem,/content/training-data/natural_language_inference/36/triples/research-problem.txt
Contribution,has research problem,text comprehension ( TC ,research-problem,/content/training-data/natural_language_inference/36/triples/research-problem.txt
Contribution,has research problem,machine reading comprehension ( MRC ,research-problem,/content/training-data/natural_language_inference/36/triples/research-problem.txt
Contribution,has research problem,textual entailment ( TE ,research-problem,/content/training-data/natural_language_inference/36/triples/research-problem.txt
Contribution,has,Baselines,baselines,/content/training-data/natural_language_inference/13/triples/baselines.txt
Baselines,has,Search QA,baselines,/content/training-data/natural_language_inference/13/triples/baselines.txt
Search QA,has,main competitor baseline,baselines,/content/training-data/natural_language_inference/13/triples/baselines.txt
main competitor baseline,is,AMANDA model,baselines,/content/training-data/natural_language_inference/13/triples/baselines.txt
Baselines,has,RACE,baselines,/content/training-data/natural_language_inference/13/triples/baselines.txt
RACE,has,key competitors,baselines,/content/training-data/natural_language_inference/13/triples/baselines.txt
key competitors,are,Stanford Attention Reader ( Stanford AR ,baselines,/content/training-data/natural_language_inference/13/triples/baselines.txt
key competitors,are,Gated Attention Reader ( GA ,baselines,/content/training-data/natural_language_inference/13/triples/baselines.txt
key competitors,are,Dynamic Fusion Networks ( DFN ,baselines,/content/training-data/natural_language_inference/13/triples/baselines.txt
Baselines,has,NarrativeQA,baselines,/content/training-data/natural_language_inference/13/triples/baselines.txt
NarrativeQA,has,baselines,baselines,/content/training-data/natural_language_inference/13/triples/baselines.txt
baselines,are,context - less sequence to sequence ( seq2seq ) model,baselines,/content/training-data/natural_language_inference/13/triples/baselines.txt
baselines,are,ASR,baselines,/content/training-data/natural_language_inference/13/triples/baselines.txt
baselines,are,BiDAF,baselines,/content/training-data/natural_language_inference/13/triples/baselines.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/13/triples/model.txt
Model,compress,input document,model,/content/training-data/natural_language_inference/13/triples/model.txt
input document,into,neural bag - of - words ( summed ) representation,model,/content/training-data/natural_language_inference/13/triples/model.txt
neural bag - of - words ( summed ) representation,re-expanded to,original sequence length,model,/content/training-data/natural_language_inference/13/triples/model.txt
neural bag - of - words ( summed ) representation,passed through,affine transformation layers,model,/content/training-data/natural_language_inference/13/triples/model.txt
input document,has,"arbitrary k times at multi-ranges ( e.g. , 1 , 2 , 4 , 10 , 25 ",model,/content/training-data/natural_language_inference/13/triples/model.txt
Model,has,Our proposed MRU encoders,model,/content/training-data/natural_language_inference/13/triples/model.txt
Our proposed MRU encoders,learns,gating vectors,model,/content/training-data/natural_language_inference/13/triples/model.txt
gating vectors,via,multiple contract - and - expand layers,model,/content/training-data/natural_language_inference/13/triples/model.txt
multiple contract - and - expand layers,at,multiple dilated resolutions,model,/content/training-data/natural_language_inference/13/triples/model.txt
Model,has,k document representations,model,/content/training-data/natural_language_inference/13/triples/model.txt
k document representations,combined and modeled with,fully connected layers,model,/content/training-data/natural_language_inference/13/triples/model.txt
fully connected layers,to form,final compositional gate,model,/content/training-data/natural_language_inference/13/triples/model.txt
final compositional gate,applied onto,original input document,model,/content/training-data/natural_language_inference/13/triples/model.txt
k document representations,at,multiple ranges and n-gram blocks,model,/content/training-data/natural_language_inference/13/triples/model.txt
Model,propose,new compositional encoder,model,/content/training-data/natural_language_inference/13/triples/model.txt
new compositional encoder,serve as,new module,model,/content/training-data/natural_language_inference/13/triples/model.txt
new module,complementary to,existing neural architectures,model,/content/training-data/natural_language_inference/13/triples/model.txt
new compositional encoder,used in - place of,standard RNN encoders,model,/content/training-data/natural_language_inference/13/triples/model.txt
Contribution,has research problem,Machine Comprehension,research-problem,/content/training-data/natural_language_inference/13/triples/research-problem.txt
Contribution,has research problem,machine comprehension ( MC ,research-problem,/content/training-data/natural_language_inference/13/triples/research-problem.txt
Contribution,has research problem,MC,research-problem,/content/training-data/natural_language_inference/13/triples/research-problem.txt
Contribution,has,Experimental Setup,experimental-setup,/content/training-data/natural_language_inference/13/triples/experimental-setup.txt
Experimental Setup,adopt,Adam optimizer,experimental-setup,/content/training-data/natural_language_inference/13/triples/experimental-setup.txt
Adam optimizer,with,learning rate,experimental-setup,/content/training-data/natural_language_inference/13/triples/experimental-setup.txt
learning rate,of,0.0003/ 0.001/0.001,experimental-setup,/content/training-data/natural_language_inference/13/triples/experimental-setup.txt
0.0003/ 0.001/0.001,for,RACE / SearchQA / NarrativeQA,experimental-setup,/content/training-data/natural_language_inference/13/triples/experimental-setup.txt
Experimental Setup,based on,TitanXP GPU,experimental-setup,/content/training-data/natural_language_inference/13/triples/experimental-setup.txt
TitanXP GPU,trained,All models,experimental-setup,/content/training-data/natural_language_inference/13/triples/experimental-setup.txt
TitanXP GPU,has,all runtime benchmarks,experimental-setup,/content/training-data/natural_language_inference/13/triples/experimental-setup.txt
Experimental Setup,implement,all models,experimental-setup,/content/training-data/natural_language_inference/13/triples/experimental-setup.txt
all models,in,TensorFlow,experimental-setup,/content/training-data/natural_language_inference/13/triples/experimental-setup.txt
Experimental Setup,has,Dropout rate,experimental-setup,/content/training-data/natural_language_inference/13/triples/experimental-setup.txt
Dropout rate,tuned amongst,"{ 0.1 , 0.2 , 0.3 }",experimental-setup,/content/training-data/natural_language_inference/13/triples/experimental-setup.txt
"{ 0.1 , 0.2 , 0.3 }",on,all layers,experimental-setup,/content/training-data/natural_language_inference/13/triples/experimental-setup.txt
all layers,including,embedding layer,experimental-setup,/content/training-data/natural_language_inference/13/triples/experimental-setup.txt
Experimental Setup,has,batch size,experimental-setup,/content/training-data/natural_language_inference/13/triples/experimental-setup.txt
batch size,set to,64/256/32,experimental-setup,/content/training-data/natural_language_inference/13/triples/experimental-setup.txt
Experimental Setup,has,Word embeddings,experimental-setup,/content/training-data/natural_language_inference/13/triples/experimental-setup.txt
Word embeddings,initialized with,300d Glo Ve vectors,experimental-setup,/content/training-data/natural_language_inference/13/triples/experimental-setup.txt
Word embeddings,not fine - tuned during,training,experimental-setup,/content/training-data/natural_language_inference/13/triples/experimental-setup.txt
Experimental Setup,has,maximum sequence lengths,experimental-setup,/content/training-data/natural_language_inference/13/triples/experimental-setup.txt
maximum sequence lengths,are,500/200/1100,experimental-setup,/content/training-data/natural_language_inference/13/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/13/triples/results.txt
Results,on,RACE,results,/content/training-data/natural_language_inference/13/triples/results.txt
RACE,has,6 % improvement,results,/content/training-data/natural_language_inference/13/triples/results.txt
6 % improvement,on,RACE - H dataset,results,/content/training-data/natural_language_inference/13/triples/results.txt
RACE,has,Sim . MRU and MRU,results,/content/training-data/natural_language_inference/13/triples/results.txt
Sim . MRU and MRU,achieve,comparable performance,results,/content/training-data/natural_language_inference/13/triples/results.txt
comparable performance,to,each other,results,/content/training-data/natural_language_inference/13/triples/results.txt
RACE,has,ensemble of Sim . MRU models,results,/content/training-data/natural_language_inference/13/triples/results.txt
ensemble of Sim . MRU models,achieve,state - of - the - art performance,results,/content/training-data/natural_language_inference/13/triples/results.txt
state - of - the - art performance,achieving,over all score,results,/content/training-data/natural_language_inference/13/triples/results.txt
over all score,of,53.3 %,results,/content/training-data/natural_language_inference/13/triples/results.txt
state - of - the - art performance,on,RACE dataset,results,/content/training-data/natural_language_inference/13/triples/results.txt
RACE,has,1.8 % improvement,results,/content/training-data/natural_language_inference/13/triples/results.txt
1.8 % improvement,on,RACE - M dataset,results,/content/training-data/natural_language_inference/13/triples/results.txt
RACE,has,GRU and LSTM models,results,/content/training-data/natural_language_inference/13/triples/results.txt
GRU and LSTM models,do not have,competitive edge,results,/content/training-data/natural_language_inference/13/triples/results.txt
RACE,has,no encoder,results,/content/training-data/natural_language_inference/13/triples/results.txt
no encoder,achieves,comparable 1 performance,results,/content/training-data/natural_language_inference/13/triples/results.txt
comparable 1 performance,to,DFN,results,/content/training-data/natural_language_inference/13/triples/results.txt
Results,on,Narrative QA benchmark,results,/content/training-data/natural_language_inference/13/triples/results.txt
Narrative QA benchmark,observe that,300d MRU,results,/content/training-data/natural_language_inference/13/triples/results.txt
300d MRU,can achieve,comparable performance,results,/content/training-data/natural_language_inference/13/triples/results.txt
comparable performance,with,BiDAF,results,/content/training-data/natural_language_inference/13/triples/results.txt
Narrative QA benchmark,compared with,BiLSTM,results,/content/training-data/natural_language_inference/13/triples/results.txt
BiLSTM,find that,MRU model,results,/content/training-data/natural_language_inference/13/triples/results.txt
MRU model,performs,competitively,results,/content/training-data/natural_language_inference/13/triples/results.txt
competitively,with,less than 1 % deprovement,results,/content/training-data/natural_language_inference/13/triples/results.txt
less than 1 % deprovement,across,all metrics,results,/content/training-data/natural_language_inference/13/triples/results.txt
BiLSTM,of,equal output dimensions ( 150 d ,results,/content/training-data/natural_language_inference/13/triples/results.txt
Narrative QA benchmark,has,MRU - LSTM,results,/content/training-data/natural_language_inference/13/triples/results.txt
MRU - LSTM,has,significantly outperforms,results,/content/training-data/natural_language_inference/13/triples/results.txt
significantly outperforms,has,all models,results,/content/training-data/natural_language_inference/13/triples/results.txt
all models,including,BiDAF,results,/content/training-data/natural_language_inference/13/triples/results.txt
Narrative QA benchmark,has,Performance improvement,results,/content/training-data/natural_language_inference/13/triples/results.txt
Performance improvement,over,vanilla BiLSTM model,results,/content/training-data/natural_language_inference/13/triples/results.txt
vanilla BiLSTM model,ranges from,1 % ? 3 %,results,/content/training-data/natural_language_inference/13/triples/results.txt
1 % ? 3 %,across,all metrics,results,/content/training-data/natural_language_inference/13/triples/results.txt
Narrative QA benchmark,performance of,our model,results,/content/training-data/natural_language_inference/13/triples/results.txt
our model,is,significantly better,results,/content/training-data/natural_language_inference/13/triples/results.txt
significantly better,than,300d LSTM model,results,/content/training-data/natural_language_inference/13/triples/results.txt
our model,also being,significantly faster,results,/content/training-data/natural_language_inference/13/triples/results.txt
Contribution,has,Approach,approach,/content/training-data/natural_language_inference/33/triples/approach.txt
Approach,in,context,approach,/content/training-data/natural_language_inference/33/triples/approach.txt
context,of,simple one - to - many multi -task learning ( MTL ) framework,approach,/content/training-data/natural_language_inference/33/triples/approach.txt
simple one - to - many multi -task learning ( MTL ) framework,wherein,single recurrent sentence encoder,approach,/content/training-data/natural_language_inference/33/triples/approach.txt
single recurrent sentence encoder,shared across,multiple tasks,approach,/content/training-data/natural_language_inference/33/triples/approach.txt
Approach,aims at,learning,approach,/content/training-data/natural_language_inference/33/triples/approach.txt
learning,has,fixed - length distributed sentence representations,approach,/content/training-data/natural_language_inference/33/triples/approach.txt
Approach,combine,benefits,approach,/content/training-data/natural_language_inference/33/triples/approach.txt
benefits,of,diverse sentence - representation learning objectives,approach,/content/training-data/natural_language_inference/33/triples/approach.txt
diverse sentence - representation learning objectives,into,single multi-task framework,approach,/content/training-data/natural_language_inference/33/triples/approach.txt
Contribution,has research problem,LEARNING GENERAL PURPOSE DISTRIBUTED SEN - TENCE REPRESENTATIONS,research-problem,/content/training-data/natural_language_inference/33/triples/research-problem.txt
Contribution,has research problem,distributed vector representations of words,research-problem,/content/training-data/natural_language_inference/33/triples/research-problem.txt
Contribution,has research problem,learning representations of sequences of words,research-problem,/content/training-data/natural_language_inference/33/triples/research-problem.txt
Contribution,has research problem,learning general - purpose sentence representations,research-problem,/content/training-data/natural_language_inference/33/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/33/triples/results.txt
Results,see,similar gains,results,/content/training-data/natural_language_inference/33/triples/results.txt
similar gains,closing,gap,results,/content/training-data/natural_language_inference/33/triples/results.txt
gap,on,supervised approaches,results,/content/training-data/natural_language_inference/33/triples/results.txt
supervised approaches,trained from,scratch,results,/content/training-data/natural_language_inference/33/triples/results.txt
similar gains,has,2.3 %,results,/content/training-data/natural_language_inference/33/triples/results.txt
similar gains,on,paraphrase identification ( MPRC ,results,/content/training-data/natural_language_inference/33/triples/results.txt
Results,observe that,sentence characteristics,results,/content/training-data/natural_language_inference/33/triples/results.txt
sentence characteristics,such as,length and word order,results,/content/training-data/natural_language_inference/33/triples/results.txt
length and word order,are,better encoded,results,/content/training-data/natural_language_inference/33/triples/results.txt
better encoded,addition of,parsing,results,/content/training-data/natural_language_inference/33/triples/results.txt
Results,observe that,learned word embeddings,results,/content/training-data/natural_language_inference/33/triples/results.txt
learned word embeddings,are,competitive,results,/content/training-data/natural_language_inference/33/triples/results.txt
competitive,with,popular methods,results,/content/training-data/natural_language_inference/33/triples/results.txt
popular methods,such as,GloVe,results,/content/training-data/natural_language_inference/33/triples/results.txt
popular methods,such as,word2vec,results,/content/training-data/natural_language_inference/33/triples/results.txt
popular methods,such as,fasttext,results,/content/training-data/natural_language_inference/33/triples/results.txt
Results,Increasing,capacity,results,/content/training-data/natural_language_inference/33/triples/results.txt
capacity,has,our sentence encoder,results,/content/training-data/natural_language_inference/33/triples/results.txt
our sentence encoder,lead to,improved transfer performance,results,/content/training-data/natural_language_inference/33/triples/results.txt
our sentence encoder,with,more hidden units ( + L ,results,/content/training-data/natural_language_inference/33/triples/results.txt
our sentence encoder,as well as,additional layer ( + 2L ,results,/content/training-data/natural_language_inference/33/triples/results.txt
Results,addition of,constituency parsing,results,/content/training-data/natural_language_inference/33/triples/results.txt
constituency parsing,has,improves,results,/content/training-data/natural_language_inference/33/triples/results.txt
improves,has,performance,results,/content/training-data/natural_language_inference/33/triples/results.txt
performance,on,sentence relatedness ( SICK - R ,results,/content/training-data/natural_language_inference/33/triples/results.txt
performance,on,entailment ( SICK - E ,results,/content/training-data/natural_language_inference/33/triples/results.txt
Results,When using,small fraction,results,/content/training-data/natural_language_inference/33/triples/results.txt
small fraction,able to,outperform,results,/content/training-data/natural_language_inference/33/triples/results.txt
outperform,has,Siamese and Multi - Perspective CNN,results,/content/training-data/natural_language_inference/33/triples/results.txt
Siamese and Multi - Perspective CNN,using,roughly 6 %,results,/content/training-data/natural_language_inference/33/triples/results.txt
roughly 6 %,of,available training set,results,/content/training-data/natural_language_inference/33/triples/results.txt
small fraction,of,training data,results,/content/training-data/natural_language_inference/33/triples/results.txt
Results,adding,more tasks,results,/content/training-data/natural_language_inference/33/triples/results.txt
more tasks,improves,transfer performance,results,/content/training-data/natural_language_inference/33/triples/results.txt
transfer performance,of,our model,results,/content/training-data/natural_language_inference/33/triples/results.txt
Results,demonstrate,substantial gains,results,/content/training-data/natural_language_inference/33/triples/results.txt
substantial gains,has,roughly 2 %,results,/content/training-data/natural_language_inference/33/triples/results.txt
roughly 2 %,over,CNN - LSTM,results,/content/training-data/natural_language_inference/33/triples/results.txt
substantial gains,has,outperforming,results,/content/training-data/natural_language_inference/33/triples/results.txt
outperforming,has,competitive supervised baseline,results,/content/training-data/natural_language_inference/33/triples/results.txt
substantial gains,has,6 %,results,/content/training-data/natural_language_inference/33/triples/results.txt
6 %,over,Infersent,results,/content/training-data/natural_language_inference/33/triples/results.txt
substantial gains,on,TREC,results,/content/training-data/natural_language_inference/33/triples/results.txt
Results,has,outperform,results,/content/training-data/natural_language_inference/33/triples/results.txt
outperform,has,Deconv LVM model,results,/content/training-data/natural_language_inference/33/triples/results.txt
Results,has,Representations,results,/content/training-data/natural_language_inference/33/triples/results.txt
Representations,learned solely from,NLI,results,/content/training-data/natural_language_inference/33/triples/results.txt
NLI,appear to encode,syntax,results,/content/training-data/natural_language_inference/33/triples/results.txt
Representations,incorporation into,our multi-task framework,results,/content/training-data/natural_language_inference/33/triples/results.txt
our multi-task framework,does not,amplify,results,/content/training-data/natural_language_inference/33/triples/results.txt
amplify,has,signal,results,/content/training-data/natural_language_inference/33/triples/results.txt
Results,observe,0.2-0.5 % improvement,results,/content/training-data/natural_language_inference/33/triples/results.txt
0.2-0.5 % improvement,over,decomposable attention model,results,/content/training-data/natural_language_inference/33/triples/results.txt
Results,observe,gains,results,/content/training-data/natural_language_inference/33/triples/results.txt
gains,of,1.1 - 2.0 %,results,/content/training-data/natural_language_inference/33/triples/results.txt
1.1 - 2.0 %,over,Infersent,results,/content/training-data/natural_language_inference/33/triples/results.txt
1.1 - 2.0 %,on,sentiment classification tasks,results,/content/training-data/natural_language_inference/33/triples/results.txt
sentiment classification tasks,name,MR,results,/content/training-data/natural_language_inference/33/triples/results.txt
sentiment classification tasks,name,CR,results,/content/training-data/natural_language_inference/33/triples/results.txt
sentiment classification tasks,name,SUBJ,results,/content/training-data/natural_language_inference/33/triples/results.txt
sentiment classification tasks,name,MPQA,results,/content/training-data/natural_language_inference/33/triples/results.txt
Results,show that,training,results,/content/training-data/natural_language_inference/33/triples/results.txt
training,has,MLP,results,/content/training-data/natural_language_inference/33/triples/results.txt
MLP,on top of,our fixed sentence representations,results,/content/training-data/natural_language_inference/33/triples/results.txt
our fixed sentence representations,has,outperforms,results,/content/training-data/natural_language_inference/33/triples/results.txt
outperforms,has,several strong & complex supervised approaches,results,/content/training-data/natural_language_inference/33/triples/results.txt
several strong & complex supervised approaches,that use,attention mechanisms,results,/content/training-data/natural_language_inference/33/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/34/triples/ablation-analysis.txt
Ablation analysis,using,1 - layer fusion block,ablation-analysis,/content/training-data/natural_language_inference/34/triples/ablation-analysis.txt
1 - layer fusion block,leads to,obvious performance loss,ablation-analysis,/content/training-data/natural_language_inference/34/triples/ablation-analysis.txt
obvious performance loss,implies,significance,ablation-analysis,/content/training-data/natural_language_inference/34/triples/ablation-analysis.txt
significance,of,performing,ablation-analysis,/content/training-data/natural_language_inference/34/triples/ablation-analysis.txt
performing,has,multi-hop reasoning,ablation-analysis,/content/training-data/natural_language_inference/34/triples/ablation-analysis.txt
multi-hop reasoning,in,Hotpot QA,ablation-analysis,/content/training-data/natural_language_inference/34/triples/ablation-analysis.txt
Ablation analysis,of,QA performances,ablation-analysis,/content/training-data/natural_language_inference/34/triples/ablation-analysis.txt
QA performances,in,development set,ablation-analysis,/content/training-data/natural_language_inference/34/triples/ablation-analysis.txt
development set,of,Hotpot QA,ablation-analysis,/content/training-data/natural_language_inference/34/triples/ablation-analysis.txt
development set,see that,each of our model components,ablation-analysis,/content/training-data/natural_language_inference/34/triples/ablation-analysis.txt
each of our model components,provide,from 1 % to 2 %,ablation-analysis,/content/training-data/natural_language_inference/34/triples/ablation-analysis.txt
from 1 % to 2 %,has,relative gain,ablation-analysis,/content/training-data/natural_language_inference/34/triples/ablation-analysis.txt
relative gain,over,QA performance,ablation-analysis,/content/training-data/natural_language_inference/34/triples/ablation-analysis.txt
Ablation analysis,has,dataset abla-tion results,ablation-analysis,/content/training-data/natural_language_inference/34/triples/ablation-analysis.txt
dataset abla-tion results,show,our model,ablation-analysis,/content/training-data/natural_language_inference/34/triples/ablation-analysis.txt
our model,has,not very sensitive,ablation-analysis,/content/training-data/natural_language_inference/34/triples/ablation-analysis.txt
not very sensitive,can achieve,more than 5 % performance gain,ablation-analysis,/content/training-data/natural_language_inference/34/triples/ablation-analysis.txt
more than 5 % performance gain,in,gold paragraphs only,ablation-analysis,/content/training-data/natural_language_inference/34/triples/ablation-analysis.txt
more than 5 % performance gain,in,supporting facts only,ablation-analysis,/content/training-data/natural_language_inference/34/triples/ablation-analysis.txt
not very sensitive,to,noisy paragraphs,ablation-analysis,/content/training-data/natural_language_inference/34/triples/ablation-analysis.txt
not very sensitive,comparing with,baseline model,ablation-analysis,/content/training-data/natural_language_inference/34/triples/ablation-analysis.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
Hyperparameters,set,relatively low threshold,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
relatively low threshold,during,selection,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
selection,on,supporting facts,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
supporting facts,to keep,high recall ( 97 % ,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
supporting facts,to keep,reasonable precision ( 69 % ,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
Hyperparameters,set,dropout rate,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
dropout rate,for,all hidden units,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
all hidden units,of,LSTM and dynamic graph attention,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
LSTM and dynamic graph attention,to,0.3 and 0.5,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
Hyperparameters,In,graph construction stage,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
graph construction stage,use,pretrained NER model,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
pretrained NER model,to extract,named entities,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
pretrained NER model,from,Stanford CoreNLP Toolkits,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
Hyperparameters,In,encoding stage,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
encoding stage,use,pre-trained BERT model,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
pre-trained BERT model,as,encoder,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
pre-trained BERT model,has,d 1,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
d 1,is,768,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
Hyperparameters,In,paragraph selection stage,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
paragraph selection stage,use,uncased version of BERT Tokenizer,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
uncased version of BERT Tokenizer,to tokenize,all passages and questions,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
Hyperparameters,For,optimization,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
optimization,use,Adam Optimizer,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
Adam Optimizer,with,initial learning rate,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
initial learning rate,of,1 e ?4,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
Hyperparameters,has,Each entity node,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
Each entity node,in,entity graphs,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
Each entity node,has,average degree,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
average degree,of,3.52,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
Hyperparameters,has,encoding vectors,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
encoding vectors,of,sentence pairs,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
encoding vectors,generated from,pre-trained BERT model,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
Hyperparameters,has,maximum number of entities,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
maximum number of entities,in,graph,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
maximum number of entities,set to,40,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
Hyperparameters,has,hidden state dimensions d 2,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
hidden state dimensions d 2,set to,300,hyperparameters,/content/training-data/natural_language_inference/34/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/34/triples/model.txt
Model,In,each round,model,/content/training-data/natural_language_inference/34/triples/model.txt
each round,has,DFGN,model,/content/training-data/natural_language_inference/34/triples/model.txt
DFGN,has,generates and reasons,model,/content/training-data/natural_language_inference/34/triples/model.txt
generates and reasons,on,dynamic graph,model,/content/training-data/natural_language_inference/34/triples/model.txt
dynamic graph,has,irrelevant entities,model,/content/training-data/natural_language_inference/34/triples/model.txt
irrelevant entities,are,masked out,model,/content/training-data/natural_language_inference/34/triples/model.txt
masked out,while,reasoning sources,model,/content/training-data/natural_language_inference/34/triples/model.txt
reasoning sources,are,preserved,model,/content/training-data/natural_language_inference/34/triples/model.txt
preserved,via,mask prediction module,model,/content/training-data/natural_language_inference/34/triples/model.txt
Model,propagate,information,model,/content/training-data/natural_language_inference/34/triples/model.txt
information,of,entity graph,model,/content/training-data/natural_language_inference/34/triples/model.txt
entity graph,back to,document representations,model,/content/training-data/natural_language_inference/34/triples/model.txt
Model,has,DFGN,model,/content/training-data/natural_language_inference/34/triples/model.txt
DFGN,constructs,dynamic entity graph,model,/content/training-data/natural_language_inference/34/triples/model.txt
dynamic entity graph,based on,entity mentions,model,/content/training-data/natural_language_inference/34/triples/model.txt
entity mentions,in,query and documents,model,/content/training-data/natural_language_inference/34/triples/model.txt
dynamic entity graph,has,process,model,/content/training-data/natural_language_inference/34/triples/model.txt
process,iterates in,multiple rounds,model,/content/training-data/natural_language_inference/34/triples/model.txt
multiple rounds,to achieve,multihop reasoning,model,/content/training-data/natural_language_inference/34/triples/model.txt
Model,has,fusion process,model,/content/training-data/natural_language_inference/34/triples/model.txt
fusion process,of,doc2 graph and graph2doc,model,/content/training-data/natural_language_inference/34/triples/model.txt
doc2 graph and graph2doc,along with,dynamic entity graph,model,/content/training-data/natural_language_inference/34/triples/model.txt
doc2 graph and graph2doc,has,jointly improve,model,/content/training-data/natural_language_inference/34/triples/model.txt
jointly improve,has,interaction,model,/content/training-data/natural_language_inference/34/triples/model.txt
interaction,leading to,less noisy entity graph,model,/content/training-data/natural_language_inference/34/triples/model.txt
less noisy entity graph,has,more accurate answers,model,/content/training-data/natural_language_inference/34/triples/model.txt
interaction,between,information,model,/content/training-data/natural_language_inference/34/triples/model.txt
information,of,documents,model,/content/training-data/natural_language_inference/34/triples/model.txt
information,of,entity graph,model,/content/training-data/natural_language_inference/34/triples/model.txt
fusion process,is,iteratively performed,model,/content/training-data/natural_language_inference/34/triples/model.txt
iteratively performed,through,document tokens and entities,model,/content/training-data/natural_language_inference/34/triples/model.txt
iteratively performed,at,each hop,model,/content/training-data/natural_language_inference/34/triples/model.txt
iteratively performed,has,final resulting answer,model,/content/training-data/natural_language_inference/34/triples/model.txt
final resulting answer,obtained from,document tokens,model,/content/training-data/natural_language_inference/34/triples/model.txt
Model,propose,Dynamically Fused Graph Network ( DFGN ,model,/content/training-data/natural_language_inference/34/triples/model.txt
Dynamically Fused Graph Network ( DFGN ),has,novel method,model,/content/training-data/natural_language_inference/34/triples/model.txt
Model,propose,fusion process,model,/content/training-data/natural_language_inference/34/triples/model.txt
fusion process,in,DFGN,model,/content/training-data/natural_language_inference/34/triples/model.txt
DFGN,to solve,unrestricted QA challenge,model,/content/training-data/natural_language_inference/34/triples/model.txt
Model,aggregate,information,model,/content/training-data/natural_language_inference/34/triples/model.txt
information,from,documents,model,/content/training-data/natural_language_inference/34/triples/model.txt
documents,name,doc2 graph,model,/content/training-data/natural_language_inference/34/triples/model.txt
documents,to,entity graph,model,/content/training-data/natural_language_inference/34/triples/model.txt
Contribution,has research problem,Multi-hop Reasoning,research-problem,/content/training-data/natural_language_inference/34/triples/research-problem.txt
Contribution,has research problem,Text - based question answering ( TBQA ,research-problem,/content/training-data/natural_language_inference/34/triples/research-problem.txt
Contribution,has research problem,Question answering ( QA ,research-problem,/content/training-data/natural_language_inference/34/triples/research-problem.txt
Contribution,has research problem,QA,research-problem,/content/training-data/natural_language_inference/34/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/34/triples/results.txt
Results,in,private test set,results,/content/training-data/natural_language_inference/34/triples/results.txt
private test set,of,Hotpot QA,results,/content/training-data/natural_language_inference/34/triples/results.txt
Hotpot QA,see that,our model,results,/content/training-data/natural_language_inference/34/triples/results.txt
our model,achieves,second best result,results,/content/training-data/natural_language_inference/34/triples/results.txt
second best result,on,leaderboard,results,/content/training-data/natural_language_inference/34/triples/results.txt
second best result,on,March 1st,results,/content/training-data/natural_language_inference/34/triples/results.txt
Results,show,our model,results,/content/training-data/natural_language_inference/34/triples/results.txt
our model,achieves,1.5 % gain,results,/content/training-data/natural_language_inference/34/triples/results.txt
1.5 % gain,with,entity graph,results,/content/training-data/natural_language_inference/34/triples/results.txt
entity graph,built from,better entity recognizer,results,/content/training-data/natural_language_inference/34/triples/results.txt
1.5 % gain,in,joint F1 - score,results,/content/training-data/natural_language_inference/34/triples/results.txt
Results,has,joint performance,results,/content/training-data/natural_language_inference/34/triples/results.txt
joint performance,of,our model,results,/content/training-data/natural_language_inference/34/triples/results.txt
our model,are,competitive,results,/content/training-data/natural_language_inference/34/triples/results.txt
competitive,against,state - of - the - art unpublished models,results,/content/training-data/natural_language_inference/34/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/67/triples/ablation-analysis.txt
Ablation analysis,shows,POS feature ( 1 ) and question - word feature ( 3 ,ablation-analysis,/content/training-data/natural_language_inference/67/triples/ablation-analysis.txt
POS feature ( 1 ) and question - word feature ( 3 ),are,two most important features,ablation-analysis,/content/training-data/natural_language_inference/67/triples/ablation-analysis.txt
Ablation analysis,combining,DCR model,ablation-analysis,/content/training-data/natural_language_inference/67/triples/ablation-analysis.txt
DCR model,with,proposed POS - trie constraints,ablation-analysis,/content/training-data/natural_language_inference/67/triples/ablation-analysis.txt
DCR model,yields,score,ablation-analysis,/content/training-data/natural_language_inference/67/triples/ablation-analysis.txt
score,similar to,DCR model,ablation-analysis,/content/training-data/natural_language_inference/67/triples/ablation-analysis.txt
DCR model,with,all possible n-gram chunks,ablation-analysis,/content/training-data/natural_language_inference/67/triples/ablation-analysis.txt
Ablation analysis,replacing,word - by - word attention,ablation-analysis,/content/training-data/natural_language_inference/67/triples/ablation-analysis.txt
word - by - word attention,with,Attentive Reader,ablation-analysis,/content/training-data/natural_language_inference/67/triples/ablation-analysis.txt
Attentive Reader,has,style attention,ablation-analysis,/content/training-data/natural_language_inference/67/triples/ablation-analysis.txt
style attention,has,decreases,ablation-analysis,/content/training-data/natural_language_inference/67/triples/ablation-analysis.txt
decreases,by about,4.5 %,ablation-analysis,/content/training-data/natural_language_inference/67/triples/ablation-analysis.txt
decreases,has,EM score,ablation-analysis,/content/training-data/natural_language_inference/67/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/67/triples/model.txt
Model,represents,answer candidates,model,/content/training-data/natural_language_inference/67/triples/model.txt
answer candidates,as,chunks,model,/content/training-data/natural_language_inference/67/triples/model.txt
chunks,to make,model,model,/content/training-data/natural_language_inference/67/triples/model.txt
model,aware of,subtle differences,model,/content/training-data/natural_language_inference/67/triples/model.txt
subtle differences,among,candidates,model,/content/training-data/natural_language_inference/67/triples/model.txt
chunks,instead of,word - level representations,model,/content/training-data/natural_language_inference/67/triples/model.txt
Model,called,dynamic chunk reader ( DCR ,model,/content/training-data/natural_language_inference/67/triples/model.txt
Model,uses,deep networks,model,/content/training-data/natural_language_inference/67/triples/model.txt
deep networks,to learn,better representations,model,/content/training-data/natural_language_inference/67/triples/model.txt
better representations,for,candidate answer chunks,model,/content/training-data/natural_language_inference/67/triples/model.txt
better representations,instead of,fixed feature representations,model,/content/training-data/natural_language_inference/67/triples/model.txt
Contribution,has research problem,Reading Comprehension,research-problem,/content/training-data/natural_language_inference/67/triples/research-problem.txt
Contribution,has research problem,neural reading comprehension ( RC ,research-problem,/content/training-data/natural_language_inference/67/triples/research-problem.txt
Contribution,has research problem,Reading comprehension - based question answering ( RCQA ,research-problem,/content/training-data/natural_language_inference/67/triples/research-problem.txt
Contribution,has research problem,RCQA,research-problem,/content/training-data/natural_language_inference/67/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
Experimental setup,set,maximum passage length,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
maximum passage length,has,pruned,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
pruned,has,all the tokens,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
all the tokens,after,300 - th token,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
300 - th token,in,training set,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
maximum passage length,to be,300 tokens,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
Experimental setup,trained,model,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
model,for,at most 30 epochs,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
Experimental setup,pre-processed,SQuAD dataset,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
SQuAD dataset,using,Stanford CoreNLP tool,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
Stanford CoreNLP tool,with,default setting,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
Stanford CoreNLP tool,to tokenize,text,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
text,obtain,POS and NE annotations,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
Experimental setup,applied,zero - padding,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
zero - padding,to,passage and question inputs,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
passage and question inputs,in,each batch,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
Experimental setup,applied,gradient clipping,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
gradient clipping,when,norm,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
norm,exceeded,10,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
norm,of,gradients,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
Experimental setup,applied,dropout,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
dropout,of rate,0.2,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
0.2,to,embedding layer,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
embedding layer,of,input bi - GRU encoder,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
Experimental setup,For,feature ranking - based system,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
feature ranking - based system,used,jforest ranker,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
jforest ranker,with,Lambda MART - Regression Tree algorithm,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
Experimental setup,used,stochastic gradient descent,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
stochastic gradient descent,with,ADAM optimizer,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
stochastic gradient descent,with,initial learning rate,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
initial learning rate,of,0.001,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
Experimental setup,has,hidden state size,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
hidden state size,has,d,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
d,set to,300,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
300,for,all GRUs,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
Experimental setup,has,All GRU weights,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
All GRU weights,initialized from,uniform distribution,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
uniform distribution,between,"( - 0.01 , 0.01 ",experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
Experimental setup,trained in,mini-batch style,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
mini-batch style,has,mini - batch size,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
mini - batch size,is,180,experimental-setup,/content/training-data/natural_language_inference/67/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/67/triples/results.txt
Results,has,our baseline system,results,/content/training-data/natural_language_inference/67/triples/results.txt
our baseline system,has,improves,results,/content/training-data/natural_language_inference/67/triples/results.txt
improves,has,10 % ( EM ,results,/content/training-data/natural_language_inference/67/triples/results.txt
10 % ( EM ),over,feature - based ranking system,results,/content/training-data/natural_language_inference/67/triples/results.txt
Results,compared to,our DCR model,results,/content/training-data/natural_language_inference/67/triples/results.txt
our DCR model,has,baseline,results,/content/training-data/natural_language_inference/67/triples/results.txt
baseline,is,more than 12 % ( EM ,results,/content/training-data/natural_language_inference/67/triples/results.txt
more than 12 % ( EM ),has,behind,results,/content/training-data/natural_language_inference/67/triples/results.txt
Results,on,SQuAD dataset,results,/content/training-data/natural_language_inference/67/triples/results.txt
SQuAD dataset,are,better,results,/content/training-data/natural_language_inference/67/triples/results.txt
better,has,our exact match ( EM ) and F1,results,/content/training-data/natural_language_inference/67/triples/results.txt
our exact match ( EM ) and F1,on,development set,results,/content/training-data/natural_language_inference/67/triples/results.txt
SQuAD dataset,are,comparable,results,/content/training-data/natural_language_inference/67/triples/results.txt
comparable,has,F1,results,/content/training-data/natural_language_inference/67/triples/results.txt
F1,on,test set,results,/content/training-data/natural_language_inference/67/triples/results.txt
Contribution,has,Dataset,datase,/content/training-data/natural_language_inference/47/triples/dataset.txt
Dataset,introduces,Stanford Natural Language Inference ( SNLI ) corpus,datase,/content/training-data/natural_language_inference/47/triples/dataset.txt
Stanford Natural Language Inference ( SNLI ) corpus,has,collection,datase,/content/training-data/natural_language_inference/47/triples/dataset.txt
collection,of,sentence pairs,datase,/content/training-data/natural_language_inference/47/triples/dataset.txt
sentence pairs,labeled for,entailment,datase,/content/training-data/natural_language_inference/47/triples/dataset.txt
sentence pairs,labeled for,contradiction,datase,/content/training-data/natural_language_inference/47/triples/dataset.txt
sentence pairs,labeled for,semantic independence,datase,/content/training-data/natural_language_inference/47/triples/dataset.txt
Dataset,At,"570,152 sentence pairs",datase,/content/training-data/natural_language_inference/47/triples/dataset.txt
"570,152 sentence pairs",has,SNLI,datase,/content/training-data/natural_language_inference/47/triples/dataset.txt
SNLI,is,two orders of magnitude larger,datase,/content/training-data/natural_language_inference/47/triples/dataset.txt
two orders of magnitude larger,than,other resources,datase,/content/training-data/natural_language_inference/47/triples/dataset.txt
other resources,of,its type,datase,/content/training-data/natural_language_inference/47/triples/dataset.txt
Dataset,In,separate validation phase,datase,/content/training-data/natural_language_inference/47/triples/dataset.txt
separate validation phase,collected,four additional judgments,datase,/content/training-data/natural_language_inference/47/triples/dataset.txt
four additional judgments,for,each label,datase,/content/training-data/natural_language_inference/47/triples/dataset.txt
each label,for,"56,941",datase,/content/training-data/natural_language_inference/47/triples/dataset.txt
"56,941",of,examples,datase,/content/training-data/natural_language_inference/47/triples/dataset.txt
four additional judgments,Of these,58 %,datase,/content/training-data/natural_language_inference/47/triples/dataset.txt
58 %,see,unanimous consensus,datase,/content/training-data/natural_language_inference/47/triples/dataset.txt
unanimous consensus,from,all five annotators,datase,/content/training-data/natural_language_inference/47/triples/dataset.txt
four additional judgments,Of these,98 %,datase,/content/training-data/natural_language_inference/47/triples/dataset.txt
98 %,of,cases,datase,/content/training-data/natural_language_inference/47/triples/dataset.txt
cases,emerge with,threeannotator consensus,datase,/content/training-data/natural_language_inference/47/triples/dataset.txt
Dataset,has,sentences and labels,datase,/content/training-data/natural_language_inference/47/triples/dataset.txt
sentences and labels,written by,humans,datase,/content/training-data/natural_language_inference/47/triples/dataset.txt
humans,in,"grounded , naturalistic context",datase,/content/training-data/natural_language_inference/47/triples/dataset.txt
Contribution,has research problem,natural language inference,research-problem,/content/training-data/natural_language_inference/47/triples/research-problem.txt
Contribution,has research problem,natural language inference ( NLI ,research-problem,/content/training-data/natural_language_inference/47/triples/research-problem.txt
Contribution,has research problem,NLI,research-problem,/content/training-data/natural_language_inference/47/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/47/triples/results.txt
Results,Of,two RNN models,results,/content/training-data/natural_language_inference/47/triples/results.txt
two RNN models,has,LSTM 's,results,/content/training-data/natural_language_inference/47/triples/results.txt
LSTM 's,has,more robust ability,results,/content/training-data/natural_language_inference/47/triples/results.txt
more robust ability,serves,well,results,/content/training-data/natural_language_inference/47/triples/results.txt
more robust ability,resulting in,performance,results,/content/training-data/natural_language_inference/47/triples/results.txt
performance,that is,essentially equivalent,results,/content/training-data/natural_language_inference/47/triples/results.txt
essentially equivalent,to,lexicalized classifier,results,/content/training-data/natural_language_inference/47/triples/results.txt
lexicalized classifier,on,test set,results,/content/training-data/natural_language_inference/47/triples/results.txt
more robust ability,to learn,long - term dependencies,results,/content/training-data/natural_language_inference/47/triples/results.txt
Results,has,sum of words model,results,/content/training-data/natural_language_inference/47/triples/results.txt
sum of words model,performed,slightly worse,results,/content/training-data/natural_language_inference/47/triples/results.txt
slightly worse,than,fundamentally similar lexicalized classifier,results,/content/training-data/natural_language_inference/47/triples/results.txt
Results,has,gap,results,/content/training-data/natural_language_inference/47/triples/results.txt
gap,between,train and test set accuracy,results,/content/training-data/natural_language_inference/47/triples/results.txt
train and test set accuracy,is,relatively small,results,/content/training-data/natural_language_inference/47/triples/results.txt
relatively small,for,all three neural network models,results,/content/training-data/natural_language_inference/47/triples/results.txt
Results,has,somewhat steeper slope,results,/content/training-data/natural_language_inference/47/triples/results.txt
somewhat steeper slope,for,LSTM,results,/content/training-data/natural_language_inference/47/triples/results.txt
somewhat steeper slope,hints that,ability to learn,results,/content/training-data/natural_language_inference/47/triples/results.txt
ability to learn,has,arbitrarily structured representations,results,/content/training-data/natural_language_inference/47/triples/results.txt
arbitrarily structured representations,give it,advantage,results,/content/training-data/natural_language_inference/47/triples/results.txt
advantage,over,more constrained lexicalized model,results,/content/training-data/natural_language_inference/47/triples/results.txt
more constrained lexicalized model,on,larger datasets,results,/content/training-data/natural_language_inference/47/triples/results.txt
arbitrarily structured representations,of,sentence meaning,results,/content/training-data/natural_language_inference/47/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/natural_language_inference/29/triples/baselines.txt
Baselines,has,BM25,baselines,/content/training-data/natural_language_inference/29/triples/baselines.txt
BM25,is,bag - of - words retrieval function,baselines,/content/training-data/natural_language_inference/29/triples/baselines.txt
bag - of - words retrieval function,ranks,set of reviews,baselines,/content/training-data/natural_language_inference/29/triples/baselines.txt
set of reviews,based on,question terms,baselines,/content/training-data/natural_language_inference/29/triples/baselines.txt
question terms,appearing in,each review,baselines,/content/training-data/natural_language_inference/29/triples/baselines.txt
Baselines,has,TF - IDF,baselines,/content/training-data/natural_language_inference/29/triples/baselines.txt
TF - IDF,name,Term Frequency - Inverse Document Frequency,baselines,/content/training-data/natural_language_inference/29/triples/baselines.txt
TF - IDF,is,numerical statistic,baselines,/content/training-data/natural_language_inference/29/triples/baselines.txt
numerical statistic,to reflect,how important,baselines,/content/training-data/natural_language_inference/29/triples/baselines.txt
how important,has,question word,baselines,/content/training-data/natural_language_inference/29/triples/baselines.txt
question word,to,review,baselines,/content/training-data/natural_language_inference/29/triples/baselines.txt
Baselines,has,QS,baselines,/content/training-data/natural_language_inference/29/triples/baselines.txt
QS,implement,query - based summarization model,baselines,/content/training-data/natural_language_inference/29/triples/baselines.txt
Baselines,has,S2SA,baselines,/content/training-data/natural_language_inference/29/triples/baselines.txt
S2SA,name,Sequence - to - sequence framework,baselines,/content/training-data/natural_language_inference/29/triples/baselines.txt
S2SA,proposed for,language generation task,baselines,/content/training-data/natural_language_inference/29/triples/baselines.txt
Baselines,has,S2SAR,baselines,/content/training-data/natural_language_inference/29/triples/baselines.txt
S2SAR,implement,simple method,baselines,/content/training-data/natural_language_inference/29/triples/baselines.txt
simple method,incorporate,review information,baselines,/content/training-data/natural_language_inference/29/triples/baselines.txt
review information,when generating,answer,baselines,/content/training-data/natural_language_inference/29/triples/baselines.txt
Baselines,has,SNet,baselines,/content/training-data/natural_language_inference/29/triples/baselines.txt
SNet,is,two - stage state - of - the - art model,baselines,/content/training-data/natural_language_inference/29/triples/baselines.txt
two - stage state - of - the - art model,synthesis,answer,baselines,/content/training-data/natural_language_inference/29/triples/baselines.txt
answer,from,spans,baselines,/content/training-data/natural_language_inference/29/triples/baselines.txt
two - stage state - of - the - art model,which extracts,some text spans,baselines,/content/training-data/natural_language_inference/29/triples/baselines.txt
some text spans,from,multiple documents context,baselines,/content/training-data/natural_language_inference/29/triples/baselines.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/29/triples/ablation-analysis.txt
Ablation analysis,find that,PAAG,ablation-analysis,/content/training-data/natural_language_inference/29/triples/ablation-analysis.txt
PAAG,has,outperforms,ablation-analysis,/content/training-data/natural_language_inference/29/triples/ablation-analysis.txt
outperforms,has,RAGFWD,ablation-analysis,/content/training-data/natural_language_inference/29/triples/ablation-analysis.txt
RAGFWD,has,4.1 %,ablation-analysis,/content/training-data/natural_language_inference/29/triples/ablation-analysis.txt
4.1 %,in terms of,BLEU,ablation-analysis,/content/training-data/natural_language_inference/29/triples/ablation-analysis.txt
Ablation analysis,find that,RAGFWD,ablation-analysis,/content/training-data/natural_language_inference/29/triples/ablation-analysis.txt
RAGFWD,achieves,4.3 % improvement,ablation-analysis,/content/training-data/natural_language_inference/29/triples/ablation-analysis.txt
4.3 % improvement,over,RAGFD,ablation-analysis,/content/training-data/natural_language_inference/29/triples/ablation-analysis.txt
4.3 % improvement,in terms of,BLEU,ablation-analysis,/content/training-data/natural_language_inference/29/triples/ablation-analysis.txt
Ablation analysis,has,slight increment,ablation-analysis,/content/training-data/natural_language_inference/29/triples/ablation-analysis.txt
slight increment,demonstrates,effectiveness,ablation-analysis,/content/training-data/natural_language_inference/29/triples/ablation-analysis.txt
effectiveness,of,discriminator,ablation-analysis,/content/training-data/natural_language_inference/29/triples/ablation-analysis.txt
slight increment,from,RAGF,ablation-analysis,/content/training-data/natural_language_inference/29/triples/ablation-analysis.txt
RAGF,to,RAGFD,ablation-analysis,/content/training-data/natural_language_inference/29/triples/ablation-analysis.txt
Ablation analysis,conclude that,performance,ablation-analysis,/content/training-data/natural_language_inference/29/triples/ablation-analysis.txt
performance,of,PAAG,ablation-analysis,/content/training-data/natural_language_inference/29/triples/ablation-analysis.txt
PAAG,benefits from,Wasserstein distance based adversarial learning,ablation-analysis,/content/training-data/natural_language_inference/29/triples/ablation-analysis.txt
Wasserstein distance based adversarial learning,with,gradient penalty,ablation-analysis,/content/training-data/natural_language_inference/29/triples/ablation-analysis.txt
Wasserstein distance based adversarial learning,can help,our model,ablation-analysis,/content/training-data/natural_language_inference/29/triples/ablation-analysis.txt
our model,to achieve,better performance,ablation-analysis,/content/training-data/natural_language_inference/29/triples/ablation-analysis.txt
better performance,than,model,ablation-analysis,/content/training-data/natural_language_inference/29/triples/ablation-analysis.txt
model,using,vanilla GAN architecture,ablation-analysis,/content/training-data/natural_language_inference/29/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/29/triples/model.txt
Model,employ,attention mechanism,model,/content/training-data/natural_language_inference/29/triples/model.txt
attention mechanism,to model,interactions,model,/content/training-data/natural_language_inference/29/triples/model.txt
interactions,between,question and reviews,model,/content/training-data/natural_language_inference/29/triples/model.txt
Model,employ,key - value memory network,model,/content/training-data/natural_language_inference/29/triples/model.txt
key - value memory network,extract,relevance values,model,/content/training-data/natural_language_inference/29/triples/model.txt
relevance values,according to,question,model,/content/training-data/natural_language_inference/29/triples/model.txt
key - value memory network,to store,product attributes,model,/content/training-data/natural_language_inference/29/triples/model.txt
Model,to tackle,meaningless answers,model,/content/training-data/natural_language_inference/29/triples/model.txt
meaningless answers,propose,adversarial learning mechanism,model,/content/training-data/natural_language_inference/29/triples/model.txt
adversarial learning mechanism,for optimizing,parameters,model,/content/training-data/natural_language_inference/29/triples/model.txt
adversarial learning mechanism,in,loss calculation,model,/content/training-data/natural_language_inference/29/triples/model.txt
Model,propose,product - aware answer generator ( PAAG ,model,/content/training-data/natural_language_inference/29/triples/model.txt
product - aware answer generator ( PAAG ),has,product related question answering model,model,/content/training-data/natural_language_inference/29/triples/model.txt
product related question answering model,incorporates,customer reviews,model,/content/training-data/natural_language_inference/29/triples/model.txt
customer reviews,with,product attributes,model,/content/training-data/natural_language_inference/29/triples/model.txt
Model,propose,recurrent neural network ( RNN ) based decoder,model,/content/training-data/natural_language_inference/29/triples/model.txt
recurrent neural network ( RNN ) based decoder,combines,product - aware review representation and attributes,model,/content/training-data/natural_language_inference/29/triples/model.txt
product - aware review representation and attributes,to generate,answer,model,/content/training-data/natural_language_inference/29/triples/model.txt
Contribution,has research problem,Product - Aware Answer Generation,research-problem,/content/training-data/natural_language_inference/29/triples/research-problem.txt
Contribution,has research problem,question - answering ( QA ,research-problem,/content/training-data/natural_language_inference/29/triples/research-problem.txt
Contribution,has research problem,reading comprehension,research-problem,/content/training-data/natural_language_inference/29/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/29/triples/experimental-setup.txt
Experimental setup,To produce,better answers,experimental-setup,/content/training-data/natural_language_inference/29/triples/experimental-setup.txt
better answers,use,beam search,experimental-setup,/content/training-data/natural_language_inference/29/triples/experimental-setup.txt
beam search,with,beam size,experimental-setup,/content/training-data/natural_language_inference/29/triples/experimental-setup.txt
beam size,has,4,experimental-setup,/content/training-data/natural_language_inference/29/triples/experimental-setup.txt
Experimental setup,implement,our model,experimental-setup,/content/training-data/natural_language_inference/29/triples/experimental-setup.txt
our model,using,TensorFlow framework,experimental-setup,/content/training-data/natural_language_inference/29/triples/experimental-setup.txt
Experimental setup,has,randomly initialize,experimental-setup,/content/training-data/natural_language_inference/29/triples/experimental-setup.txt
randomly initialize,has,network parameters,experimental-setup,/content/training-data/natural_language_inference/29/triples/experimental-setup.txt
network parameters,at,beginning,experimental-setup,/content/training-data/natural_language_inference/29/triples/experimental-setup.txt
beginning,of,experiments,experimental-setup,/content/training-data/natural_language_inference/29/triples/experimental-setup.txt
network parameters,Without using,pre-trained embeddings,experimental-setup,/content/training-data/natural_language_inference/29/triples/experimental-setup.txt
Experimental setup,has,All the RNN networks,experimental-setup,/content/training-data/natural_language_inference/29/triples/experimental-setup.txt
All the RNN networks,have,512 hidden units,experimental-setup,/content/training-data/natural_language_inference/29/triples/experimental-setup.txt
Experimental setup,has,dimension,experimental-setup,/content/training-data/natural_language_inference/29/triples/experimental-setup.txt
dimension,of,word embedding,experimental-setup,/content/training-data/natural_language_inference/29/triples/experimental-setup.txt
word embedding,is,256,experimental-setup,/content/training-data/natural_language_inference/29/triples/experimental-setup.txt
Experimental setup,has,Adagrad,experimental-setup,/content/training-data/natural_language_inference/29/triples/experimental-setup.txt
Adagrad,with,learning rate,experimental-setup,/content/training-data/natural_language_inference/29/triples/experimental-setup.txt
learning rate,has,0.1,experimental-setup,/content/training-data/natural_language_inference/29/triples/experimental-setup.txt
0.1,to optimize,parameters,experimental-setup,/content/training-data/natural_language_inference/29/triples/experimental-setup.txt
Adagrad,with,batch size,experimental-setup,/content/training-data/natural_language_inference/29/triples/experimental-setup.txt
batch size,is,64,experimental-setup,/content/training-data/natural_language_inference/29/triples/experimental-setup.txt
Experimental setup,train,our model and all baseline models,experimental-setup,/content/training-data/natural_language_inference/29/triples/experimental-setup.txt
our model and all baseline models,on,NVIDIA Tesla P40 GPU,experimental-setup,/content/training-data/natural_language_inference/29/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/29/triples/results.txt
Results,find,noticeable gap,results,/content/training-data/natural_language_inference/29/triples/results.txt
noticeable gap,between,S2SAR and PAAG,results,/content/training-data/natural_language_inference/29/triples/results.txt
Results,see that,PAAG,results,/content/training-data/natural_language_inference/29/triples/results.txt
PAAG,over,stateof - the - art baseline SNet,results,/content/training-data/natural_language_inference/29/triples/results.txt
stateof - the - art baseline SNet,achieves,111 %,results,/content/training-data/natural_language_inference/29/triples/results.txt
stateof - the - art baseline SNet,achieves,8 %,results,/content/training-data/natural_language_inference/29/triples/results.txt
stateof - the - art baseline SNet,achieves,62.73 % increment,results,/content/training-data/natural_language_inference/29/triples/results.txt
stateof - the - art baseline SNet,in terms of,BLEU,results,/content/training-data/natural_language_inference/29/triples/results.txt
stateof - the - art baseline SNet,in terms of,embedding greedy,results,/content/training-data/natural_language_inference/29/triples/results.txt
stateof - the - art baseline SNet,in terms of,consistency score,results,/content/training-data/natural_language_inference/29/triples/results.txt
PAAG,has,outperforms,results,/content/training-data/natural_language_inference/29/triples/results.txt
outperforms,has,other baseline models,results,/content/training-data/natural_language_inference/29/triples/results.txt
other baseline models,in,both sentence fluency and consistency,results,/content/training-data/natural_language_inference/29/triples/results.txt
both sentence fluency and consistency,with,facts,results,/content/training-data/natural_language_inference/29/triples/results.txt
outperforms,has,all the baseline,results,/content/training-data/natural_language_inference/29/triples/results.txt
all the baseline,in,semantic distance,results,/content/training-data/natural_language_inference/29/triples/results.txt
semantic distance,with respect to,ground truth,results,/content/training-data/natural_language_inference/29/triples/results.txt
all the baseline,has,significantly,results,/content/training-data/natural_language_inference/29/triples/results.txt
Results,has,small increment,results,/content/training-data/natural_language_inference/29/triples/results.txt
small increment,of,S2 SAR,results,/content/training-data/natural_language_inference/29/triples/results.txt
S2 SAR,with respect to,S2SA,results,/content/training-data/natural_language_inference/29/triples/results.txt
S2SA,in,all metrics,results,/content/training-data/natural_language_inference/29/triples/results.txt
Contribution,has,Approach,approach,/content/training-data/natural_language_inference/59/triples/approach.txt
Approach,examine,simple model family,approach,/content/training-data/natural_language_inference/59/triples/approach.txt
simple model family,has,decomposable attention model,approach,/content/training-data/natural_language_inference/59/triples/approach.txt
decomposable attention model,shown promise in modeling,natural language inference,approach,/content/training-data/natural_language_inference/59/triples/approach.txt
Approach,to mitigate,data sparsity,approach,/content/training-data/natural_language_inference/59/triples/approach.txt
data sparsity,modify,input representation,approach,/content/training-data/natural_language_inference/59/triples/approach.txt
input representation,of,decomposable attention model,approach,/content/training-data/natural_language_inference/59/triples/approach.txt
input representation,to use,sums,approach,/content/training-data/natural_language_inference/59/triples/approach.txt
sums,of,character n-gram embeddings,approach,/content/training-data/natural_language_inference/59/triples/approach.txt
character n-gram embeddings,instead of,word embeddings,approach,/content/training-data/natural_language_inference/59/triples/approach.txt
Approach,pretrain,all our model parameters,approach,/content/training-data/natural_language_inference/59/triples/approach.txt
all our model parameters,on,"noisy , automatically collected question - paraphrase corpus",approach,/content/training-data/natural_language_inference/59/triples/approach.txt
"noisy , automatically collected question - paraphrase corpus",followed by,fine - tuning,approach,/content/training-data/natural_language_inference/59/triples/approach.txt
fine - tuning,has,parameters,approach,/content/training-data/natural_language_inference/59/triples/approach.txt
parameters,on,Quora dataset,approach,/content/training-data/natural_language_inference/59/triples/approach.txt
"noisy , automatically collected question - paraphrase corpus",name,Paralex,approach,/content/training-data/natural_language_inference/59/triples/approach.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/natural_language_inference/59/triples/hyperparameters.txt
Hyperparameters,by,grid search,hyperparameters,/content/training-data/natural_language_inference/59/triples/hyperparameters.txt
grid search,has,shape of all feedforward networks,hyperparameters,/content/training-data/natural_language_inference/59/triples/hyperparameters.txt
shape of all feedforward networks,has,two layers,hyperparameters,/content/training-data/natural_language_inference/59/triples/hyperparameters.txt
two layers,with,400 and 200 width,hyperparameters,/content/training-data/natural_language_inference/59/triples/hyperparameters.txt
grid search,has,prediction threshold,hyperparameters,/content/training-data/natural_language_inference/59/triples/hyperparameters.txt
prediction threshold,has,positive paraphrase,hyperparameters,/content/training-data/natural_language_inference/59/triples/hyperparameters.txt
positive paraphrase,for,score ? 0.3,hyperparameters,/content/training-data/natural_language_inference/59/triples/hyperparameters.txt
grid search,has,character n -gram sizes,hyperparameters,/content/training-data/natural_language_inference/59/triples/hyperparameters.txt
character n -gram sizes,has,5,hyperparameters,/content/training-data/natural_language_inference/59/triples/hyperparameters.txt
grid search,has,learning rate,hyperparameters,/content/training-data/natural_language_inference/59/triples/hyperparameters.txt
learning rate,has,0.1,hyperparameters,/content/training-data/natural_language_inference/59/triples/hyperparameters.txt
0.1,for,pretraining and tuning,hyperparameters,/content/training-data/natural_language_inference/59/triples/hyperparameters.txt
grid search,has,dropout ratio,hyperparameters,/content/training-data/natural_language_inference/59/triples/hyperparameters.txt
dropout ratio,has,0.1,hyperparameters,/content/training-data/natural_language_inference/59/triples/hyperparameters.txt
0.1,for,tuning,hyperparameters,/content/training-data/natural_language_inference/59/triples/hyperparameters.txt
grid search,has,context size,hyperparameters,/content/training-data/natural_language_inference/59/triples/hyperparameters.txt
context size,has,1,hyperparameters,/content/training-data/natural_language_inference/59/triples/hyperparameters.txt
grid search,has,batch size,hyperparameters,/content/training-data/natural_language_inference/59/triples/hyperparameters.txt
batch size,has,256,hyperparameters,/content/training-data/natural_language_inference/59/triples/hyperparameters.txt
256,for,pretraining,hyperparameters,/content/training-data/natural_language_inference/59/triples/hyperparameters.txt
batch size,has,64,hyperparameters,/content/training-data/natural_language_inference/59/triples/hyperparameters.txt
64,for,tuning,hyperparameters,/content/training-data/natural_language_inference/59/triples/hyperparameters.txt
grid search,has,embedding dimension,hyperparameters,/content/training-data/natural_language_inference/59/triples/hyperparameters.txt
embedding dimension,has,300,hyperparameters,/content/training-data/natural_language_inference/59/triples/hyperparameters.txt
grid search,on,development set,hyperparameters,/content/training-data/natural_language_inference/59/triples/hyperparameters.txt
Contribution,has research problem,Neural Paraphrase Identification of Questions,research-problem,/content/training-data/natural_language_inference/59/triples/research-problem.txt
Contribution,has research problem,paraphrase identification of questions,research-problem,/content/training-data/natural_language_inference/59/triples/research-problem.txt
Contribution,has research problem,Question paraphrase identification,research-problem,/content/training-data/natural_language_inference/59/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/59/triples/results.txt
Results,note,our best performing model,results,/content/training-data/natural_language_inference/59/triples/results.txt
our best performing model,is,pt - DECATT char,results,/content/training-data/natural_language_inference/59/triples/results.txt
pt - DECATT char,leverages,full power,results,/content/training-data/natural_language_inference/59/triples/results.txt
full power,of,character embeddings,results,/content/training-data/natural_language_inference/59/triples/results.txt
Results,has,Our basic decomposable attention model DECATT word,results,/content/training-data/natural_language_inference/59/triples/results.txt
Our basic decomposable attention model DECATT word,is,better,results,/content/training-data/natural_language_inference/59/triples/results.txt
better,than,most of the models,results,/content/training-data/natural_language_inference/59/triples/results.txt
most of the models,used,GloVe embeddings,results,/content/training-data/natural_language_inference/59/triples/results.txt
Our basic decomposable attention model DECATT word,without,pre-trained embeddings,results,/content/training-data/natural_language_inference/59/triples/results.txt
Results,has,DECATT char model,results,/content/training-data/natural_language_inference/59/triples/results.txt
DECATT char model,without,any pretrained embeddings,results,/content/training-data/natural_language_inference/59/triples/results.txt
DECATT char model,outperforms,DE - CATT glove,results,/content/training-data/natural_language_inference/59/triples/results.txt
DE - CATT glove,that uses,task - agnostic GloVe embeddings,results,/content/training-data/natural_language_inference/59/triples/results.txt
Results,when,character n-gram embeddings,results,/content/training-data/natural_language_inference/59/triples/results.txt
character n-gram embeddings,are,pre-trained,results,/content/training-data/natural_language_inference/59/triples/results.txt
pre-trained,in,task - specific manner,results,/content/training-data/natural_language_inference/59/triples/results.txt
task - specific manner,in,DECATT paralex ? char model,results,/content/training-data/natural_language_inference/59/triples/results.txt
task - specific manner,observe,significant boost,results,/content/training-data/natural_language_inference/59/triples/results.txt
significant boost,in,performance,results,/content/training-data/natural_language_inference/59/triples/results.txt
Results,observe,simple FFNN baselines,results,/content/training-data/natural_language_inference/59/triples/results.txt
simple FFNN baselines,has,work better,results,/content/training-data/natural_language_inference/59/triples/results.txt
work better,than,more complex Siamese and Multi - Perspective CNN or LSTM models,results,/content/training-data/natural_language_inference/59/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/81/triples/ablation-analysis.txt
Ablation analysis,result in,less performance degradation,ablation-analysis,/content/training-data/natural_language_inference/81/triples/ablation-analysis.txt
less performance degradation,Replacing,bidirectional GRUs,ablation-analysis,/content/training-data/natural_language_inference/81/triples/ablation-analysis.txt
bidirectional GRUs,with,unidirectional GRUs,ablation-analysis,/content/training-data/natural_language_inference/81/triples/ablation-analysis.txt
less performance degradation,Replacing,selfattention layers,ablation-analysis,/content/training-data/natural_language_inference/81/triples/ablation-analysis.txt
selfattention layers,with,mean - pooling,ablation-analysis,/content/training-data/natural_language_inference/81/triples/ablation-analysis.txt
Ablation analysis,has,Both the coarse - grain module and the fine - grain module,ablation-analysis,/content/training-data/natural_language_inference/81/triples/ablation-analysis.txt
Both the coarse - grain module and the fine - grain module,contribute to,model performance,ablation-analysis,/content/training-data/natural_language_inference/81/triples/ablation-analysis.txt
Ablation analysis,has,fine - grain - only model,ablation-analysis,/content/training-data/natural_language_inference/81/triples/ablation-analysis.txt
fine - grain - only model,has,under-performs,ablation-analysis,/content/training-data/natural_language_inference/81/triples/ablation-analysis.txt
under-performs,has,coarse - grain - only model,ablation-analysis,/content/training-data/natural_language_inference/81/triples/ablation-analysis.txt
under-performs,has,consistently,ablation-analysis,/content/training-data/natural_language_inference/81/triples/ablation-analysis.txt
consistently,across,almost all length measures,ablation-analysis,/content/training-data/natural_language_inference/81/triples/ablation-analysis.txt
fine - grain - only model,has,matches or outperforms,ablation-analysis,/content/training-data/natural_language_inference/81/triples/ablation-analysis.txt
matches or outperforms,has,coarse - grain - only model,ablation-analysis,/content/training-data/natural_language_inference/81/triples/ablation-analysis.txt
coarse - grain - only model,on,examples,ablation-analysis,/content/training-data/natural_language_inference/81/triples/ablation-analysis.txt
examples,with,large number,ablation-analysis,/content/training-data/natural_language_inference/81/triples/ablation-analysis.txt
large number,of,support documents,ablation-analysis,/content/training-data/natural_language_inference/81/triples/ablation-analysis.txt
examples,with,long support documents,ablation-analysis,/content/training-data/natural_language_inference/81/triples/ablation-analysis.txt
Ablation analysis,Replacing,encoder,ablation-analysis,/content/training-data/natural_language_inference/81/triples/ablation-analysis.txt
encoder,with,projection,ablation-analysis,/content/training-data/natural_language_inference/81/triples/ablation-analysis.txt
projection,over,word embeddings,ablation-analysis,/content/training-data/natural_language_inference/81/triples/ablation-analysis.txt
projection,result in,significant performance drop,ablation-analysis,/content/training-data/natural_language_inference/81/triples/ablation-analysis.txt
Contribution,has,Experiments,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
Experiments,has,Tasks,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
Tasks,has,MULTI - EVIDENCE QUESTION ANSWERING,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
MULTI - EVIDENCE QUESTION ANSWERING,has,Hyperparameters,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
Hyperparameters,split,symbolic query relations,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
symbolic query relations,into,words,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
Hyperparameters,use,fixed Glo Ve embeddings,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
fixed Glo Ve embeddings,as well as,character ngram embeddings,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
Hyperparameters,has,tokenize,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
tokenize,using,Stanford CoreNLP,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
Hyperparameters,trained using,ADAM,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
MULTI - EVIDENCE QUESTION ANSWERING,has,Results,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
Results,has,CFC,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
CFC,achieves,state - of - the - art results,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
state - of - the - art results,on,masked and unmasked versions,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
masked and unmasked versions,of,WikiHop,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
Results,on,"blind , held - out WikiHop test set",experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
"blind , held - out WikiHop test set",has,CFC,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
CFC,achieves,new best accuracy,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
new best accuracy,of,70.6 %,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
MULTI - EVIDENCE QUESTION ANSWERING,ON,WIKIHOP,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
Tasks,has,RERANKING EXTRACTIVE QUESTION ANSWERING,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
RERANKING EXTRACTIVE QUESTION ANSWERING,has,Our experimental results,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
Our experimental results,show,reranking,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
reranking,using,CFC,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
reranking,provides,consistent performance gains,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
consistent performance gains,over only using,span extraction question answering model,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
RERANKING EXTRACTIVE QUESTION ANSWERING,has,reranking,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
reranking,using,CFC,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
reranking,has,improves,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
improves,has,performance,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
RERANKING EXTRACTIVE QUESTION ANSWERING,ON,TRIVIAQA,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
RERANKING EXTRACTIVE QUESTION ANSWERING,On,whole Trivia QA dev set,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
whole Trivia QA dev set,has,reranking,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
reranking,using,CFC,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
reranking,results in,again,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
again,of,3.1 % EM and 3.0 % F1,experiments,/content/training-data/natural_language_inference/81/triples/experiments.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/81/triples/model.txt
Model,In,fine - grain reasoning,model,/content/training-data/natural_language_inference/81/triples/model.txt
fine - grain reasoning,has,model,model,/content/training-data/natural_language_inference/81/triples/model.txt
model,matches,specific finegrain contexts,model,/content/training-data/natural_language_inference/81/triples/model.txt
specific finegrain contexts,to gauge,relevance,model,/content/training-data/natural_language_inference/81/triples/model.txt
relevance,of,candidate,model,/content/training-data/natural_language_inference/81/triples/model.txt
Model,In,coarse - grain reasoning,model,/content/training-data/natural_language_inference/81/triples/model.txt
coarse - grain reasoning,has,model,model,/content/training-data/natural_language_inference/81/triples/model.txt
model,builds,coarse summary,model,/content/training-data/natural_language_inference/81/triples/model.txt
coarse summary,of,support documents,model,/content/training-data/natural_language_inference/81/triples/model.txt
coarse summary,conditioned on,query,model,/content/training-data/natural_language_inference/81/triples/model.txt
Model,has,CFC,model,/content/training-data/natural_language_inference/81/triples/model.txt
CFC,inspired by,coarse - grain reasoning,model,/content/training-data/natural_language_inference/81/triples/model.txt
CFC,inspired by,fine - grain reasoning,model,/content/training-data/natural_language_inference/81/triples/model.txt
Model,has,Each module,model,/content/training-data/natural_language_inference/81/triples/model.txt
Each module,employs,novel hierarchical attention,model,/content/training-data/natural_language_inference/81/triples/model.txt
novel hierarchical attention,has,hierarchy,model,/content/training-data/natural_language_inference/81/triples/model.txt
hierarchy,to combine,information,model,/content/training-data/natural_language_inference/81/triples/model.txt
information,conditioned on,query,model,/content/training-data/natural_language_inference/81/triples/model.txt
information,conditioned on,candidates,model,/content/training-data/natural_language_inference/81/triples/model.txt
information,from,support documents,model,/content/training-data/natural_language_inference/81/triples/model.txt
hierarchy,of,coattention,model,/content/training-data/natural_language_inference/81/triples/model.txt
hierarchy,of,self - attention,model,/content/training-data/natural_language_inference/81/triples/model.txt
Model,has,Our multi-evidence QA model,model,/content/training-data/natural_language_inference/81/triples/model.txt
Our multi-evidence QA model,name,Coarse - grain Fine - grain Coattention Network ( CFC ,model,/content/training-data/natural_language_inference/81/triples/model.txt
Our multi-evidence QA model,selects among,set,model,/content/training-data/natural_language_inference/81/triples/model.txt
set,given,set,model,/content/training-data/natural_language_inference/81/triples/model.txt
set,of,support documents,model,/content/training-data/natural_language_inference/81/triples/model.txt
set,of,query,model,/content/training-data/natural_language_inference/81/triples/model.txt
set,of,candidate answers,model,/content/training-data/natural_language_inference/81/triples/model.txt
Contribution,has research problem,MULTI - EVIDENCE QUESTION ANSWERING,research-problem,/content/training-data/natural_language_inference/81/triples/research-problem.txt
Contribution,has research problem,question answering,research-problem,/content/training-data/natural_language_inference/81/triples/research-problem.txt
Contribution,has research problem,question answering ( QA ,research-problem,/content/training-data/natural_language_inference/81/triples/research-problem.txt
Contribution,has research problem,neural question answering,research-problem,/content/training-data/natural_language_inference/81/triples/research-problem.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/7/triples/ablation-analysis.txt
Ablation analysis,has,passage - aligned question representation,ablation-analysis,/content/training-data/natural_language_inference/7/triples/ablation-analysis.txt
passage - aligned question representation,is,crucial,ablation-analysis,/content/training-data/natural_language_inference/7/triples/ablation-analysis.txt
Ablation analysis,observe,general improvements,ablation-analysis,/content/training-data/natural_language_inference/7/triples/ablation-analysis.txt
general improvements,when using,labels,ablation-analysis,/content/training-data/natural_language_inference/7/triples/ablation-analysis.txt
labels,that,closely align,ablation-analysis,/content/training-data/natural_language_inference/7/triples/ablation-analysis.txt
closely align,with,task,ablation-analysis,/content/training-data/natural_language_inference/7/triples/ablation-analysis.txt
Ablation analysis,observe,interactions between the endpoints,ablation-analysis,/content/training-data/natural_language_inference/7/triples/ablation-analysis.txt
interactions between the endpoints,using,spanlevel FFNN,ablation-analysis,/content/training-data/natural_language_inference/7/triples/ablation-analysis.txt
interactions between the endpoints,has,RASOR,ablation-analysis,/content/training-data/natural_language_inference/7/triples/ablation-analysis.txt
RASOR,outperforms,endpoint prediction model,ablation-analysis,/content/training-data/natural_language_inference/7/triples/ablation-analysis.txt
endpoint prediction model,by,1.1,ablation-analysis,/content/training-data/natural_language_inference/7/triples/ablation-analysis.txt
1.1,in,exact match,ablation-analysis,/content/training-data/natural_language_inference/7/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/7/triples/model.txt
Model,demonstrate,training,model,/content/training-data/natural_language_inference/7/triples/model.txt
training,with,global normalization,model,/content/training-data/natural_language_inference/7/triples/model.txt
global normalization,over,all possible spans,model,/content/training-data/natural_language_inference/7/triples/model.txt
global normalization,leads to,significant increase,model,/content/training-data/natural_language_inference/7/triples/model.txt
significant increase,in,performance,model,/content/training-data/natural_language_inference/7/triples/model.txt
Model,demonstrate,directly classifying,model,/content/training-data/natural_language_inference/7/triples/model.txt
directly classifying,each of,competing spans,model,/content/training-data/natural_language_inference/7/triples/model.txt
Model,present,novel neural architecture,model,/content/training-data/natural_language_inference/7/triples/model.txt
novel neural architecture,called,RASOR,model,/content/training-data/natural_language_inference/7/triples/model.txt
novel neural architecture,builds,fixed - length span representations,model,/content/training-data/natural_language_inference/7/triples/model.txt
fixed - length span representations,reusing,recurrent computations,model,/content/training-data/natural_language_inference/7/triples/model.txt
recurrent computations,for,shared substructures,model,/content/training-data/natural_language_inference/7/triples/model.txt
Contribution,has research problem,EXTRACTIVE QUESTION ANSWERING,research-problem,/content/training-data/natural_language_inference/7/triples/research-problem.txt
Contribution,has research problem,answer extraction,research-problem,/content/training-data/natural_language_inference/7/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
Experimental setup,couple,input and forget gates,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
input and forget gates,in,our LSTMs,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
Experimental setup,use,single dropout mask,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
single dropout mask,to apply,dropout,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
dropout,across,all LSTM time - steps,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
Experimental setup,To choose,final model configuration,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
final model configuration,ran,grid searches,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
grid searches,over,dimensionality,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
dimensionality,of,LSTM hidden states,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
grid searches,over,width and depth,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
width and depth,of,feed forward neural networks,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
grid searches,over,dropout,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
dropout,for,LSTMs,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
grid searches,over,number,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
number,of,stacked LSTM layers,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
grid searches,over,"decay multiplier [ 0.9 , 0.95 , 1.0 ]",experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
"decay multiplier [ 0.9 , 0.95 , 1.0 ]",multiply,learning rate every 10 k steps,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
Experimental setup,has,two - layer BiLSTMs,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
two - layer BiLSTMs,for,span encoder,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
two - layer BiLSTMs,for,passage - independent question representation,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
Experimental setup,has,models,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
models,implemented using,TensorFlow,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
models,trained on,SQUAD training set,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
SQUAD training set,using,ADAM optimizer,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
ADAM optimizer,with,mini-batch size,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
mini-batch size,of,4,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
models,trained using,10 asynchronous training threads,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
10 asynchronous training threads,on,single machine,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
Experimental setup,has,learning rate decay,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
learning rate decay,of,5 % every 10 k steps,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
Experimental setup,has,Hidden layers,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
Hidden layers,in,feed forward neural networks,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
Hidden layers,use,rectified linear units,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
Experimental setup,has,dropout,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
dropout,of,0.1,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
Experimental setup,has,best model,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
best model,uses,50d LSTM states,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
Experimental setup,has,Answer candidates,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
Answer candidates,limited to,spans,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
spans,with,at most 30 words,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
Experimental setup,represent,words,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
words,using,300 dimensional GloVe embeddings,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
300 dimensional GloVe embeddings,cover,200 k words,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
300 dimensional GloVe embeddings,trained on,corpus,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
corpus,of,840 bn words,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
300 dimensional GloVe embeddings,has,all out of vocabulary ( OOV ) words,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
all out of vocabulary ( OOV ) words,projected onto,one,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
one,of,1 m randomly initialized 300d embeddings,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
words,in,question and document,experimental-setup,/content/training-data/natural_language_inference/7/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/7/triples/results.txt
Results,has,RASOR,results,/content/training-data/natural_language_inference/7/triples/results.txt
RASOR,achieves,error reduction,results,/content/training-data/natural_language_inference/7/triples/results.txt
error reduction,of,more than 50 %,results,/content/training-data/natural_language_inference/7/triples/results.txt
more than 50 %,relative to,human performance upper bound,results,/content/training-data/natural_language_inference/7/triples/results.txt
more than 50 %,in terms of,exact match and F1,results,/content/training-data/natural_language_inference/7/triples/results.txt
RASOR,model,efficiently and explicitly,results,/content/training-data/natural_language_inference/7/triples/results.txt
efficiently and explicitly,has,quadratic number of possible answers,results,/content/training-data/natural_language_inference/7/triples/results.txt
quadratic number of possible answers,leads to,14 % error reduction,results,/content/training-data/natural_language_inference/7/triples/results.txt
14 % error reduction,over,best performing Match - LSTM model,results,/content/training-data/natural_language_inference/7/triples/results.txt
Contribution,has,Dataset,datase,/content/training-data/natural_language_inference/1/triples/dataset.txt
Dataset,contains,more than 100 k questions,datase,/content/training-data/natural_language_inference/1/triples/dataset.txt
Dataset,collected,first large - scale dataset of questions and answers,datase,/content/training-data/natural_language_inference/1/triples/dataset.txt
first large - scale dataset of questions and answers,called,SimpleQuestions,datase,/content/training-data/natural_language_inference/1/triples/dataset.txt
first large - scale dataset of questions and answers,based on,KB,datase,/content/training-data/natural_language_inference/1/triples/dataset.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/natural_language_inference/1/triples/hyperparameters.txt
Hyperparameters,has,margin,hyperparameters,/content/training-data/natural_language_inference/1/triples/hyperparameters.txt
margin,set to,0.1,hyperparameters,/content/training-data/natural_language_inference/1/triples/hyperparameters.txt
Hyperparameters,has,embedding dimension and the learning rate,hyperparameters,/content/training-data/natural_language_inference/1/triples/hyperparameters.txt
embedding dimension and the learning rate,chosen among,"{ 64 , 128 , 256 } and { 1 , 0.1 , ... , 1.0e ? 4 }",hyperparameters,/content/training-data/natural_language_inference/1/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/1/triples/model.txt
Model,setting of,simple QA,model,/content/training-data/natural_language_inference/1/triples/model.txt
simple QA,corresponds to,elementary operation,model,/content/training-data/natural_language_inference/1/triples/model.txt
elementary operation,of performing,single lookup,model,/content/training-data/natural_language_inference/1/triples/model.txt
single lookup,in,memory,model,/content/training-data/natural_language_inference/1/triples/model.txt
Model,has,Memory Networks,model,/content/training-data/natural_language_inference/1/triples/model.txt
Memory Networks,are,learning systems,model,/content/training-data/natural_language_inference/1/triples/model.txt
learning systems,focus on cases where,relationship,model,/content/training-data/natural_language_inference/1/triples/model.txt
relationship,performed by,embedding,model,/content/training-data/natural_language_inference/1/triples/model.txt
embedding,in,same vector space,model,/content/training-data/natural_language_inference/1/triples/model.txt
embedding,between,input and response languages ( here natural language ,model,/content/training-data/natural_language_inference/1/triples/model.txt
embedding,between,"storage language ( here , the facts from KBs ",model,/content/training-data/natural_language_inference/1/triples/model.txt
learning systems,centered around,memory component,model,/content/training-data/natural_language_inference/1/triples/model.txt
memory component,can be,read and written to,model,/content/training-data/natural_language_inference/1/triples/model.txt
Model,present,embedding - based QA system,model,/content/training-data/natural_language_inference/1/triples/model.txt
embedding - based QA system,developed under,framework of Memory Networks ( Mem NNs ,model,/content/training-data/natural_language_inference/1/triples/model.txt
Contribution,has research problem,Large - scale Simple Question Answering,research-problem,/content/training-data/natural_language_inference/1/triples/research-problem.txt
Contribution,has research problem,large - scale question answering,research-problem,/content/training-data/natural_language_inference/1/triples/research-problem.txt
Contribution,has research problem,simple question answering,research-problem,/content/training-data/natural_language_inference/1/triples/research-problem.txt
Contribution,has research problem,Simple Question Answering,research-problem,/content/training-data/natural_language_inference/1/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/1/triples/results.txt
Results,Using,bigger FB5M as KB,results,/content/training-data/natural_language_inference/1/triples/results.txt
bigger FB5M as KB,not change,performance,results,/content/training-data/natural_language_inference/1/triples/results.txt
performance,on,SimpleQuestions,results,/content/training-data/natural_language_inference/1/triples/results.txt
Results,has,Importance of data sources,results,/content/training-data/natural_language_inference/1/triples/results.txt
Importance of data sources,has,paraphrases,results,/content/training-data/natural_language_inference/1/triples/results.txt
paraphrases,not seem to,help much,results,/content/training-data/natural_language_inference/1/triples/results.txt
help much,except when,training,results,/content/training-data/natural_language_inference/1/triples/results.txt
training,only with,synthetic questions,results,/content/training-data/natural_language_inference/1/triples/results.txt
synthetic questions,have,dramatic impact,results,/content/training-data/natural_language_inference/1/triples/results.txt
dramatic impact,on,performance,results,/content/training-data/natural_language_inference/1/triples/results.txt
performance,on,Reverb,results,/content/training-data/natural_language_inference/1/triples/results.txt
help much,on,WebQuestions and SimpleQuestions,results,/content/training-data/natural_language_inference/1/triples/results.txt
Results,has,Transfer learning on Reverb,results,/content/training-data/natural_language_inference/1/triples/results.txt
Transfer learning on Reverb,training on,both datasets,results,/content/training-data/natural_language_inference/1/triples/results.txt
both datasets,improves,performance,results,/content/training-data/natural_language_inference/1/triples/results.txt
Transfer learning on Reverb,has,best results,results,/content/training-data/natural_language_inference/1/triples/results.txt
best results,are,67 % accuracy,results,/content/training-data/natural_language_inference/1/triples/results.txt
best results,are,68 %,results,/content/training-data/natural_language_inference/1/triples/results.txt
68 %,for,ensemble of 5 models,results,/content/training-data/natural_language_inference/1/triples/results.txt
Transfer learning on Reverb,notice,models,results,/content/training-data/natural_language_inference/1/triples/results.txt
models,trained on,single QA dataset,results,/content/training-data/natural_language_inference/1/triples/results.txt
single QA dataset,perform,poorly,results,/content/training-data/natural_language_inference/1/triples/results.txt
poorly,on,other datasets,results,/content/training-data/natural_language_inference/1/triples/results.txt
Results,On,WebQuestions,results,/content/training-data/natural_language_inference/1/triples/results.txt
WebQuestions,has,86 % of the questions,results,/content/training-data/natural_language_inference/1/triples/results.txt
86 % of the questions,has,performance,results,/content/training-data/natural_language_inference/1/triples/results.txt
performance,from,36.2 %,results,/content/training-data/natural_language_inference/1/triples/results.txt
36.2 %,to,41.0 % F1-score,results,/content/training-data/natural_language_inference/1/triples/results.txt
performance,has,increases significantly,results,/content/training-data/natural_language_inference/1/triples/results.txt
86 % of the questions,answered with,single supporting fact,results,/content/training-data/natural_language_inference/1/triples/results.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/23/triples/model.txt
Model,identify,attention scoring function,model,/content/training-data/natural_language_inference/23/triples/model.txt
attention scoring function,utilizing,all layers,model,/content/training-data/natural_language_inference/23/triples/model.txt
all layers,with,less training burden,model,/content/training-data/natural_language_inference/23/triples/model.txt
all layers,of,representation,model,/content/training-data/natural_language_inference/23/triples/model.txt
Model,leads to,attention,model,/content/training-data/natural_language_inference/23/triples/model.txt
attention,thoroughly captures,complete information,model,/content/training-data/natural_language_inference/23/triples/model.txt
complete information,between,question and the context,model,/content/training-data/natural_language_inference/23/triples/model.txt
Model,has,innovations,model,/content/training-data/natural_language_inference/23/triples/model.txt
innovations,integrated into,new end - to - end structure,model,/content/training-data/natural_language_inference/23/triples/model.txt
new end - to - end structure,called,FusionNet,model,/content/training-data/natural_language_inference/23/triples/model.txt
Model,With,fully - aware attention,model,/content/training-data/natural_language_inference/23/triples/model.txt
fully - aware attention,put forward,multi -level attention mechanism,model,/content/training-data/natural_language_inference/23/triples/model.txt
multi -level attention mechanism,to understand,information,model,/content/training-data/natural_language_inference/23/triples/model.txt
information,in,question,model,/content/training-data/natural_language_inference/23/triples/model.txt
information,exploit it,layer by layer,model,/content/training-data/natural_language_inference/23/triples/model.txt
layer by layer,on,context side,model,/content/training-data/natural_language_inference/23/triples/model.txt
Contribution,has research problem,MACHINE COMPREHENSION,research-problem,/content/training-data/natural_language_inference/23/triples/research-problem.txt
Contribution,has research problem,"Teaching machines to read , process and comprehend text and then answer questions",research-problem,/content/training-data/natural_language_inference/23/triples/research-problem.txt
Contribution,has research problem,machine reading comprehension ( MRC ,research-problem,/content/training-data/natural_language_inference/23/triples/research-problem.txt
Contribution,has research problem,language understanding,research-problem,/content/training-data/natural_language_inference/23/triples/research-problem.txt
Contribution,has research problem,MRC,research-problem,/content/training-data/natural_language_inference/23/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/23/triples/results.txt
Results,shows,FusionNet,results,/content/training-data/natural_language_inference/23/triples/results.txt
FusionNet,is,better,results,/content/training-data/natural_language_inference/23/triples/results.txt
better,at,language understanding,results,/content/training-data/natural_language_inference/23/triples/results.txt
language understanding,of both,context and question,results,/content/training-data/natural_language_inference/23/triples/results.txt
Results,see that,our models,results,/content/training-data/natural_language_inference/23/triples/results.txt
our models,not only perform,well,results,/content/training-data/natural_language_inference/23/triples/results.txt
well,on,original SQuAD dataset,results,/content/training-data/natural_language_inference/23/triples/results.txt
our models,also,outperform,results,/content/training-data/natural_language_inference/23/triples/results.txt
outperform,has,all previous models,results,/content/training-data/natural_language_inference/23/triples/results.txt
all previous models,by,more than 5 %,results,/content/training-data/natural_language_inference/23/triples/results.txt
more than 5 %,in,EM score,results,/content/training-data/natural_language_inference/23/triples/results.txt
more than 5 %,on,adversarial datasets,results,/content/training-data/natural_language_inference/23/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/80/triples/ablation-analysis.txt
Ablation analysis,demonstrates,best performance,ablation-analysis,/content/training-data/natural_language_inference/80/triples/ablation-analysis.txt
best performance,with,450 - dimensional BiLSTMs,ablation-analysis,/content/training-data/natural_language_inference/80/triples/ablation-analysis.txt
Ablation analysis,illustrate,importance,ablation-analysis,/content/training-data/natural_language_inference/80/triples/ablation-analysis.txt
importance,of,our proposed dependent reading strategy,ablation-analysis,/content/training-data/natural_language_inference/80/triples/ablation-analysis.txt
our proposed dependent reading strategy,leads to,significant improvement,ablation-analysis,/content/training-data/natural_language_inference/80/triples/ablation-analysis.txt
significant improvement,in,encoding stage,ablation-analysis,/content/training-data/natural_language_inference/80/triples/ablation-analysis.txt
Ablation analysis,see that,all modifications,ablation-analysis,/content/training-data/natural_language_inference/80/triples/ablation-analysis.txt
all modifications,lead to,new model,ablation-analysis,/content/training-data/natural_language_inference/80/triples/ablation-analysis.txt
Ablation analysis,Among,all components,ablation-analysis,/content/training-data/natural_language_inference/80/triples/ablation-analysis.txt
all components,has,three of them,ablation-analysis,/content/training-data/natural_language_inference/80/triples/ablation-analysis.txt
three of them,have,noticeable influences,ablation-analysis,/content/training-data/natural_language_inference/80/triples/ablation-analysis.txt
three of them,has,max pooling,ablation-analysis,/content/training-data/natural_language_inference/80/triples/ablation-analysis.txt
three of them,has,difference in the attention stage,ablation-analysis,/content/training-data/natural_language_inference/80/triples/ablation-analysis.txt
three of them,has,dependent reading,ablation-analysis,/content/training-data/natural_language_inference/80/triples/ablation-analysis.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/natural_language_inference/80/triples/hyperparameters.txt
Hyperparameters,use,fairly small batch size,hyperparameters,/content/training-data/natural_language_inference/80/triples/hyperparameters.txt
fairly small batch size,of,32,hyperparameters,/content/training-data/natural_language_inference/80/triples/hyperparameters.txt
Hyperparameters,use,pre-trained 300 - D Glove 840B vectors,hyperparameters,/content/training-data/natural_language_inference/80/triples/hyperparameters.txt
pre-trained 300 - D Glove 840B vectors,to initialize,our word embedding vectors,hyperparameters,/content/training-data/natural_language_inference/80/triples/hyperparameters.txt
Hyperparameters,has,All hidden states,hyperparameters,/content/training-data/natural_language_inference/80/triples/hyperparameters.txt
All hidden states,of,BiLSTMs,hyperparameters,/content/training-data/natural_language_inference/80/triples/hyperparameters.txt
BiLSTMs,during,input encoding and inference,hyperparameters,/content/training-data/natural_language_inference/80/triples/hyperparameters.txt
input encoding and inference,have,450 dimensions,hyperparameters,/content/training-data/natural_language_inference/80/triples/hyperparameters.txt
Hyperparameters,has,initial learning rate,hyperparameters,/content/training-data/natural_language_inference/80/triples/hyperparameters.txt
initial learning rate,is,0.0004,hyperparameters,/content/training-data/natural_language_inference/80/triples/hyperparameters.txt
Hyperparameters,has,weights,hyperparameters,/content/training-data/natural_language_inference/80/triples/hyperparameters.txt
weights,learned by,minimizing,hyperparameters,/content/training-data/natural_language_inference/80/triples/hyperparameters.txt
minimizing,has,log - loss,hyperparameters,/content/training-data/natural_language_inference/80/triples/hyperparameters.txt
log - loss,on,training data,hyperparameters,/content/training-data/natural_language_inference/80/triples/hyperparameters.txt
minimizing,via,Adam optimizer,hyperparameters,/content/training-data/natural_language_inference/80/triples/hyperparameters.txt
Hyperparameters,During,training,hyperparameters,/content/training-data/natural_language_inference/80/triples/hyperparameters.txt
training,has,word embeddings,hyperparameters,/content/training-data/natural_language_inference/80/triples/hyperparameters.txt
word embeddings,updated to learn,effective representations,hyperparameters,/content/training-data/natural_language_inference/80/triples/hyperparameters.txt
effective representations,for,NLI task,hyperparameters,/content/training-data/natural_language_inference/80/triples/hyperparameters.txt
Hyperparameters,To avoid,overfitting,hyperparameters,/content/training-data/natural_language_inference/80/triples/hyperparameters.txt
overfitting,use,dropout,hyperparameters,/content/training-data/natural_language_inference/80/triples/hyperparameters.txt
dropout,with,rate,hyperparameters,/content/training-data/natural_language_inference/80/triples/hyperparameters.txt
rate,of,0.4,hyperparameters,/content/training-data/natural_language_inference/80/triples/hyperparameters.txt
dropout,for,regularization,hyperparameters,/content/training-data/natural_language_inference/80/triples/hyperparameters.txt
dropout,applied to,all feedforward connections,hyperparameters,/content/training-data/natural_language_inference/80/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/80/triples/model.txt
Model,employs,soft attention mechanism,model,/content/training-data/natural_language_inference/80/triples/model.txt
soft attention mechanism,to extract,relevant information,model,/content/training-data/natural_language_inference/80/triples/model.txt
relevant information,from,these encodings,model,/content/training-data/natural_language_inference/80/triples/model.txt
Model,has,augmented sentence representations,model,/content/training-data/natural_language_inference/80/triples/model.txt
augmented sentence representations,passed to,inference stage,model,/content/training-data/natural_language_inference/80/triples/model.txt
inference stage,uses,similar dependent reading strategy,model,/content/training-data/natural_language_inference/80/triples/model.txt
similar dependent reading strategy,in,both directions,model,/content/training-data/natural_language_inference/80/triples/model.txt
Model,has,decision,model,/content/training-data/natural_language_inference/80/triples/model.txt
decision,made through,multi - layer perceptron ( MLP ,model,/content/training-data/natural_language_inference/80/triples/model.txt
multi - layer perceptron ( MLP ),based on,aggregated information,model,/content/training-data/natural_language_inference/80/triples/model.txt
Model,propose,dependent reading bidirectional LSTM ( DR - BiLSTM ) model,model,/content/training-data/natural_language_inference/80/triples/model.txt
Model,first encodes,premise u and a hypothesis v,model,/content/training-data/natural_language_inference/80/triples/model.txt
premise u and a hypothesis v,considering,dependency,model,/content/training-data/natural_language_inference/80/triples/model.txt
dependency,on,each other,model,/content/training-data/natural_language_inference/80/triples/model.txt
Contribution,has research problem,Natural Language Inference,research-problem,/content/training-data/natural_language_inference/80/triples/research-problem.txt
Contribution,has research problem,natural language inference ( NLI ,research-problem,/content/training-data/natural_language_inference/80/triples/research-problem.txt
Contribution,has research problem,NLI,research-problem,/content/training-data/natural_language_inference/80/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/80/triples/results.txt
Results,utilizing,trivial preprocessing step,results,/content/training-data/natural_language_inference/80/triples/results.txt
trivial preprocessing step,yields to,further improvements,results,/content/training-data/natural_language_inference/80/triples/results.txt
further improvements,of,0.4 % and 0.3 %,results,/content/training-data/natural_language_inference/80/triples/results.txt
further improvements,for,single and ensemble DR - BiLSTM models,results,/content/training-data/natural_language_inference/80/triples/results.txt
Results,has,ensemble model,results,/content/training-data/natural_language_inference/80/triples/results.txt
ensemble model,has,considerably outperforms,results,/content/training-data/natural_language_inference/80/triples/results.txt
considerably outperforms,has,current state - of - the - art,results,/content/training-data/natural_language_inference/80/triples/results.txt
current state - of - the - art,by obtaining,89.3 % accuracy,results,/content/training-data/natural_language_inference/80/triples/results.txt
Results,has,our single model,results,/content/training-data/natural_language_inference/80/triples/results.txt
our single model,obtains,state - of - the - art performance,results,/content/training-data/natural_language_inference/80/triples/results.txt
state - of - the - art performance,over,reported single and ensemble models,results,/content/training-data/natural_language_inference/80/triples/results.txt
reported single and ensemble models,by performing,simple preprocessing step,results,/content/training-data/natural_language_inference/80/triples/results.txt
our single model,name,DR - BiLSTM ( Single ) + Process,results,/content/training-data/natural_language_inference/80/triples/results.txt
Results,has,DR - BiLSTM ( Ensem . ) + Process,results,/content/training-data/natural_language_inference/80/triples/results.txt
DR - BiLSTM ( Ensem . ) + Process,has,outperforms,results,/content/training-data/natural_language_inference/80/triples/results.txt
outperforms,has,existing state - of - the - art,results,/content/training-data/natural_language_inference/80/triples/results.txt
existing state - of - the - art,has,remarkably,results,/content/training-data/natural_language_inference/80/triples/results.txt
existing state - of - the - art,has,0.7 % improvement,results,/content/training-data/natural_language_inference/80/triples/results.txt
Results,has,DR - BiLSTM ( Single ,results,/content/training-data/natural_language_inference/80/triples/results.txt
DR - BiLSTM ( Single ),obtains,accuracy,results,/content/training-data/natural_language_inference/80/triples/results.txt
accuracy,of,88.5 %,results,/content/training-data/natural_language_inference/80/triples/results.txt
88.5 %,has,considerably outperforms,results,/content/training-data/natural_language_inference/80/triples/results.txt
considerably outperforms,has,previous non-ensemble models,results,/content/training-data/natural_language_inference/80/triples/results.txt
DR - BiLSTM ( Single ),achieves,88.5 % accuracy,results,/content/training-data/natural_language_inference/80/triples/results.txt
88.5 % accuracy,is,best reported result,results,/content/training-data/natural_language_inference/80/triples/results.txt
best reported result,among,existing single models,results,/content/training-data/natural_language_inference/80/triples/results.txt
88.5 % accuracy,on,test set,results,/content/training-data/natural_language_inference/80/triples/results.txt
Results,has,DR - BiLSTM ( Ensemble ,results,/content/training-data/natural_language_inference/80/triples/results.txt
DR - BiLSTM ( Ensemble ),achieves,accuracy,results,/content/training-data/natural_language_inference/80/triples/results.txt
accuracy,of,89.3 %,results,/content/training-data/natural_language_inference/80/triples/results.txt
89.3 %,has,best result,results,/content/training-data/natural_language_inference/80/triples/results.txt
best result,observed on,SNLI,results,/content/training-data/natural_language_inference/80/triples/results.txt
Results,has,preprocessing mechanism,results,/content/training-data/natural_language_inference/80/triples/results.txt
preprocessing mechanism,leads to,further improvements,results,/content/training-data/natural_language_inference/80/triples/results.txt
further improvements,of,0.4 % and 0.3 %,results,/content/training-data/natural_language_inference/80/triples/results.txt
0.4 % and 0.3 %,for,our single and ensemble models,results,/content/training-data/natural_language_inference/80/triples/results.txt
0.4 % and 0.3 %,on,SNLI test set,results,/content/training-data/natural_language_inference/80/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/26/triples/ablation-analysis.txt
Ablation analysis,observe that,concatenation,ablation-analysis,/content/training-data/natural_language_inference/26/triples/ablation-analysis.txt
concatenation,yield,improvement,ablation-analysis,/content/training-data/natural_language_inference/26/triples/ablation-analysis.txt
improvement,verifying that,integrating,ablation-analysis,/content/training-data/natural_language_inference/26/triples/ablation-analysis.txt
integrating,be,quite useful,ablation-analysis,/content/training-data/natural_language_inference/26/triples/ablation-analysis.txt
quite useful,for,language understanding,ablation-analysis,/content/training-data/natural_language_inference/26/triples/ablation-analysis.txt
integrating,has,contextual semantics,ablation-analysis,/content/training-data/natural_language_inference/26/triples/ablation-analysis.txt
Ablation analysis,has,SemBERT,ablation-analysis,/content/training-data/natural_language_inference/26/triples/ablation-analysis.txt
SemBERT,shows that,SemBERT,ablation-analysis,/content/training-data/natural_language_inference/26/triples/ablation-analysis.txt
SemBERT,works,more effectively,ablation-analysis,/content/training-data/natural_language_inference/26/triples/ablation-analysis.txt
more effectively,for,integrating,ablation-analysis,/content/training-data/natural_language_inference/26/triples/ablation-analysis.txt
integrating,both,plain contextual representation,ablation-analysis,/content/training-data/natural_language_inference/26/triples/ablation-analysis.txt
integrating,both,contextual semantics,ablation-analysis,/content/training-data/natural_language_inference/26/triples/ablation-analysis.txt
SemBERT,has,outperforms,ablation-analysis,/content/training-data/natural_language_inference/26/triples/ablation-analysis.txt
outperforms,has,simple BERT + SRL model,ablation-analysis,/content/training-data/natural_language_inference/26/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/26/triples/model.txt
Model,consists of,three components,model,/content/training-data/natural_language_inference/26/triples/model.txt
three components,has,sequence encoder,model,/content/training-data/natural_language_inference/26/triples/model.txt
sequence encoder,where,pre-trained language model,model,/content/training-data/natural_language_inference/26/triples/model.txt
pre-trained language model,used to build,representation,model,/content/training-data/natural_language_inference/26/triples/model.txt
representation,for input,raw texts,model,/content/training-data/natural_language_inference/26/triples/model.txt
sequence encoder,where,semantic role labels,model,/content/training-data/natural_language_inference/26/triples/model.txt
semantic role labels,mapped to,embedding,model,/content/training-data/natural_language_inference/26/triples/model.txt
embedding,in,parallel,model,/content/training-data/natural_language_inference/26/triples/model.txt
three components,has,an out - ofshelf semantic role labeler,model,/content/training-data/natural_language_inference/26/triples/model.txt
an out - ofshelf semantic role labeler,to annotate,input sentences,model,/content/training-data/natural_language_inference/26/triples/model.txt
input sentences,with,variety of semantic role labels,model,/content/training-data/natural_language_inference/26/triples/model.txt
three components,has,semantic integration component,model,/content/training-data/natural_language_inference/26/triples/model.txt
semantic integration component,to integrate,text representation,model,/content/training-data/natural_language_inference/26/triples/model.txt
text representation,with,contextual explicit semantic embedding,model,/content/training-data/natural_language_inference/26/triples/model.txt
contextual explicit semantic embedding,to obtain,joint representation,model,/content/training-data/natural_language_inference/26/triples/model.txt
joint representation,for,downstream tasks,model,/content/training-data/natural_language_inference/26/triples/model.txt
Model,enrich,sentence contextual semantics,model,/content/training-data/natural_language_inference/26/triples/model.txt
sentence contextual semantics,in,multiple predicate - specific argument sequences,model,/content/training-data/natural_language_inference/26/triples/model.txt
multiple predicate - specific argument sequences,by presenting,SemBERT : Semantics - aware BERT,model,/content/training-data/natural_language_inference/26/triples/model.txt
SemBERT : Semantics - aware BERT,is,fine - tuned BERT,model,/content/training-data/natural_language_inference/26/triples/model.txt
fine - tuned BERT,with,explicit contextual semantic clues,model,/content/training-data/natural_language_inference/26/triples/model.txt
Model,has,proposed SemBERT,model,/content/training-data/natural_language_inference/26/triples/model.txt
proposed SemBERT,learns,representation,model,/content/training-data/natural_language_inference/26/triples/model.txt
representation,in,fine - grained manner,model,/content/training-data/natural_language_inference/26/triples/model.txt
proposed SemBERT,takes,strengths,model,/content/training-data/natural_language_inference/26/triples/model.txt
strengths,of,BERT,model,/content/training-data/natural_language_inference/26/triples/model.txt
BERT,on,plain context representation,model,/content/training-data/natural_language_inference/26/triples/model.txt
proposed SemBERT,takes,explicit semantics,model,/content/training-data/natural_language_inference/26/triples/model.txt
explicit semantics,for,deeper meaning representation,model,/content/training-data/natural_language_inference/26/triples/model.txt
Contribution,has research problem,language representations,research-problem,/content/training-data/natural_language_inference/26/triples/research-problem.txt
Contribution,has research problem,learning universal language representations,research-problem,/content/training-data/natural_language_inference/26/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
Experimental setup,set,initial learning rate,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
initial learning rate,with,warm - up rate,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
warm - up rate,of,0.1,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
initial learning rate,with,L2 weight decay,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
L2 weight decay,of,0.01,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
initial learning rate,in,"{ 8e -6 , 1 e - 5 , 2 e - 5 , 3 e - 5 }",experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
Experimental setup,use,pre-trained weights,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
pre-trained weights,of,BERT,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
Experimental setup,has,maximum number of epochs,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
maximum number of epochs,set in,"[ 2 , 5 ]",experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
maximum number of epochs,depending on,tasks,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
Experimental setup,has,default maximum number,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
default maximum number,of,predicateargument structures m,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
predicateargument structures m,set to,3,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
Experimental setup,has,Our implementation,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
Our implementation,based on,PyTorch implementation,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
PyTorch implementation,of,BERT,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
Experimental setup,has,Texts,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
Texts,with,maximum length,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
maximum length,of,128 or 200,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
128 or 200,for,other tasks,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
maximum length,of,384,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
384,for,SQuAD,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
Texts,tokenized using,wordpieces,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
Experimental setup,has,batch size,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
batch size,selected in,"{ 16 , 24 , 32 }",experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
Experimental setup,has,dimension,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
dimension,of,SRL embedding,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
SRL embedding,set to,10,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
Experimental setup,follow,same fine - tuning procedure,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
same fine - tuning procedure,as,BERT,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
BERT,has,all the layers,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
all the layers,tuned with,moderate model size,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
moderate model size,as,extra SRL embedding volume,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
extra SRL embedding volume,is,less than 15 %,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
less than 15 %,of,original encoder size,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
moderate model size,has,increasing,experimental-setup,/content/training-data/natural_language_inference/26/triples/experimental-setup.txt
Contribution,has,Approach,approach,/content/training-data/natural_language_inference/62/triples/approach.txt
Approach,propose,end - to - end MRC model,approach,/content/training-data/natural_language_inference/62/triples/approach.txt
end - to - end MRC model,named,Knowledge Aided Reader ( KAR ,approach,/content/training-data/natural_language_inference/62/triples/approach.txt
Knowledge Aided Reader ( KAR ),explicitly uses,extracted general knowledge,approach,/content/training-data/natural_language_inference/62/triples/approach.txt
extracted general knowledge,to assist,attention mechanisms,approach,/content/training-data/natural_language_inference/62/triples/approach.txt
Approach,propose,data enrichment method,approach,/content/training-data/natural_language_inference/62/triples/approach.txt
data enrichment method,to extract,inter-word semantic connections,approach,/content/training-data/natural_language_inference/62/triples/approach.txt
inter-word semantic connections,as,general knowledge,approach,/content/training-data/natural_language_inference/62/triples/approach.txt
general knowledge,from,each given passage - question pair,approach,/content/training-data/natural_language_inference/62/triples/approach.txt
data enrichment method,which uses,WordNet,approach,/content/training-data/natural_language_inference/62/triples/approach.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/62/triples/ablation-analysis.txt
Ablation analysis,replacing,knowledge aided attention mechanisms,ablation-analysis,/content/training-data/natural_language_inference/62/triples/ablation-analysis.txt
knowledge aided attention mechanisms,with,mutual attention,ablation-analysis,/content/training-data/natural_language_inference/62/triples/ablation-analysis.txt
knowledge aided attention mechanisms,with,self attention,ablation-analysis,/content/training-data/natural_language_inference/62/triples/ablation-analysis.txt
knowledge aided attention mechanisms,find that,F 1 score,ablation-analysis,/content/training-data/natural_language_inference/62/triples/ablation-analysis.txt
F 1 score,of,KAR,ablation-analysis,/content/training-data/natural_language_inference/62/triples/ablation-analysis.txt
F 1 score,drops by,7.8,ablation-analysis,/content/training-data/natural_language_inference/62/triples/ablation-analysis.txt
7.8,on,AddSent,ablation-analysis,/content/training-data/natural_language_inference/62/triples/ablation-analysis.txt
F 1 score,drops by,4.2,ablation-analysis,/content/training-data/natural_language_inference/62/triples/ablation-analysis.txt
4.2,on,development set,ablation-analysis,/content/training-data/natural_language_inference/62/triples/ablation-analysis.txt
F 1 score,drops by,9.1,ablation-analysis,/content/training-data/natural_language_inference/62/triples/ablation-analysis.txt
9.1,on,AddOneSent,ablation-analysis,/content/training-data/natural_language_inference/62/triples/ablation-analysis.txt
Contribution,has research problem,Machine Reading Comprehension,research-problem,/content/training-data/natural_language_inference/62/triples/research-problem.txt
Contribution,has research problem,Machine Reading Comprehension ( MRC ,research-problem,/content/training-data/natural_language_inference/62/triples/research-problem.txt
Contribution,has research problem,MRC,research-problem,/content/training-data/natural_language_inference/62/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/62/triples/experimental-setup.txt
Experimental setup,To boost,performance,experimental-setup,/content/training-data/natural_language_inference/62/triples/experimental-setup.txt
performance,apply,exponential moving average,experimental-setup,/content/training-data/natural_language_inference/62/triples/experimental-setup.txt
exponential moving average,with,decay rate,experimental-setup,/content/training-data/natural_language_inference/62/triples/experimental-setup.txt
decay rate,of,0.999,experimental-setup,/content/training-data/natural_language_inference/62/triples/experimental-setup.txt
Experimental setup,For,dense layers and the BiLSTMs,experimental-setup,/content/training-data/natural_language_inference/62/triples/experimental-setup.txt
dense layers and the BiLSTMs,set,dimensionality unit d,experimental-setup,/content/training-data/natural_language_inference/62/triples/experimental-setup.txt
dimensionality unit d,to,600,experimental-setup,/content/training-data/natural_language_inference/62/triples/experimental-setup.txt
Experimental setup,For,model optimization,experimental-setup,/content/training-data/natural_language_inference/62/triples/experimental-setup.txt
model optimization,apply,"Adam ( Kingma and Ba , 2014 ) optimizer",experimental-setup,/content/training-data/natural_language_inference/62/triples/experimental-setup.txt
"Adam ( Kingma and Ba , 2014 ) optimizer",with,minibatch size,experimental-setup,/content/training-data/natural_language_inference/62/triples/experimental-setup.txt
minibatch size,of,32,experimental-setup,/content/training-data/natural_language_inference/62/triples/experimental-setup.txt
"Adam ( Kingma and Ba , 2014 ) optimizer",with,learning rate,experimental-setup,/content/training-data/natural_language_inference/62/triples/experimental-setup.txt
learning rate,of,0.0005,experimental-setup,/content/training-data/natural_language_inference/62/triples/experimental-setup.txt
Experimental setup,implement,KAR,experimental-setup,/content/training-data/natural_language_inference/62/triples/experimental-setup.txt
KAR,with,TensorFlow 1.11.0,experimental-setup,/content/training-data/natural_language_inference/62/triples/experimental-setup.txt
Experimental setup,tokenize,MRC dataset,experimental-setup,/content/training-data/natural_language_inference/62/triples/experimental-setup.txt
MRC dataset,with,spa Cy 2.0.13,experimental-setup,/content/training-data/natural_language_inference/62/triples/experimental-setup.txt
Experimental setup,manipulate,WordNet 3.0,experimental-setup,/content/training-data/natural_language_inference/62/triples/experimental-setup.txt
WordNet 3.0,with,NLTK 3.3,experimental-setup,/content/training-data/natural_language_inference/62/triples/experimental-setup.txt
Experimental setup,To avoid,overfitting,experimental-setup,/content/training-data/natural_language_inference/62/triples/experimental-setup.txt
overfitting,apply,dropout,experimental-setup,/content/training-data/natural_language_inference/62/triples/experimental-setup.txt
dropout,with,dropout rate,experimental-setup,/content/training-data/natural_language_inference/62/triples/experimental-setup.txt
dropout rate,of,0.3,experimental-setup,/content/training-data/natural_language_inference/62/triples/experimental-setup.txt
dropout,to,dense layers,experimental-setup,/content/training-data/natural_language_inference/62/triples/experimental-setup.txt
dropout,to,BiLSTMs,experimental-setup,/content/training-data/natural_language_inference/62/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/62/triples/results.txt
Results,has,KAR,results,/content/training-data/natural_language_inference/62/triples/results.txt
KAR,on,development set,results,/content/training-data/natural_language_inference/62/triples/results.txt
development set,achieves,F 1 score,results,/content/training-data/natural_language_inference/62/triples/results.txt
F 1 score,of,80.8,results,/content/training-data/natural_language_inference/62/triples/results.txt
development set,achieves,EM,results,/content/training-data/natural_language_inference/62/triples/results.txt
EM,of,71.9,results,/content/training-data/natural_language_inference/62/triples/results.txt
development set,achieves,even better,results,/content/training-data/natural_language_inference/62/triples/results.txt
even better,than,final performance,results,/content/training-data/natural_language_inference/62/triples/results.txt
final performance,of,several strong baselines,results,/content/training-data/natural_language_inference/62/triples/results.txt
several strong baselines,such as,DCN ( EM / F1 : 65.4 / 75.6 ,results,/content/training-data/natural_language_inference/62/triples/results.txt
several strong baselines,such as,BiDAF ( EM / F1 : 67.7 / 77.3 ,results,/content/training-data/natural_language_inference/62/triples/results.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/76/triples/model.txt
Model,proposing,attentive neural network,model,/content/training-data/natural_language_inference/76/triples/model.txt
attentive neural network,capable of,reasoning,model,/content/training-data/natural_language_inference/76/triples/model.txt
reasoning,over,entailments,model,/content/training-data/natural_language_inference/76/triples/model.txt
entailments,of,pairs,model,/content/training-data/natural_language_inference/76/triples/model.txt
pairs,of,words and phrases,model,/content/training-data/natural_language_inference/76/triples/model.txt
entailments,by processing,hypothesis,model,/content/training-data/natural_language_inference/76/triples/model.txt
hypothesis,conditioned on,premise,model,/content/training-data/natural_language_inference/76/triples/model.txt
Model,extend,neural word - by - word attention mechanism,model,/content/training-data/natural_language_inference/76/triples/model.txt
neural word - by - word attention mechanism,to encourage,reasoning,model,/content/training-data/natural_language_inference/76/triples/model.txt
reasoning,over,entailments,model,/content/training-data/natural_language_inference/76/triples/model.txt
entailments,of,pairs,model,/content/training-data/natural_language_inference/76/triples/model.txt
pairs,of,words and phrases,model,/content/training-data/natural_language_inference/76/triples/model.txt
Model,present,neural model,model,/content/training-data/natural_language_inference/76/triples/model.txt
neural model,based on,LSTMs,model,/content/training-data/natural_language_inference/76/triples/model.txt
neural model,reads,two sentences,model,/content/training-data/natural_language_inference/76/triples/model.txt
two sentences,in,one go,model,/content/training-data/natural_language_inference/76/triples/model.txt
Contribution,has research problem,Recognizing textual entailment ( RTE ,research-problem,/content/training-data/natural_language_inference/76/triples/research-problem.txt
Contribution,has research problem,RTE,research-problem,/content/training-data/natural_language_inference/76/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/76/triples/results.txt
Results,Allowing,model,results,/content/training-data/natural_language_inference/76/triples/results.txt
model,attend,hypothesis,results,/content/training-data/natural_language_inference/76/triples/results.txt
hypothesis,not,improve,results,/content/training-data/natural_language_inference/76/triples/results.txt
improve,has,performance,results,/content/training-data/natural_language_inference/76/triples/results.txt
performance,for,RTE,results,/content/training-data/natural_language_inference/76/triples/results.txt
hypothesis,based on,premise,results,/content/training-data/natural_language_inference/76/triples/results.txt
Results,processing,hypothesis,results,/content/training-data/natural_language_inference/76/triples/results.txt
hypothesis,conditioned on,premise,results,/content/training-data/natural_language_inference/76/triples/results.txt
hypothesis,gives,improvement,results,/content/training-data/natural_language_inference/76/triples/results.txt
improvement,of,3.3 percentage points,results,/content/training-data/natural_language_inference/76/triples/results.txt
3.3 percentage points,over,Bowman et al. 's LSTM,results,/content/training-data/natural_language_inference/76/triples/results.txt
3.3 percentage points,in,accuracy,results,/content/training-data/natural_language_inference/76/triples/results.txt
Results,has,Our LSTM,results,/content/training-data/natural_language_inference/76/triples/results.txt
Our LSTM,has,outperforms,results,/content/training-data/natural_language_inference/76/triples/results.txt
outperforms,by,2.7 percentage points,results,/content/training-data/natural_language_inference/76/triples/results.txt
outperforms,has,simple lexicalized classifier,results,/content/training-data/natural_language_inference/76/triples/results.txt
Results,Enabling,model,results,/content/training-data/natural_language_inference/76/triples/results.txt
model,attend,output vectors,results,/content/training-data/natural_language_inference/76/triples/results.txt
output vectors,of,premise,results,/content/training-data/natural_language_inference/76/triples/results.txt
output vectors,yields,another 1.2 percentage point,results,/content/training-data/natural_language_inference/76/triples/results.txt
another 1.2 percentage point,has,improvement,results,/content/training-data/natural_language_inference/76/triples/results.txt
improvement,compared to,attending,results,/content/training-data/natural_language_inference/76/triples/results.txt
attending,based only on,last output vector,results,/content/training-data/natural_language_inference/76/triples/results.txt
last output vector,of,premise,results,/content/training-data/natural_language_inference/76/triples/results.txt
output vectors,for,every word,results,/content/training-data/natural_language_inference/76/triples/results.txt
every word,in,hypothesis,results,/content/training-data/natural_language_inference/76/triples/results.txt
Results,incorporating,attention mechanism,results,/content/training-data/natural_language_inference/76/triples/results.txt
attention mechanism,found,0.9 percentage point,results,/content/training-data/natural_language_inference/76/triples/results.txt
0.9 percentage point,has,improvement,results,/content/training-data/natural_language_inference/76/triples/results.txt
improvement,over,single LSTM,results,/content/training-data/natural_language_inference/76/triples/results.txt
attention mechanism,found,1.4 percentage point,results,/content/training-data/natural_language_inference/76/triples/results.txt
1.4 percentage point,has,increase,results,/content/training-data/natural_language_inference/76/triples/results.txt
increase,over,benchmark model,results,/content/training-data/natural_language_inference/76/triples/results.txt
benchmark model,uses,two LSTMs,results,/content/training-data/natural_language_inference/76/triples/results.txt
two LSTMs,for,conditional encoding,results,/content/training-data/natural_language_inference/76/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/natural_language_inference/9/triples/baselines.txt
Baselines,include,LSTM,baselines,/content/training-data/natural_language_inference/9/triples/baselines.txt
Baselines,include,End - to - end Memory Networks ( N2N ,baselines,/content/training-data/natural_language_inference/9/triples/baselines.txt
Baselines,include,Dynamic Memory Networks ( DMN + ,baselines,/content/training-data/natural_language_inference/9/triples/baselines.txt
Baselines,include,Gated End - to - end Memory Networks ( GMe m N2N ,baselines,/content/training-data/natural_language_inference/9/triples/baselines.txt
Baselines,include,Differentiable Neural Computer ( DNC ,baselines,/content/training-data/natural_language_inference/9/triples/baselines.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/9/triples/ablation-analysis.txt
Ablation analysis,When,number of layers,ablation-analysis,/content/training-data/natural_language_inference/9/triples/ablation-analysis.txt
number of layers,is,only one,ablation-analysis,/content/training-data/natural_language_inference/9/triples/ablation-analysis.txt
number of layers,has,model,ablation-analysis,/content/training-data/natural_language_inference/9/triples/ablation-analysis.txt
model,lacks,reasoning capability,ablation-analysis,/content/training-data/natural_language_inference/9/triples/ablation-analysis.txt
Ablation analysis,In the case of,10 k dataset,ablation-analysis,/content/training-data/natural_language_inference/9/triples/ablation-analysis.txt
10 k dataset,has,many layers ( 6 ) and hidden dimensions ( 200 ,ablation-analysis,/content/training-data/natural_language_inference/9/triples/ablation-analysis.txt
many layers ( 6 ) and hidden dimensions ( 200 ),helps,reasoning,ablation-analysis,/content/training-data/natural_language_inference/9/triples/ablation-analysis.txt
Ablation analysis,In the case of,1 k dataset,ablation-analysis,/content/training-data/natural_language_inference/9/triples/ablation-analysis.txt
1 k dataset,when,too many layers,ablation-analysis,/content/training-data/natural_language_inference/9/triples/ablation-analysis.txt
too many layers,correctly training,model,ablation-analysis,/content/training-data/natural_language_inference/9/triples/ablation-analysis.txt
too many layers,becomes,increasingly difficult,ablation-analysis,/content/training-data/natural_language_inference/9/triples/ablation-analysis.txt
Ablation analysis,hypothesized that,larger hidden state,ablation-analysis,/content/training-data/natural_language_inference/9/triples/ablation-analysis.txt
larger hidden state,required for,real data,ablation-analysis,/content/training-data/natural_language_inference/9/triples/ablation-analysis.txt
Ablation analysis,Increasing,dimension,ablation-analysis,/content/training-data/natural_language_inference/9/triples/ablation-analysis.txt
dimension,of,hidden state,ablation-analysis,/content/training-data/natural_language_inference/9/triples/ablation-analysis.txt
hidden state,in,dialog 's Task 6 ( DSTC2 ,ablation-analysis,/content/training-data/natural_language_inference/9/triples/ablation-analysis.txt
dialog 's Task 6 ( DSTC2 ),has,helps,ablation-analysis,/content/training-data/natural_language_inference/9/triples/ablation-analysis.txt
hidden state,to,100,ablation-analysis,/content/training-data/natural_language_inference/9/triples/ablation-analysis.txt
Ablation analysis,Adding,reset gate,ablation-analysis,/content/training-data/natural_language_inference/9/triples/ablation-analysis.txt
reset gate,has,helps,ablation-analysis,/content/training-data/natural_language_inference/9/triples/ablation-analysis.txt
Ablation analysis,has,vector gates,ablation-analysis,/content/training-data/natural_language_inference/9/triples/ablation-analysis.txt
vector gates,in,bAbI story - based QA 10 k dataset,ablation-analysis,/content/training-data/natural_language_inference/9/triples/ablation-analysis.txt
bAbI story - based QA 10 k dataset,sometimes,help,ablation-analysis,/content/training-data/natural_language_inference/9/triples/ablation-analysis.txt
Ablation analysis,Including,vector gates,ablation-analysis,/content/training-data/natural_language_inference/9/triples/ablation-analysis.txt
vector gates,has,hurts,ablation-analysis,/content/training-data/natural_language_inference/9/triples/ablation-analysis.txt
hurts,in,1 k datasets,ablation-analysis,/content/training-data/natural_language_inference/9/triples/ablation-analysis.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
Hyperparameters,withhold,10 %,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
10 %,of,training,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
10 %,for,development,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
Hyperparameters,use,hidden state size,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
hidden state size,of,50,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
50,by,deafult,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
Hyperparameters,repeat,each training procedure,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
each training procedure,has,10 times ( 50 times for 10 k ,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
10 times ( 50 times for 10 k ),with,new random initialization,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
new random initialization,of,weights,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
Hyperparameters,has,Forget bias,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
Forget bias,of,2.5,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
2.5,used for,update gates,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
Hyperparameters,has,loss function,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
loss function,is,cross entropy,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
cross entropy,between,v and the one - hot vector,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
v and the one - hot vector,of,true answer,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
Hyperparameters,has,loss,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
loss,minimized by,stochastic gradient descent,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
stochastic gradient descent,for,maximally 500 epochs,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
Hyperparameters,has,Batch sizes,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
Batch sizes,of,128,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
128,for,bAbI QA 10 k,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
Batch sizes,of,32,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
32,for,bAbI story - based QA 1k,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
32,for,bAb I dialog,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
32,for,DSTC2 dialog,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
Hyperparameters,has,learning rate,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
learning rate,controlled by,AdaGrad,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
AdaGrad,with,initial learning rate,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
initial learning rate,of,0.5 ( 0.1 for QA 10 k ,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
Hyperparameters,has,L2 weight decay,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
L2 weight decay,of,0.001 ( 0.0005 for QA 10 k ,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
0.001 ( 0.0005 for QA 10 k ),used for,all weights,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
Hyperparameters,has,training,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
training,is,early stopped,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
early stopped,if,loss,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
loss,has,not decrease,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
not decrease,for,50 epochs,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
loss,on,development data,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
Hyperparameters,has,weights,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
weights,in,input and output modules,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
input and output modules,initialized with,zero mean,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
input and output modules,initialized with,standard deviation of 1 / ? d,hyperparameters,/content/training-data/natural_language_inference/9/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/9/triples/model.txt
Model,has,Query - Reduction Network 1 ( QRN ,model,/content/training-data/natural_language_inference/9/triples/model.txt
Query - Reduction Network 1 ( QRN ),considers,context sentences,model,/content/training-data/natural_language_inference/9/triples/model.txt
context sentences,as,sequence,model,/content/training-data/natural_language_inference/9/triples/model.txt
sequence,of,state - changing triggers,model,/content/training-data/natural_language_inference/9/triples/model.txt
state - changing triggers,observes,through time,model,/content/training-data/natural_language_inference/9/triples/model.txt
through time,transforms,original query,model,/content/training-data/natural_language_inference/9/triples/model.txt
original query,to,more informed query,model,/content/training-data/natural_language_inference/9/triples/model.txt
Query - Reduction Network 1 ( QRN ),is,single recurrent unit,model,/content/training-data/natural_language_inference/9/triples/model.txt
single recurrent unit,that addresses,long - term dependency problem,model,/content/training-data/natural_language_inference/9/triples/model.txt
long - term dependency problem,by simplifying,recurrent update,model,/content/training-data/natural_language_inference/9/triples/model.txt
long - term dependency problem,of,most RNN - based models,model,/content/training-data/natural_language_inference/9/triples/model.txt
long - term dependency problem,taking,advantage,model,/content/training-data/natural_language_inference/9/triples/model.txt
advantage,of,RNN 's capability,model,/content/training-data/natural_language_inference/9/triples/model.txt
RNN 's capability,to model,sequential data,model,/content/training-data/natural_language_inference/9/triples/model.txt
Query - Reduction Network 1 ( QRN ),better encodes,locality information,model,/content/training-data/natural_language_inference/9/triples/model.txt
locality information,has,query updates,model,/content/training-data/natural_language_inference/9/triples/model.txt
query updates,performed,locally,model,/content/training-data/natural_language_inference/9/triples/model.txt
Contribution,has research problem,QUESTION ANSWERING,research-problem,/content/training-data/natural_language_inference/9/triples/research-problem.txt
Contribution,has research problem,question answering when reasoning over multiple facts,research-problem,/content/training-data/natural_language_inference/9/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/9/triples/results.txt
Results,In,10 k dataset,results,/content/training-data/natural_language_inference/9/triples/results.txt
10 k dataset,has,average accuracy,results,/content/training-data/natural_language_inference/9/triples/results.txt
average accuracy,of,QRN 's ' 6r200 ' ( 6 layers + reset gate + d = 200 ) model,results,/content/training-data/natural_language_inference/9/triples/results.txt
QRN 's ' 6r200 ' ( 6 layers + reset gate + d = 200 ) model,outperforms,all previous models,results,/content/training-data/natural_language_inference/9/triples/results.txt
all previous models,achieving,nearly perfect score,results,/content/training-data/natural_language_inference/9/triples/results.txt
nearly perfect score,of,99.7 %,results,/content/training-data/natural_language_inference/9/triples/results.txt
all previous models,by,large margin ( 2.5 + % ,results,/content/training-data/natural_language_inference/9/triples/results.txt
Results,In,1 k data,results,/content/training-data/natural_language_inference/9/triples/results.txt
1 k data,has,QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 ,results,/content/training-data/natural_language_inference/9/triples/results.txt
QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 ),outperforms,all other models,results,/content/training-data/natural_language_inference/9/triples/results.txt
all other models,by,large margin ( 2.8 + % ,results,/content/training-data/natural_language_inference/9/triples/results.txt
Results,has,QRN,results,/content/training-data/natural_language_inference/9/triples/results.txt
QRN,outperforms,previous work,results,/content/training-data/natural_language_inference/9/triples/results.txt
previous work,by,large margin ( 2.0 + % ,results,/content/training-data/natural_language_inference/9/triples/results.txt
Contribution,has,Dataset,datase,/content/training-data/natural_language_inference/64/triples/dataset.txt
Dataset,built,ASNQ,datase,/content/training-data/natural_language_inference/64/triples/dataset.txt
ASNQ,by transforming,recently released Natural Questions ( NQ ) corpus,datase,/content/training-data/natural_language_inference/64/triples/dataset.txt
recently released Natural Questions ( NQ ) corpus,from,MR,datase,/content/training-data/natural_language_inference/64/triples/dataset.txt
MR,to,AS2 task,datase,/content/training-data/natural_language_inference/64/triples/dataset.txt
ASNQ,for,AS2,datase,/content/training-data/natural_language_inference/64/triples/dataset.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/natural_language_inference/64/triples/hyperparameters.txt
Hyperparameters,set,max number of epochs,hyperparameters,/content/training-data/natural_language_inference/64/triples/hyperparameters.txt
max number of epochs,equal to,3 and 9,hyperparameters,/content/training-data/natural_language_inference/64/triples/hyperparameters.txt
3 and 9,for,adapt and transfer steps,hyperparameters,/content/training-data/natural_language_inference/64/triples/hyperparameters.txt
Hyperparameters,set,maximum sequence length,hyperparameters,/content/training-data/natural_language_inference/64/triples/hyperparameters.txt
maximum sequence length,for,BERT / RoBERTa,hyperparameters,/content/training-data/natural_language_inference/64/triples/hyperparameters.txt
BERT / RoBERTa,to,128 tokens,hyperparameters,/content/training-data/natural_language_inference/64/triples/hyperparameters.txt
Hyperparameters,apply,early stopping,hyperparameters,/content/training-data/natural_language_inference/64/triples/hyperparameters.txt
early stopping,on,dev. set,hyperparameters,/content/training-data/natural_language_inference/64/triples/hyperparameters.txt
dev. set,of,target corpus,hyperparameters,/content/training-data/natural_language_inference/64/triples/hyperparameters.txt
Hyperparameters,adopt,Adam optimizer,hyperparameters,/content/training-data/natural_language_inference/64/triples/hyperparameters.txt
Adam optimizer,with,learning rate,hyperparameters,/content/training-data/natural_language_inference/64/triples/hyperparameters.txt
learning rate,of,2e - 5,hyperparameters,/content/training-data/natural_language_inference/64/triples/hyperparameters.txt
2e - 5,for,transfer step,hyperparameters,/content/training-data/natural_language_inference/64/triples/hyperparameters.txt
transfer step,on,ASNQ dataset,hyperparameters,/content/training-data/natural_language_inference/64/triples/hyperparameters.txt
learning rate,of,1e - 6,hyperparameters,/content/training-data/natural_language_inference/64/triples/hyperparameters.txt
1e - 6,for,adapt step,hyperparameters,/content/training-data/natural_language_inference/64/triples/hyperparameters.txt
adapt step,on,target dataset,hyperparameters,/content/training-data/natural_language_inference/64/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/64/triples/model.txt
Model,study,Transformer - based models,model,/content/training-data/natural_language_inference/64/triples/model.txt
Transformer - based models,provide,effective solutions,model,/content/training-data/natural_language_inference/64/triples/model.txt
effective solutions,to tackle,instability,model,/content/training-data/natural_language_inference/64/triples/model.txt
instability,of,finetuning step,model,/content/training-data/natural_language_inference/64/triples/model.txt
effective solutions,to tackle,data scarceness problem,model,/content/training-data/natural_language_inference/64/triples/model.txt
data scarceness problem,for,AS2,model,/content/training-data/natural_language_inference/64/triples/model.txt
Transformer - based models,for,AS2,model,/content/training-data/natural_language_inference/64/triples/model.txt
Model,show,transferred model,model,/content/training-data/natural_language_inference/64/triples/model.txt
transferred model,can be,effectively adapted,model,/content/training-data/natural_language_inference/64/triples/model.txt
effectively adapted,with,subsequent finetuning step,model,/content/training-data/natural_language_inference/64/triples/model.txt
subsequent finetuning step,when using,target data,model,/content/training-data/natural_language_inference/64/triples/model.txt
target data,of,small size,model,/content/training-data/natural_language_inference/64/triples/model.txt
effectively adapted,to,target domain,model,/content/training-data/natural_language_inference/64/triples/model.txt
Model,has,improve,model,/content/training-data/natural_language_inference/64/triples/model.txt
improve,has,stability,model,/content/training-data/natural_language_inference/64/triples/model.txt
stability,by adding,intermediate fine - tuning step,model,/content/training-data/natural_language_inference/64/triples/model.txt
intermediate fine - tuning step,aims at,specializing,model,/content/training-data/natural_language_inference/64/triples/model.txt
specializing,to,target task ( AS2 ,model,/content/training-data/natural_language_inference/64/triples/model.txt
stability,of,Transformer models,model,/content/training-data/natural_language_inference/64/triples/model.txt
Contribution,has research problem,Answer Sentence Selection,research-problem,/content/training-data/natural_language_inference/64/triples/research-problem.txt
Contribution,has research problem,Question Answering,research-problem,/content/training-data/natural_language_inference/64/triples/research-problem.txt
Contribution,has research problem,Question Answering ( QA ,research-problem,/content/training-data/natural_language_inference/64/triples/research-problem.txt
Contribution,has research problem,answer sentence selection ( AS2 ,research-problem,/content/training-data/natural_language_inference/64/triples/research-problem.txt
Contribution,has research problem,AS2,research-problem,/content/training-data/natural_language_inference/64/triples/research-problem.txt
Contribution,has research problem,QA,research-problem,/content/training-data/natural_language_inference/64/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/64/triples/results.txt
Results,has,TANDA,results,/content/training-data/natural_language_inference/64/triples/results.txt
TANDA,provides,large improvement,results,/content/training-data/natural_language_inference/64/triples/results.txt
large improvement,over,state of the art,results,/content/training-data/natural_language_inference/64/triples/results.txt
TANDA,improves,all the models,results,/content/training-data/natural_language_inference/64/triples/results.txt
all the models,name,BERT - Base,results,/content/training-data/natural_language_inference/64/triples/results.txt
all the models,name,RoBERTa- Base,results,/content/training-data/natural_language_inference/64/triples/results.txt
all the models,name,BERT - Large,results,/content/training-data/natural_language_inference/64/triples/results.txt
all the models,name,RoBERTa - Large,results,/content/training-data/natural_language_inference/64/triples/results.txt
all the models,has,outperforming,results,/content/training-data/natural_language_inference/64/triples/results.txt
outperforming,has,previous state of the art,results,/content/training-data/natural_language_inference/64/triples/results.txt
Results,has,RoBERTa- Large TANDA,results,/content/training-data/natural_language_inference/64/triples/results.txt
RoBERTa- Large TANDA,using,ASNQ,results,/content/training-data/natural_language_inference/64/triples/results.txt
RoBERTa- Large TANDA,establish,impressive new state of the art,results,/content/training-data/natural_language_inference/64/triples/results.txt
impressive new state of the art,of,0.920 and 0.933,results,/content/training-data/natural_language_inference/64/triples/results.txt
0.920 and 0.933,in,MAP and MRR,results,/content/training-data/natural_language_inference/64/triples/results.txt
impressive new state of the art,for,AS2,results,/content/training-data/natural_language_inference/64/triples/results.txt
AS2,on,WikiQA,results,/content/training-data/natural_language_inference/64/triples/results.txt
Results,has,RoBERTa - Large TANDA,results,/content/training-data/natural_language_inference/64/triples/results.txt
RoBERTa - Large TANDA,with,ASNQ,results,/content/training-data/natural_language_inference/64/triples/results.txt
RoBERTa - Large TANDA,establishes,impressive performance,results,/content/training-data/natural_language_inference/64/triples/results.txt
impressive performance,of,0.943,results,/content/training-data/natural_language_inference/64/triples/results.txt
0.943,in,MAP,results,/content/training-data/natural_language_inference/64/triples/results.txt
impressive performance,of,0.974,results,/content/training-data/natural_language_inference/64/triples/results.txt
0.974,in,MRR,results,/content/training-data/natural_language_inference/64/triples/results.txt
impressive performance,has,outperforming,results,/content/training-data/natural_language_inference/64/triples/results.txt
outperforming,has,previous state of the art,results,/content/training-data/natural_language_inference/64/triples/results.txt
Contribution,has,Experiments,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
Experiments,has,Experimental setup,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
Experimental setup,pre-train,large model,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
large model,with,12 layers,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
12 layers,in,encoder and decoder,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
large model,with,hidden size,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
hidden size,of,1024,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
Experimental setup,use,combination,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
combination,of,text infilling,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
combination,of,sentence permutation,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
Experimental setup,Following,RoBERTa,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
RoBERTa,use,batch size,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
batch size,of,8000,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
RoBERTa,train,model,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
model,for,500000 steps,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
Experimental setup,has,Documents,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
Documents,tokenized with,same byte - pair encoding,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
same byte - pair encoding,as,GPT - 2,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
Experimental setup,permute,all sentences,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
Experimental setup,dis abled,dropout,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
dropout,for,final 10 %,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
final 10 %,of,training steps,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
final 10 %,To help,model,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
model,better fit,data,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
Experimental setup,mask,30 %,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
30 %,of,tokens,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
tokens,in,each document,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
Experiments,has,Tasks,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
Tasks,experiment with,several text generation tasks,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
several text generation tasks,experiment on,original WMT16 Romanian - English,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
original WMT16 Romanian - English,augmented with,back - translation data,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
original WMT16 Romanian - English,has,Preliminary results,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
Preliminary results,suggested,our approach,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
our approach,prone to,overfitting,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
our approach,was,less effective,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
less effective,without,back - translation data,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
several text generation tasks,use,recently proposed ELI5 dataset,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
recently proposed ELI5 dataset,find,BART,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
BART,has,outperforms,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
outperforms,by,1.2 ROUGE - L,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
outperforms,has,best previous work,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
several text generation tasks,present,results,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
results,on,two summarization datasets,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
two summarization datasets,name,CNN / DailyMail,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
two summarization datasets,name,XSum,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
two summarization datasets,has,BART,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
BART,has,outperforms,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
outperforms,has,all existing work,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
outperforms,has,best previous work,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
best previous work,by,roughly 6.0 points,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
roughly 6.0 points,representing,significant advance,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
significant advance,in,performance,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
roughly 6.0 points,on,all ROUGE metrics,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
best previous work,leverages,BERT,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
several text generation tasks,evaluate,dialogue response generation,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
dialogue response generation,has,BART,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
BART,has,outperforms,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
outperforms,has,previous work,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
outperforms,on,two automated metrics,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
dialogue response generation,on,CONVAI2,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
Experiments,has,Baselines,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
Baselines,has,RoBERTa,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
RoBERTa,pre-trained with,same resources,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
RoBERTa,pre-trained with,different objective,experiments,/content/training-data/natural_language_inference/41/triples/experiments.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/41/triples/model.txt
Model,has,Pretraining,model,/content/training-data/natural_language_inference/41/triples/model.txt
Pretraining,has,two stages,model,/content/training-data/natural_language_inference/41/triples/model.txt
two stages,has,sequence - to - sequence model,model,/content/training-data/natural_language_inference/41/triples/model.txt
sequence - to - sequence model,is,learned,model,/content/training-data/natural_language_inference/41/triples/model.txt
learned,to reconstruct,original text,model,/content/training-data/natural_language_inference/41/triples/model.txt
two stages,has,text,model,/content/training-data/natural_language_inference/41/triples/model.txt
text,corrupted with,arbitrary noising function,model,/content/training-data/natural_language_inference/41/triples/model.txt
Model,has,BART,model,/content/training-data/natural_language_inference/41/triples/model.txt
BART,is,denoising autoencoder,model,/content/training-data/natural_language_inference/41/triples/model.txt
denoising autoencoder,built with,sequence - to - sequence model,model,/content/training-data/natural_language_inference/41/triples/model.txt
denoising autoencoder,applicable to,very wide range of end tasks,model,/content/training-data/natural_language_inference/41/triples/model.txt
BART,uses,standard Tranformer - based neural machine translation architecture,model,/content/training-data/natural_language_inference/41/triples/model.txt
standard Tranformer - based neural machine translation architecture,seen as,generalizing,model,/content/training-data/natural_language_inference/41/triples/model.txt
generalizing,has,BERT,model,/content/training-data/natural_language_inference/41/triples/model.txt
BERT,due to,bidirectional encoder,model,/content/training-data/natural_language_inference/41/triples/model.txt
generalizing,has,GPT,model,/content/training-data/natural_language_inference/41/triples/model.txt
GPT,with,left - to - right decoder,model,/content/training-data/natural_language_inference/41/triples/model.txt
Model,present,BART,model,/content/training-data/natural_language_inference/41/triples/model.txt
BART,pre-trains,model,model,/content/training-data/natural_language_inference/41/triples/model.txt
model,combining,Bidirectional and Auto - Regressive Transformers,model,/content/training-data/natural_language_inference/41/triples/model.txt
Contribution,has research problem,Sequence - to - Sequence Pre-training,research-problem,/content/training-data/natural_language_inference/41/triples/research-problem.txt
Contribution,has research problem,pretraining sequence - to - sequence models,research-problem,/content/training-data/natural_language_inference/41/triples/research-problem.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/31/triples/ablation-analysis.txt
Ablation analysis,has,second baseline,ablation-analysis,/content/training-data/natural_language_inference/31/triples/ablation-analysis.txt
second baseline,show,vanilla residual connections,ablation-analysis,/content/training-data/natural_language_inference/31/triples/ablation-analysis.txt
vanilla residual connections,without direct access to,original pointwise features,ablation-analysis,/content/training-data/natural_language_inference/31/triples/ablation-analysis.txt
vanilla residual connections,are,not enough,ablation-analysis,/content/training-data/natural_language_inference/31/triples/ablation-analysis.txt
not enough,to model,relations,ablation-analysis,/content/training-data/natural_language_inference/31/triples/ablation-analysis.txt
relations,in,many text matching tasks,ablation-analysis,/content/training-data/natural_language_inference/31/triples/ablation-analysis.txt
Ablation analysis,has,first ablation baseline,ablation-analysis,/content/training-data/natural_language_inference/31/triples/ablation-analysis.txt
first ablation baseline,shows,without richer features,ablation-analysis,/content/training-data/natural_language_inference/31/triples/ablation-analysis.txt
without richer features,as,alignment input,ablation-analysis,/content/training-data/natural_language_inference/31/triples/ablation-analysis.txt
without richer features,has,performance,ablation-analysis,/content/training-data/natural_language_inference/31/triples/ablation-analysis.txt
performance,has,degrades significantly,ablation-analysis,/content/training-data/natural_language_inference/31/triples/ablation-analysis.txt
performance,on,all datasets,ablation-analysis,/content/training-data/natural_language_inference/31/triples/ablation-analysis.txt
Ablation analysis,has,simpler implementation,ablation-analysis,/content/training-data/natural_language_inference/31/triples/ablation-analysis.txt
simpler implementation,leads to,evidently worse performance,ablation-analysis,/content/training-data/natural_language_inference/31/triples/ablation-analysis.txt
simpler implementation,of,fusion layer,ablation-analysis,/content/training-data/natural_language_inference/31/triples/ablation-analysis.txt
Ablation analysis,see that,parallel blocks,ablation-analysis,/content/training-data/natural_language_inference/31/triples/ablation-analysis.txt
parallel blocks,perform,worse,ablation-analysis,/content/training-data/natural_language_inference/31/triples/ablation-analysis.txt
worse,than,stacked blocks,ablation-analysis,/content/training-data/natural_language_inference/31/triples/ablation-analysis.txt
worse,supports,preference,ablation-analysis,/content/training-data/natural_language_inference/31/triples/ablation-analysis.txt
preference,for,deeper models,ablation-analysis,/content/training-data/natural_language_inference/31/triples/ablation-analysis.txt
deeper models,over,wider ones,ablation-analysis,/content/training-data/natural_language_inference/31/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/31/triples/model.txt
Model,has,components,model,/content/training-data/natural_language_inference/31/triples/model.txt
components,has,contextual features,model,/content/training-data/natural_language_inference/31/triples/model.txt
contextual features,name,Encoded vectors,model,/content/training-data/natural_language_inference/31/triples/model.txt
components,has,previous aligned features,model,/content/training-data/natural_language_inference/31/triples/model.txt
previous aligned features,name,Residual vectors,model,/content/training-data/natural_language_inference/31/triples/model.txt
components,has,original point - wise features,model,/content/training-data/natural_language_inference/31/triples/model.txt
original point - wise features,name,Embedding vectors,model,/content/training-data/natural_language_inference/31/triples/model.txt
Model,has,same - structured blocks,model,/content/training-data/natural_language_inference/31/triples/model.txt
same - structured blocks,process,sequences,model,/content/training-data/natural_language_inference/31/triples/model.txt
sequences,has,consecutively,model,/content/training-data/natural_language_inference/31/triples/model.txt
same - structured blocks,consisting of,encoding,model,/content/training-data/natural_language_inference/31/triples/model.txt
same - structured blocks,consisting of,alignment,model,/content/training-data/natural_language_inference/31/triples/model.txt
same - structured blocks,consisting of,fusion layers,model,/content/training-data/natural_language_inference/31/triples/model.txt
same - structured blocks,connected by,augmented version of residual connections,model,/content/training-data/natural_language_inference/31/triples/model.txt
Model,has,implementation,model,/content/training-data/natural_language_inference/31/triples/model.txt
implementation,of,each layer,model,/content/training-data/natural_language_inference/31/triples/model.txt
each layer,kept,as simple as possible,model,/content/training-data/natural_language_inference/31/triples/model.txt
Model,has,pooling layer,model,/content/training-data/natural_language_inference/31/triples/model.txt
pooling layer,aggregates,sequential representations,model,/content/training-data/natural_language_inference/31/triples/model.txt
sequential representations,into,vectors,model,/content/training-data/natural_language_inference/31/triples/model.txt
vectors,processed by,prediction layer,model,/content/training-data/natural_language_inference/31/triples/model.txt
prediction layer,to give,final prediction,model,/content/training-data/natural_language_inference/31/triples/model.txt
Model,has,embedding layer,model,/content/training-data/natural_language_inference/31/triples/model.txt
embedding layer,embeds,discrete tokens,model,/content/training-data/natural_language_inference/31/triples/model.txt
Model,presents,RE2,model,/content/training-data/natural_language_inference/31/triples/model.txt
RE2,has,fast and strong neural architecture,model,/content/training-data/natural_language_inference/31/triples/model.txt
fast and strong neural architecture,with,multiple alignment processes,model,/content/training-data/natural_language_inference/31/triples/model.txt
fast and strong neural architecture,for,general purpose text matching,model,/content/training-data/natural_language_inference/31/triples/model.txt
Contribution,has research problem,Text Matching,research-problem,/content/training-data/natural_language_inference/31/triples/research-problem.txt
Contribution,has,Experimental Setup,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
Experimental Setup,use,"Adam optimizer ( Kingma and Ba , 2015 ",experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
Experimental Setup,use,exponentially decaying learning rate,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
exponentially decaying learning rate,with,linear warmup,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
Experimental Setup,train on,Nvidia P100 GPUs,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
Experimental Setup,scale,summation,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
summation,in,augmented residual connections,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
summation,by,1 / ? 2,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
1 / ? 2,when,n ? 3,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
1 / ? 2,to preserve,variance,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
variance,under,assumption,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
assumption,that,two addends,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
two addends,have,same variance,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
Experimental Setup,implement,model,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
model,with,TensorFlow,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
Experimental Setup,has,kernel size,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
kernel size,of,convolutional encoder,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
kernel size,set to,3,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
Experimental Setup,has,initial learning rate,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
initial learning rate,tuned from,0.0001 to 0.003,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
Experimental Setup,has,prediction layer,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
prediction layer,is,two - layer feed - forward network,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
Experimental Setup,has,batch size,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
batch size,tuned from,64 to 512,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
Experimental Setup,has,threshold,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
threshold,for,gradient clipping,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
gradient clipping,set to,5,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
Experimental Setup,has,hidden size,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
hidden size,set to,150,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
Experimental Setup,has,number of blocks,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
number of blocks,tuned in,range,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
range,from,1 to 3,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
Experimental Setup,has,Glo Ve word vectors,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
Glo Ve word vectors,fixed during,training,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
Experimental Setup,has,Embeddings,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
Embeddings,of,out - ofvocabulary words,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
Embeddings,initialized to,zeros,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
Experimental Setup,has,Activations,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
Activations,in,all feed - forward networks,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
Activations,are,GeLU activations,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
Experimental Setup,has,All other parameters,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
All other parameters,are,normalized,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
normalized,by,weight normalization,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
All other parameters,are,initialized,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
initialized,with,He initialization,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
Experimental Setup,has,number of layers,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
number of layers,of,convolutional encoder,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
convolutional encoder,tuned from,1,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
1,to,3,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
Experimental Setup,has,Word embeddings,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
Word embeddings,initialized with,840B - 300d,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
Experimental Setup,has,Dropout,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
Dropout,with,keep probability,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
keep probability,applied before,fully - connected,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
keep probability,applied before,convolutional layer,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
keep probability,of,0.8,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
Experimental Setup,tokenize,sentences,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
sentences,with,NLTK toolkit,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
sentences,convert,lower cases,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
sentences,remove,all punctuations,experimental-setup,/content/training-data/natural_language_inference/31/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/31/triples/results.txt
Results,has,method,results,/content/training-data/natural_language_inference/31/triples/results.txt
method,perform,well,results,/content/training-data/natural_language_inference/31/triples/results.txt
well,in,answer selection task,results,/content/training-data/natural_language_inference/31/triples/results.txt
well,without,any taskspecific modifications,results,/content/training-data/natural_language_inference/31/triples/results.txt
Results,on,WikiQA dataset,results,/content/training-data/natural_language_inference/31/triples/results.txt
WikiQA dataset,obtain,result,results,/content/training-data/natural_language_inference/31/triples/results.txt
result,on par with,state - of - the - art,results,/content/training-data/natural_language_inference/31/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/natural_language_inference/50/triples/baselines.txt
Baselines,has,YahooCQA,baselines,/content/training-data/natural_language_inference/50/triples/baselines.txt
YahooCQA,has,implementation,baselines,/content/training-data/natural_language_inference/50/triples/baselines.txt
implementation,of,Convolutional Neural Tensor Network,baselines,/content/training-data/natural_language_inference/50/triples/baselines.txt
implementation,of,vanilla CNN model,baselines,/content/training-data/natural_language_inference/50/triples/baselines.txt
implementation,of,Okapi BM - 25 benchmark,baselines,/content/training-data/natural_language_inference/50/triples/baselines.txt
YahooCQA,has,key competitors,baselines,/content/training-data/natural_language_inference/50/triples/baselines.txt
key competitors,are,Neural Tensor LSTM ( NTN - LSTM ,baselines,/content/training-data/natural_language_inference/50/triples/baselines.txt
key competitors,are,HD - LSTM,baselines,/content/training-data/natural_language_inference/50/triples/baselines.txt
HD - LSTM,from,Tay et al.,baselines,/content/training-data/natural_language_inference/50/triples/baselines.txt
Baselines,has,WikiQA,baselines,/content/training-data/natural_language_inference/50/triples/baselines.txt
WikiQA,has,key competitors,baselines,/content/training-data/natural_language_inference/50/triples/baselines.txt
key competitors,are,Paragraph Vector ( PV ,baselines,/content/training-data/natural_language_inference/50/triples/baselines.txt
key competitors,are,PV + Cnt models,baselines,/content/training-data/natural_language_inference/50/triples/baselines.txt
PV + Cnt models,of,Le and Mikolv,baselines,/content/training-data/natural_language_inference/50/triples/baselines.txt
key competitors,are,CNN + Cnt model,baselines,/content/training-data/natural_language_inference/50/triples/baselines.txt
CNN + Cnt model,from,Yu et al.,baselines,/content/training-data/natural_language_inference/50/triples/baselines.txt
key competitors,are,LCLR ( Yih et al . ,baselines,/content/training-data/natural_language_inference/50/triples/baselines.txt
WikiQA,compare with,AP - CNN,baselines,/content/training-data/natural_language_inference/50/triples/baselines.txt
WikiQA,compare with,QA - BiLSTM / CNN,baselines,/content/training-data/natural_language_inference/50/triples/baselines.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/natural_language_inference/50/triples/hyperparameters.txt
Hyperparameters,adopt,AdaGrad optimizer,hyperparameters,/content/training-data/natural_language_inference/50/triples/hyperparameters.txt
AdaGrad optimizer,with,initial learning rate,hyperparameters,/content/training-data/natural_language_inference/50/triples/hyperparameters.txt
initial learning rate,tuned amongst,"{ 0.2 , 0.1 , 0.05 , 0.01 }",hyperparameters,/content/training-data/natural_language_inference/50/triples/hyperparameters.txt
Hyperparameters,has,L2 regularization,hyperparameters,/content/training-data/natural_language_inference/50/triples/hyperparameters.txt
L2 regularization,is,tuned,hyperparameters,/content/training-data/natural_language_inference/50/triples/hyperparameters.txt
tuned,amongst,"{ 0.001 , 0.0001 , 0.00001 }",hyperparameters,/content/training-data/natural_language_inference/50/triples/hyperparameters.txt
Hyperparameters,has,negative sampling rate,hyperparameters,/content/training-data/natural_language_inference/50/triples/hyperparameters.txt
negative sampling rate,is,tuned,hyperparameters,/content/training-data/natural_language_inference/50/triples/hyperparameters.txt
tuned,from,2 to 8,hyperparameters,/content/training-data/natural_language_inference/50/triples/hyperparameters.txt
Hyperparameters,has,batch size,hyperparameters,/content/training-data/natural_language_inference/50/triples/hyperparameters.txt
batch size,is,tuned,hyperparameters,/content/training-data/natural_language_inference/50/triples/hyperparameters.txt
tuned,amongst,"{ 50 , 100 , 200 }",hyperparameters,/content/training-data/natural_language_inference/50/triples/hyperparameters.txt
Hyperparameters,has,Models,hyperparameters,/content/training-data/natural_language_inference/50/triples/hyperparameters.txt
Models,trained for,25 epochs,hyperparameters,/content/training-data/natural_language_inference/50/triples/hyperparameters.txt
Models,has,model parameters,hyperparameters,/content/training-data/natural_language_inference/50/triples/hyperparameters.txt
model parameters,are,saved,hyperparameters,/content/training-data/natural_language_inference/50/triples/hyperparameters.txt
saved,each time,performance,hyperparameters,/content/training-data/natural_language_inference/50/triples/hyperparameters.txt
performance,is,topped,hyperparameters,/content/training-data/natural_language_inference/50/triples/hyperparameters.txt
performance,on,validation set,hyperparameters,/content/training-data/natural_language_inference/50/triples/hyperparameters.txt
Hyperparameters,has,Hyper QA,hyperparameters,/content/training-data/natural_language_inference/50/triples/hyperparameters.txt
Hyper QA,implemented in,Tensor - Flow,hyperparameters,/content/training-data/natural_language_inference/50/triples/hyperparameters.txt
Hyperparameters,has,dimension,hyperparameters,/content/training-data/natural_language_inference/50/triples/hyperparameters.txt
dimension,of,projection layer,hyperparameters,/content/training-data/natural_language_inference/50/triples/hyperparameters.txt
projection layer,is,tuned,hyperparameters,/content/training-data/natural_language_inference/50/triples/hyperparameters.txt
tuned,amongst,"{ 100 , 200 , 300 , 400 }",hyperparameters,/content/training-data/natural_language_inference/50/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/50/triples/model.txt
Model,has,neural ranking,model,/content/training-data/natural_language_inference/50/triples/model.txt
neural ranking,models,relationships,model,/content/training-data/natural_language_inference/50/triples/model.txt
relationships,between,QA pairs,model,/content/training-data/natural_language_inference/50/triples/model.txt
QA pairs,in,Hyperbolic space,model,/content/training-data/natural_language_inference/50/triples/model.txt
Hyperbolic space,instead of,Euclidean space,model,/content/training-data/natural_language_inference/50/triples/model.txt
Model,has,Hyperbolic space,model,/content/training-data/natural_language_inference/50/triples/model.txt
Hyperbolic space,is,embedding space,model,/content/training-data/natural_language_inference/50/triples/model.txt
embedding space,with,constant negative curvature,model,/content/training-data/natural_language_inference/50/triples/model.txt
constant negative curvature,in which,distance,model,/content/training-data/natural_language_inference/50/triples/model.txt
distance,is,increasing exponentially,model,/content/training-data/natural_language_inference/50/triples/model.txt
distance,towards,border,model,/content/training-data/natural_language_inference/50/triples/model.txt
Model,propose,extremely simple neural ranking model,model,/content/training-data/natural_language_inference/50/triples/model.txt
extremely simple neural ranking model,for,question answering,model,/content/training-data/natural_language_inference/50/triples/model.txt
Contribution,has research problem,Neural Question Answering,research-problem,/content/training-data/natural_language_inference/50/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/50/triples/results.txt
Results,on,TrecQA ( raw ,results,/content/training-data/natural_language_inference/50/triples/results.txt
TrecQA ( raw ),has,Hyper QA,results,/content/training-data/natural_language_inference/50/triples/results.txt
Hyper QA,achieves,competitive performance,results,/content/training-data/natural_language_inference/50/triples/results.txt
competitive performance,on,MAP and MRR metrics,results,/content/training-data/natural_language_inference/50/triples/results.txt
Hyper QA,has,outperforms,results,/content/training-data/natural_language_inference/50/triples/results.txt
outperforms,has,basic CNN model of ( S&M ,results,/content/training-data/natural_language_inference/50/triples/results.txt
basic CNN model of ( S&M ),by,2 % ? 3 %,results,/content/training-data/natural_language_inference/50/triples/results.txt
basic CNN model of ( S&M ),in terms of,MAP / MRR,results,/content/training-data/natural_language_inference/50/triples/results.txt
Results,on,TrecQA ( clean ,results,/content/training-data/natural_language_inference/50/triples/results.txt
TrecQA ( clean ),has,Hyper QA,results,/content/training-data/natural_language_inference/50/triples/results.txt
Hyper QA,has,outperforms,results,/content/training-data/natural_language_inference/50/triples/results.txt
outperforms,has,MP - CNN,results,/content/training-data/natural_language_inference/50/triples/results.txt
outperforms,has,AP - CNN,results,/content/training-data/natural_language_inference/50/triples/results.txt
outperforms,has,QA - CNN,results,/content/training-data/natural_language_inference/50/triples/results.txt
Results,on,SemEvalCQA,results,/content/training-data/natural_language_inference/50/triples/results.txt
SemEvalCQA,has,performance,results,/content/training-data/natural_language_inference/50/triples/results.txt
performance,of,our model,results,/content/training-data/natural_language_inference/50/triples/results.txt
our model,is,marginally short,results,/content/training-data/natural_language_inference/50/triples/results.txt
marginally short,from,best performing model,results,/content/training-data/natural_language_inference/50/triples/results.txt
our model,on,MAP,results,/content/training-data/natural_language_inference/50/triples/results.txt
SemEvalCQA,has,proposed approach,results,/content/training-data/natural_language_inference/50/triples/results.txt
proposed approach,achieves,highly competitive performance,results,/content/training-data/natural_language_inference/50/triples/results.txt
SemEvalCQA,obtained,best P@1 performance over all,results,/content/training-data/natural_language_inference/50/triples/results.txt
best P@1 performance over all,has,outperforming,results,/content/training-data/natural_language_inference/50/triples/results.txt
outperforming,has,state - of - the - art AI - CNN model,results,/content/training-data/natural_language_inference/50/triples/results.txt
state - of - the - art AI - CNN model,by,3 %,results,/content/training-data/natural_language_inference/50/triples/results.txt
3 %,in terms of,P@1,results,/content/training-data/natural_language_inference/50/triples/results.txt
Contribution,has,Experiments,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
Experiments,has,Tasks,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
Tasks,has,Multi-mention Reading Comprehension,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
Multi-mention Reading Comprehension,has,Experimental setup,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
Experimental setup,use,uncased version,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
uncased version,of,BERT base,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
Experimental setup,use,batch size,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
batch size,of,192,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
192,for,two open - domain QA tasks,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
batch size,of,20,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
20,for,two reading comprehension tasks,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
Experimental setup,For,opendomain QA tasks,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
opendomain QA tasks,retrieve,50 Wikipedia articles,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
50 Wikipedia articles,through,TF - IDF,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
Experimental setup,try,"10 , 20 , 40 and 80 paragraphs",experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
"10 , 20 , 40 and 80 paragraphs",to choose,number of paragraphs,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
number of paragraphs,to use on,test set,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
"10 , 20 , 40 and 80 paragraphs",on,development set,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
Experimental setup,To avoid,local optima,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
local optima,perform,annealing,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
Multi-mention Reading Comprehension,has,Results,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
Results,has,our method,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
our method,achieves,new state - of the - art,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
new state - of the - art,on,NARRATIVEQA,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
new state - of the - art,on,TRIVIAQA - OPEN,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
new state - of the - art,on,NATURALQUESTIONS - OPEN,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
our method,achieves,comparable,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
comparable,to,state - of - the - art,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
comparable,on,TRIVIAQA,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
Results,has,MML,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
MML,achieves,comparable result,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
comparable result,to,First - Only baseline,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
Results,has,our learning method,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
our learning method,has,outperforms,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
outperforms,by,2 + F1 / ROUGE - L / EM,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
outperforms,on,all datasets,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
Results,observe,First - Only,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
First - Only,is,strong baseline,experiments,/content/training-data/natural_language_inference/92/triples/experiments.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/92/triples/model.txt
Model,formulate,wide range of weakly supervised QA tasks,model,/content/training-data/natural_language_inference/92/triples/model.txt
wide range of weakly supervised QA tasks,as,discrete latent - variable learning problems,model,/content/training-data/natural_language_inference/92/triples/model.txt
Model,model,set of possible solutions,model,/content/training-data/natural_language_inference/92/triples/model.txt
set of possible solutions,as,discrete latent variable,model,/content/training-data/natural_language_inference/92/triples/model.txt
Model,has,learning challenge,model,/content/training-data/natural_language_inference/92/triples/model.txt
learning challenge,to determine,which solution,model,/content/training-data/natural_language_inference/92/triples/model.txt
which solution,in,set,model,/content/training-data/natural_language_inference/92/triples/model.txt
which solution,is,correct one,model,/content/training-data/natural_language_inference/92/triples/model.txt
correct one,while estimating,complete QA model,model,/content/training-data/natural_language_inference/92/triples/model.txt
Model,has,hard updates,model,/content/training-data/natural_language_inference/92/triples/model.txt
hard updates,has,strongly enforce,model,/content/training-data/natural_language_inference/92/triples/model.txt
strongly enforce,has,prior beliefs,model,/content/training-data/natural_language_inference/92/triples/model.txt
prior beliefs,there is,single correct solution,model,/content/training-data/natural_language_inference/92/triples/model.txt
Model,develop,learning strategy,model,/content/training-data/natural_language_inference/92/triples/model.txt
learning strategy,uses,hard - EM - style parameter updates,model,/content/training-data/natural_language_inference/92/triples/model.txt
Model,updates,model parameters,model,/content/training-data/natural_language_inference/92/triples/model.txt
model parameters,to further encourage,own prediction,model,/content/training-data/natural_language_inference/92/triples/model.txt
Model,predicts,most likely solution,model,/content/training-data/natural_language_inference/92/triples/model.txt
most likely solution,according to,current model,model,/content/training-data/natural_language_inference/92/triples/model.txt
current model,from,precomputed set,model,/content/training-data/natural_language_inference/92/triples/model.txt
Contribution,has research problem,Weakly Supervised Question Answering,research-problem,/content/training-data/natural_language_inference/92/triples/research-problem.txt
Contribution,has research problem,question answering ( QA ,research-problem,/content/training-data/natural_language_inference/92/triples/research-problem.txt
Contribution,has research problem,QA,research-problem,/content/training-data/natural_language_inference/92/triples/research-problem.txt
Contribution,has,Dataset,datase,/content/training-data/natural_language_inference/72/triples/dataset.txt
Dataset,contains,long support passages,datase,/content/training-data/natural_language_inference/72/triples/dataset.txt
long support passages,includes,answers,datase,/content/training-data/natural_language_inference/72/triples/dataset.txt
answers,which are,single - or multiword medical entities,datase,/content/training-data/natural_language_inference/72/triples/dataset.txt
Dataset,contains,"around 100,000 queries",datase,/content/training-data/natural_language_inference/72/triples/dataset.txt
"around 100,000 queries",on,"12,000 case reports",datase,/content/training-data/natural_language_inference/72/triples/dataset.txt
Dataset,use,learning points,datase,/content/training-data/natural_language_inference/72/triples/dataset.txt
learning points,to create,queries,datase,/content/training-data/natural_language_inference/72/triples/dataset.txt
queries,by blanking out,medical entity,datase,/content/training-data/natural_language_inference/72/triples/dataset.txt
Dataset,construct,"queries , answers and supporting passages",datase,/content/training-data/natural_language_inference/72/triples/dataset.txt
"queries , answers and supporting passages",from,BMJ Case Reports,datase,/content/training-data/natural_language_inference/72/triples/dataset.txt
Dataset,has,case report,datase,/content/training-data/natural_language_inference/72/triples/dataset.txt
case report,contains,Learning points section,datase,/content/training-data/natural_language_inference/72/triples/dataset.txt
Learning points section,summarizing,key pieces,datase,/content/training-data/natural_language_inference/72/triples/dataset.txt
key pieces,of,information,datase,/content/training-data/natural_language_inference/72/triples/dataset.txt
case report,is,detailed description,datase,/content/training-data/natural_language_inference/72/triples/dataset.txt
detailed description,of,clinical case,datase,/content/training-data/natural_language_inference/72/triples/dataset.txt
clinical case,focuses on,rare diseases,datase,/content/training-data/natural_language_inference/72/triples/dataset.txt
clinical case,focuses on,unusual presentation of common conditions,datase,/content/training-data/natural_language_inference/72/triples/dataset.txt
clinical case,focuses on,novel treatment methods,datase,/content/training-data/natural_language_inference/72/triples/dataset.txt
Contribution,has,Baselines,baselines,/content/training-data/natural_language_inference/72/triples/baselines.txt
Baselines,include,distance - based method,baselines,/content/training-data/natural_language_inference/72/triples/baselines.txt
distance - based method,that uses,word embeddings,baselines,/content/training-data/natural_language_inference/72/triples/baselines.txt
Baselines,trained,4 - gram Kneser - Ney model,baselines,/content/training-data/natural_language_inference/72/triples/baselines.txt
4 - gram Kneser - Ney model,on,CliCR training data,baselines,/content/training-data/natural_language_inference/72/triples/baselines.txt
CliCR training data,with,multi-word entities,baselines,/content/training-data/natural_language_inference/72/triples/baselines.txt
multi-word entities,represented as,single token,baselines,/content/training-data/natural_language_inference/72/triples/baselines.txt
CliCR training data,using,SRILM,baselines,/content/training-data/natural_language_inference/72/triples/baselines.txt
Contribution,has research problem,Machine Reading Comprehension,research-problem,/content/training-data/natural_language_inference/72/triples/research-problem.txt
Contribution,has research problem,machine comprehension,research-problem,/content/training-data/natural_language_inference/72/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/72/triples/results.txt
Results,observed that,GA - NoEnt,results,/content/training-data/natural_language_inference/72/triples/results.txt
GA - NoEnt,tends to predict,longer answers,results,/content/training-data/natural_language_inference/72/triples/results.txt
longer answers,than,GA - Ent / Anonym,results,/content/training-data/natural_language_inference/72/triples/results.txt
Results,see that,answer prediction,results,/content/training-data/natural_language_inference/72/triples/results.txt
answer prediction,based on,contextual representation,results,/content/training-data/natural_language_inference/72/triples/results.txt
contextual representation,of,queries and passages ( sim -entity ,results,/content/training-data/natural_language_inference/72/triples/results.txt
answer prediction,achieves,strong base performance,results,/content/training-data/natural_language_inference/72/triples/results.txt
strong base performance,that is,outperformed,results,/content/training-data/natural_language_inference/72/triples/results.txt
outperformed,by,GA,results,/content/training-data/natural_language_inference/72/triples/results.txt
Results,has,embedding - metric score,results,/content/training-data/natural_language_inference/72/triples/results.txt
embedding - metric score,is,higher,results,/content/training-data/natural_language_inference/72/triples/results.txt
Results,has,language model,results,/content/training-data/natural_language_inference/72/triples/results.txt
language model,performs,poorly,results,/content/training-data/natural_language_inference/72/triples/results.txt
poorly,on,EM and F1,results,/content/training-data/natural_language_inference/72/triples/results.txt
Results,has,GA reader,results,/content/training-data/natural_language_inference/72/triples/results.txt
GA reader,performs,well,results,/content/training-data/natural_language_inference/72/triples/results.txt
well,across,all entity set - ups,results,/content/training-data/natural_language_inference/72/triples/results.txt
Results,has,SA reader,results,/content/training-data/natural_language_inference/72/triples/results.txt
SA reader,are,far below,results,/content/training-data/natural_language_inference/72/triples/results.txt
far below,has,per-formance,results,/content/training-data/natural_language_inference/72/triples/results.txt
per-formance,of,GA reader,results,/content/training-data/natural_language_inference/72/triples/results.txt
GA reader,performs,much better,results,/content/training-data/natural_language_inference/72/triples/results.txt
much better,than on,non-anonymized ones,results,/content/training-data/natural_language_inference/72/triples/results.txt
much better,on,anonymized entities,results,/content/training-data/natural_language_inference/72/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/38/triples/ablation-analysis.txt
Ablation analysis,of,single model,ablation-analysis,/content/training-data/natural_language_inference/38/triples/ablation-analysis.txt
single model,has,contribute,ablation-analysis,/content/training-data/natural_language_inference/38/triples/ablation-analysis.txt
contribute,towards,model 's performance,ablation-analysis,/content/training-data/natural_language_inference/38/triples/ablation-analysis.txt
model 's performance,has,POS tags,ablation-analysis,/content/training-data/natural_language_inference/38/triples/ablation-analysis.txt
POS tags,to be,more important,ablation-analysis,/content/training-data/natural_language_inference/38/triples/ablation-analysis.txt
model 's performance,both,syntactic embeddings,ablation-analysis,/content/training-data/natural_language_inference/38/triples/ablation-analysis.txt
model 's performance,both,semantic embeddings,ablation-analysis,/content/training-data/natural_language_inference/38/triples/ablation-analysis.txt
single model,on,SQ u AD dev set,ablation-analysis,/content/training-data/natural_language_inference/38/triples/ablation-analysis.txt
Ablation analysis,For ablating,integral query matching,ablation-analysis,/content/training-data/natural_language_inference/38/triples/ablation-analysis.txt
integral query matching,shows that,integral information,ablation-analysis,/content/training-data/natural_language_inference/38/triples/ablation-analysis.txt
integral information,of,query,ablation-analysis,/content/training-data/natural_language_inference/38/triples/ablation-analysis.txt
query,for,each word,ablation-analysis,/content/training-data/natural_language_inference/38/triples/ablation-analysis.txt
each word,in,passage,ablation-analysis,/content/training-data/natural_language_inference/38/triples/ablation-analysis.txt
each word,is,crucial,ablation-analysis,/content/training-data/natural_language_inference/38/triples/ablation-analysis.txt
integral query matching,has,result,ablation-analysis,/content/training-data/natural_language_inference/38/triples/ablation-analysis.txt
result,has,drops,ablation-analysis,/content/training-data/natural_language_inference/38/triples/ablation-analysis.txt
drops,about,2 %,ablation-analysis,/content/training-data/natural_language_inference/38/triples/ablation-analysis.txt
Ablation analysis,For,context - based similarity matching,ablation-analysis,/content/training-data/natural_language_inference/38/triples/ablation-analysis.txt
context - based similarity matching,took out,M 3,ablation-analysis,/content/training-data/natural_language_inference/38/triples/ablation-analysis.txt
M 3,proved to be,contributory,ablation-analysis,/content/training-data/natural_language_inference/38/triples/ablation-analysis.txt
contributory,to,performance,ablation-analysis,/content/training-data/natural_language_inference/38/triples/ablation-analysis.txt
performance,of,full - orientation matching,ablation-analysis,/content/training-data/natural_language_inference/38/triples/ablation-analysis.txt
M 3,from,linear function,ablation-analysis,/content/training-data/natural_language_inference/38/triples/ablation-analysis.txt
Ablation analysis,has,query - based similarity matching,ablation-analysis,/content/training-data/natural_language_inference/38/triples/ablation-analysis.txt
query - based similarity matching,accounts for,about 10 % performance degradation,ablation-analysis,/content/training-data/natural_language_inference/38/triples/ablation-analysis.txt
about 10 % performance degradation,proves,effectiveness,ablation-analysis,/content/training-data/natural_language_inference/38/triples/ablation-analysis.txt
effectiveness,of,alignment context words,ablation-analysis,/content/training-data/natural_language_inference/38/triples/ablation-analysis.txt
alignment context words,against,query,ablation-analysis,/content/training-data/natural_language_inference/38/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/38/triples/model.txt
Model,introduce,Multi - layer Embedding with Memory Networks ( MEMEN ,model,/content/training-data/natural_language_inference/38/triples/model.txt
Multi - layer Embedding with Memory Networks ( MEMEN ),has,end - to - end neural network,model,/content/training-data/natural_language_inference/38/triples/model.txt
end - to - end neural network,for,machine comprehension task,model,/content/training-data/natural_language_inference/38/triples/model.txt
Model,consists of,three parts,model,/content/training-data/natural_language_inference/38/triples/model.txt
three parts,has,high - efficiency multilayer memory network,model,/content/training-data/natural_language_inference/38/triples/model.txt
high - efficiency multilayer memory network,of,full - orientation matching,model,/content/training-data/natural_language_inference/38/triples/model.txt
full - orientation matching,to match,question and context,model,/content/training-data/natural_language_inference/38/triples/model.txt
three parts,has,pointer - network based answer boundary prediction layer,model,/content/training-data/natural_language_inference/38/triples/model.txt
pointer - network based answer boundary prediction layer,to get,location,model,/content/training-data/natural_language_inference/38/triples/model.txt
location,of,answer,model,/content/training-data/natural_language_inference/38/triples/model.txt
answer,in,passage,model,/content/training-data/natural_language_inference/38/triples/model.txt
three parts,has,encoding,model,/content/training-data/natural_language_inference/38/triples/model.txt
encoding,in which,add,model,/content/training-data/natural_language_inference/38/triples/model.txt
add,has,useful syntactic and semantic information,model,/content/training-data/natural_language_inference/38/triples/model.txt
useful syntactic and semantic information,in,embedding,model,/content/training-data/natural_language_inference/38/triples/model.txt
embedding,of,every word,model,/content/training-data/natural_language_inference/38/triples/model.txt
encoding,of,context and query,model,/content/training-data/natural_language_inference/38/triples/model.txt
Contribution,has research problem,Machine Comprehension,research-problem,/content/training-data/natural_language_inference/38/triples/research-problem.txt
Contribution,has research problem,Machine comprehension ( MC ) style question answering,research-problem,/content/training-data/natural_language_inference/38/triples/research-problem.txt
Contribution,has research problem,Machine comprehension ( MC ,research-problem,/content/training-data/natural_language_inference/38/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
Experimental setup,to transform,passage and question,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
passage and question,in,Stanford CoreNLP utilities,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
Stanford CoreNLP utilities,use,part - of - speech tagger,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
Stanford CoreNLP utilities,use,named - entity recognition tagger,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
Experimental setup,set,hidden size,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
hidden size,as,100,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
100,for,all the LSTM and GRU layers,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
Experimental setup,apply,dropout,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
dropout,between,layers,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
layers,with,dropout ratio,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
dropout ratio,as,0.2,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
Experimental setup,use,100 one dimensional,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
100 one dimensional,has,filters,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
filters,with,width,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
width,of,5,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
5,for,each one,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
filters,for,CNN,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
CNN,in,character level embedding,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
Experimental setup,use,"AdaDelta ( Zeiler , 2012 ) optimizer",experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
"AdaDelta ( Zeiler , 2012 ) optimizer",with,initial learning rate,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
initial learning rate,as,0.001,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
Experimental setup,For,memory networks,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
memory networks,set,number of layer,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
number of layer,as,3,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
Experimental setup,For,skip - gram model,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
skip - gram model,has,our model,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
our model,refers to,word2 vec module,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
word2 vec module,in,open source software library,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
open source software library,name,Tensorflow,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
word2 vec module,has,skip window,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
skip window,set as,2,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
Experimental setup,To improve,reliability and stabllity,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
reliability and stabllity,screen out,sentences,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
sentences,whose,length,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
length,are,shorter than 9,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
Experimental setup,has,tokenizers,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
tokenizers,use in,step,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
step,of,preprocessing,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
preprocessing,has,data,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
tokenizers,from,Stanford CoreNLP,experimental-setup,/content/training-data/natural_language_inference/38/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/natural_language_inference/38/triples/results.txt
Results,see,our model,results,/content/training-data/natural_language_inference/38/triples/results.txt
our model,achieves,state - of - the - art result,results,/content/training-data/natural_language_inference/38/triples/results.txt
state - of - the - art result,on,all subsets on TriviaQA,results,/content/training-data/natural_language_inference/38/triples/results.txt
our model,has,outperforms,results,/content/training-data/natural_language_inference/38/triples/results.txt
outperforms,has,all other baselines,results,/content/training-data/natural_language_inference/38/triples/results.txt
Results,use,Stanford Question Answering Dataset ( SQuAD ) v 1.1,results,/content/training-data/natural_language_inference/38/triples/results.txt
Stanford Question Answering Dataset ( SQuAD ) v 1.1,has,our model,results,/content/training-data/natural_language_inference/38/triples/results.txt
our model,is,competitive,results,/content/training-data/natural_language_inference/38/triples/results.txt
competitive,to,state - of - the - art method,results,/content/training-data/natural_language_inference/38/triples/results.txt
state - of - the - art method,achieves,exact match score,results,/content/training-data/natural_language_inference/38/triples/results.txt
exact match score,of,75.37 %,results,/content/training-data/natural_language_inference/38/triples/results.txt
state - of - the - art method,achieves,F1 score,results,/content/training-data/natural_language_inference/38/triples/results.txt
F1 score,of,82 . 66 %,results,/content/training-data/natural_language_inference/38/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/natural_language_inference/6/triples/ablation-analysis.txt
Ablation analysis,seen that,all tasks,ablation-analysis,/content/training-data/natural_language_inference/6/triples/ablation-analysis.txt
all tasks,benefit from,deeper models,ablation-analysis,/content/training-data/natural_language_inference/6/triples/ablation-analysis.txt
Ablation analysis,achieve,good convergence,ablation-analysis,/content/training-data/natural_language_inference/6/triples/ablation-analysis.txt
good convergence,with,deeper models,ablation-analysis,/content/training-data/natural_language_inference/6/triples/ablation-analysis.txt
Ablation analysis,has,Multitask learning,ablation-analysis,/content/training-data/natural_language_inference/6/triples/ablation-analysis.txt
Multitask learning,has,NLI objective,ablation-analysis,/content/training-data/natural_language_inference/6/triples/ablation-analysis.txt
NLI objective,leads to,better performance,ablation-analysis,/content/training-data/natural_language_inference/6/triples/ablation-analysis.txt
better performance,comes at,cost,ablation-analysis,/content/training-data/natural_language_inference/6/triples/ablation-analysis.txt
cost,of,worse cross - lingual transfer performance,ablation-analysis,/content/training-data/natural_language_inference/6/triples/ablation-analysis.txt
worse cross - lingual transfer performance,in,XNLI and Tatoeba,ablation-analysis,/content/training-data/natural_language_inference/6/triples/ablation-analysis.txt
better performance,on,English NLI test set,ablation-analysis,/content/training-data/natural_language_inference/6/triples/ablation-analysis.txt
Contribution,has,Experiments,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
Experiments,has,BUCC : bitext mining,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
BUCC : bitext mining,outperform,Artetxe and Schwenk ( 2018 ,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
BUCC : bitext mining,has,our system,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
our system,establishes,new state - of - the - art,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
new state - of - the - art,for,all language pairs,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
new state - of - the - art,with the exception of,English - Chinese test,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
Experiments,has,MLDoc : cross - lingual classification,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
MLDoc : cross - lingual classification,has,our system,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
our system,obtains,best published results,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
best published results,for,5 of the 7 transfer languages,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
Experiments,has,XNLI : cross - lingual NLI,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
XNLI : cross - lingual NLI,outperform,all baselines,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
all baselines,by,substantial margin,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
XNLI : cross - lingual NLI,achieve,remarkable good results,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
remarkable good results,on,low - resource languages,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
low - resource languages,like,Swahili,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
XNLI : cross - lingual NLI,has,Our proposed method,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
Our proposed method,obtains,best results,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
best results,in,zero - shot cross - lingual transfer,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
zero - shot cross - lingual transfer,for,all languages but Spanish,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
XNLI : cross - lingual NLI,has,transfer results,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
transfer results,are,strong and homogeneous,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
strong and homogeneous,across,all languages,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
XNLI : cross - lingual NLI,has,zero - short performance,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
zero - short performance,is,( at most ) 5 % lower,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
 at most ) 5 % lower,than,English,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
English,including,distant languages,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
distant languages,like,"Arabic , Chinese and Vietnamese",experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
Experiments,has,Tatoeba : similarity search,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
Tatoeba : similarity search,are,55,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
55,with,less than 20 %,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
Tatoeba : similarity search,are,48 languages,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
48 languages,with,error rate,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
error rate,has,below 10 %,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
Tatoeba : similarity search,are,15 languages,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
15 languages,with error rates,above 50 %,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
Tatoeba : similarity search,has,similarity error rates,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
similarity error rates,has,below 5 %,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
below 5 %,for,37 languages,experiments,/content/training-data/natural_language_inference/6/triples/experiments.txt
Contribution,Code,https://github.com / facebookresearch/LASER,code,/content/training-data/natural_language_inference/6/triples/code.txt
Contribution,has,Model,model,/content/training-data/natural_language_inference/6/triples/model.txt
Model,interested in,universal language agnostic sentence embeddings,model,/content/training-data/natural_language_inference/6/triples/model.txt
universal language agnostic sentence embeddings,that is,vector representations,model,/content/training-data/natural_language_inference/6/triples/model.txt
vector representations,of,sentences,model,/content/training-data/natural_language_inference/6/triples/model.txt
sentences,that are,general,model,/content/training-data/natural_language_inference/6/triples/model.txt
general,with respect to,two dimensions,model,/content/training-data/natural_language_inference/6/triples/model.txt
two dimensions,name,input language,model,/content/training-data/natural_language_inference/6/triples/model.txt
two dimensions,name,NLP task,model,/content/training-data/natural_language_inference/6/triples/model.txt
Model,train,single encoder,model,/content/training-data/natural_language_inference/6/triples/model.txt
single encoder,to handle,multiple languages,model,/content/training-data/natural_language_inference/6/triples/model.txt
Contribution,has research problem,Massively Multilingual Sentence Embeddings,research-problem,/content/training-data/natural_language_inference/6/triples/research-problem.txt
Contribution,has research problem,joint multilingual sentence representations,research-problem,/content/training-data/natural_language_inference/6/triples/research-problem.txt
Contribution,has,Baselines,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
Baselines,For,Gigaword dataset,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
Gigaword dataset,compare our models with,Feat2s,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
Feat2s,is an,RNN sequence - to - sequence model,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
RNN sequence - to - sequence model,with,lexical and statistical features,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
lexical and statistical features,in,encoder,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
Gigaword dataset,compare our models with,Luong - NMT,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
Luong - NMT,is a,two - layer LSTM encoder - decoder model,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
Gigaword dataset,compare our models with,RAS - Elman,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
RAS - Elman,uses,attentive CNN encoder,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
RAS - Elman,uses,Elman RNN decoder,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
Gigaword dataset,compare our models with,ABS +,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
ABS +,is a,fine tuned version of ABS,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
fine tuned version of ABS,uses,attentive CNN encoder,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
fine tuned version of ABS,uses,NNLM decoder,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
Gigaword dataset,compare our models with,SEASS,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
SEASS,uses,BiGRU encoders,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
SEASS,uses,GRU decoders,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
GRU decoders,with,selective encoding,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
Baselines,For,CNN dataset,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
CNN dataset,compare our models with,Distraction - M3,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
Distraction - M3,uses,sequence - to - sequence abstractive model,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
sequence - to - sequence abstractive model,with,distraction - based networks,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
CNN dataset,compare our models with,GBA,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
GBA,is a,graph - based attentional neural abstractive model,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
CNN dataset,compare our models with,Lead - 3,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
Lead - 3,extracts,first three sentences of the document,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
first three sentences of the document,as,summary,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
CNN dataset,compare our models with,Bi - GRU,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
Bi - GRU,is a,non-hierarchical one - layer sequence - to - sequence abstractive baseline,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
CNN dataset,compare our models with,LexRank,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
LexRank,extracts,texts,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
texts,using,LexRank,baselines,/content/training-data/text_summarization/4/triples/baselines.txt
Contribution,has,Model,model,/content/training-data/text_summarization/4/triples/model.txt
Model,using,entity encoders,model,/content/training-data/text_summarization/4/triples/model.txt
entity encoders,with,selective disambiguation,model,/content/training-data/text_summarization/4/triples/model.txt
entity encoders,constructing,topic vectors,model,/content/training-data/text_summarization/4/triples/model.txt
topic vectors,using,firm attention,model,/content/training-data/text_summarization/4/triples/model.txt
Model,method,effectively apply linked entities,model,/content/training-data/text_summarization/4/triples/model.txt
effectively apply linked entities,in,sequence - tosequence models,model,/content/training-data/text_summarization/4/triples/model.txt
sequence - tosequence models,called,Entity2Topic ( E2T ,model,/content/training-data/text_summarization/4/triples/model.txt
Model,module,E2T,model,/content/training-data/text_summarization/4/triples/model.txt
E2T,encodes,entities extracted from the original text,model,/content/training-data/text_summarization/4/triples/model.txt
entities extracted from the original text,by,entity linking system ( ELS ,model,/content/training-data/text_summarization/4/triples/model.txt
E2T,constructs,vector,model,/content/training-data/text_summarization/4/triples/model.txt
vector,representing,topic,model,/content/training-data/text_summarization/4/triples/model.txt
topic,of,summary to be generated,model,/content/training-data/text_summarization/4/triples/model.txt
E2T,informs,decoder,model,/content/training-data/text_summarization/4/triples/model.txt
decoder,about,constructed topic vector,model,/content/training-data/text_summarization/4/triples/model.txt
Contribution,has research problem,Abstractive Summarization,research-problem,/content/training-data/text_summarization/4/triples/research-problem.txt
Contribution,has research problem,Text summarization,research-problem,/content/training-data/text_summarization/4/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
Experimental setup,reduce,"size of the input , output , and entity vocabularies",experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
"size of the input , output , and entity vocabularies",to,at most 50 K,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
Experimental setup,set,batch sizes,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
batch sizes,of,Gigaword and CNN datasets,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
Gigaword and CNN datasets,to,80 and 10,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
Experimental setup,use,dropout,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
dropout,on,all non-linear connections,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
all non-linear connections,with,dropout rate of 0.5,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
Experimental setup,use,beam search,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
beam search,of size,10,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
10,to generate,summary,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
Experimental setup,replace,less frequent words,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
less frequent words,to,< unk >,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
Experimental setup,For,firm attention,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
firm attention,tuned,k,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
k,by calculating,perplexity,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
perplexity,of,model,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
model,starting with,"smaller values ( i.e. k = 1 , 2 , 5 , 10 , 20 , ... ",experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
model,stopping when,perplexity of the model becomes worse,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
perplexity of the model becomes worse,than,previous model,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
Experimental setup,For,CNN,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
CNN,set,"h = 3 , 4 , 5",experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
"h = 3 , 4 , 5",with,"400 , 300 , 300 feature maps",experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
Experimental setup,For,GRUs,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
GRUs,set,state size,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
state size,to,500,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
Experimental setup,Training is done via,stochastic gradient descent,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
stochastic gradient descent,over,shuffled mini-batches,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
shuffled mini-batches,with,Adadelta update rule,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
shuffled mini-batches,with,"l 2 constraint ( Hinton et al. , 2012 ) of 3",experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
Experimental setup,perform,early stopping,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
early stopping,using,subset of the given development dataset,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
Experimental setup,initialize,word and entity vectors,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
word and entity vectors,use,pre-trained vectors,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
pre-trained vectors,name,300D Glove,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
pre-trained vectors,name,1000D wiki2vec,experimental-setup,/content/training-data/text_summarization/4/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/text_summarization/4/triples/results.txt
Results,In,Gigaword dataset,results,/content/training-data/text_summarization/4/triples/results.txt
Gigaword dataset,where,texts are short,results,/content/training-data/text_summarization/4/triples/results.txt
Gigaword dataset,has,our best model,results,/content/training-data/text_summarization/4/triples/results.txt
our best model,achieves,comparable performance,results,/content/training-data/text_summarization/4/triples/results.txt
comparable performance,with,current state - of - the - art,results,/content/training-data/text_summarization/4/triples/results.txt
Results,In,CNN dataset,results,/content/training-data/text_summarization/4/triples/results.txt
CNN dataset,where,texts are longer,results,/content/training-data/text_summarization/4/triples/results.txt
CNN dataset,has,our best model,results,/content/training-data/text_summarization/4/triples/results.txt
our best model,outperforms,all the previous models,results,/content/training-data/text_summarization/4/triples/results.txt
Results,has,E2T,results,/content/training-data/text_summarization/4/triples/results.txt
E2T,achieves,significant improvement,results,/content/training-data/text_summarization/4/triples/results.txt
significant improvement,over,baseline model BASE,results,/content/training-data/text_summarization/4/triples/results.txt
baseline model BASE,with,at least 2 ROUGE,results,/content/training-data/text_summarization/4/triples/results.txt
at least 2 ROUGE,has,1 points increase,results,/content/training-data/text_summarization/4/triples/results.txt
1 points increase,in,Gigaword dataset,results,/content/training-data/text_summarization/4/triples/results.txt
baseline model BASE,with,6 ROUGE,results,/content/training-data/text_summarization/4/triples/results.txt
6 ROUGE,has,1 points increase,results,/content/training-data/text_summarization/4/triples/results.txt
1 points increase,in,CNN dataset,results,/content/training-data/text_summarization/4/triples/results.txt
Results,Among,model variants,results,/content/training-data/text_summarization/4/triples/results.txt
model variants,has,CNN - based encoder,results,/content/training-data/text_summarization/4/triples/results.txt
CNN - based encoder,with,selective disambiguation,results,/content/training-data/text_summarization/4/triples/results.txt
CNN - based encoder,with,firm attention,results,/content/training-data/text_summarization/4/triples/results.txt
CNN - based encoder,performs,best,results,/content/training-data/text_summarization/4/triples/results.txt
Contribution,has,Approach,approach,/content/training-data/text_summarization/8/triples/approach.txt
Approach,employ,masking,approach,/content/training-data/text_summarization/8/triples/approach.txt
masking,to constrain,copying words,approach,/content/training-data/text_summarization/8/triples/approach.txt
copying words,to,selected parts,approach,/content/training-data/text_summarization/8/triples/approach.txt
selected parts,of,text,approach,/content/training-data/text_summarization/8/triples/approach.txt
masking,produces,grammatical outputs,approach,/content/training-data/text_summarization/8/triples/approach.txt
Approach,first selects,selection mask,approach,/content/training-data/text_summarization/8/triples/approach.txt
selection mask,by this mask,constrains a standard neural model,approach,/content/training-data/text_summarization/8/triples/approach.txt
selection mask,for,source document,approach,/content/training-data/text_summarization/8/triples/approach.txt
Approach,incorporates,separate content selection system,approach,/content/training-data/text_summarization/8/triples/approach.txt
separate content selection system,to decide,relevant aspects,approach,/content/training-data/text_summarization/8/triples/approach.txt
relevant aspects,of,source document,approach,/content/training-data/text_summarization/8/triples/approach.txt
Approach,consider,bottom - up attention,approach,/content/training-data/text_summarization/8/triples/approach.txt
Approach,frame,selection task,approach,/content/training-data/text_summarization/8/triples/approach.txt
selection task,as a,sequence - tagging problem,approach,/content/training-data/text_summarization/8/triples/approach.txt
sequence - tagging problem,with the objective of,identifying tokens,approach,/content/training-data/text_summarization/8/triples/approach.txt
identifying tokens,from,document,approach,/content/training-data/text_summarization/8/triples/approach.txt
identifying tokens,part of,summary,approach,/content/training-data/text_summarization/8/triples/approach.txt
Contribution,has research problem,Abstractive Summarization,research-problem,/content/training-data/text_summarization/8/triples/research-problem.txt
Contribution,has research problem,Text summarization,research-problem,/content/training-data/text_summarization/8/triples/research-problem.txt
Contribution,has research problem,neural abstractive summarization,research-problem,/content/training-data/text_summarization/8/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/text_summarization/8/triples/experimental-setup.txt
Experimental setup,use,AllenNLP,experimental-setup,/content/training-data/text_summarization/8/triples/experimental-setup.txt
AllenNLP,for,content selector,experimental-setup,/content/training-data/text_summarization/8/triples/experimental-setup.txt
Experimental setup,use,Open NMT - py,experimental-setup,/content/training-data/text_summarization/8/triples/experimental-setup.txt
Open NMT - py,for,abstractive models,experimental-setup,/content/training-data/text_summarization/8/triples/experimental-setup.txt
Experimental setup,minimum length of,generated summary,experimental-setup,/content/training-data/text_summarization/8/triples/experimental-setup.txt
generated summary,is set to,35,experimental-setup,/content/training-data/text_summarization/8/triples/experimental-setup.txt
35,for,CNN - DM,experimental-setup,/content/training-data/text_summarization/8/triples/experimental-setup.txt
generated summary,is set to,6,experimental-setup,/content/training-data/text_summarization/8/triples/experimental-setup.txt
6,for,NYT,experimental-setup,/content/training-data/text_summarization/8/triples/experimental-setup.txt
Experimental setup,has,Length penalty parameter,experimental-setup,/content/training-data/text_summarization/8/triples/experimental-setup.txt
Length penalty parameter,ranging from,0.6 to 1.4,experimental-setup,/content/training-data/text_summarization/8/triples/experimental-setup.txt
Experimental setup,has,copy mask,experimental-setup,/content/training-data/text_summarization/8/triples/experimental-setup.txt
copy mask,ranging from,0.1 to 0.2,experimental-setup,/content/training-data/text_summarization/8/triples/experimental-setup.txt
Experimental setup,has,copy attention normalization parameter,experimental-setup,/content/training-data/text_summarization/8/triples/experimental-setup.txt
copy attention normalization parameter,to,2,experimental-setup,/content/training-data/text_summarization/8/triples/experimental-setup.txt
Experimental setup,has,coverage penalty parameter,experimental-setup,/content/training-data/text_summarization/8/triples/experimental-setup.txt
coverage penalty parameter,set to,10,experimental-setup,/content/training-data/text_summarization/8/triples/experimental-setup.txt
Experimental setup,has,inference parameters,experimental-setup,/content/training-data/text_summarization/8/triples/experimental-setup.txt
inference parameters,tuned on,200 example subset,experimental-setup,/content/training-data/text_summarization/8/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/text_summarization/8/triples/results.txt
Results,with,CopyTransformer and coverage penalty,results,/content/training-data/text_summarization/8/triples/results.txt
CopyTransformer and coverage penalty,indicate,slight improvement,results,/content/training-data/text_summarization/8/triples/results.txt
slight improvement,across,all three scores,results,/content/training-data/text_summarization/8/triples/results.txt
Results,observe,no significant difference,results,/content/training-data/text_summarization/8/triples/results.txt
no significant difference,between,Pointer - Generator and CopyTransformer with bottom - up attention,results,/content/training-data/text_summarization/8/triples/results.txt
Results,on,CNN - DM corpus,results,/content/training-data/text_summarization/8/triples/results.txt
CNN - DM corpus,using,coverage inference penalty,results,/content/training-data/text_summarization/8/triples/results.txt
coverage inference penalty,scores,the same,results,/content/training-data/text_summarization/8/triples/results.txt
the same,as a,full coverage mechanism,results,/content/training-data/text_summarization/8/triples/results.txt
full coverage mechanism,without requiring,additional model parameters,results,/content/training-data/text_summarization/8/triples/results.txt
full coverage mechanism,without requiring,model fine - tuning,results,/content/training-data/text_summarization/8/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/text_summarization/10/triples/ablation-analysis.txt
Ablation analysis,has,Soft - sharing vs. Hard - sharing,ablation-analysis,/content/training-data/text_summarization/10/triples/ablation-analysis.txt
Soft - sharing vs. Hard - sharing,prove,soft - sharing method,ablation-analysis,/content/training-data/text_summarization/10/triples/ablation-analysis.txt
soft - sharing method,is statistically significantly better than,hard - sharing,ablation-analysis,/content/training-data/text_summarization/10/triples/ablation-analysis.txt
hard - sharing,with,p < 0.001,ablation-analysis,/content/training-data/text_summarization/10/triples/ablation-analysis.txt
p < 0.001,in,all metrics,ablation-analysis,/content/training-data/text_summarization/10/triples/ablation-analysis.txt
Soft - sharing vs. Hard - sharing,choose,soft - sharing,ablation-analysis,/content/training-data/text_summarization/10/triples/ablation-analysis.txt
soft - sharing,over,hard - sharing,ablation-analysis,/content/training-data/text_summarization/10/triples/ablation-analysis.txt
soft - sharing,because of,more expressive parameter sharing,ablation-analysis,/content/training-data/text_summarization/10/triples/ablation-analysis.txt
Ablation analysis,has,Quantitative Improvements in Saliency Detection,ablation-analysis,/content/training-data/text_summarization/10/triples/ablation-analysis.txt
Quantitative Improvements in Saliency Detection,results are,2 - way - QG MTL model ( with question generation ,ablation-analysis,/content/training-data/text_summarization/10/triples/ablation-analysis.txt
2 - way - QG MTL model ( with question generation ),versus,baseline improvement,ablation-analysis,/content/training-data/text_summarization/10/triples/ablation-analysis.txt
baseline improvement,is,stat. significant ( p < 0.01 ,ablation-analysis,/content/training-data/text_summarization/10/triples/ablation-analysis.txt
Ablation analysis,has,Qualitative Examples on Entailment and Saliency Improvements,ablation-analysis,/content/training-data/text_summarization/10/triples/ablation-analysis.txt
Qualitative Examples on Entailment and Saliency Improvements,has,3 - way multi-task model,ablation-analysis,/content/training-data/text_summarization/10/triples/ablation-analysis.txt
3 - way multi-task model,generates,summaries,ablation-analysis,/content/training-data/text_summarization/10/triples/ablation-analysis.txt
summaries,are both better at,logical entailment,ablation-analysis,/content/training-data/text_summarization/10/triples/ablation-analysis.txt
summaries,are both better at,contain more salient information,ablation-analysis,/content/training-data/text_summarization/10/triples/ablation-analysis.txt
Ablation analysis,has,Quantitative Improvements in Entailment,ablation-analysis,/content/training-data/text_summarization/10/triples/ablation-analysis.txt
Quantitative Improvements in Entailment,found,our 2 - way MTL model,ablation-analysis,/content/training-data/text_summarization/10/triples/ablation-analysis.txt
our 2 - way MTL model,with,entailment generation,ablation-analysis,/content/training-data/text_summarization/10/triples/ablation-analysis.txt
entailment generation,reduces,extraneous count,ablation-analysis,/content/training-data/text_summarization/10/triples/ablation-analysis.txt
extraneous count,by,17.2 % w.r.t. the baseline,ablation-analysis,/content/training-data/text_summarization/10/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/text_summarization/10/triples/model.txt
Model,present,novel multi-task learning architectures,model,/content/training-data/text_summarization/10/triples/model.txt
novel multi-task learning architectures,empirically show,substantially better,model,/content/training-data/text_summarization/10/triples/model.txt
substantially better,to share,higherlevel semantic layers,model,/content/training-data/text_summarization/10/triples/model.txt
higherlevel semantic layers,keeping,lower - level ( lexico- syntactic ) layers unshared,model,/content/training-data/text_summarization/10/triples/model.txt
higherlevel semantic layers,between,three aforementioned tasks,model,/content/training-data/text_summarization/10/triples/model.txt
novel multi-task learning architectures,based on,multi-layered encoder and decoder models,model,/content/training-data/text_summarization/10/triples/model.txt
Contribution,has research problem,Multi - Task Summarization,research-problem,/content/training-data/text_summarization/10/triples/research-problem.txt
Contribution,has research problem,abstractive summarization,research-problem,/content/training-data/text_summarization/10/triples/research-problem.txt
Contribution,has research problem,abstractive text summarization,research-problem,/content/training-data/text_summarization/10/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/text_summarization/10/triples/results.txt
Results,For,multi-task learning with question generation,results,/content/training-data/text_summarization/10/triples/results.txt
multi-task learning with question generation,improvements,statistically significant,results,/content/training-data/text_summarization/10/triples/results.txt
statistically significant,for,CNN / DailyMail,results,/content/training-data/text_summarization/10/triples/results.txt
CNN / DailyMail,in,ROUGE - 1 ( p < 0.01 ,results,/content/training-data/text_summarization/10/triples/results.txt
CNN / DailyMail,in,ROUGE - L ( p < 0.05 ,results,/content/training-data/text_summarization/10/triples/results.txt
CNN / DailyMail,in,METEOR ( p < 0.01 ,results,/content/training-data/text_summarization/10/triples/results.txt
statistically significant,for,Gigaword,results,/content/training-data/text_summarization/10/triples/results.txt
Gigaword,in,all metrics ( p < 0.01 ,results,/content/training-data/text_summarization/10/triples/results.txt
Results,has,Multi - Task with Entailment Generation,results,/content/training-data/text_summarization/10/triples/results.txt
Multi - Task with Entailment Generation,shows,multi-task setting,results,/content/training-data/text_summarization/10/triples/results.txt
multi-task setting,better than,our strong baseline models,results,/content/training-data/text_summarization/10/triples/results.txt
Results,has,Pointer + Coverage Baseline,results,/content/training-data/text_summarization/10/triples/results.txt
Pointer + Coverage Baseline,On,Gigaword dataset,results,/content/training-data/text_summarization/10/triples/results.txt
Gigaword dataset,has,baseline model,results,/content/training-data/text_summarization/10/triples/results.txt
baseline model,performs better than,all previous works,results,/content/training-data/text_summarization/10/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/text_summarization/3/triples/baselines.txt
Baselines,has,lvt5 k - lsent,baselines,/content/training-data/text_summarization/3/triples/baselines.txt
lvt5 k - lsent,uses,temporal attention,baselines,/content/training-data/text_summarization/3/triples/baselines.txt
temporal attention,to keep track of,past attentive weights,baselines,/content/training-data/text_summarization/3/triples/baselines.txt
past attentive weights,of,decoder,baselines,/content/training-data/text_summarization/3/triples/baselines.txt
temporal attention,restrains,repetition,baselines,/content/training-data/text_summarization/3/triples/baselines.txt
repetition,in,later sequences,baselines,/content/training-data/text_summarization/3/triples/baselines.txt
Baselines,has,CGU,baselines,/content/training-data/text_summarization/3/triples/baselines.txt
CGU,for,global encoding,baselines,/content/training-data/text_summarization/3/triples/baselines.txt
global encoding,sets,convolutional gated unit,baselines,/content/training-data/text_summarization/3/triples/baselines.txt
global encoding,sets,self - attention,baselines,/content/training-data/text_summarization/3/triples/baselines.txt
Baselines,has,Seq2seq + att,baselines,/content/training-data/text_summarization/3/triples/baselines.txt
Seq2seq + att,is,two - layer BiLSTM encoder,baselines,/content/training-data/text_summarization/3/triples/baselines.txt
Seq2seq + att,is,one - layer LSTM decoder,baselines,/content/training-data/text_summarization/3/triples/baselines.txt
one - layer LSTM decoder,equipped with,attention,baselines,/content/training-data/text_summarization/3/triples/baselines.txt
Baselines,has,Pointer - generator,baselines,/content/training-data/text_summarization/3/triples/baselines.txt
Pointer - generator,is,integrated pointer network,baselines,/content/training-data/text_summarization/3/triples/baselines.txt
Pointer - generator,is,seq2seq model,baselines,/content/training-data/text_summarization/3/triples/baselines.txt
Baselines,has,RAS - Elman,baselines,/content/training-data/text_summarization/3/triples/baselines.txt
RAS - Elman,is a,convolution encoder,baselines,/content/training-data/text_summarization/3/triples/baselines.txt
RAS - Elman,is a,Elman RNN decoder,baselines,/content/training-data/text_summarization/3/triples/baselines.txt
Elman RNN decoder,with,attention,baselines,/content/training-data/text_summarization/3/triples/baselines.txt
Baselines,has,ABS +,baselines,/content/training-data/text_summarization/3/triples/baselines.txt
ABS +,is a,tuned ABS model,baselines,/content/training-data/text_summarization/3/triples/baselines.txt
Baselines,has,SEASS,baselines,/content/training-data/text_summarization/3/triples/baselines.txt
SEASS,includes,additional selective gate,baselines,/content/training-data/text_summarization/3/triples/baselines.txt
additional selective gate,to control,information flow,baselines,/content/training-data/text_summarization/3/triples/baselines.txt
information flow,from,encoder,baselines,/content/training-data/text_summarization/3/triples/baselines.txt
encoder,to,decoder,baselines,/content/training-data/text_summarization/3/triples/baselines.txt
Contribution,Code,https :// github.com/wprojectsn/codes,code,/content/training-data/text_summarization/3/triples/code.txt
Contribution,has,Model,model,/content/training-data/text_summarization/3/triples/model.txt
Model,alleviates,OOV problems,model,/content/training-data/text_summarization/3/triples/model.txt
Model,optimized end - to - end,network,model,/content/training-data/text_summarization/3/triples/model.txt
network,using,reinforcement learning,model,/content/training-data/text_summarization/3/triples/model.txt
reinforcement learning,with,distant - supervision strategy,model,/content/training-data/text_summarization/3/triples/model.txt
Model,employs,another pointer,model,/content/training-data/text_summarization/3/triples/model.txt
another pointer,to generalize,detailed words,model,/content/training-data/text_summarization/3/triples/model.txt
detailed words,according to,upper level of expressions,model,/content/training-data/text_summarization/3/triples/model.txt
Model,uses,pointer network,model,/content/training-data/text_summarization/3/triples/model.txt
pointer network,to capture,salient information,model,/content/training-data/text_summarization/3/triples/model.txt
salient information,from,source text,model,/content/training-data/text_summarization/3/triples/model.txt
Model,has,optimization function,model,/content/training-data/text_summarization/3/triples/model.txt
optimization function,adaptive so as to cater for,different datasets,model,/content/training-data/text_summarization/3/triples/model.txt
different datasets,with,distantly - supervised training,model,/content/training-data/text_summarization/3/triples/model.txt
Model,propose,novel model,model,/content/training-data/text_summarization/3/triples/model.txt
novel model,based on,concept pointer generator,model,/content/training-data/text_summarization/3/triples/model.txt
concept pointer generator,encourages the generation of,conceptual and abstract words,model,/content/training-data/text_summarization/3/triples/model.txt
Contribution,has research problem,Abstractive Summarization,research-problem,/content/training-data/text_summarization/3/triples/research-problem.txt
Contribution,has research problem,Abstractive summarization ( ABS ,research-problem,/content/training-data/text_summarization/3/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
Experimental setup,optimization using,RL rewards,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
RL rewards,for,RG - L,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
RG - L,at,50 K iterations,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
50 K iterations,on,Gigaword,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
RG - L,at,95 K iterations,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
95 K iterations,on,DUC - 2004,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
Experimental setup,took,distancesupervised training,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
distancesupervised training,at,5 K iterations,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
5 K iterations,on,DUC - 2004,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
distancesupervised training,at,6.5 K iterations,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
6.5 K iterations,on,Gigaword,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
Experimental setup,trained,concept pointer generator,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
concept pointer generator,for,450 k iterations,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
450 k iterations,yielded,best performance,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
Experimental setup,set to,0.1,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
0.1,has,accumulator value,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
Experimental setup,set to,0.15,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
0.15,has,initial learning rate,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
Experimental setup,has,models,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
models,on,single GTX TI - TAN GPU machine,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
Experimental setup,has,hidden state size,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
hidden state size,set to,256,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
Experimental setup,has,word embeddings,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
word embeddings,fine - tune,during training,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
word embeddings,initialize,128 - d vectors,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
Experimental setup,has,vocabulary size,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
vocabulary size,set to,150 k,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
150 k,for,both the source and target text,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
Experimental setup,used,gradient clipping,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
gradient clipping,with,maximum gradient norm,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
maximum gradient norm,of,2,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
Experimental setup,used,Adagrad optimizer,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
Adagrad optimizer,with,batch size of 64,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
batch size of 64,to minimize,loss,experimental-setup,/content/training-data/text_summarization/3/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/text_summarization/3/triples/results.txt
Results,In terms of,pointer generator performance,results,/content/training-data/text_summarization/3/triples/results.txt
pointer generator performance,has,concept pointer,results,/content/training-data/text_summarization/3/triples/results.txt
concept pointer,improvements made,statistically significant ( p < 0.01 ,results,/content/training-data/text_summarization/3/triples/results.txt
statistically significant ( p < 0.01 ),across,all metrics,results,/content/training-data/text_summarization/3/triples/results.txt
Results,observe,model,results,/content/training-data/text_summarization/3/triples/results.txt
model,outperformed,all the strong state of - the - art models,results,/content/training-data/text_summarization/3/triples/results.txt
model,outperformed,in all metrics,results,/content/training-data/text_summarization/3/triples/results.txt
in all metrics,except for,RG - 2,results,/content/training-data/text_summarization/3/triples/results.txt
RG - 2,on,Gigaword,results,/content/training-data/text_summarization/3/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
Baselines,for,LCSTS,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
LCSTS,has,Copy - Net,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
Copy - Net,is,attention - based seq2seq model,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
attention - based seq2seq model,with,copy mechanism,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
LCSTS,has,RNN and RNN - context,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
RNN and RNN - context,are,RNNbased seq2seq models,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
RNNbased seq2seq models,without and with,attention mechanism,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
LCSTS,has,SRB,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
SRB,improves,semantic relevance,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
semantic relevance,between,source text and summary,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
LCSTS,has,DRGD,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
DRGD,is,conventional seq2seq,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
conventional seq2seq,with,deep recurrent generative decoder,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
Baselines,for,Gigaword,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
Gigaword,has,ABS and ABS +,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
ABS and ABS +,are,models,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
models,with,local attention and handcrafted features,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
Gigaword,has,Feats,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
Feats,is,fully RNN seq2seq model,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
fully RNN seq2seq model,with,some specific methods,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
some specific methods,to control,vocabulary size,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
Gigaword,has,RAS - LSTM and RAS - Elman,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
RAS - LSTM and RAS - Elman,are,seq2seq models,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
seq2seq models,with,convolutional encoder,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
seq2seq models,with,LSTM decoder,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
seq2seq models,with,Elman RNN decoder,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
Gigaword,has,DRGD,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
DRGD,baseline for,Gigaword,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
Gigaword,has,SEASS,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
SEASS,is a,seq2seq model,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
seq2seq model,with,selective gate mechanism,baselines,/content/training-data/text_summarization/11/triples/baselines.txt
Contribution,has,Model,model,/content/training-data/text_summarization/11/triples/model.txt
Model,set,convolutional gated unit,model,/content/training-data/text_summarization/11/triples/model.txt
convolutional gated unit,to perform,global encoding,model,/content/training-data/text_summarization/11/triples/model.txt
global encoding,on,source context,model,/content/training-data/text_summarization/11/triples/model.txt
Model,of,global encoding,model,/content/training-data/text_summarization/11/triples/model.txt
global encoding,for,abstractive summarization,model,/content/training-data/text_summarization/11/triples/model.txt
Model,has,gate,model,/content/training-data/text_summarization/11/triples/model.txt
gate,based on,convolutional neural network ( CNN ,model,/content/training-data/text_summarization/11/triples/model.txt
gate,filters,each encoder output,model,/content/training-data/text_summarization/11/triples/model.txt
each encoder output,based on,global context,model,/content/training-data/text_summarization/11/triples/model.txt
global context,due to,parameter sharing,model,/content/training-data/text_summarization/11/triples/model.txt
Contribution,has research problem,Abstractive Summarization,research-problem,/content/training-data/text_summarization/11/triples/research-problem.txt
Contribution,has research problem,neural abstractive summarization,research-problem,/content/training-data/text_summarization/11/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/text_summarization/11/triples/experimental-setup.txt
Experimental setup,use,"Adam optimizer ( Kingma and Ba , 2014 ",experimental-setup,/content/training-data/text_summarization/11/triples/experimental-setup.txt
"Adam optimizer ( Kingma and Ba , 2014 )",with,"default setting ? = 0.001 , ? 1 = 0.9 , ? 2 = 0.999 and = 1 10 ?8",experimental-setup,/content/training-data/text_summarization/11/triples/experimental-setup.txt
Experimental setup,implement,our experiments,experimental-setup,/content/training-data/text_summarization/11/triples/experimental-setup.txt
our experiments,in,PyTorch on an NVIDIA 1080 Ti GPU,experimental-setup,/content/training-data/text_summarization/11/triples/experimental-setup.txt
Experimental setup,has,Gradient clipping,experimental-setup,/content/training-data/text_summarization/11/triples/experimental-setup.txt
Gradient clipping,applied with,"range [ - 10 , 10 ]",experimental-setup,/content/training-data/text_summarization/11/triples/experimental-setup.txt
Experimental setup,has,learning rate,experimental-setup,/content/training-data/text_summarization/11/triples/experimental-setup.txt
learning rate,halved,every epoch,experimental-setup,/content/training-data/text_summarization/11/triples/experimental-setup.txt
Experimental setup,has,batch size,experimental-setup,/content/training-data/text_summarization/11/triples/experimental-setup.txt
batch size,set to,64,experimental-setup,/content/training-data/text_summarization/11/triples/experimental-setup.txt
Experimental setup,word embedding dimension and the number of hidden units,512,experimental-setup,/content/training-data/text_summarization/11/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/text_summarization/11/triples/results.txt
Results,Compared with,conventional seq2seq model,results,/content/training-data/text_summarization/11/triples/results.txt
conventional seq2seq model,owns an advantage,our model,results,/content/training-data/text_summarization/11/triples/results.txt
our model,of,ROUGE - 2 score 3.7 and 1.5,results,/content/training-data/text_summarization/11/triples/results.txt
ROUGE - 2 score 3.7 and 1.5,on,LCSTS and Gigaword,results,/content/training-data/text_summarization/11/triples/results.txt
Results,on,two datasets,results,/content/training-data/text_summarization/11/triples/results.txt
two datasets,has,our model,results,/content/training-data/text_summarization/11/triples/results.txt
our model,achieves,advantages,results,/content/training-data/text_summarization/11/triples/results.txt
advantages,of,ROUGE score,results,/content/training-data/text_summarization/11/triples/results.txt
ROUGE score,over,baselines,results,/content/training-data/text_summarization/11/triples/results.txt
ROUGE score,on,LCSTS,results,/content/training-data/text_summarization/11/triples/results.txt
LCSTS,are,significant,results,/content/training-data/text_summarization/11/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
Baselines,has,Seq2seq + RL,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
Seq2seq + RL,implement,Reinforcement Learning ( RL ) models,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
Reinforcement Learning ( RL ) models,with,reward metrics,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
reward metrics,of,Entailment and ROUGE - 2,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
Baselines,has,ABS,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
ABS,apply,seq2seq model,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
seq2seq model,to,abstractive sentence summarization,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
Baselines,has,Seq2seq,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
Seq2seq,is a,standard seq2seq model,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
standard seq2seq model,with,attention mechanism,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
Baselines,has,Seq2seq + selective,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
Seq2seq + selective,employ,selective encoding model,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
selective encoding model,to control,information flow,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
information flow,from,encoder to decoder,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
Baselines,has,Seq2seq + MTL,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
Seq2seq + MTL,with,entailment - aware encoder,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
entailment - aware encoder,applies,multi-task learning ( MTL ) framework,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
multi-task learning ( MTL ) framework,to,seq2seq model,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
Baselines,has,Seq2seq + MTL ( Share decoder ,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
Seq2seq + MTL ( Share decoder ),propose,multi - task learning ( MTL ) framework,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
multi - task learning ( MTL ) framework,in which,decoder,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
decoder,is shared for,summarization generation and entailment generation task,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
Baselines,has,Seq2seq + ROUGE -2 RAML,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
Seq2seq + ROUGE -2 RAML,apply,ROUGE - 2 RAML training,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
ROUGE - 2 RAML training,for,seq2seq model,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
Baselines,has,Seq2seq + ERAML,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
Seq2seq + ERAML,with,entailment - aware decoder,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
entailment - aware decoder,conducts,Entailment Reward Augmented Maximum Likelihood ( ERAML ) training framework,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
Baselines,has,ABS +,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
ABS +,propose,neural machine translation model,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
neural machine translation model,with,two - layer LSTMs,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
two - layer LSTMs,for,encoder - decoder,baselines,/content/training-data/text_summarization/14/triples/baselines.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
Ablation analysis,has,Could the entailment recognition also be improved ?,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
Could the entailment recognition also be improved ?,shows,our summarization model,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
our summarization model,with,MTL,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
MTL,outperforms,basic seq2seq model,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
Could the entailment recognition also be improved ?,has,accuracy,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
accuracy,of,entailment recognition,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
Ablation analysis,has,Does our summarization model learn entailment knowledge ?,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
Does our summarization model learn entailment knowledge ?,adopt,entailmentbased strategies,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
entailmentbased strategies,has,entailment score,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
entailment score,rises to,0.63,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
0.63,for,seq2seq model,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
Does our summarization model learn entailment knowledge ?,Note,entailment score,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
entailment score,is,0.57,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
0.57,for,seq2seq model,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
seq2seq model,with,selective encoding,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
Does our summarization model learn entailment knowledge ?,For,test set,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
test set,has,average entailment score,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
average entailment score,for,reference,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
reference,is,0.72,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
average entailment score,for,basic seq2seq model,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
basic seq2seq model,entailment score is,0.46,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
Does our summarization model learn entailment knowledge ?,conclude,our model,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
our model,successfully learned,entailment knowledge,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
Does our summarization model learn entailment knowledge ?,has,selective mechanism,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
selective mechanism,filter out,secondary information,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
secondary information,in,input,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
Does our summarization model learn entailment knowledge ?,has,Entailment - aware selective model,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
Entailment - aware selective model,achieves,high entailment reward,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
high entailment reward,of,0.71,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
Ablation analysis,has,Is it less abstractive for our model ?,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
Is it less abstractive for our model ?,shows that,seq2seq model,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
seq2seq model,produces,more novel words,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
more novel words,than,our model,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
our model,indicating,lower degree of abstraction,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
Is it less abstractive for our model ?,exclude,all the words not in the reference,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
all the words not in the reference,has,model,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
model,generates,more novel words,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
more novel words,suggesting that,our model,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
our model,provides,compromise solution,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
compromise solution,for,informativeness and correctness,ablation-analysis,/content/training-data/text_summarization/14/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/text_summarization/14/triples/model.txt
Model,share,encoder,model,/content/training-data/text_summarization/14/triples/model.txt
encoder,of,summarization generation system,model,/content/training-data/text_summarization/14/triples/model.txt
summarization generation system,with,entailment recognition system,model,/content/training-data/text_summarization/14/triples/model.txt
Model,propose,entailment - aware encoder,model,/content/training-data/text_summarization/14/triples/model.txt
Model,propose,entailment - aware decoder,model,/content/training-data/text_summarization/14/triples/model.txt
Model,propose,entailment Reward Augmented Maximum Likelihood ( RAML ) training,model,/content/training-data/text_summarization/14/triples/model.txt
entailment Reward Augmented Maximum Likelihood ( RAML ) training,encourages,decoder of the summarization system,model,/content/training-data/text_summarization/14/triples/model.txt
decoder of the summarization system,to produce,summary,model,/content/training-data/text_summarization/14/triples/model.txt
summary,entailed by,source,model,/content/training-data/text_summarization/14/triples/model.txt
Contribution,has research problem,Abstractive Sentence Summarization,research-problem,/content/training-data/text_summarization/14/triples/research-problem.txt
Contribution,has research problem,sentence summarization,research-problem,/content/training-data/text_summarization/14/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/text_summarization/14/triples/results.txt
Results,on,DUC 2004,results,/content/training-data/text_summarization/14/triples/results.txt
DUC 2004,show,Seq2seq + selective + MTL + ERAML model,results,/content/training-data/text_summarization/14/triples/results.txt
Seq2seq + selective + MTL + ERAML model,achieves,significant improvements,results,/content/training-data/text_summarization/14/triples/results.txt
significant improvements,over,baseline models,results,/content/training-data/text_summarization/14/triples/results.txt
significant improvements,surpassing,Feats2s,results,/content/training-data/text_summarization/14/triples/results.txt
Feats2s,by,"0.98 % ROUGE - 1 , 0.78 % ROUGE - 2 and 0.65 % ROUGE - L",results,/content/training-data/text_summarization/14/triples/results.txt
"0.98 % ROUGE - 1 , 0.78 % ROUGE - 2 and 0.65 % ROUGE - L",without,fine - tuning,results,/content/training-data/text_summarization/14/triples/results.txt
Results,on,Gigaword Corpus,results,/content/training-data/text_summarization/14/triples/results.txt
Gigaword Corpus,has,Our model,results,/content/training-data/text_summarization/14/triples/results.txt
Our model,performs better than,previous works,results,/content/training-data/text_summarization/14/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/text_summarization/12/triples/baselines.txt
Baselines,has,ABS +,baselines,/content/training-data/text_summarization/12/triples/baselines.txt
ABS +,Based on,ABS model,baselines,/content/training-data/text_summarization/12/triples/baselines.txt
ABS model,further with,two - layer LSTMs,baselines,/content/training-data/text_summarization/12/triples/baselines.txt
two - layer LSTMs,for,encoder - decoder,baselines,/content/training-data/text_summarization/12/triples/baselines.txt
encoder - decoder,with,500 hidden units,baselines,/content/training-data/text_summarization/12/triples/baselines.txt
500 hidden units,in,each layer,baselines,/content/training-data/text_summarization/12/triples/baselines.txt
ABS +,Based on,s 2 s+ att,baselines,/content/training-data/text_summarization/12/triples/baselines.txt
s 2 s+ att,implement,sequence - to sequence model,baselines,/content/training-data/text_summarization/12/triples/baselines.txt
sequence - to sequence model,with,attention,baselines,/content/training-data/text_summarization/12/triples/baselines.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/text_summarization/12/triples/hyperparameters.txt
Hyperparameters,apply,gradient clipping,hyperparameters,/content/training-data/text_summarization/12/triples/hyperparameters.txt
gradient clipping,with,"range [ ? 5 , 5 ]",hyperparameters,/content/training-data/text_summarization/12/triples/hyperparameters.txt
"range [ ? 5 , 5 ]",during,training,hyperparameters,/content/training-data/text_summarization/12/triples/hyperparameters.txt
Hyperparameters,use,Adam,hyperparameters,/content/training-data/text_summarization/12/triples/hyperparameters.txt
Adam,as,optimizing algorithm,hyperparameters,/content/training-data/text_summarization/12/triples/hyperparameters.txt
Adam,learning rate,0.001,hyperparameters,/content/training-data/text_summarization/12/triples/hyperparameters.txt
Adam,two momentum parameters,? 1 = 0.9 and ? 2 = 0.999,hyperparameters,/content/training-data/text_summarization/12/triples/hyperparameters.txt
Hyperparameters,use,mini-batch size 64,hyperparameters,/content/training-data/text_summarization/12/triples/hyperparameters.txt
mini-batch size 64,by,grid search,hyperparameters,/content/training-data/text_summarization/12/triples/hyperparameters.txt
grid search,To both speedup,training and converge quickly,hyperparameters,/content/training-data/text_summarization/12/triples/hyperparameters.txt
Hyperparameters,initialize,model parameters randomly,hyperparameters,/content/training-data/text_summarization/12/triples/hyperparameters.txt
model parameters randomly,using,Gaussian distribution,hyperparameters,/content/training-data/text_summarization/12/triples/hyperparameters.txt
Gaussian distribution,with,Xavier scheme,hyperparameters,/content/training-data/text_summarization/12/triples/hyperparameters.txt
Hyperparameters,During,training,hyperparameters,/content/training-data/text_summarization/12/triples/hyperparameters.txt
training,test,model performance ( ROUGE - 2 F1 ,hyperparameters,/content/training-data/text_summarization/12/triples/hyperparameters.txt
model performance ( ROUGE - 2 F1 ),on,development set,hyperparameters,/content/training-data/text_summarization/12/triples/hyperparameters.txt
development set,for,"every 2,000 batches",hyperparameters,/content/training-data/text_summarization/12/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/text_summarization/12/triples/model.txt
Model,treat,sentence summarization,model,/content/training-data/text_summarization/12/triples/model.txt
sentence summarization,as a,threephase task,model,/content/training-data/text_summarization/12/triples/model.txt
threephase task,name,encoding,model,/content/training-data/text_summarization/12/triples/model.txt
threephase task,name,selection,model,/content/training-data/text_summarization/12/triples/model.txt
threephase task,name,decoding,model,/content/training-data/text_summarization/12/triples/model.txt
Model,consists of,sentence encoder,model,/content/training-data/text_summarization/12/triples/model.txt
Model,consists of,selective gate network,model,/content/training-data/text_summarization/12/triples/model.txt
Model,consists of,summary decoder,model,/content/training-data/text_summarization/12/triples/model.txt
Model,First,sentence encoder,model,/content/training-data/text_summarization/12/triples/model.txt
sentence encoder,reads,input words,model,/content/training-data/text_summarization/12/triples/model.txt
input words,through,RNN unit,model,/content/training-data/text_summarization/12/triples/model.txt
RNN unit,to construct,first level sentence representation,model,/content/training-data/text_summarization/12/triples/model.txt
Model,has,selective mechanism,model,/content/training-data/text_summarization/12/triples/model.txt
selective mechanism,controls,information flow,model,/content/training-data/text_summarization/12/triples/model.txt
information flow,from,encoder to decoder,model,/content/training-data/text_summarization/12/triples/model.txt
encoder to decoder,by applying,gate network,model,/content/training-data/text_summarization/12/triples/model.txt
gate network,according to,sentence information,model,/content/training-data/text_summarization/12/triples/model.txt
Model,has,selective gate network,model,/content/training-data/text_summarization/12/triples/model.txt
selective gate network,selects,encoded information,model,/content/training-data/text_summarization/12/triples/model.txt
encoded information,to construct,second level sentence representation,model,/content/training-data/text_summarization/12/triples/model.txt
Model,has,attention - equipped decoder,model,/content/training-data/text_summarization/12/triples/model.txt
attention - equipped decoder,generates,summary,model,/content/training-data/text_summarization/12/triples/model.txt
summary,using,second level sentence representation,model,/content/training-data/text_summarization/12/triples/model.txt
Model,propose,Selective Encoding for Abstractive Sentence Summarization ( SEASS ,model,/content/training-data/text_summarization/12/triples/model.txt
Contribution,has research problem,Abstractive Sentence Summarization,research-problem,/content/training-data/text_summarization/12/triples/research-problem.txt
Contribution,has research problem,sentence summarization,research-problem,/content/training-data/text_summarization/12/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/text_summarization/12/triples/results.txt
Results,has,DUC 2004,results,/content/training-data/text_summarization/12/triples/results.txt
DUC 2004,Compared to,ABS + model,results,/content/training-data/text_summarization/12/triples/results.txt
ABS + model,tuned using,DUC 2003 data,results,/content/training-data/text_summarization/12/triples/results.txt
ABS + model,has,our model,results,/content/training-data/text_summarization/12/triples/results.txt
our model,performs,significantly better,results,/content/training-data/text_summarization/12/triples/results.txt
significantly better,by,1.07 ROUGE - 2 recall score,results,/content/training-data/text_summarization/12/triples/results.txt
DUC 2004,has,SEASS,results,/content/training-data/text_summarization/12/triples/results.txt
SEASS,achieves,"29.21 , 9.56 and 25.51",results,/content/training-data/text_summarization/12/triples/results.txt
"29.21 , 9.56 and 25.51",for,"ROUGE 1 , 2 and L recall",results,/content/training-data/text_summarization/12/triples/results.txt
SEASS,outperforms,all the baseline methods,results,/content/training-data/text_summarization/12/triples/results.txt
Results,has,English Gigaword,results,/content/training-data/text_summarization/12/triples/results.txt
English Gigaword,has,SEASS model,results,/content/training-data/text_summarization/12/triples/results.txt
SEASS model,with,beam search,results,/content/training-data/text_summarization/12/triples/results.txt
beam search,outperforms,all baseline models,results,/content/training-data/text_summarization/12/triples/results.txt
all baseline models,by,large margin,results,/content/training-data/text_summarization/12/triples/results.txt
SEASS model,Compared to,ABS model,results,/content/training-data/text_summarization/12/triples/results.txt
ABS model,has,6.22 ROUGE - 2 F1 relative gain,results,/content/training-data/text_summarization/12/triples/results.txt
SEASS model,Compared to,highest CAs 2s baseline,results,/content/training-data/text_summarization/12/triples/results.txt
highest CAs 2s baseline,passes,significant test,results,/content/training-data/text_summarization/12/triples/results.txt
significant test,according to,official ROUGE script,results,/content/training-data/text_summarization/12/triples/results.txt
highest CAs 2s baseline,achieves,1.57 ROUGE - 2 F1 improvement,results,/content/training-data/text_summarization/12/triples/results.txt
SEASS model,Even for,greedy search,results,/content/training-data/text_summarization/12/triples/results.txt
greedy search,still performs better than,other methods,results,/content/training-data/text_summarization/12/triples/results.txt
SEASS model,For the popular,ROUGE - 2 metric,results,/content/training-data/text_summarization/12/triples/results.txt
ROUGE - 2 metric,performs better than,previous works,results,/content/training-data/text_summarization/12/triples/results.txt
ROUGE - 2 metric,achieves,17.54 F1 score,results,/content/training-data/text_summarization/12/triples/results.txt
Contribution,has,Approach,approach,/content/training-data/text_summarization/5/triples/approach.txt
Approach,call,summarization system,approach,/content/training-data/text_summarization/5/triples/approach.txt
summarization system,name,Re 3 Sum,approach,/content/training-data/text_summarization/5/triples/approach.txt
Re 3 Sum,consists of,three modules,approach,/content/training-data/text_summarization/5/triples/approach.txt
three modules,name,Retrieve,approach,/content/training-data/text_summarization/5/triples/approach.txt
three modules,name,Rerank,approach,/content/training-data/text_summarization/5/triples/approach.txt
three modules,name,Rewrite,approach,/content/training-data/text_summarization/5/triples/approach.txt
Approach,extend,seq2seq model,approach,/content/training-data/text_summarization/5/triples/approach.txt
seq2seq model,to jointly learn,template saliency measurement ( Rerank ,approach,/content/training-data/text_summarization/5/triples/approach.txt
seq2seq model,to jointly learn,final summary generation ( Rewrite ,approach,/content/training-data/text_summarization/5/triples/approach.txt
Approach,In,Rewrite,approach,/content/training-data/text_summarization/5/triples/approach.txt
Rewrite,generated,summary,approach,/content/training-data/text_summarization/5/triples/approach.txt
summary,according to,hidden states,approach,/content/training-data/text_summarization/5/triples/approach.txt
hidden states,of both,sentence and template,approach,/content/training-data/text_summarization/5/triples/approach.txt
Approach,In,Rerank,approach,/content/training-data/text_summarization/5/triples/approach.txt
Rerank,measure,informativeness,approach,/content/training-data/text_summarization/5/triples/approach.txt
informativeness,of,candidate template,approach,/content/training-data/text_summarization/5/triples/approach.txt
candidate template,with,highest predicted informativeness,approach,/content/training-data/text_summarization/5/triples/approach.txt
highest predicted informativeness,is regarded as,actual soft template,approach,/content/training-data/text_summarization/5/triples/approach.txt
candidate template,according to,hidden state relevance,approach,/content/training-data/text_summarization/5/triples/approach.txt
hidden state relevance,to,input sentence,approach,/content/training-data/text_summarization/5/triples/approach.txt
Approach,utilize,widely - used Information Retrieval ( IR ) platform,approach,/content/training-data/text_summarization/5/triples/approach.txt
widely - used Information Retrieval ( IR ) platform,to find out,candidate soft templates,approach,/content/training-data/text_summarization/5/triples/approach.txt
candidate soft templates,from,training corpus,approach,/content/training-data/text_summarization/5/triples/approach.txt
Approach,has,Recurrent Neural Network ( RNN ) encoder,approach,/content/training-data/text_summarization/5/triples/approach.txt
Recurrent Neural Network ( RNN ) encoder,applied to,convert,approach,/content/training-data/text_summarization/5/triples/approach.txt
convert,into,hidden states,approach,/content/training-data/text_summarization/5/triples/approach.txt
hidden states,name,input sentence,approach,/content/training-data/text_summarization/5/triples/approach.txt
hidden states,name,each candidate template,approach,/content/training-data/text_summarization/5/triples/approach.txt
Approach,combine,seq2seq and template based summarization approaches,approach,/content/training-data/text_summarization/5/triples/approach.txt
Contribution,has,Baselines,baselines,/content/training-data/text_summarization/5/triples/baselines.txt
Baselines,has,OpenNMT,baselines,/content/training-data/text_summarization/5/triples/baselines.txt
OpenNMT,implement,standard attentional seq2seq model,baselines,/content/training-data/text_summarization/5/triples/baselines.txt
Baselines,has,PIPELINE,baselines,/content/training-data/text_summarization/5/triples/baselines.txt
PIPELINE,trains,Rerank module,baselines,/content/training-data/text_summarization/5/triples/baselines.txt
PIPELINE,trains,Rewrite module,baselines,/content/training-data/text_summarization/5/triples/baselines.txt
Baselines,has,FTSum,baselines,/content/training-data/text_summarization/5/triples/baselines.txt
FTSum,encoded,facts,baselines,/content/training-data/text_summarization/5/triples/baselines.txt
facts,extracted from,source sentence,baselines,/content/training-data/text_summarization/5/triples/baselines.txt
source sentence,to improve,generated summaries,baselines,/content/training-data/text_summarization/5/triples/baselines.txt
generated summaries,both,faithfulness,baselines,/content/training-data/text_summarization/5/triples/baselines.txt
generated summaries,both,informativeness,baselines,/content/training-data/text_summarization/5/triples/baselines.txt
Contribution,Code,http://www4.comp.polyu.edu.hk/cszqcao/,code,/content/training-data/text_summarization/5/triples/code.txt
Contribution,has research problem,Neural Summarization,research-problem,/content/training-data/text_summarization/5/triples/research-problem.txt
Contribution,has research problem,abstractive sentence summarization,research-problem,/content/training-data/text_summarization/5/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/text_summarization/5/triples/experimental-setup.txt
Experimental setup,add,argument,experimental-setup,/content/training-data/text_summarization/5/triples/experimental-setup.txt
argument,name,replace unk,experimental-setup,/content/training-data/text_summarization/5/triples/experimental-setup.txt
replace unk,to replace,generated unknown words,experimental-setup,/content/training-data/text_summarization/5/triples/experimental-setup.txt
generated unknown words,with,source word,experimental-setup,/content/training-data/text_summarization/5/triples/experimental-setup.txt
source word,that holds,highest attention weight,experimental-setup,/content/training-data/text_summarization/5/triples/experimental-setup.txt
Experimental setup,use,popular seq2seq framework,experimental-setup,/content/training-data/text_summarization/5/triples/experimental-setup.txt
popular seq2seq framework,name,Open - NMT,experimental-setup,/content/training-data/text_summarization/5/triples/experimental-setup.txt
Experimental setup,introduce,additional length penalty argument,experimental-setup,/content/training-data/text_summarization/5/triples/experimental-setup.txt
additional length penalty argument,name,alpha 1,experimental-setup,/content/training-data/text_summarization/5/triples/experimental-setup.txt
alpha 1,to encourage,longer generation,experimental-setup,/content/training-data/text_summarization/5/triples/experimental-setup.txt
Experimental setup,retain,default settings,experimental-setup,/content/training-data/text_summarization/5/triples/experimental-setup.txt
default settings,of,Open NMT,experimental-setup,/content/training-data/text_summarization/5/triples/experimental-setup.txt
Open NMT,to build,network architecture,experimental-setup,/content/training-data/text_summarization/5/triples/experimental-setup.txt
Experimental setup,has,encoder and decoder structures,experimental-setup,/content/training-data/text_summarization/5/triples/experimental-setup.txt
encoder and decoder structures,are,two - layer bidirectional Long Short Term Memory Networks ( LSTMs ,experimental-setup,/content/training-data/text_summarization/5/triples/experimental-setup.txt
Experimental setup,dimensions,word embeddings and RNN,experimental-setup,/content/training-data/text_summarization/5/triples/experimental-setup.txt
word embeddings and RNN,are both,500,experimental-setup,/content/training-data/text_summarization/5/triples/experimental-setup.txt
Experimental setup,On,our computer,experimental-setup,/content/training-data/text_summarization/5/triples/experimental-setup.txt
our computer,Memory,16G,experimental-setup,/content/training-data/text_summarization/5/triples/experimental-setup.txt
our computer,CPU,i7-7700 K,experimental-setup,/content/training-data/text_summarization/5/triples/experimental-setup.txt
our computer,GPU,GTX 1080,experimental-setup,/content/training-data/text_summarization/5/triples/experimental-setup.txt
our computer,training spends,about 2 days,experimental-setup,/content/training-data/text_summarization/5/triples/experimental-setup.txt
Experimental setup,During test,beam search,experimental-setup,/content/training-data/text_summarization/5/triples/experimental-setup.txt
beam search,of size,5,experimental-setup,/content/training-data/text_summarization/5/triples/experimental-setup.txt
5,to generate,summaries,experimental-setup,/content/training-data/text_summarization/5/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/text_summarization/5/triples/results.txt
Results,measure,linguistic quality,results,/content/training-data/text_summarization/5/triples/results.txt
linguistic quality,of,generated summaries,results,/content/training-data/text_summarization/5/triples/results.txt
linguistic quality,performance of,Re 3 Sum,results,/content/training-data/text_summarization/5/triples/results.txt
Re 3 Sum,almost the same as,soft templates,results,/content/training-data/text_summarization/5/triples/results.txt
Results,examine,performance,results,/content/training-data/text_summarization/5/triples/results.txt
performance,of directly regarding,soft templates,results,/content/training-data/text_summarization/5/triples/results.txt
soft templates,as,output summaries,results,/content/training-data/text_summarization/5/triples/results.txt
soft templates,introduce,five types of different soft templates,results,/content/training-data/text_summarization/5/triples/results.txt
five types of different soft templates,comparing,Max and First,results,/content/training-data/text_summarization/5/triples/results.txt
Max and First,observe that,improving capacity,results,/content/training-data/text_summarization/5/triples/results.txt
improving capacity,of,Retrieve module,results,/content/training-data/text_summarization/5/triples/results.txt
Retrieve module,is,high,results,/content/training-data/text_summarization/5/triples/results.txt
five types of different soft templates,has,Optimal,results,/content/training-data/text_summarization/5/triples/results.txt
Optimal,greatly exceeds,all the state - of - the - art approaches,results,/content/training-data/text_summarization/5/triples/results.txt
five types of different soft templates,has,Rerank,results,/content/training-data/text_summarization/5/triples/results.txt
Rerank,largely outperforms,First,results,/content/training-data/text_summarization/5/triples/results.txt
five types of different soft templates,performance of,Random,results,/content/training-data/text_summarization/5/triples/results.txt
Random,is,terrible,results,/content/training-data/text_summarization/5/triples/results.txt
Results,investigate,soft templates,results,/content/training-data/text_summarization/5/triples/results.txt
soft templates,with,different templates given,results,/content/training-data/text_summarization/5/triples/results.txt
different templates given,has,our model,results,/content/training-data/text_summarization/5/triples/results.txt
our model,likely to generate,dissimilar summaries,results,/content/training-data/text_summarization/5/triples/results.txt
soft templates,affect,our model,results,/content/training-data/text_summarization/5/triples/results.txt
soft templates,provided,more high - quality templates,results,/content/training-data/text_summarization/5/triples/results.txt
more high - quality templates,achieved,higher ROUGE scores,results,/content/training-data/text_summarization/5/triples/results.txt
soft templates,manually inspect,summaries,results,/content/training-data/text_summarization/5/triples/results.txt
summaries,find,outputs of Re 3 Sum,results,/content/training-data/text_summarization/5/triples/results.txt
outputs of Re 3 Sum,longer and more flu - ent than,outputs of OpenNMT,results,/content/training-data/text_summarization/5/triples/results.txt
summaries,generated by,different methods,results,/content/training-data/text_summarization/5/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/text_summarization/0/triples/baselines.txt
Baselines,has,CGU,baselines,/content/training-data/text_summarization/0/triples/baselines.txt
CGU,propose to use,convolutional gated unit,baselines,/content/training-data/text_summarization/0/triples/baselines.txt
Baselines,has,S2S,baselines,/content/training-data/text_summarization/0/triples/baselines.txt
S2S,name,Sequence - to - sequence framework,baselines,/content/training-data/text_summarization/0/triples/baselines.txt
Baselines,has,S2SR,baselines,/content/training-data/text_summarization/0/triples/baselines.txt
S2SR,add,reader attention on attention distribution ? t,baselines,/content/training-data/text_summarization/0/triples/baselines.txt
reader attention on attention distribution ? t,in,decoding step,baselines,/content/training-data/text_summarization/0/triples/baselines.txt
Baselines,has,LEAD1,baselines,/content/training-data/text_summarization/0/triples/baselines.txt
LEAD1,selects,first sentence of document as the summary,baselines,/content/training-data/text_summarization/0/triples/baselines.txt
Baselines,has,TextRank,baselines,/content/training-data/text_summarization/0/triples/baselines.txt
TextRank,add,each sentence,baselines,/content/training-data/text_summarization/0/triples/baselines.txt
each sentence,as,vertex,baselines,/content/training-data/text_summarization/0/triples/baselines.txt
TextRank,use,link,baselines,/content/training-data/text_summarization/0/triples/baselines.txt
link,to represent,semantic similarity,baselines,/content/training-data/text_summarization/0/triples/baselines.txt
TextRank,propose to build,graph,baselines,/content/training-data/text_summarization/0/triples/baselines.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/text_summarization/0/triples/ablation-analysis.txt
Ablation analysis,has,RASG w/o DM,ablation-analysis,/content/training-data/text_summarization/0/triples/ablation-analysis.txt
RASG w/o DM,offers a decrease of,10 . 22 %,ablation-analysis,/content/training-data/text_summarization/0/triples/ablation-analysis.txt
10 . 22 %,compared with,RASG,ablation-analysis,/content/training-data/text_summarization/0/triples/ablation-analysis.txt
RASG,in terms of,ROUGE - L,ablation-analysis,/content/training-data/text_summarization/0/triples/ablation-analysis.txt
Ablation analysis,has,goal tracker,ablation-analysis,/content/training-data/text_summarization/0/triples/ablation-analysis.txt
goal tracker,compared with,RASG and RASG w / o GT,ablation-analysis,/content/training-data/text_summarization/0/triples/ablation-analysis.txt
RASG and RASG w / o GT,offers,RASG w/ o GTD,ablation-analysis,/content/training-data/text_summarization/0/triples/ablation-analysis.txt
RASG w/ o GTD,decrease of,45. 23 % and 17.88 %,ablation-analysis,/content/training-data/text_summarization/0/triples/ablation-analysis.txt
45. 23 % and 17.88 %,in terms of,ROUGE - 1,ablation-analysis,/content/training-data/text_summarization/0/triples/ablation-analysis.txt
Ablation analysis,has,discriminator,ablation-analysis,/content/training-data/text_summarization/0/triples/ablation-analysis.txt
discriminator,increment of,17.51 %,ablation-analysis,/content/training-data/text_summarization/0/triples/ablation-analysis.txt
17.51 %,from,RASG w / o GTD to RASG w / o GT,ablation-analysis,/content/training-data/text_summarization/0/triples/ablation-analysis.txt
RASG w / o GTD to RASG w / o GT,in terms of,ROUGE - L,ablation-analysis,/content/training-data/text_summarization/0/triples/ablation-analysis.txt
discriminator,provides,scalar training signal L g c,ablation-analysis,/content/training-data/text_summarization/0/triples/ablation-analysis.txt
scalar training signal L g c,for,generator training,ablation-analysis,/content/training-data/text_summarization/0/triples/ablation-analysis.txt
discriminator,provides,feature vector F ( m t ,ablation-analysis,/content/training-data/text_summarization/0/triples/ablation-analysis.txt
feature vector F ( m t ),for,goal tracker,ablation-analysis,/content/training-data/text_summarization/0/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/text_summarization/0/triples/model.txt
Model,treat,decoder attention weights,model,/content/training-data/text_summarization/0/triples/model.txt
decoder attention weights,as,focused aspect of the generated summary,model,/content/training-data/text_summarization/0/triples/model.txt
focused aspect of the generated summary,a.k.a.,decoder focused aspect,model,/content/training-data/text_summarization/0/triples/model.txt
Model,After,each decoding step,model,/content/training-data/text_summarization/0/triples/model.txt
each decoding step,designed,supervisor,model,/content/training-data/text_summarization/0/triples/model.txt
supervisor,to measure,distance,model,/content/training-data/text_summarization/0/triples/model.txt
distance,between,reader focused aspect and the decoder focused aspect,model,/content/training-data/text_summarization/0/triples/model.txt
Model,calculate alignment between,reader comments words and document words,model,/content/training-data/text_summarization/0/triples/model.txt
reader comments words and document words,regarded as,reader attention,model,/content/training-data/text_summarization/0/triples/model.txt
reader attention,representing,reader focused aspect,model,/content/training-data/text_summarization/0/triples/model.txt
Model,training of,framework RASG,model,/content/training-data/text_summarization/0/triples/model.txt
framework RASG,conducted in,adversarial way,model,/content/training-data/text_summarization/0/triples/model.txt
Model,propose,summarization framework,model,/content/training-data/text_summarization/0/triples/model.txt
summarization framework,incorporates,reader comments,model,/content/training-data/text_summarization/0/triples/model.txt
reader comments,to improve,summarization performance,model,/content/training-data/text_summarization/0/triples/model.txt
summarization framework,named,reader - aware summary generator ( RASG ,model,/content/training-data/text_summarization/0/triples/model.txt
Model,employed,seq2seq architecture with attention mechanism,model,/content/training-data/text_summarization/0/triples/model.txt
seq2seq architecture with attention mechanism,as,basic summary generator,model,/content/training-data/text_summarization/0/triples/model.txt
Contribution,has research problem,Abstractive Text Summarization,research-problem,/content/training-data/text_summarization/0/triples/research-problem.txt
Contribution,has research problem,neural abstractive summarization,research-problem,/content/training-data/text_summarization/0/triples/research-problem.txt
Contribution,has research problem,reader - aware abstractive summary generation,research-problem,/content/training-data/text_summarization/0/triples/research-problem.txt
Contribution,has research problem,abstractive summarization,research-problem,/content/training-data/text_summarization/0/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/text_summarization/0/triples/experimental-setup.txt
Experimental setup,employ,beam search,experimental-setup,/content/training-data/text_summarization/0/triples/experimental-setup.txt
beam search,with,beam size 5,experimental-setup,/content/training-data/text_summarization/0/triples/experimental-setup.txt
beam size 5,to generate,more fluency summary sentence,experimental-setup,/content/training-data/text_summarization/0/triples/experimental-setup.txt
Experimental setup,use,Adagrad optimizer,experimental-setup,/content/training-data/text_summarization/0/triples/experimental-setup.txt
Adagrad optimizer,as,optimizing algorithm,experimental-setup,/content/training-data/text_summarization/0/triples/experimental-setup.txt
Experimental setup,implement,our experiments,experimental-setup,/content/training-data/text_summarization/0/triples/experimental-setup.txt
our experiments,in,TensorFlow,experimental-setup,/content/training-data/text_summarization/0/triples/experimental-setup.txt
TensorFlow,on,NVIDIA P40 GPU,experimental-setup,/content/training-data/text_summarization/0/triples/experimental-setup.txt
Experimental setup,has,word embedding dimension,experimental-setup,/content/training-data/text_summarization/0/triples/experimental-setup.txt
word embedding dimension,set to,256,experimental-setup,/content/training-data/text_summarization/0/triples/experimental-setup.txt
Experimental setup,has,number of hidden units,experimental-setup,/content/training-data/text_summarization/0/triples/experimental-setup.txt
number of hidden units,has,512,experimental-setup,/content/training-data/text_summarization/0/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/text_summarization/0/triples/results.txt
Results,worth noticing,baseline model S2SR,results,/content/training-data/text_summarization/0/triples/results.txt
baseline model S2SR,achieves better performance than,S2S,results,/content/training-data/text_summarization/0/triples/results.txt
Results,see that,RASG,results,/content/training-data/text_summarization/0/triples/results.txt
RASG,achieves,"11.0 % , 9.1 % and 6.6 % increment",results,/content/training-data/text_summarization/0/triples/results.txt
"11.0 % , 9.1 % and 6.6 % increment",over,state - of - the - art method CGU,results,/content/training-data/text_summarization/0/triples/results.txt
state - of - the - art method CGU,in terms of,"ROUGE - 1 , ROUGE - 2 and ROUGE - L",results,/content/training-data/text_summarization/0/triples/results.txt
Contribution,has,Model,model,/content/training-data/text_summarization/2/triples/model.txt
Model,present,structure - infused copy mechanisms,model,/content/training-data/text_summarization/2/triples/model.txt
structure - infused copy mechanisms,to facilitate copying,source words and relations,model,/content/training-data/text_summarization/2/triples/model.txt
source words and relations,to,summary,model,/content/training-data/text_summarization/2/triples/model.txt
summary,based on,semantic and structural importance,model,/content/training-data/text_summarization/2/triples/model.txt
semantic and structural importance,in,source sentences,model,/content/training-data/text_summarization/2/triples/model.txt
Model,incorporating,source syntactic structure,model,/content/training-data/text_summarization/2/triples/model.txt
source syntactic structure,in,neural sentence summarization,model,/content/training-data/text_summarization/2/triples/model.txt
neural sentence summarization,to help,system,model,/content/training-data/text_summarization/2/triples/model.txt
system,identify,summary - worthy content,model,/content/training-data/text_summarization/2/triples/model.txt
system,compose,summaries,model,/content/training-data/text_summarization/2/triples/model.txt
summaries,that preserve,important meaning of the source texts,model,/content/training-data/text_summarization/2/triples/model.txt
Contribution,has research problem,Abstractive Summarization,research-problem,/content/training-data/text_summarization/2/triples/research-problem.txt
Contribution,has research problem,summarization,research-problem,/content/training-data/text_summarization/2/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/text_summarization/2/triples/results.txt
Results,on,Gigaword valid - 2000 dataset,results,/content/training-data/text_summarization/2/triples/results.txt
Gigaword valid - 2000 dataset,found,""" Struct + Hidden "" architecture",results,/content/training-data/text_summarization/2/triples/results.txt
""" Struct + Hidden "" architecture",directly concatenates,structural embeddings,results,/content/training-data/text_summarization/2/triples/results.txt
structural embeddings,with,encoder hidden states,results,/content/training-data/text_summarization/2/triples/results.txt
""" Struct + Hidden "" architecture",outperforms,""" Struct + Input """,results,/content/training-data/text_summarization/2/triples/results.txt
Gigaword valid - 2000 dataset,has,Struct + 2 Way + Word,results,/content/training-data/text_summarization/2/triples/results.txt
Struct + 2 Way + Word,demonstrates,strong performance,results,/content/training-data/text_summarization/2/triples/results.txt
Struct + 2 Way + Word,achieving,"43.21 % , 21. 84 % , and 40.86 % F 1 scores",results,/content/training-data/text_summarization/2/triples/results.txt
"43.21 % , 21. 84 % , and 40.86 % F 1 scores",for,"R - 1 , R - 2 , and R - L",results,/content/training-data/text_summarization/2/triples/results.txt
Gigaword valid - 2000 dataset,present,"R - 1 , R - 2 , and R - L scores",results,/content/training-data/text_summarization/2/triples/results.txt
"R - 1 , R - 2 , and R - L scores",measures,"overlapped unigrams , bigrams , and longest common subsequences",results,/content/training-data/text_summarization/2/triples/results.txt
"overlapped unigrams , bigrams , and longest common subsequences",between,system and reference summaries,results,/content/training-data/text_summarization/2/triples/results.txt
Gigaword valid - 2000 dataset,observe,models,results,/content/training-data/text_summarization/2/triples/results.txt
models,with,structure - infused copy mechanisms,results,/content/training-data/text_summarization/2/triples/results.txt
structure - infused copy mechanisms,superior to,baseline,results,/content/training-data/text_summarization/2/triples/results.txt
Contribution,has,Approach,approach,/content/training-data/text_summarization/13/triples/approach.txt
Approach,prune down,length of the source sequence,approach,/content/training-data/text_summarization/13/triples/approach.txt
Approach,For,document summarization,approach,/content/training-data/text_summarization/13/triples/approach.txt
document summarization,call,coarse - to - fine attention,approach,/content/training-data/text_summarization/13/triples/approach.txt
document summarization,means,dividing the document,approach,/content/training-data/text_summarization/13/triples/approach.txt
dividing the document,into,chunks of text,approach,/content/training-data/text_summarization/13/triples/approach.txt
document summarization,sparsely attending to,one or a few chunks at a time,approach,/content/training-data/text_summarization/13/triples/approach.txt
one or a few chunks at a time,applying,usual full attention,approach,/content/training-data/text_summarization/13/triples/approach.txt
usual full attention,over,those chunks,approach,/content/training-data/text_summarization/13/triples/approach.txt
one or a few chunks at a time,using,hard attention,approach,/content/training-data/text_summarization/13/triples/approach.txt
Approach,to use,two - layer hierarchical attention,approach,/content/training-data/text_summarization/13/triples/approach.txt
two - layer hierarchical attention,Instead of,naively attending to all the words,approach,/content/training-data/text_summarization/13/triples/approach.txt
naively attending to all the words,of,source,approach,/content/training-data/text_summarization/13/triples/approach.txt
Approach,to scale,attention models,approach,/content/training-data/text_summarization/13/triples/approach.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/text_summarization/13/triples/ablation-analysis.txt
Ablation analysis,has,Attention Heatmaps,ablation-analysis,/content/training-data/text_summarization/13/triples/ablation-analysis.txt
Attention Heatmaps,In,C2 F,ablation-analysis,/content/training-data/text_summarization/13/triples/ablation-analysis.txt
C2 F,get,very sharp attention,ablation-analysis,/content/training-data/text_summarization/13/triples/ablation-analysis.txt
very sharp attention,on,some rows,ablation-analysis,/content/training-data/text_summarization/13/triples/ablation-analysis.txt
Attention Heatmaps,In,HIER,ablation-analysis,/content/training-data/text_summarization/13/triples/ablation-analysis.txt
HIER,observe,attention becomes washed out,ablation-analysis,/content/training-data/text_summarization/13/triples/ablation-analysis.txt
attention becomes washed out,averaging,all of the encoder hidden states,ablation-analysis,/content/training-data/text_summarization/13/triples/ablation-analysis.txt
Ablation analysis,has,Sharpness of Attention,ablation-analysis,/content/training-data/text_summarization/13/triples/ablation-analysis.txt
Sharpness of Attention,compute,entropy numbers,ablation-analysis,/content/training-data/text_summarization/13/triples/ablation-analysis.txt
entropy numbers,by averaging,over all generated words,ablation-analysis,/content/training-data/text_summarization/13/triples/ablation-analysis.txt
over all generated words,in,validation set,ablation-analysis,/content/training-data/text_summarization/13/triples/ablation-analysis.txt
Sharpness of Attention,note,entropy of C2F,ablation-analysis,/content/training-data/text_summarization/13/triples/ablation-analysis.txt
entropy of C2F,is,very low,ablation-analysis,/content/training-data/text_summarization/13/triples/ablation-analysis.txt
Sharpness of Attention,has,model,ablation-analysis,/content/training-data/text_summarization/13/triples/ablation-analysis.txt
model,learns to focus on,few top - level chunks,ablation-analysis,/content/training-data/text_summarization/13/triples/ablation-analysis.txt
few top - level chunks,of,document,ablation-analysis,/content/training-data/text_summarization/13/triples/ablation-analysis.txt
document,over,course of generation,ablation-analysis,/content/training-data/text_summarization/13/triples/ablation-analysis.txt
Contribution,has research problem,Document Summarization,research-problem,/content/training-data/text_summarization/13/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
Experimental setup,At,test time,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
test time,run,beam search,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
beam search,to produce,summary,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
summary,with,beam size of 5,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
Experimental setup,use,2 layer LSTMs,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
2 layer LSTMs,with,500 hidden units,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
Experimental setup,use,dropout,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
dropout,between,stacked LSTM hidden states and before the final word generator layer,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
stacked LSTM hidden states and before the final word generator layer,to regularize,dropout probability 0.3,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
Experimental setup,For,convolutional layers,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
convolutional layers,use,kernel width,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
kernel width,of,6 and 600 filters,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
Experimental setup,implemented using,Torch,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
Torch,based on,past version of the Open NMT system,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
Experimental setup,initialize,all other parameters,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
all other parameters,as uniform in,"interval [ ? 0.1 , 0.1 ]",experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
Experimental setup,initialize,learning rate,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
learning rate,to,1,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
1,for,rest of the model,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
learning rate,to,0.1,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
0.1,for,top - level encoder,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
learning rate,begin decaying it by,a factor of 0.5,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
a factor of 0.5,after,validation perplexity,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
validation perplexity,stops,decreasing,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
Experimental setup,initialize,word embeddings,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
word embeddings,with,300 dimensional word2vec embeddings,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
Experimental setup,has,Positional embeddings,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
Positional embeddings,have,dimension 25,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
Experimental setup,train with,minibatch stochastic gradient descent ( SGD ,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
minibatch stochastic gradient descent ( SGD ),with,batch size 20,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
batch size 20,for,20 epochs,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
minibatch stochastic gradient descent ( SGD ),renormalizing gradients,below norm 5,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
Experimental setup,ran,our experiments,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
our experiments,on,12GB Geforce GTX Titan X GPU,experimental-setup,/content/training-data/text_summarization/13/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/text_summarization/13/triples/results.txt
Results,has,ILP model,results,/content/training-data/text_summarization/13/triples/results.txt
ILP model,ROUGE scores,surprisingly low,results,/content/training-data/text_summarization/13/triples/results.txt
Results,has,C2 F,results,/content/training-data/text_summarization/13/triples/results.txt
C2 F,significantly worse than,soft attention results,results,/content/training-data/text_summarization/13/triples/results.txt
Contribution,has,Model,model,/content/training-data/text_summarization/7/triples/model.txt
Model,additional component,wordfrequency estimation ( WFE ) sub-model,model,/content/training-data/text_summarization/7/triples/model.txt
Model,has,WFE sub-model,model,/content/training-data/text_summarization/7/triples/model.txt
WFE sub-model,explicitly manages,how many times each word has been generated so far,model,/content/training-data/text_summarization/7/triples/model.txt
WFE sub-model,explicitly manages,might be generated in the future,model,/content/training-data/text_summarization/7/triples/model.txt
Model,jointly estimate,upper-bound frequency,model,/content/training-data/text_summarization/7/triples/model.txt
upper-bound frequency,of,each target vocabulary,model,/content/training-data/text_summarization/7/triples/model.txt
each target vocabulary,that can occur in,summary,model,/content/training-data/text_summarization/7/triples/model.txt
summary,during,encoding process,model,/content/training-data/text_summarization/7/triples/model.txt
upper-bound frequency,control,output words,model,/content/training-data/text_summarization/7/triples/model.txt
output words,in,each decoding step,model,/content/training-data/text_summarization/7/triples/model.txt
Contribution,has research problem,Neural Abstractive Summarization,research-problem,/content/training-data/text_summarization/7/triples/research-problem.txt
Contribution,has research problem,abstractive summarization ( ABS ,research-problem,/content/training-data/text_summarization/7/triples/research-problem.txt
Contribution,has,Baselines,baselines,/content/training-data/text_summarization/1/triples/baselines.txt
Baselines,has,Truncated Sampling,baselines,/content/training-data/text_summarization/1/triples/baselines.txt
Truncated Sampling,randomly samples,words,baselines,/content/training-data/text_summarization/1/triples/baselines.txt
words,from,top - 10 candidates,baselines,/content/training-data/text_summarization/1/triples/baselines.txt
top - 10 candidates,of,distribution,baselines,/content/training-data/text_summarization/1/triples/baselines.txt
distribution,at,decoding step,baselines,/content/training-data/text_summarization/1/triples/baselines.txt
Baselines,has,Mixture Decoder,baselines,/content/training-data/text_summarization/1/triples/baselines.txt
Mixture Decoder,conducts,parallel greedy decoding,baselines,/content/training-data/text_summarization/1/triples/baselines.txt
Mixture Decoder,constructs,hard - MoE of K decoders,baselines,/content/training-data/text_summarization/1/triples/baselines.txt
hard - MoE of K decoders,with,uniform mixing coefficient,baselines,/content/training-data/text_summarization/1/triples/baselines.txt
Baselines,has,Beam Search,baselines,/content/training-data/text_summarization/1/triples/baselines.txt
Beam Search,keeps,K hypotheses,baselines,/content/training-data/text_summarization/1/triples/baselines.txt
K hypotheses,with,highest log-probability scores,baselines,/content/training-data/text_summarization/1/triples/baselines.txt
highest log-probability scores,at,each decoding step,baselines,/content/training-data/text_summarization/1/triples/baselines.txt
Baselines,has,Mixture Selector ( Ours ,baselines,/content/training-data/text_summarization/1/triples/baselines.txt
Mixture Selector ( Ours ),construct,hard - MoE of K SELECTORs,baselines,/content/training-data/text_summarization/1/triples/baselines.txt
hard - MoE of K SELECTORs,with,uniform mixing coefficient,baselines,/content/training-data/text_summarization/1/triples/baselines.txt
uniform mixing coefficient,infers,K different focus from source sequence,baselines,/content/training-data/text_summarization/1/triples/baselines.txt
Contribution,has,Model,model,/content/training-data/text_summarization/1/triples/model.txt
Model,separates,diversification and generation stages,model,/content/training-data/text_summarization/1/triples/model.txt
Model,has,diversification stage,model,/content/training-data/text_summarization/1/triples/model.txt
diversification stage,leverages,content selection,model,/content/training-data/text_summarization/1/triples/model.txt
content selection,map,source to multiple sequences,model,/content/training-data/text_summarization/1/triples/model.txt
Model,has,generation stage,model,/content/training-data/text_summarization/1/triples/model.txt
generation stage,uses,standard encoder - decoder model,model,/content/training-data/text_summarization/1/triples/model.txt
standard encoder - decoder model,generate,target sequence given each selected content from the source,model,/content/training-data/text_summarization/1/triples/model.txt
Model,present,generic module,model,/content/training-data/text_summarization/1/triples/model.txt
generic module,called,SELECTOR,model,/content/training-data/text_summarization/1/triples/model.txt
SELECTOR,specialized for,diversification,model,/content/training-data/text_summarization/1/triples/model.txt
generic module,used as,plug - and - play,model,/content/training-data/text_summarization/1/triples/model.txt
plug - and - play,to,arbitrary encoder - decoder model,model,/content/training-data/text_summarization/1/triples/model.txt
arbitrary encoder - decoder model,for,generation without architecture change,model,/content/training-data/text_summarization/1/triples/model.txt
Contribution,has research problem,Diverse Sequence Generation,research-problem,/content/training-data/text_summarization/1/triples/research-problem.txt
Contribution,has research problem,Generating diverse sequences,research-problem,/content/training-data/text_summarization/1/triples/research-problem.txt
Contribution,has research problem,Generating target sequences given a source sequence,research-problem,/content/training-data/text_summarization/1/triples/research-problem.txt
Contribution,has research problem,sequence generation,research-problem,/content/training-data/text_summarization/1/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/text_summarization/1/triples/experimental-setup.txt
Experimental setup,tie,weights,experimental-setup,/content/training-data/text_summarization/1/triples/experimental-setup.txt
weights,of,encoder embedding,experimental-setup,/content/training-data/text_summarization/1/triples/experimental-setup.txt
weights,of,decoder embedding,experimental-setup,/content/training-data/text_summarization/1/triples/experimental-setup.txt
weights,of,decoder output layers,experimental-setup,/content/training-data/text_summarization/1/triples/experimental-setup.txt
Experimental setup,use,"Adam ( Kingma and Ba , 2015 ) optimizer",experimental-setup,/content/training-data/text_summarization/1/triples/experimental-setup.txt
"Adam ( Kingma and Ba , 2015 ) optimizer",with,learning rate 0.001,experimental-setup,/content/training-data/text_summarization/1/triples/experimental-setup.txt
"Adam ( Kingma and Ba , 2015 ) optimizer",with,momentum parmeters ? 1 = 0.9 and ? 2 = 0.999,experimental-setup,/content/training-data/text_summarization/1/triples/experimental-setup.txt
Experimental setup,implemented in,PyTorch,experimental-setup,/content/training-data/text_summarization/1/triples/experimental-setup.txt
Experimental setup,has,Minibatch size,experimental-setup,/content/training-data/text_summarization/1/triples/experimental-setup.txt
Minibatch size,is,64 and 32,experimental-setup,/content/training-data/text_summarization/1/triples/experimental-setup.txt
64 and 32,for,question generation and abstractive summarization,experimental-setup,/content/training-data/text_summarization/1/triples/experimental-setup.txt
Experimental setup,trained on,single Tesla P40 GPU,experimental-setup,/content/training-data/text_summarization/1/triples/experimental-setup.txt
single Tesla P40 GPU,based on,NAVER Smart Machine Learning ( NSML ) platform,experimental-setup,/content/training-data/text_summarization/1/triples/experimental-setup.txt
Experimental setup,train,up to 20 epochs,experimental-setup,/content/training-data/text_summarization/1/triples/experimental-setup.txt
up to 20 epochs,select,checkpoint,experimental-setup,/content/training-data/text_summarization/1/triples/experimental-setup.txt
checkpoint,with,best oracle metric,experimental-setup,/content/training-data/text_summarization/1/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/text_summarization/1/triples/results.txt
Results,has,Diversity vs. Accuracy Trade - off,results,/content/training-data/text_summarization/1/triples/results.txt
Diversity vs. Accuracy Trade - off,show,mixture SELECTOR method,results,/content/training-data/text_summarization/1/triples/results.txt
mixture SELECTOR method,achieves,best trade - off,results,/content/training-data/text_summarization/1/triples/results.txt
best trade - off,between,diversity and accuracy,results,/content/training-data/text_summarization/1/triples/results.txt
mixture SELECTOR method,outperforms,all baselines,results,/content/training-data/text_summarization/1/triples/results.txt
all baselines,in,Top - 1 and oracle metrics,results,/content/training-data/text_summarization/1/triples/results.txt
Diversity vs. Accuracy Trade - off,has,our method,results,/content/training-data/text_summarization/1/triples/results.txt
our method,scores,state - of - the - art BLEU,results,/content/training-data/text_summarization/1/triples/results.txt
state - of - the - art BLEU,in,question generation,results,/content/training-data/text_summarization/1/triples/results.txt
question generation,on,SQuAD and ROUGE,results,/content/training-data/text_summarization/1/triples/results.txt
SQuAD and ROUGE,in,abstractive summarization in CNN - DM,results,/content/training-data/text_summarization/1/triples/results.txt
Results,has,Diversity vs. Number of Mixtures,results,/content/training-data/text_summarization/1/triples/results.txt
Diversity vs. Number of Mixtures,compare,effect of number of mixtures,results,/content/training-data/text_summarization/1/triples/results.txt
effect of number of mixtures,in,SELECTOR and Mixture Decoder,results,/content/training-data/text_summarization/1/triples/results.txt
Diversity vs. Number of Mixtures,for,Mixture Decoder,results,/content/training-data/text_summarization/1/triples/results.txt
Mixture Decoder,show,pairwise similarity increases ( diversity ?,results,/content/training-data/text_summarization/1/triples/results.txt
pairwise similarity increases ( diversity ?),when,number of mixtures increases,results,/content/training-data/text_summarization/1/triples/results.txt
Contribution,has,Model,model,/content/training-data/text_summarization/9/triples/model.txt
Model,consists of,conditional recurrent neural network,model,/content/training-data/text_summarization/9/triples/model.txt
conditional recurrent neural network,acts as,decoder,model,/content/training-data/text_summarization/9/triples/model.txt
decoder,to generate,summary,model,/content/training-data/text_summarization/9/triples/model.txt
summary,of,input sentence,model,/content/training-data/text_summarization/9/triples/model.txt
Model,has,decoder,model,/content/training-data/text_summarization/9/triples/model.txt
decoder,takes,conditioning input,model,/content/training-data/text_summarization/9/triples/model.txt
conditioning input,at,every time step,model,/content/training-data/text_summarization/9/triples/model.txt
conditioning input,output of,encoder module,model,/content/training-data/text_summarization/9/triples/model.txt
Model,has,encoder,model,/content/training-data/text_summarization/9/triples/model.txt
encoder,computes scores over,words,model,/content/training-data/text_summarization/9/triples/model.txt
words,in,input sentence,model,/content/training-data/text_summarization/9/triples/model.txt
encoder,uses,convolutional network,model,/content/training-data/text_summarization/9/triples/model.txt
convolutional network,to encode,input words,model,/content/training-data/text_summarization/9/triples/model.txt
Model,has,decoder and encoder,model,/content/training-data/text_summarization/9/triples/model.txt
decoder and encoder,jointly trained on,data set,model,/content/training-data/text_summarization/9/triples/model.txt
data set,consisting of,sentence - summary pairs,model,/content/training-data/text_summarization/9/triples/model.txt
Contribution,has research problem,Abstractive Sentence Summarization,research-problem,/content/training-data/text_summarization/9/triples/research-problem.txt
Contribution,has research problem,text summarization,research-problem,/content/training-data/text_summarization/9/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/text_summarization/9/triples/experimental-setup.txt
Experimental setup,optimize,our loss,experimental-setup,/content/training-data/text_summarization/9/triples/experimental-setup.txt
our loss,used,stochastic gradient descent,experimental-setup,/content/training-data/text_summarization/9/triples/experimental-setup.txt
stochastic gradient descent,with,mini-batches,experimental-setup,/content/training-data/text_summarization/9/triples/experimental-setup.txt
mini-batches,of size,32,experimental-setup,/content/training-data/text_summarization/9/triples/experimental-setup.txt
Experimental setup,implemented,our models,experimental-setup,/content/training-data/text_summarization/9/triples/experimental-setup.txt
our models,in,Torch library,experimental-setup,/content/training-data/text_summarization/9/triples/experimental-setup.txt
Experimental setup,For,decoder,experimental-setup,/content/training-data/text_summarization/9/triples/experimental-setup.txt
decoder,experimented with,Elman RNN,experimental-setup,/content/training-data/text_summarization/9/triples/experimental-setup.txt
decoder,experimented with,Long - Short Term Memory ( LSTM ) architecture,experimental-setup,/content/training-data/text_summarization/9/triples/experimental-setup.txt
Experimental setup,chose,hyper - parameters,experimental-setup,/content/training-data/text_summarization/9/triples/experimental-setup.txt
hyper - parameters,based on,grid search,experimental-setup,/content/training-data/text_summarization/9/triples/experimental-setup.txt
hyper - parameters,picked the one which gave,best perplexity,experimental-setup,/content/training-data/text_summarization/9/triples/experimental-setup.txt
best perplexity,on,validation set,experimental-setup,/content/training-data/text_summarization/9/triples/experimental-setup.txt
Experimental setup,has,final Elman architecture ( RAS - Elman ,experimental-setup,/content/training-data/text_summarization/9/triples/experimental-setup.txt
final Elman architecture ( RAS - Elman ),uses,single layer,experimental-setup,/content/training-data/text_summarization/9/triples/experimental-setup.txt
single layer,with,"H = 512 , ? = 0.5 , ? = 2 , and ? = 10",experimental-setup,/content/training-data/text_summarization/9/triples/experimental-setup.txt
Experimental setup,has,LSTM model ( RAS - LSTM ,experimental-setup,/content/training-data/text_summarization/9/triples/experimental-setup.txt
LSTM model ( RAS - LSTM ),has,single layer,experimental-setup,/content/training-data/text_summarization/9/triples/experimental-setup.txt
single layer,with,"H = 512 , ? = 0.1 , ? = 2 , and ? = 10",experimental-setup,/content/training-data/text_summarization/9/triples/experimental-setup.txt
Experimental setup,During,training,experimental-setup,/content/training-data/text_summarization/9/triples/experimental-setup.txt
training,measure,perplexity,experimental-setup,/content/training-data/text_summarization/9/triples/experimental-setup.txt
perplexity,of,summaries,experimental-setup,/content/training-data/text_summarization/9/triples/experimental-setup.txt
summaries,in,validation set,experimental-setup,/content/training-data/text_summarization/9/triples/experimental-setup.txt
training,adjust,hyper - parameters,experimental-setup,/content/training-data/text_summarization/9/triples/experimental-setup.txt
hyper - parameters,such as,learning rate,experimental-setup,/content/training-data/text_summarization/9/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/text_summarization/9/triples/results.txt
Results,shows,RAS - Elman and RAS - LSTM models,results,/content/training-data/text_summarization/9/triples/results.txt
RAS - Elman and RAS - LSTM models,achieve,lower perplexity,results,/content/training-data/text_summarization/9/triples/results.txt
lower perplexity,than,ABS,results,/content/training-data/text_summarization/9/triples/results.txt
Results,has,ROUGE results,results,/content/training-data/text_summarization/9/triples/results.txt
ROUGE results,show,our models,results,/content/training-data/text_summarization/9/triples/results.txt
our models,comfortably outperform,ABS and ABS +,results,/content/training-data/text_summarization/9/triples/results.txt
ABS and ABS +,by,wide margin,results,/content/training-data/text_summarization/9/triples/results.txt
wide margin,on,all metrics,results,/content/training-data/text_summarization/9/triples/results.txt
Results,has,RAS - LSTM,results,/content/training-data/text_summarization/9/triples/results.txt
RAS - LSTM,performs slightly worse than,RAS - Elman,results,/content/training-data/text_summarization/9/triples/results.txt
Results,On,DUC - 2004,results,/content/training-data/text_summarization/9/triples/results.txt
DUC - 2004,show,our models,results,/content/training-data/text_summarization/9/triples/results.txt
our models,better than,ABS +,results,/content/training-data/text_summarization/9/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
Baselines,has,lvt2k - 1sent and lvt5k - 1sent,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
lvt2k - 1sent and lvt5k - 1sent,utilize,trick,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
trick,to control,vocabulary size,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
vocabulary size,to improve,training efficiency,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
Baselines,has,RNN and RNN - context,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
RNN and RNN - context,are,two seq2seq architectures,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
Baselines,has,ASC+ FSC,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
ASC+ FSC,uses,generative model,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
generative model,with,attention mechanism,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
attention mechanism,to conduct,sentence compression problem,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
Baselines,has,ABS and ABS +,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
ABS and ABS +,with,local attention modeling,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
local attention modeling,for,abstractive sentence summarization,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
Baselines,has,TOPIARY,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
TOPIARY,for,compressive text summarization,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
compressive text summarization,combines,system using linguistic based transformations,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
compressive text summarization,combines,an unsupervised topic detection algorithm,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
Baselines,has,LenEmb,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
LenEmb,control,summary length,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
summary length,by considering,length embedding vector,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
length embedding vector,as,input,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
Baselines,has,Copy Net,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
Copy Net,integrates,copying mechanism,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
copying mechanism,into,sequence - to sequence framework,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
Baselines,has,RAS - LSTM and RAS - Elman,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
RAS - LSTM and RAS - Elman,use,convolutional encoders,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
convolutional encoders,to handle,source information,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
RAS - LSTM and RAS - Elman,consider,words and word positions,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
words and word positions,as,input,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
Baselines,has,MOSES +,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
MOSES +,uses,phrasebased statistical machine translation system,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
phrasebased statistical machine translation system,trained on,Gigaword,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
Gigaword,to produce,summaries,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
Baselines,has,RNN - distract,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
RNN - distract,uses,new attention mechanism,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
new attention mechanism,by distracting,historical attention,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
historical attention,in,decoding steps,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
Baselines,has,ABS +,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
ABS +,combined with,additional log - linear extractive summarization model,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
additional log - linear extractive summarization model,with,handcrafted features,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
ABS +,trained on,Gigaword corpus,baselines,/content/training-data/text_summarization/6/triples/baselines.txt
Contribution,has,Model,model,/content/training-data/text_summarization/6/triples/model.txt
Model,employ,Variational Auto - Encoders ( VAEs ,model,/content/training-data/text_summarization/6/triples/model.txt
Variational Auto - Encoders ( VAEs ),as,base model,model,/content/training-data/text_summarization/6/triples/model.txt
base model,for,our generative framework,model,/content/training-data/text_summarization/6/triples/model.txt
our generative framework,can handle,inference problem,model,/content/training-data/text_summarization/6/triples/model.txt
inference problem,associated with,complex generative modeling,model,/content/training-data/text_summarization/6/triples/model.txt
Model,add,historical dependencies,model,/content/training-data/text_summarization/6/triples/model.txt
historical dependencies,on,latent variables,model,/content/training-data/text_summarization/6/triples/model.txt
latent variables,of,VAEs,model,/content/training-data/text_summarization/6/triples/model.txt
Model,design,new framework,model,/content/training-data/text_summarization/6/triples/model.txt
new framework,based on,sequence to - sequence oriented encoder - decoder model,model,/content/training-data/text_summarization/6/triples/model.txt
sequence to - sequence oriented encoder - decoder model,equipped with,latent structure modeling component,model,/content/training-data/text_summarization/6/triples/model.txt
Model,decoded,target summaries,model,/content/training-data/text_summarization/6/triples/model.txt
target summaries,based on both,discriminative deterministic variables,model,/content/training-data/text_summarization/6/triples/model.txt
target summaries,based on both,generative latent structural information,model,/content/training-data/text_summarization/6/triples/model.txt
Model,propose,deep recurrent generative decoder ( DRGD ,model,/content/training-data/text_summarization/6/triples/model.txt
deep recurrent generative decoder ( DRGD ),for,latent structure modeling,model,/content/training-data/text_summarization/6/triples/model.txt
Model,integrated,standard discriminative deterministic decoder and the recurrent generative decoder,model,/content/training-data/text_summarization/6/triples/model.txt
standard discriminative deterministic decoder and the recurrent generative decoder,into,unified decoding framework,model,/content/training-data/text_summarization/6/triples/model.txt
Contribution,has research problem,Abstractive Text Summarization,research-problem,/content/training-data/text_summarization/6/triples/research-problem.txt
Contribution,has research problem,Automatic summarization,research-problem,/content/training-data/text_summarization/6/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/text_summarization/6/triples/experimental-setup.txt
Experimental setup,beam size of,decoder,experimental-setup,/content/training-data/text_summarization/6/triples/experimental-setup.txt
decoder,set to,10,experimental-setup,/content/training-data/text_summarization/6/triples/experimental-setup.txt
Experimental setup,For,DUC - 2004,experimental-setup,/content/training-data/text_summarization/6/triples/experimental-setup.txt
DUC - 2004,maximum length of,summaries,experimental-setup,/content/training-data/text_summarization/6/triples/experimental-setup.txt
summaries,is,75 bytes,experimental-setup,/content/training-data/text_summarization/6/triples/experimental-setup.txt
Experimental setup,For,dataset of LCSTS,experimental-setup,/content/training-data/text_summarization/6/triples/experimental-setup.txt
dataset of LCSTS,dimension of,hidden states and latent variables,experimental-setup,/content/training-data/text_summarization/6/triples/experimental-setup.txt
hidden states and latent variables,to,500,experimental-setup,/content/training-data/text_summarization/6/triples/experimental-setup.txt
dataset of LCSTS,dimension of,word embeddings,experimental-setup,/content/training-data/text_summarization/6/triples/experimental-setup.txt
word embeddings,is,350,experimental-setup,/content/training-data/text_summarization/6/triples/experimental-setup.txt
dataset of LCSTS,maximum length of,documents and summaries,experimental-setup,/content/training-data/text_summarization/6/triples/experimental-setup.txt
documents and summaries,is,120 and 25,experimental-setup,/content/training-data/text_summarization/6/triples/experimental-setup.txt
Experimental setup,has,neural network based framework,experimental-setup,/content/training-data/text_summarization/6/triples/experimental-setup.txt
neural network based framework,implemented using,Theano,experimental-setup,/content/training-data/text_summarization/6/triples/experimental-setup.txt
Experimental setup,has,Adadelta,experimental-setup,/content/training-data/text_summarization/6/triples/experimental-setup.txt
Adadelta,with,hyperparameter ? = 0.95,experimental-setup,/content/training-data/text_summarization/6/triples/experimental-setup.txt
hyperparameter ? = 0.95,used for,gradient based optimization,experimental-setup,/content/training-data/text_summarization/6/triples/experimental-setup.txt
Experimental setup,on,English dataset Gigawords,experimental-setup,/content/training-data/text_summarization/6/triples/experimental-setup.txt
English dataset Gigawords,set,dimension,experimental-setup,/content/training-data/text_summarization/6/triples/experimental-setup.txt
dimension,of,hidden states and latent variables,experimental-setup,/content/training-data/text_summarization/6/triples/experimental-setup.txt
hidden states and latent variables,to,500,experimental-setup,/content/training-data/text_summarization/6/triples/experimental-setup.txt
dimension,of,word embeddings,experimental-setup,/content/training-data/text_summarization/6/triples/experimental-setup.txt
word embeddings,to,300,experimental-setup,/content/training-data/text_summarization/6/triples/experimental-setup.txt
English dataset Gigawords,maximum length of,documents and summaries,experimental-setup,/content/training-data/text_summarization/6/triples/experimental-setup.txt
documents and summaries,is,100 and 50,experimental-setup,/content/training-data/text_summarization/6/triples/experimental-setup.txt
English dataset Gigawords,has,batch size,experimental-setup,/content/training-data/text_summarization/6/triples/experimental-setup.txt
batch size,of,mini-batch training,experimental-setup,/content/training-data/text_summarization/6/triples/experimental-setup.txt
mini-batch training,is,256,experimental-setup,/content/training-data/text_summarization/6/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/text_summarization/6/triples/results.txt
Results,has,ROUGE Evaluation,results,/content/training-data/text_summarization/6/triples/results.txt
ROUGE Evaluation,on,Chinese dataset LCSTS,results,/content/training-data/text_summarization/6/triples/results.txt
Chinese dataset LCSTS,has,Our model DRGD,results,/content/training-data/text_summarization/6/triples/results.txt
Our model DRGD,achieves,best performance,results,/content/training-data/text_summarization/6/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/topic_models/0/triples/baselines.txt
Baselines,has,TF - IDF,baselines,/content/training-data/topic_models/0/triples/baselines.txt
TF - IDF,is a,standard term frequency - inverse document frequency ( TF - IDF ) based document representation,baselines,/content/training-data/topic_models/0/triples/baselines.txt
standard term frequency - inverse document frequency ( TF - IDF ) based document representation,followed by,multi-class logistic regression ( LR ,baselines,/content/training-data/topic_models/0/triples/baselines.txt
Baselines,has,SMM,baselines,/content/training-data/topic_models/0/triples/baselines.txt
SMM,is,non-Bayesian SMM with 1 regularization over the rows in T matrix,baselines,/content/training-data/topic_models/0/triples/baselines.txt
Baselines,has,NVDM,baselines,/content/training-data/topic_models/0/triples/baselines.txt
NVDM,extract,embeddings from NVDM,baselines,/content/training-data/topic_models/0/triples/baselines.txt
embeddings from NVDM,use them for,training linear classifiers,baselines,/content/training-data/topic_models/0/triples/baselines.txt
Baselines,has,ULMFiT,baselines,/content/training-data/topic_models/0/triples/baselines.txt
ULMFiT,is,universal language model,baselines,/content/training-data/topic_models/0/triples/baselines.txt
universal language model,fine - tuned for,classification,baselines,/content/training-data/topic_models/0/triples/baselines.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/topic_models/0/triples/hyperparameters.txt
Hyperparameters,has,regularization weight,hyperparameters,/content/training-data/topic_models/0/triples/hyperparameters.txt
regularization weight,from,"? = { 0.0001 , . . . , 10.0 }",hyperparameters,/content/training-data/topic_models/0/triples/hyperparameters.txt
Hyperparameters,has,embedding dimension,hyperparameters,/content/training-data/topic_models/0/triples/hyperparameters.txt
embedding dimension,chosen from,"K = { 100 , . . . , 800 }",hyperparameters,/content/training-data/topic_models/0/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/topic_models/0/triples/model.txt
Model,learn to represent,each document,model,/content/training-data/topic_models/0/triples/model.txt
each document,in the form of,Gaussian distribution,model,/content/training-data/topic_models/0/triples/model.txt
Gaussian distribution,encoding,uncertainty in its covariance,model,/content/training-data/topic_models/0/triples/model.txt
Model,present,Bayesian subspace multinomial model ( Bayesian SMM ,model,/content/training-data/topic_models/0/triples/model.txt
Bayesian subspace multinomial model ( Bayesian SMM ),as a,generative model,model,/content/training-data/topic_models/0/triples/model.txt
generative model,for,bag - ofwords representation of documents,model,/content/training-data/topic_models/0/triples/model.txt
Model,propose,generative Gaussian classifier,model,/content/training-data/topic_models/0/triples/model.txt
generative Gaussian classifier,exploits,uncertainty,model,/content/training-data/topic_models/0/triples/model.txt
uncertainty,for,topic identification ( ID ,model,/content/training-data/topic_models/0/triples/model.txt
Model,extended,proposed VB framework,model,/content/training-data/topic_models/0/triples/model.txt
proposed VB framework,for,subspace n-gram model,model,/content/training-data/topic_models/0/triples/model.txt
subspace n-gram model,can model,n-gram distribution of words in sentences,model,/content/training-data/topic_models/0/triples/model.txt
Contribution,has research problem,Learning document embeddings,research-problem,/content/training-data/topic_models/0/triples/research-problem.txt
Contribution,has research problem,topic identification,research-problem,/content/training-data/topic_models/0/triples/research-problem.txt
Contribution,has research problem,L EARNING word and document embeddings,research-problem,/content/training-data/topic_models/0/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/topic_models/0/triples/results.txt
Results,has,Fisher speech corpora,results,/content/training-data/topic_models/0/triples/results.txt
Fisher speech corpora,with,manual and automatic transcriptions,results,/content/training-data/topic_models/0/triples/results.txt
Fisher speech corpora,see that,GLCU,results,/content/training-data/topic_models/0/triples/results.txt
GLCU,has,much lower cross - entropy,results,/content/training-data/topic_models/0/triples/results.txt
much lower cross - entropy,than,GLC,results,/content/training-data/topic_models/0/triples/results.txt
GLCU,exploits,uncertainty in document embeddings,results,/content/training-data/topic_models/0/triples/results.txt
Fisher speech corpora,see that,our proposed systems,results,/content/training-data/topic_models/0/triples/results.txt
our proposed systems,achieve,consistently better accuracies,results,/content/training-data/topic_models/0/triples/results.txt
Results,has,20 Newsgroups dataset,results,/content/training-data/topic_models/0/triples/results.txt
20 Newsgroups dataset,see that,topic ID systems based on Bayesian SMM and logistic regression,results,/content/training-data/topic_models/0/triples/results.txt
topic ID systems based on Bayesian SMM and logistic regression,better than,all the other models,results,/content/training-data/topic_models/0/triples/results.txt
all the other models,except for,purely discriminative CNN model,results,/content/training-data/topic_models/0/triples/results.txt
20 Newsgroups dataset,see that,topic ID systems based on Bayesian SMM,results,/content/training-data/topic_models/0/triples/results.txt
topic ID systems based on Bayesian SMM,consistently better than,"variational auto encoder inspired NVDM , and ( non-Bayesian ) SMM",results,/content/training-data/topic_models/0/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/phrase_grounding/0/triples/ablation-analysis.txt
Ablation analysis,applying,softmax,ablation-analysis,/content/training-data/phrase_grounding/0/triples/ablation-analysis.txt
softmax,leads to,very negative effect,ablation-analysis,/content/training-data/phrase_grounding/0/triples/ablation-analysis.txt
very negative effect,on,performance,ablation-analysis,/content/training-data/phrase_grounding/0/triples/ablation-analysis.txt
Ablation analysis,using,level - attention mechanism,ablation-analysis,/content/training-data/phrase_grounding/0/triples/ablation-analysis.txt
level - attention mechanism,based on,multi-level feature maps,ablation-analysis,/content/training-data/phrase_grounding/0/triples/ablation-analysis.txt
level - attention mechanism,significantly improves,performance,ablation-analysis,/content/training-data/phrase_grounding/0/triples/ablation-analysis.txt
performance,over,single visual - textual feature comparison,ablation-analysis,/content/training-data/phrase_grounding/0/triples/ablation-analysis.txt
Ablation analysis,also see that,non-linear mapping,ablation-analysis,/content/training-data/phrase_grounding/0/triples/ablation-analysis.txt
non-linear mapping,seems,more important,ablation-analysis,/content/training-data/phrase_grounding/0/triples/ablation-analysis.txt
more important,on,visual side,ablation-analysis,/content/training-data/phrase_grounding/0/triples/ablation-analysis.txt
Ablation analysis,show,importance,ablation-analysis,/content/training-data/phrase_grounding/0/triples/ablation-analysis.txt
importance,of using,strong contextualized text embedding,ablation-analysis,/content/training-data/phrase_grounding/0/triples/ablation-analysis.txt
strong contextualized text embedding,as,performance,ablation-analysis,/content/training-data/phrase_grounding/0/triples/ablation-analysis.txt
performance,has,drops significantly,ablation-analysis,/content/training-data/phrase_grounding/0/triples/ablation-analysis.txt
Ablation analysis,see that,non-linear mapping,ablation-analysis,/content/training-data/phrase_grounding/0/triples/ablation-analysis.txt
non-linear mapping,in,model,ablation-analysis,/content/training-data/phrase_grounding/0/triples/ablation-analysis.txt
non-linear mapping,is,really important,ablation-analysis,/content/training-data/phrase_grounding/0/triples/ablation-analysis.txt
Ablation analysis,replacing,any mapping,ablation-analysis,/content/training-data/phrase_grounding/0/triples/ablation-analysis.txt
any mapping,with,linear one,ablation-analysis,/content/training-data/phrase_grounding/0/triples/ablation-analysis.txt
linear one,significantly degrades,performance,ablation-analysis,/content/training-data/phrase_grounding/0/triples/ablation-analysis.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
Hyperparameters,use,? = 0.25,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
? = 0.25,for,Leaky ReLU,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
Leaky ReLU,in,non-linear mappings,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
Hyperparameters,use,D = 1024,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
D = 1024,for,common space mapping dimension,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
Hyperparameters,use,batch size,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
batch size,of,B = 32,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
Hyperparameters,regularize,weights,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
weights,with,l 2 regularization,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
l 2 regularization,with,reg value = 0.0005,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
weights,of,mappings,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
Hyperparameters,For,VGG,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
VGG,take,outputs,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
outputs,map to,semantic feature maps,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
semantic feature maps,with dimension,18181024,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
outputs,from,"{ conv 4 1 , conv 4 3 , conv5 1 , conv5 3 }",hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
Hyperparameters,for,PNAS - Net,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
PNAS - Net,take,outputs,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
outputs,from,"{ Cell 5 , Cell 7 , Cell 9 , Cell 11 }",hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
Hyperparameters,has,Image - caption pairs,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
Image - caption pairs,sampled,randomly,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
randomly,with,uniform distribution,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
Hyperparameters,has,common space mapping weights,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
common space mapping weights,are,trainable,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
Hyperparameters,has,Both visual and textual networks weights,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
Both visual and textual networks weights,fixed during,training,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
Hyperparameters,train,network,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
network,with,Adam optimizer,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
Adam optimizer,with,lr = 0.001,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
lr = 0.001,where,learning rate,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
learning rate,divided by,2,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
2,again at,15 - th epoch,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
2,once at,10 - th epoch,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
network,for,20 epochs,hyperparameters,/content/training-data/phrase_grounding/0/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/phrase_grounding/0/triples/model.txt
Model,explicitly learn,non-linear mapping,model,/content/training-data/phrase_grounding/0/triples/model.txt
non-linear mapping,of,visual and textual modalities,model,/content/training-data/phrase_grounding/0/triples/model.txt
visual and textual modalities,into,common space,model,/content/training-data/phrase_grounding/0/triples/model.txt
visual and textual modalities,at,different granularity,model,/content/training-data/phrase_grounding/0/triples/model.txt
different granularity,for,each domain,model,/content/training-data/phrase_grounding/0/triples/model.txt
Model,has,common space mapping,model,/content/training-data/phrase_grounding/0/triples/model.txt
common space mapping,exploited at,test - time,model,/content/training-data/phrase_grounding/0/triples/model.txt
test - time,with,multi - level multimodal attention mechanism,model,/content/training-data/phrase_grounding/0/triples/model.txt
multi - level multimodal attention mechanism,where,natural formalism,model,/content/training-data/phrase_grounding/0/triples/model.txt
natural formalism,solve,phrase grounding task,model,/content/training-data/phrase_grounding/0/triples/model.txt
phrase grounding task,has,elegantly and effectively,model,/content/training-data/phrase_grounding/0/triples/model.txt
natural formalism,for computing,attention heatmaps,model,/content/training-data/phrase_grounding/0/triples/model.txt
attention heatmaps,at,each level,model,/content/training-data/phrase_grounding/0/triples/model.txt
natural formalism,for computing,attended features,model,/content/training-data/phrase_grounding/0/triples/model.txt
natural formalism,for computing,pertinence scoring,model,/content/training-data/phrase_grounding/0/triples/model.txt
common space mapping,trained with,weak supervision,model,/content/training-data/phrase_grounding/0/triples/model.txt
Contribution,has research problem,Image - Phrase Grounding,research-problem,/content/training-data/phrase_grounding/0/triples/research-problem.txt
Contribution,has research problem,phrase grounding,research-problem,/content/training-data/phrase_grounding/0/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/phrase_grounding/0/triples/results.txt
Results,for,some sentences,results,/content/training-data/phrase_grounding/0/triples/results.txt
some sentences,has,4th model,results,/content/training-data/phrase_grounding/0/triples/results.txt
4th model,been,selected,results,/content/training-data/phrase_grounding/0/triples/results.txt
Results,has,full sentence selection,results,/content/training-data/phrase_grounding/0/triples/results.txt
full sentence selection,relies mostly on,3rd level,results,/content/training-data/phrase_grounding/0/triples/results.txt
Results,show that,method,results,/content/training-data/phrase_grounding/0/triples/results.txt
method,has,significantly outperforms,results,/content/training-data/phrase_grounding/0/triples/results.txt
significantly outperforms,in,all conditions and all datasets,results,/content/training-data/phrase_grounding/0/triples/results.txt
significantly outperforms,has,all state - of - the - art methods,results,/content/training-data/phrase_grounding/0/triples/results.txt
Results,show that,1st level,results,/content/training-data/phrase_grounding/0/triples/results.txt
1st level,exploited mostly for,animals and people categories,results,/content/training-data/phrase_grounding/0/triples/results.txt
Results,show that,3rd level,results,/content/training-data/phrase_grounding/0/triples/results.txt
3rd level,dominates,selection,results,/content/training-data/phrase_grounding/0/triples/results.txt
Results,show that,4th level,results,/content/training-data/phrase_grounding/0/triples/results.txt
4th level,important for,several categories,results,/content/training-data/phrase_grounding/0/triples/results.txt
several categories,such as,scene and animals,results,/content/training-data/phrase_grounding/0/triples/results.txt
Results,on,Flickr30 k,results,/content/training-data/phrase_grounding/0/triples/results.txt
Flickr30 k,observe that,our method,results,/content/training-data/phrase_grounding/0/triples/results.txt
our method,obtains,higher performance,results,/content/training-data/phrase_grounding/0/triples/results.txt
higher performance,on,almost all categories,results,/content/training-data/phrase_grounding/0/triples/results.txt
Flickr30 k,has,model,results,/content/training-data/phrase_grounding/0/triples/results.txt
model,based on,PNASNet,results,/content/training-data/phrase_grounding/0/triples/results.txt
PNASNet,has,consistently outperforms,results,/content/training-data/phrase_grounding/0/triples/results.txt
consistently outperforms,has,state - of - the - art,results,/content/training-data/phrase_grounding/0/triples/results.txt
state - of - the - art,on,all categories,results,/content/training-data/phrase_grounding/0/triples/results.txt
state - of - the - art,on,both metrics,results,/content/training-data/phrase_grounding/0/triples/results.txt
Contribution,has,Dataset,datase,/content/training-data/sarcasm_detection/0/triples/dataset.txt
Dataset,make available,first corpus,datase,/content/training-data/sarcasm_detection/0/triples/dataset.txt
first corpus,for,sarcasm detection,datase,/content/training-data/sarcasm_detection/0/triples/dataset.txt
first corpus,has,unbalanced and self - annotated labels,datase,/content/training-data/sarcasm_detection/0/triples/dataset.txt
Dataset,With,more than a million examples,datase,/content/training-data/sarcasm_detection/0/triples/dataset.txt
more than a million examples,exceeds,previous sarcasm corpora,datase,/content/training-data/sarcasm_detection/0/triples/dataset.txt
previous sarcasm corpora,by,an order of magnitude in size,datase,/content/training-data/sarcasm_detection/0/triples/dataset.txt
more than a million examples,of,sarcastic statements,datase,/content/training-data/sarcasm_detection/0/triples/dataset.txt
sarcastic statements,each provided with,"author , topic , and contex information",datase,/content/training-data/sarcasm_detection/0/triples/dataset.txt
Contribution,Code,https://github.com/NLPrinceton/,code,/content/training-data/sarcasm_detection/0/triples/code.txt
Contribution,has research problem,sarcasm detection,research-problem,/content/training-data/sarcasm_detection/0/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sarcasm_detection/0/triples/results.txt
Results,perform,reasonably well,results,/content/training-data/sarcasm_detection/0/triples/results.txt
Results,perform,much better,results,/content/training-data/sarcasm_detection/0/triples/results.txt
much better,than,random baseline,results,/content/training-data/sarcasm_detection/0/triples/results.txt
Results,has,clear scope,results,/content/training-data/sarcasm_detection/0/triples/results.txt
clear scope,for,improvement,results,/content/training-data/sarcasm_detection/0/triples/results.txt
clear scope,for,machine learning methods,results,/content/training-data/sarcasm_detection/0/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/sarcasm_detection/1/triples/baselines.txt
Baselines,has,CNN,baselines,/content/training-data/sarcasm_detection/1/triples/baselines.txt
CNN,has,individual CNN version,baselines,/content/training-data/sarcasm_detection/1/triples/baselines.txt
Baselines,has,CNN - SVM,baselines,/content/training-data/sarcasm_detection/1/triples/baselines.txt
CNN - SVM,consists of,CNN,baselines,/content/training-data/sarcasm_detection/1/triples/baselines.txt
CNN,for,content modeling,baselines,/content/training-data/sarcasm_detection/1/triples/baselines.txt
CNN - SVM,consists of,other pre-trained CNNs,baselines,/content/training-data/sarcasm_detection/1/triples/baselines.txt
other pre-trained CNNs,for extracting,"sentiment , emotion and personality features",baselines,/content/training-data/sarcasm_detection/1/triples/baselines.txt
"sentiment , emotion and personality features",from,given comment,baselines,/content/training-data/sarcasm_detection/1/triples/baselines.txt
Baselines,has,Bag - of - Words,baselines,/content/training-data/sarcasm_detection/1/triples/baselines.txt
Bag - of - Words,uses,comment 's,baselines,/content/training-data/sarcasm_detection/1/triples/baselines.txt
comment 's,has,word - counts,baselines,/content/training-data/sarcasm_detection/1/triples/baselines.txt
word - counts,as,features,baselines,/content/training-data/sarcasm_detection/1/triples/baselines.txt
features,in,vector,baselines,/content/training-data/sarcasm_detection/1/triples/baselines.txt
Baselines,has,CUE - CNN,baselines,/content/training-data/sarcasm_detection/1/triples/baselines.txt
CUE - CNN,models,user embeddings,baselines,/content/training-data/sarcasm_detection/1/triples/baselines.txt
user embeddings,method akin to,ParagraphVector,baselines,/content/training-data/sarcasm_detection/1/triples/baselines.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/sarcasm_detection/1/triples/ablation-analysis.txt
Ablation analysis,test,performance,ablation-analysis,/content/training-data/sarcasm_detection/1/triples/ablation-analysis.txt
performance,for,content - based CNN only,ablation-analysis,/content/training-data/sarcasm_detection/1/triples/ablation-analysis.txt
content - based CNN only,include,contextual features,ablation-analysis,/content/training-data/sarcasm_detection/1/triples/ablation-analysis.txt
contextual features,has,effect,ablation-analysis,/content/training-data/sarcasm_detection/1/triples/ablation-analysis.txt
effect,of,discourse features,ablation-analysis,/content/training-data/sarcasm_detection/1/triples/ablation-analysis.txt
discourse features,getting,increase,ablation-analysis,/content/training-data/sarcasm_detection/1/triples/ablation-analysis.txt
increase,of,3 % in F1,ablation-analysis,/content/training-data/sarcasm_detection/1/triples/ablation-analysis.txt
discourse features,primarily seen in,Pol dataset,ablation-analysis,/content/training-data/sarcasm_detection/1/triples/ablation-analysis.txt
content - based CNN only,introduced,user embeddings,ablation-analysis,/content/training-data/sarcasm_detection/1/triples/ablation-analysis.txt
user embeddings,observed,major boost,ablation-analysis,/content/training-data/sarcasm_detection/1/triples/ablation-analysis.txt
major boost,in,performance,ablation-analysis,/content/training-data/sarcasm_detection/1/triples/ablation-analysis.txt
content - based CNN only,provides,worst relative performance,ablation-analysis,/content/training-data/sarcasm_detection/1/triples/ablation-analysis.txt
worst relative performance,with,almost 10 % lesser accuracy,ablation-analysis,/content/training-data/sarcasm_detection/1/triples/ablation-analysis.txt
almost 10 % lesser accuracy,than,optimal,ablation-analysis,/content/training-data/sarcasm_detection/1/triples/ablation-analysis.txt
Ablation analysis,use of,CCA,ablation-analysis,/content/training-data/sarcasm_detection/1/triples/ablation-analysis.txt
CCA,for,generation,ablation-analysis,/content/training-data/sarcasm_detection/1/triples/ablation-analysis.txt
generation,of,user embeddings,ablation-analysis,/content/training-data/sarcasm_detection/1/triples/ablation-analysis.txt
user embeddings,replace it with,simple concatenation,ablation-analysis,/content/training-data/sarcasm_detection/1/triples/ablation-analysis.txt
simple concatenation,causes,significant drop,ablation-analysis,/content/training-data/sarcasm_detection/1/triples/ablation-analysis.txt
significant drop,in,performance,ablation-analysis,/content/training-data/sarcasm_detection/1/triples/ablation-analysis.txt
Ablation analysis,has,CASCADE,ablation-analysis,/content/training-data/sarcasm_detection/1/triples/ablation-analysis.txt
CASCADE,consisting of,CNN,ablation-analysis,/content/training-data/sarcasm_detection/1/triples/ablation-analysis.txt
CNN,with,user embeddings and contextual discourse features,ablation-analysis,/content/training-data/sarcasm_detection/1/triples/ablation-analysis.txt
user embeddings and contextual discourse features,provide,best performance,ablation-analysis,/content/training-data/sarcasm_detection/1/triples/ablation-analysis.txt
best performance,in,all three datasets,ablation-analysis,/content/training-data/sarcasm_detection/1/triples/ablation-analysis.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sarcasm_detection/1/triples/hyperparameters.txt
Hyperparameters,For,batched - modeling,hyperparameters,/content/training-data/sarcasm_detection/1/triples/hyperparameters.txt
batched - modeling,of,comments,hyperparameters,/content/training-data/sarcasm_detection/1/triples/hyperparameters.txt
comments,in,CNNs,hyperparameters,/content/training-data/sarcasm_detection/1/triples/hyperparameters.txt
comments,has,each comment,hyperparameters,/content/training-data/sarcasm_detection/1/triples/hyperparameters.txt
each comment,is,restricted or padded,hyperparameters,/content/training-data/sarcasm_detection/1/triples/hyperparameters.txt
restricted or padded,for,uniformity,hyperparameters,/content/training-data/sarcasm_detection/1/triples/hyperparameters.txt
restricted or padded,to,100 words,hyperparameters,/content/training-data/sarcasm_detection/1/triples/hyperparameters.txt
Hyperparameters,To optimize,parameters,hyperparameters,/content/training-data/sarcasm_detection/1/triples/hyperparameters.txt
parameters,has,Adam optimizer,hyperparameters,/content/training-data/sarcasm_detection/1/triples/hyperparameters.txt
Adam optimizer,starting with,initial learning rate,hyperparameters,/content/training-data/sarcasm_detection/1/triples/hyperparameters.txt
initial learning rate,of,1e ? 4,hyperparameters,/content/training-data/sarcasm_detection/1/triples/hyperparameters.txt
Hyperparameters,has,Training termination,hyperparameters,/content/training-data/sarcasm_detection/1/triples/hyperparameters.txt
Training termination,decided using,early stopping technique,hyperparameters,/content/training-data/sarcasm_detection/1/triples/hyperparameters.txt
early stopping technique,with,patience,hyperparameters,/content/training-data/sarcasm_detection/1/triples/hyperparameters.txt
patience,of,12,hyperparameters,/content/training-data/sarcasm_detection/1/triples/hyperparameters.txt
Hyperparameters,holdout,10 %,hyperparameters,/content/training-data/sarcasm_detection/1/triples/hyperparameters.txt
10 %,of,training data,hyperparameters,/content/training-data/sarcasm_detection/1/triples/hyperparameters.txt
10 %,for,validation,hyperparameters,/content/training-data/sarcasm_detection/1/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sarcasm_detection/1/triples/model.txt
Model,done by,document modeling,model,/content/training-data/sarcasm_detection/1/triples/model.txt
document modeling,of,consolidated comments,model,/content/training-data/sarcasm_detection/1/triples/model.txt
consolidated comments,belonging to,same forum,model,/content/training-data/sarcasm_detection/1/triples/model.txt
Model,performs,user profiling,model,/content/training-data/sarcasm_detection/1/triples/model.txt
user profiling,to create,user embeddings,model,/content/training-data/sarcasm_detection/1/triples/model.txt
user embeddings,that capture,indicative behavioral traits,model,/content/training-data/sarcasm_detection/1/triples/model.txt
indicative behavioral traits,for,sarcasm,model,/content/training-data/sarcasm_detection/1/triples/model.txt
Model,performs,content - modeling,model,/content/training-data/sarcasm_detection/1/triples/model.txt
content - modeling,using,Convolutional Neural Network ( CNN ,model,/content/training-data/sarcasm_detection/1/triples/model.txt
Convolutional Neural Network ( CNN ),to extract,syntactic features,model,/content/training-data/sarcasm_detection/1/triples/model.txt
Model,After,contextual modeling phase,model,/content/training-data/sarcasm_detection/1/triples/model.txt
contextual modeling phase,has,CASCADE,model,/content/training-data/sarcasm_detection/1/triples/model.txt
CASCADE,provided with,comment,model,/content/training-data/sarcasm_detection/1/triples/model.txt
comment,for,sarcasm detection,model,/content/training-data/sarcasm_detection/1/triples/model.txt
Model,extracts,contextual information,model,/content/training-data/sarcasm_detection/1/triples/model.txt
contextual information,from,discourse,model,/content/training-data/sarcasm_detection/1/triples/model.txt
discourse,in,discussion forums,model,/content/training-data/sarcasm_detection/1/triples/model.txt
discourse,of,comments,model,/content/training-data/sarcasm_detection/1/triples/model.txt
Model,has,CNN representation,model,/content/training-data/sarcasm_detection/1/triples/model.txt
CNN representation,concatenated with,relevant user embedding and discourse features,model,/content/training-data/sarcasm_detection/1/triples/model.txt
CNN representation,to get,final representation,model,/content/training-data/sarcasm_detection/1/triples/model.txt
final representation,used for,classification,model,/content/training-data/sarcasm_detection/1/triples/model.txt
Model,propose,hybrid network,model,/content/training-data/sarcasm_detection/1/triples/model.txt
hybrid network,named,CASCADE,model,/content/training-data/sarcasm_detection/1/triples/model.txt
hybrid network,utilizes,content and contextual - information,model,/content/training-data/sarcasm_detection/1/triples/model.txt
content and contextual - information,required for,sarcasm detection,model,/content/training-data/sarcasm_detection/1/triples/model.txt
Model,makes use of,users ' historical posts,model,/content/training-data/sarcasm_detection/1/triples/model.txt
users ' historical posts,to model,writing style ( stylometry ) and personality indicators,model,/content/training-data/sarcasm_detection/1/triples/model.txt
writing style ( stylometry ) and personality indicators,fused into,comprehensive user embeddings,model,/content/training-data/sarcasm_detection/1/triples/model.txt
comprehensive user embeddings,using,multi-view fusion approach,model,/content/training-data/sarcasm_detection/1/triples/model.txt
multi-view fusion approach,name,Canonical Correlation Analysis ( CCA ,model,/content/training-data/sarcasm_detection/1/triples/model.txt
Contribution,has research problem,Contextual Sarcasm Detection,research-problem,/content/training-data/sarcasm_detection/1/triples/research-problem.txt
Contribution,has research problem,automated sarcasm detection,research-problem,/content/training-data/sarcasm_detection/1/triples/research-problem.txt
Contribution,has research problem,sarcasm detection,research-problem,/content/training-data/sarcasm_detection/1/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sarcasm_detection/1/triples/results.txt
Results,Amongst,neural networks,results,/content/training-data/sarcasm_detection/1/triples/results.txt
neural networks,has,CNN baseline,results,/content/training-data/sarcasm_detection/1/triples/results.txt
CNN baseline,receives,least performance,results,/content/training-data/sarcasm_detection/1/triples/results.txt
Results,has,improved performance,results,/content/training-data/sarcasm_detection/1/triples/results.txt
improved performance,on,Main imbalanced dataset,results,/content/training-data/sarcasm_detection/1/triples/results.txt
Main imbalanced dataset,establishes it as,real - world deployable network,results,/content/training-data/sarcasm_detection/1/triples/results.txt
Main imbalanced dataset,reflects,robustness,results,/content/training-data/sarcasm_detection/1/triples/results.txt
robustness,towards,class imbalance,results,/content/training-data/sarcasm_detection/1/triples/results.txt
Results,has,CASCADE,results,/content/training-data/sarcasm_detection/1/triples/results.txt
CASCADE,achieve,major improvement,results,/content/training-data/sarcasm_detection/1/triples/results.txt
major improvement,across,all datasets,results,/content/training-data/sarcasm_detection/1/triples/results.txt
major improvement,with,statistical significance,results,/content/training-data/sarcasm_detection/1/triples/results.txt
CASCADE,comfortably beats,state - of - the - art neural models,results,/content/training-data/sarcasm_detection/1/triples/results.txt
state - of - the - art neural models,name,CNN - SVM,results,/content/training-data/sarcasm_detection/1/triples/results.txt
state - of - the - art neural models,name,CUE - CNN,results,/content/training-data/sarcasm_detection/1/triples/results.txt
Results,has,lowest performance,results,/content/training-data/sarcasm_detection/1/triples/results.txt
lowest performance,obtained by,Bag - of - words approach,results,/content/training-data/sarcasm_detection/1/triples/results.txt
Results,has,CUE - CNN,results,/content/training-data/sarcasm_detection/1/triples/results.txt
CUE - CNN,generates,user embeddings,results,/content/training-data/sarcasm_detection/1/triples/results.txt
user embeddings,using,method,results,/content/training-data/sarcasm_detection/1/triples/results.txt
method,similar to,ParagraphVector,results,/content/training-data/sarcasm_detection/1/triples/results.txt
Contribution,has,Approach,approach,/content/training-data/passage_re-ranking/0/triples/approach.txt
Approach,explore,alternative approach,approach,/content/training-data/passage_re-ranking/0/triples/approach.txt
alternative approach,based on,enriching,approach,/content/training-data/passage_re-ranking/0/triples/approach.txt
enriching,has,document representation,approach,/content/training-data/passage_re-ranking/0/triples/approach.txt
Approach,train,sequence - to - sequence model,approach,/content/training-data/passage_re-ranking/0/triples/approach.txt
sequence - to - sequence model,given,document,approach,/content/training-data/passage_re-ranking/0/triples/approach.txt
document,generates,possible questions,approach,/content/training-data/passage_re-ranking/0/triples/approach.txt
possible questions,that,document,approach,/content/training-data/passage_re-ranking/0/triples/approach.txt
document,might,answer,approach,/content/training-data/passage_re-ranking/0/triples/approach.txt
sequence - to - sequence model,Focusing on,question answering,approach,/content/training-data/passage_re-ranking/0/triples/approach.txt
Contribution,has,Baselines,baselines,/content/training-data/passage_re-ranking/0/triples/baselines.txt
Baselines,has,BM25,baselines,/content/training-data/passage_re-ranking/0/triples/baselines.txt
BM25,to rank,passages,baselines,/content/training-data/passage_re-ranking/0/triples/baselines.txt
BM25,use,Anserini open - source IR toolkit,baselines,/content/training-data/passage_re-ranking/0/triples/baselines.txt
Anserini open - source IR toolkit,to index,original ( non -expanded ) documents,baselines,/content/training-data/passage_re-ranking/0/triples/baselines.txt
Baselines,has,BM25 + Doc2query,baselines,/content/training-data/passage_re-ranking/0/triples/baselines.txt
BM25 + Doc2query,expand,documents,baselines,/content/training-data/passage_re-ranking/0/triples/baselines.txt
documents,using,proposed Doc2query method,baselines,/content/training-data/passage_re-ranking/0/triples/baselines.txt
BM25 + Doc2query,index and rank,expanded documents,baselines,/content/training-data/passage_re-ranking/0/triples/baselines.txt
expanded documents,exactly as in,BM25 method,baselines,/content/training-data/passage_re-ranking/0/triples/baselines.txt
Baselines,has,BM25 + Doc2query + BERT,baselines,/content/training-data/passage_re-ranking/0/triples/baselines.txt
BM25 + Doc2query + BERT,"expand , index , and retrieve",documents,baselines,/content/training-data/passage_re-ranking/0/triples/baselines.txt
documents,as in,BM25 + Doc2query condition,baselines,/content/training-data/passage_re-ranking/0/triples/baselines.txt
BM25 + Doc2query + BERT,further re-rank,documents,baselines,/content/training-data/passage_re-ranking/0/triples/baselines.txt
documents,with,BERT,baselines,/content/training-data/passage_re-ranking/0/triples/baselines.txt
Baselines,has,RM3,baselines,/content/training-data/passage_re-ranking/0/triples/baselines.txt
RM3,compare,document expansion,baselines,/content/training-data/passage_re-ranking/0/triples/baselines.txt
document expansion,with,query expansion,baselines,/content/training-data/passage_re-ranking/0/triples/baselines.txt
document expansion,applied,RM3 query expansion technique,baselines,/content/training-data/passage_re-ranking/0/triples/baselines.txt
Baselines,has,BM25 + BERT,baselines,/content/training-data/passage_re-ranking/0/triples/baselines.txt
BM25 + BERT,further re-rank,documents,baselines,/content/training-data/passage_re-ranking/0/triples/baselines.txt
documents,with,BERT,baselines,/content/training-data/passage_re-ranking/0/triples/baselines.txt
BM25 + BERT,index and retrieve,documents,baselines,/content/training-data/passage_re-ranking/0/triples/baselines.txt
documents,as in,BM25 condition,baselines,/content/training-data/passage_re-ranking/0/triples/baselines.txt
Contribution,has research problem,Document Expansion,research-problem,/content/training-data/passage_re-ranking/0/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/passage_re-ranking/0/triples/results.txt
Results,expand,MS MARCO documents,results,/content/training-data/passage_re-ranking/0/triples/results.txt
MS MARCO documents,obtain,MRR@10,results,/content/training-data/passage_re-ranking/0/triples/results.txt
MRR@10,of,18.8,results,/content/training-data/passage_re-ranking/0/triples/results.txt
MS MARCO documents,using,only new words,results,/content/training-data/passage_re-ranking/0/triples/results.txt
MS MARCO documents,retrieve,development set queries,results,/content/training-data/passage_re-ranking/0/triples/results.txt
development set queries,with,BM25,results,/content/training-data/passage_re-ranking/0/triples/results.txt
Results,find that,Recall@1000,results,/content/training-data/passage_re-ranking/0/triples/results.txt
Recall@1000,of,MS MARCO development set,results,/content/training-data/passage_re-ranking/0/triples/results.txt
MS MARCO development set,increased from,85.3 ( BM25 ,results,/content/training-data/passage_re-ranking/0/triples/results.txt
85.3 ( BM25 ),to,89.3 ( BM25 + Doc2query ,results,/content/training-data/passage_re-ranking/0/triples/results.txt
Results,find that,query expansion,results,/content/training-data/passage_re-ranking/0/triples/results.txt
query expansion,with,RM3,results,/content/training-data/passage_re-ranking/0/triples/results.txt
RM3,has,hurts,results,/content/training-data/passage_re-ranking/0/triples/results.txt
hurts,in,both datasets,results,/content/training-data/passage_re-ranking/0/triples/results.txt
Results,achieve,higher MRR@10,results,/content/training-data/passage_re-ranking/0/triples/results.txt
higher MRR@10,of,21.5,results,/content/training-data/passage_re-ranking/0/triples/results.txt
higher MRR@10,when,documents,results,/content/training-data/passage_re-ranking/0/triples/results.txt
documents,expanded with,both types of words,results,/content/training-data/passage_re-ranking/0/triples/results.txt
Results,shows that,document expansion,results,/content/training-data/passage_re-ranking/0/triples/results.txt
document expansion,more effective than,query expansion,results,/content/training-data/passage_re-ranking/0/triples/results.txt
Results,notice that,model,results,/content/training-data/passage_re-ranking/0/triples/results.txt
model,produces,words,results,/content/training-data/passage_re-ranking/0/triples/results.txt
words,not present in,input document,results,/content/training-data/passage_re-ranking/0/triples/results.txt
words,characterized as,expansion,results,/content/training-data/passage_re-ranking/0/triples/results.txt
expansion,by,synonyms and other related terms,results,/content/training-data/passage_re-ranking/0/triples/results.txt
model,copy,some words,results,/content/training-data/passage_re-ranking/0/triples/results.txt
some words,from,input document,results,/content/training-data/passage_re-ranking/0/triples/results.txt
Results,has,Document expansion,results,/content/training-data/passage_re-ranking/0/triples/results.txt
Document expansion,with,our method ( BM25 + Doc2query ,results,/content/training-data/passage_re-ranking/0/triples/results.txt
our method ( BM25 + Doc2query ),improves,retrieval effectiveness,results,/content/training-data/passage_re-ranking/0/triples/results.txt
retrieval effectiveness,by,15 %,results,/content/training-data/passage_re-ranking/0/triples/results.txt
retrieval effectiveness,for,both datasets,results,/content/training-data/passage_re-ranking/0/triples/results.txt
Results,has,Our full re-ranking condition ( BM25 + Doc2query + BERT ,results,/content/training-data/passage_re-ranking/0/triples/results.txt
Our full re-ranking condition ( BM25 + Doc2query + BERT ),beats,BM25 + BERT alone,results,/content/training-data/passage_re-ranking/0/triples/results.txt
Results,has,Our method without a re-ranker ( BM25 + Doc2query ,results,/content/training-data/passage_re-ranking/0/triples/results.txt
Our method without a re-ranker ( BM25 + Doc2query ),is,seven times faster,results,/content/training-data/passage_re-ranking/0/triples/results.txt
seven times faster,than,neural re-ranker,results,/content/training-data/passage_re-ranking/0/triples/results.txt
neural re-ranker,that has,three points higher MRR@10,results,/content/training-data/passage_re-ranking/0/triples/results.txt
Our method without a re-ranker ( BM25 + Doc2query ),adds,small latency,results,/content/training-data/passage_re-ranking/0/triples/results.txt
small latency,over,baseline BM25 ( 50 ms vs. 90 ms ,results,/content/training-data/passage_re-ranking/0/triples/results.txt
Results,Expanding with,copied words,results,/content/training-data/passage_re-ranking/0/triples/results.txt
copied words,gives,MRR@10,results,/content/training-data/passage_re-ranking/0/triples/results.txt
MRR@10,of,19.7,results,/content/training-data/passage_re-ranking/0/triples/results.txt
Results,combine,document expansion,results,/content/training-data/passage_re-ranking/0/triples/results.txt
document expansion,with,state - of - the - art re-ranker ( BM25 + Doc2query + BERT ,results,/content/training-data/passage_re-ranking/0/triples/results.txt
state - of - the - art re-ranker ( BM25 + Doc2query + BERT ),achieve,best - known results,results,/content/training-data/passage_re-ranking/0/triples/results.txt
best - known results,on,TREC CAR,results,/content/training-data/passage_re-ranking/0/triples/results.txt
state - of - the - art re-ranker ( BM25 + Doc2query + BERT ),for,MS MARCO,results,/content/training-data/passage_re-ranking/0/triples/results.txt
MS MARCO,near,state of the art,results,/content/training-data/passage_re-ranking/0/triples/results.txt
Contribution,has,Approach,approach,/content/training-data/passage_re-ranking/1/triples/approach.txt
Approach,re-purposed,BERT,approach,/content/training-data/passage_re-ranking/1/triples/approach.txt
BERT,as,passage re-ranker,approach,/content/training-data/passage_re-ranking/1/triples/approach.txt
Contribution,has,Experiments,experiments,/content/training-data/passage_re-ranking/1/triples/experiments.txt
Experiments,has,TREC - CAR,experiments,/content/training-data/passage_re-ranking/1/triples/experiments.txt
TREC - CAR,For,fine - tuning,experiments,/content/training-data/passage_re-ranking/1/triples/experiments.txt
fine - tuning,has,data,experiments,/content/training-data/passage_re-ranking/1/triples/experiments.txt
data,generate,our query - passage pairs,experiments,/content/training-data/passage_re-ranking/1/triples/experiments.txt
our query - passage pairs,by retrieving,top ten passages,experiments,/content/training-data/passage_re-ranking/1/triples/experiments.txt
top ten passages,from,entire TREC - CAR corpus,experiments,/content/training-data/passage_re-ranking/1/triples/experiments.txt
entire TREC - CAR corpus,using,BM25,experiments,/content/training-data/passage_re-ranking/1/triples/experiments.txt
TREC - CAR,train,400 k iterations,experiments,/content/training-data/passage_re-ranking/1/triples/experiments.txt
TREC - CAR,train,12.8 M examples,experiments,/content/training-data/passage_re-ranking/1/triples/experiments.txt
Experiments,has,MS MARCO,experiments,/content/training-data/passage_re-ranking/1/triples/experiments.txt
MS MARCO,has,Hyperparameters,experiments,/content/training-data/passage_re-ranking/1/triples/experiments.txt
Hyperparameters,fine - tune,model,experiments,/content/training-data/passage_re-ranking/1/triples/experiments.txt
model,with,batch size,experiments,/content/training-data/passage_re-ranking/1/triples/experiments.txt
batch size,of,32,experiments,/content/training-data/passage_re-ranking/1/triples/experiments.txt
model,using,TPUs,experiments,/content/training-data/passage_re-ranking/1/triples/experiments.txt
model,for,400 k iterations,experiments,/content/training-data/passage_re-ranking/1/triples/experiments.txt
model,takes,approximately 70 hours,experiments,/content/training-data/passage_re-ranking/1/triples/experiments.txt
Hyperparameters,use,dropout probability,experiments,/content/training-data/passage_re-ranking/1/triples/experiments.txt
dropout probability,of,0.1,experiments,/content/training-data/passage_re-ranking/1/triples/experiments.txt
0.1,on,all layers,experiments,/content/training-data/passage_re-ranking/1/triples/experiments.txt
Hyperparameters,use,"ADAM ( Kingma & Ba , 2014 ",experiments,/content/training-data/passage_re-ranking/1/triples/experiments.txt
"ADAM ( Kingma & Ba , 2014 )",with,initial learning rate,experiments,/content/training-data/passage_re-ranking/1/triples/experiments.txt
initial learning rate,set to,"3 10 ?6 , ? 1 = 0.9 , ? 2 = 0.999",experiments,/content/training-data/passage_re-ranking/1/triples/experiments.txt
"ADAM ( Kingma & Ba , 2014 )",with,learning rate warmup,experiments,/content/training-data/passage_re-ranking/1/triples/experiments.txt
learning rate warmup,over,"first 10,000 steps",experiments,/content/training-data/passage_re-ranking/1/triples/experiments.txt
"ADAM ( Kingma & Ba , 2014 )",with,L2 weight decay,experiments,/content/training-data/passage_re-ranking/1/triples/experiments.txt
L2 weight decay,of,0.01,experiments,/content/training-data/passage_re-ranking/1/triples/experiments.txt
"ADAM ( Kingma & Ba , 2014 )",with,linear decay,experiments,/content/training-data/passage_re-ranking/1/triples/experiments.txt
linear decay,of,learning rate,experiments,/content/training-data/passage_re-ranking/1/triples/experiments.txt
Contribution,has research problem,PASSAGE RE - RANKING,research-problem,/content/training-data/passage_re-ranking/1/triples/research-problem.txt
Contribution,has research problem,query - based passage re-ranking,research-problem,/content/training-data/passage_re-ranking/1/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/passage_re-ranking/1/triples/results.txt
Results,Despite,training,results,/content/training-data/passage_re-ranking/1/triples/results.txt
training,has,proposed BERT - based models,results,/content/training-data/passage_re-ranking/1/triples/results.txt
proposed BERT - based models,surpass,previous state - of - the - art models,results,/content/training-data/passage_re-ranking/1/triples/results.txt
previous state - of - the - art models,by,large margin,results,/content/training-data/passage_re-ranking/1/triples/results.txt
training,on,fraction of the data available,results,/content/training-data/passage_re-ranking/1/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/temporal_information_extraction/0/triples/ablation-analysis.txt
Ablation analysis,running,transitive closure module,ablation-analysis,/content/training-data/temporal_information_extraction/0/triples/ablation-analysis.txt
transitive closure module,after,temporal rule - based sieve ( RB + TR ,ablation-analysis,/content/training-data/temporal_information_extraction/0/triples/ablation-analysis.txt
temporal rule - based sieve ( RB + TR ),results in,improving recall,ablation-analysis,/content/training-data/temporal_information_extraction/0/triples/ablation-analysis.txt
Ablation analysis,Combining,rule - based and machine - learned sieves ( RB + ML ,ablation-analysis,/content/training-data/temporal_information_extraction/0/triples/ablation-analysis.txt
rule - based and machine - learned sieves ( RB + ML ),yields,slight improvement,ablation-analysis,/content/training-data/temporal_information_extraction/0/triples/ablation-analysis.txt
slight improvement,compared with,enabling only the machine - learned sieve,ablation-analysis,/content/training-data/temporal_information_extraction/0/triples/ablation-analysis.txt
enabling only the machine - learned sieve,in,system ( ML ,ablation-analysis,/content/training-data/temporal_information_extraction/0/triples/ablation-analysis.txt
Ablation analysis,Introducing,temporal reasoner module,ablation-analysis,/content/training-data/temporal_information_extraction/0/triples/ablation-analysis.txt
temporal reasoner module,between,two sieves ( RB + TR + ML ,ablation-analysis,/content/training-data/temporal_information_extraction/0/triples/ablation-analysis.txt
two sieves ( RB + TR + ML ),proves to be,even more beneficial,ablation-analysis,/content/training-data/temporal_information_extraction/0/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/temporal_information_extraction/0/triples/model.txt
Model,take as input,document,model,/content/training-data/temporal_information_extraction/0/triples/model.txt
document,annotated with,temporal entities,model,/content/training-data/temporal_information_extraction/0/triples/model.txt
temporal entities,including,document creation time ( DCT ,model,/content/training-data/temporal_information_extraction/0/triples/model.txt
temporal entities,including,events and time expressions ( timexes ,model,/content/training-data/temporal_information_extraction/0/triples/model.txt
temporal entities,according to,TimeML guidelines,model,/content/training-data/temporal_information_extraction/0/triples/model.txt
Model,name,CATENA system,model,/content/training-data/temporal_information_extraction/0/triples/model.txt
CATENA system,includes,two main classification modules,model,/content/training-data/temporal_information_extraction/0/triples/model.txt
two main classification modules,other for,causal relations,model,/content/training-data/temporal_information_extraction/0/triples/model.txt
two main classification modules,one for,temporal,model,/content/training-data/temporal_information_extraction/0/triples/model.txt
Model,has,output,model,/content/training-data/temporal_information_extraction/0/triples/model.txt
output,is,same document,model,/content/training-data/temporal_information_extraction/0/triples/model.txt
same document,with,temporal links ( TLINKs ,model,/content/training-data/temporal_information_extraction/0/triples/model.txt
temporal links ( TLINKs ),set between,pairs,model,/content/training-data/temporal_information_extraction/0/triples/model.txt
pairs,of,temporal entities,model,/content/training-data/temporal_information_extraction/0/triples/model.txt
output,has,document,model,/content/training-data/temporal_information_extraction/0/triples/model.txt
document,annotated with,causal relations ( CLINKs ,model,/content/training-data/temporal_information_extraction/0/triples/model.txt
causal relations ( CLINKs ),between,event pairs,model,/content/training-data/temporal_information_extraction/0/triples/model.txt
output,has,modules,model,/content/training-data/temporal_information_extraction/0/triples/model.txt
modules,for,temporal and causal relation classification,model,/content/training-data/temporal_information_extraction/0/triples/model.txt
temporal and causal relation classification,rely both on,sieve - based architecture,model,/content/training-data/temporal_information_extraction/0/triples/model.txt
sieve - based architecture,in which,remaining unlabelled pairs,model,/content/training-data/temporal_information_extraction/0/triples/model.txt
remaining unlabelled pairs,after running,rule - based component,model,/content/training-data/temporal_information_extraction/0/triples/model.txt
temporal and causal relation classification,rely both on,transitive reasoner,model,/content/training-data/temporal_information_extraction/0/triples/model.txt
transitive reasoner,fed into,supervised classifier,model,/content/training-data/temporal_information_extraction/0/triples/model.txt
Contribution,has research problem,temporal and causal relation extraction and classification,research-problem,/content/training-data/temporal_information_extraction/0/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/temporal_information_extraction/0/triples/results.txt
Results,has,CATENA,results,/content/training-data/temporal_information_extraction/0/triples/results.txt
CATENA,is,best performing system,results,/content/training-data/temporal_information_extraction/0/triples/results.txt
best performing system,in,both tasks,results,/content/training-data/temporal_information_extraction/0/triples/results.txt
CATENA,performs best on,timex - timex and event - timex relations,results,/content/training-data/temporal_information_extraction/0/triples/results.txt
timex - timex and event - timex relations,while,CAEVO,results,/content/training-data/temporal_information_extraction/0/triples/results.txt
CAEVO,achieves,best results,results,/content/training-data/temporal_information_extraction/0/triples/results.txt
best results,on,event - DCT and event - event pairs,results,/content/training-data/temporal_information_extraction/0/triples/results.txt
Contribution,has,Approach,approach,/content/training-data/temporal_information_extraction/1/triples/approach.txt
Approach,has,structured approach,approach,/content/training-data/temporal_information_extraction/1/triples/approach.txt
structured approach,gives rise to,semisupervised method,approach,/content/training-data/temporal_information_extraction/1/triples/approach.txt
semisupervised method,take advantage of,readily available unlabeled data,approach,/content/training-data/temporal_information_extraction/1/triples/approach.txt
Approach,propose,structured learning approach,approach,/content/training-data/temporal_information_extraction/1/triples/approach.txt
structured learning approach,to,temporal relation extraction,approach,/content/training-data/temporal_information_extraction/1/triples/approach.txt
temporal relation extraction,where,local models,approach,/content/training-data/temporal_information_extraction/1/triples/approach.txt
local models,are,updated,approach,/content/training-data/temporal_information_extraction/1/triples/approach.txt
updated,based on,feedback,approach,/content/training-data/temporal_information_extraction/1/triples/approach.txt
feedback,from,global inferences,approach,/content/training-data/temporal_information_extraction/1/triples/approach.txt
Contribution,has,Baselines,baselines,/content/training-data/temporal_information_extraction/1/triples/baselines.txt
Baselines,has,regularized averaged perceptron ( AP ,baselines,/content/training-data/temporal_information_extraction/1/triples/baselines.txt
regularized averaged perceptron ( AP ),implemented in,LBJava package,baselines,/content/training-data/temporal_information_extraction/1/triples/baselines.txt
Baselines,On top of,first baseline,baselines,/content/training-data/temporal_information_extraction/1/triples/baselines.txt
first baseline,performed,global inference,baselines,/content/training-data/temporal_information_extraction/1/triples/baselines.txt
Baselines,used,same feature set,baselines,/content/training-data/temporal_information_extraction/1/triples/baselines.txt
same feature set,as in,proposed structured perceptron ( SP ,baselines,/content/training-data/temporal_information_extraction/1/triples/baselines.txt
same feature set,as in,CoDL,baselines,/content/training-data/temporal_information_extraction/1/triples/baselines.txt
Contribution,has research problem,Temporal Relation Extraction,research-problem,/content/training-data/temporal_information_extraction/1/triples/research-problem.txt
Contribution,has research problem,Identifying temporal relations between events,research-problem,/content/training-data/temporal_information_extraction/1/triples/research-problem.txt
Contribution,has research problem,temporal processing,research-problem,/content/training-data/temporal_information_extraction/1/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/temporal_information_extraction/1/triples/results.txt
Results,has,Comparison with CAEVO,results,/content/training-data/temporal_information_extraction/1/triples/results.txt
Comparison with CAEVO,has,SP + ILP,results,/content/training-data/temporal_information_extraction/1/triples/results.txt
SP + ILP,outperformed,CAEVO,results,/content/training-data/temporal_information_extraction/1/triples/results.txt
Results,has,TE3 Task C,results,/content/training-data/temporal_information_extraction/1/triples/results.txt
TE3 Task C,applying,postfiltering method,results,/content/training-data/temporal_information_extraction/1/triples/results.txt
postfiltering method,able to achieve,better performances,results,/content/training-data/temporal_information_extraction/1/triples/results.txt
better performances,using,SP + ILP,results,/content/training-data/temporal_information_extraction/1/triples/results.txt
TE3 Task C,has,AP + ILP,results,/content/training-data/temporal_information_extraction/1/triples/results.txt
AP + ILP,even worse than,AP,results,/content/training-data/temporal_information_extraction/1/triples/results.txt
TE3 Task C,improvement of,SP + ILP,results,/content/training-data/temporal_information_extraction/1/triples/results.txt
SP + ILP,over,AP,results,/content/training-data/temporal_information_extraction/1/triples/results.txt
AP,was,small,results,/content/training-data/temporal_information_extraction/1/triples/results.txt
Results,has,TE3 Task C - Relation Only,results,/content/training-data/temporal_information_extraction/1/triples/results.txt
TE3 Task C - Relation Only,see,UT - Time,results,/content/training-data/temporal_information_extraction/1/triples/results.txt
UT - Time,is about,3 %,results,/content/training-data/temporal_information_extraction/1/triples/results.txt
3 %,better than,AP - 1,results,/content/training-data/temporal_information_extraction/1/triples/results.txt
AP - 1,in,absolute value of F 1,results,/content/training-data/temporal_information_extraction/1/triples/results.txt
TE3 Task C - Relation Only,On top of,AP - 2,results,/content/training-data/temporal_information_extraction/1/triples/results.txt
AP - 2,has,global inference step,results,/content/training-data/temporal_information_extraction/1/triples/results.txt
global inference step,enforcing,symmetry and transitivity constraints,results,/content/training-data/temporal_information_extraction/1/triples/results.txt
symmetry and transitivity constraints,further improve,F 1 score,results,/content/training-data/temporal_information_extraction/1/triples/results.txt
F 1 score,by,9.3 %,results,/content/training-data/temporal_information_extraction/1/triples/results.txt
TE3 Task C - Relation Only,has,SP + ILP,results,/content/training-data/temporal_information_extraction/1/triples/results.txt
SP + ILP,further improved,performance,results,/content/training-data/temporal_information_extraction/1/triples/results.txt
performance,in,"precision , recall , and F 1",results,/content/training-data/temporal_information_extraction/1/triples/results.txt
SP + ILP,reaching,F 1 score,results,/content/training-data/temporal_information_extraction/1/triples/results.txt
F 1 score,of,67.2 %,results,/content/training-data/temporal_information_extraction/1/triples/results.txt
Contribution,has,Dataset,datase,/content/training-data/relation_extraction/4/triples/dataset.txt
Dataset,name,TAC Relation Extraction Dataset ( TACRED ,datase,/content/training-data/relation_extraction/4/triples/dataset.txt
TAC Relation Extraction Dataset ( TACRED ),make it available through,Linguistic Data Consortium ( LDC ,datase,/content/training-data/relation_extraction/4/triples/dataset.txt
Linguistic Data Consortium ( LDC ),to respect,copyrights,datase,/content/training-data/relation_extraction/4/triples/dataset.txt
copyrights,on,underlying text,datase,/content/training-data/relation_extraction/4/triples/dataset.txt
Dataset,markedly improve,availability of supervised training data,datase,/content/training-data/relation_extraction/4/triples/dataset.txt
availability of supervised training data,by using,Mechanical Turk crowd annotation,datase,/content/training-data/relation_extraction/4/triples/dataset.txt
Mechanical Turk crowd annotation,to produce,large supervised training dataset,datase,/content/training-data/relation_extraction/4/triples/dataset.txt
large supervised training dataset,suitable for,common relations,datase,/content/training-data/relation_extraction/4/triples/dataset.txt
common relations,used in,TAC KBP evaluations,datase,/content/training-data/relation_extraction/4/triples/dataset.txt
common relations,between,"people , organizations and locations",datase,/content/training-data/relation_extraction/4/triples/dataset.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
Ablation analysis,At,hop - 0 level,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
hop - 0 level,provide more,negative examples,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
hop - 0 level,has,F 1 score,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
F 1 score,keeps,increasing,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
hop - 0 level,has,precision,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
precision,has,increases,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
hop - 0 level,while,recall,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
recall,stays,almost unchanged,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
Ablation analysis,At,hop - all level,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
hop - all level,has,F 1 score,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
F 1 score,increases by,Performance,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
Performance,by,sentence length,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
Ablation analysis,observe that,slot types,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
slot types,with,relatively sparse training examples,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
slot types,improved by,position - aware attention model,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
Ablation analysis,find that,Performance,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
Performance,of,all models,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
Performance,degrades,substantially,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
substantially,as,sentences,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
sentences,get,longer,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
Ablation analysis,find,model,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
model,to pay,more attention,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
more attention,to,words,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
words,informative for,relation,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
Ablation analysis,shows how,slot filling evaluation scores,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
slot filling evaluation scores,has,change,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
change,as,"amount of negative ( i.e. , no relation ) training data",ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
"amount of negative ( i.e. , no relation ) training data",provided to,our proposed model,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
Ablation analysis,compared with,CNN - PE model,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
CNN - PE model,has,position - aware attention model,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
position - aware attention model,achieves,improved F 1 scores,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
improved F 1 scores,with,top 5 slot types,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
top 5 slot types,being,org : members,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
top 5 slot types,being,per: country of death,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
top 5 slot types,being,org : shareholders,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
top 5 slot types,being,per:children,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
top 5 slot types,being,per:religion,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
improved F 1 scores,on,30 out of the 41 slot types,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
Ablation analysis,compared with,SDP - LSTM model,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
SDP - LSTM model,has,our model,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
our model,achieves,improved F 1 scores,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
improved F 1 scores,with,top 5 slot,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
top 5 slot,being,org : political / religious affiliation,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
top 5 slot,being,per: country of death,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
top 5 slot,being,org : alternate names,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
top 5 slot,being,per:religion,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
top 5 slot,being,per: alternate names,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
improved F 1 scores,on,26 out of the 41 slot types,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
Ablation analysis,observe,model,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
model,tends to put,lot of weight,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
lot of weight,onto,object entities,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
Ablation analysis,presents,results of an ablation test,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
results of an ablation test,of,our position - aware attention model,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
our position - aware attention model,contributes,about 1.5 % F 1,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
about 1.5 % F 1,where,position - aware term,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
position - aware term,contributes,about 1 % F 1 score,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
our position - aware attention model,on,development set of TACRED,ablation-analysis,/content/training-data/relation_extraction/4/triples/ablation-analysis.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/relation_extraction/4/triples/hyperparameters.txt
Hyperparameters,minimize,cross - entropy loss,hyperparameters,/content/training-data/relation_extraction/4/triples/hyperparameters.txt
cross - entropy loss,over,all 42 relations,hyperparameters,/content/training-data/relation_extraction/4/triples/hyperparameters.txt
cross - entropy loss,using,AdaGrad,hyperparameters,/content/training-data/relation_extraction/4/triples/hyperparameters.txt
Hyperparameters,set,p,hyperparameters,/content/training-data/relation_extraction/4/triples/hyperparameters.txt
p,to be,0.06,hyperparameters,/content/training-data/relation_extraction/4/triples/hyperparameters.txt
0.06,for,SDP - LSTM model,hyperparameters,/content/training-data/relation_extraction/4/triples/hyperparameters.txt
p,to be,0.04,hyperparameters,/content/training-data/relation_extraction/4/triples/hyperparameters.txt
0.04,for,all other models,hyperparameters,/content/training-data/relation_extraction/4/triples/hyperparameters.txt
Hyperparameters,apply,Dropout,hyperparameters,/content/training-data/relation_extraction/4/triples/hyperparameters.txt
Dropout,with,p = 0.5,hyperparameters,/content/training-data/relation_extraction/4/triples/hyperparameters.txt
p = 0.5,to,CNNs and LSTMs,hyperparameters,/content/training-data/relation_extraction/4/triples/hyperparameters.txt
Hyperparameters,use,pre-trained GloVe vectors,hyperparameters,/content/training-data/relation_extraction/4/triples/hyperparameters.txt
pre-trained GloVe vectors,to initialize,word embeddings,hyperparameters,/content/training-data/relation_extraction/4/triples/hyperparameters.txt
Hyperparameters,For,all the LSTM layers,hyperparameters,/content/training-data/relation_extraction/4/triples/hyperparameters.txt
all the LSTM layers,find that,2 - layer stacked LSTMs,hyperparameters,/content/training-data/relation_extraction/4/triples/hyperparameters.txt
2 - layer stacked LSTMs,work better than,one - layer LSTMs,hyperparameters,/content/training-data/relation_extraction/4/triples/hyperparameters.txt
Hyperparameters,map,words,hyperparameters,/content/training-data/relation_extraction/4/triples/hyperparameters.txt
words,occur less than,2 times,hyperparameters,/content/training-data/relation_extraction/4/triples/hyperparameters.txt
2 times,in,training set,hyperparameters,/content/training-data/relation_extraction/4/triples/hyperparameters.txt
words,to,special < UNK > token,hyperparameters,/content/training-data/relation_extraction/4/triples/hyperparameters.txt
Hyperparameters,During,training,hyperparameters,/content/training-data/relation_extraction/4/triples/hyperparameters.txt
training,find,word dropout strategy,hyperparameters,/content/training-data/relation_extraction/4/triples/hyperparameters.txt
word dropout strategy,randomly set,token,hyperparameters,/content/training-data/relation_extraction/4/triples/hyperparameters.txt
token,to be,< UNK >,hyperparameters,/content/training-data/relation_extraction/4/triples/hyperparameters.txt
< UNK >,with,probability p,hyperparameters,/content/training-data/relation_extraction/4/triples/hyperparameters.txt
word dropout strategy,to be,very effective,hyperparameters,/content/training-data/relation_extraction/4/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/relation_extraction/4/triples/model.txt
Model,means that,neural attention model,model,/content/training-data/relation_extraction/4/triples/model.txt
neural attention model,effectively exploit,combination of semantic similarity - based attention and positionbased attention,model,/content/training-data/relation_extraction/4/triples/model.txt
Model,has,architecture,model,/content/training-data/relation_extraction/4/triples/model.txt
architecture,better customized for,slot filling task,model,/content/training-data/relation_extraction/4/triples/model.txt
architecture,has,word representations,model,/content/training-data/relation_extraction/4/triples/model.txt
word representations,augmented by,extra distributed representations of word position,model,/content/training-data/relation_extraction/4/triples/model.txt
extra distributed representations of word position,relative to,subject and object,model,/content/training-data/relation_extraction/4/triples/model.txt
subject and object,of,putative relation,model,/content/training-data/relation_extraction/4/triples/model.txt
Model,propose,"new , effective neural network sequence model",model,/content/training-data/relation_extraction/4/triples/model.txt
"new , effective neural network sequence model",for,relation classification,model,/content/training-data/relation_extraction/4/triples/model.txt
Contribution,has research problem,populate knowledge bases with facts,research-problem,/content/training-data/relation_extraction/4/triples/research-problem.txt
Contribution,has research problem,populate a knowledge base with relational facts,research-problem,/content/training-data/relation_extraction/4/triples/research-problem.txt
Contribution,has research problem,relation extraction,research-problem,/content/training-data/relation_extraction/4/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/relation_extraction/4/triples/results.txt
Results,find,training,results,/content/training-data/relation_extraction/4/triples/results.txt
training,has,our logistic regression model,results,/content/training-data/relation_extraction/4/triples/results.txt
our logistic regression model,on,TACRED,results,/content/training-data/relation_extraction/4/triples/results.txt
our logistic regression model,on,2 million bootstrapped examples,results,/content/training-data/relation_extraction/4/triples/results.txt
2 million bootstrapped examples,used in,2015 Stanford system,results,/content/training-data/relation_extraction/4/triples/results.txt
2015 Stanford system,combining it with,patterns,results,/content/training-data/relation_extraction/4/triples/results.txt
Results,run,ensemble,results,/content/training-data/relation_extraction/4/triples/results.txt
ensemble,of,position - aware attention model,results,/content/training-data/relation_extraction/4/triples/results.txt
position - aware attention model,further pushes,F 1 score,results,/content/training-data/relation_extraction/4/triples/results.txt
F 1 score,by,1.6 %,results,/content/training-data/relation_extraction/4/triples/results.txt
position - aware attention model,takes,majority votes,results,/content/training-data/relation_extraction/4/triples/results.txt
majority votes,from,5 runs,results,/content/training-data/relation_extraction/4/triples/results.txt
5 runs,with,random initializations,results,/content/training-data/relation_extraction/4/triples/results.txt
Results,has,Errors,results,/content/training-data/relation_extraction/4/triples/results.txt
Errors,in,hop - 0 predictions,results,/content/training-data/relation_extraction/4/triples/results.txt
hop - 0 predictions,easily propagate to,hop - 1 predictions,results,/content/training-data/relation_extraction/4/triples/results.txt
Results,has,proposed position - aware mechanism,results,/content/training-data/relation_extraction/4/triples/results.txt
proposed position - aware mechanism,achieves,F 1 score,results,/content/training-data/relation_extraction/4/triples/results.txt
F 1 score,of,65.4 %,results,/content/training-data/relation_extraction/4/triples/results.txt
65.4 %,with,absolute increase,results,/content/training-data/relation_extraction/4/triples/results.txt
absolute increase,of,3.9 %,results,/content/training-data/relation_extraction/4/triples/results.txt
3.9 %,over,best baseline neural model ( LSTM ,results,/content/training-data/relation_extraction/4/triples/results.txt
absolute increase,of,7.9 %,results,/content/training-data/relation_extraction/4/triples/results.txt
7.9 %,over,baseline logistic regression system,results,/content/training-data/relation_extraction/4/triples/results.txt
proposed position - aware mechanism,is,very effective,results,/content/training-data/relation_extraction/4/triples/results.txt
Results,has,positional embeddings,results,/content/training-data/relation_extraction/4/triples/results.txt
positional embeddings,help increase,F 1,results,/content/training-data/relation_extraction/4/triples/results.txt
F 1,by around,2 %,results,/content/training-data/relation_extraction/4/triples/results.txt
2 %,over,plain CNN model,results,/content/training-data/relation_extraction/4/triples/results.txt
Results,has,Endto - end cold start,results,/content/training-data/relation_extraction/4/triples/results.txt
Endto - end cold start,has,slot filling scores,results,/content/training-data/relation_extraction/4/triples/results.txt
slot filling scores,conflate,performance,results,/content/training-data/relation_extraction/4/triples/results.txt
performance,of,"all modules in the system ( i.e. , entity recognizer , entity linker and relation extractor ",results,/content/training-data/relation_extraction/4/triples/results.txt
Results,has,CNN - based models,results,/content/training-data/relation_extraction/4/triples/results.txt
CNN - based models,tend to have,higher precision,results,/content/training-data/relation_extraction/4/triples/results.txt
Results,has,RNN - based models,results,/content/training-data/relation_extraction/4/triples/results.txt
RNN - based models,have,better recall,results,/content/training-data/relation_extraction/4/triples/results.txt
Results,observe,all neural models,results,/content/training-data/relation_extraction/4/triples/results.txt
all neural models,achieve,higher F 1 scores,results,/content/training-data/relation_extraction/4/triples/results.txt
higher F 1 scores,demonstrates,effectiveness,results,/content/training-data/relation_extraction/4/triples/results.txt
effectiveness,of,neural models,results,/content/training-data/relation_extraction/4/triples/results.txt
effectiveness,for,relation extraction,results,/content/training-data/relation_extraction/4/triples/results.txt
higher F 1 scores,than,logistic regression and patterns systems,results,/content/training-data/relation_extraction/4/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/relation_extraction/8/triples/baselines.txt
Baselines,has,MultiR,baselines,/content/training-data/relation_extraction/8/triples/baselines.txt
MultiR,is,multi-instance learning method,baselines,/content/training-data/relation_extraction/8/triples/baselines.txt
Baselines,has,Mintz,baselines,/content/training-data/relation_extraction/8/triples/baselines.txt
Mintz,represents,traditional distantsupervision - based model,baselines,/content/training-data/relation_extraction/8/triples/baselines.txt
Baselines,has,MIML,baselines,/content/training-data/relation_extraction/8/triples/baselines.txt
MIML,is,multi-instance multilabel model,baselines,/content/training-data/relation_extraction/8/triples/baselines.txt
Contribution,has,Model,model,/content/training-data/relation_extraction/8/triples/model.txt
Model,In,learning process,model,/content/training-data/relation_extraction/8/triples/model.txt
learning process,has,uncertainty,model,/content/training-data/relation_extraction/8/triples/model.txt
uncertainty,alleviates,wrong label problem,model,/content/training-data/relation_extraction/8/triples/model.txt
uncertainty,of,instance labels,model,/content/training-data/relation_extraction/8/triples/model.txt
Model,adopt,convolutional architecture,model,/content/training-data/relation_extraction/8/triples/model.txt
convolutional architecture,to automatically learn,relevant features,model,/content/training-data/relation_extraction/8/triples/model.txt
relevant features,without,complicated NLP preprocessing,model,/content/training-data/relation_extraction/8/triples/model.txt
Model,design,objective function,model,/content/training-data/relation_extraction/8/triples/model.txt
objective function,at,bag level,model,/content/training-data/relation_extraction/8/triples/model.txt
Model,has,distant supervised relation extraction,model,/content/training-data/relation_extraction/8/triples/model.txt
distant supervised relation extraction,treated as,multi-instance problem,model,/content/training-data/relation_extraction/8/triples/model.txt
Model,has,piecewise max pooling procedure,model,/content/training-data/relation_extraction/8/triples/model.txt
piecewise max pooling procedure,returns,maximum value,model,/content/training-data/relation_extraction/8/triples/model.txt
maximum value,in,each segment,model,/content/training-data/relation_extraction/8/triples/model.txt
each segment,instead of,single maximum value,model,/content/training-data/relation_extraction/8/triples/model.txt
single maximum value,over,entire sentence,model,/content/training-data/relation_extraction/8/triples/model.txt
Model,To capture,structural and other latent information,model,/content/training-data/relation_extraction/8/triples/model.txt
structural and other latent information,divide,convolution results,model,/content/training-data/relation_extraction/8/triples/model.txt
convolution results,into,three segments,model,/content/training-data/relation_extraction/8/triples/model.txt
three segments,based on,positions,model,/content/training-data/relation_extraction/8/triples/model.txt
positions,of,two given entities,model,/content/training-data/relation_extraction/8/triples/model.txt
structural and other latent information,devise,piecewise max pooling layer,model,/content/training-data/relation_extraction/8/triples/model.txt
piecewise max pooling layer,instead of,single max pooling layer,model,/content/training-data/relation_extraction/8/triples/model.txt
Model,propose,novel model dubbed Piecewise Convolutional Neural Networks ( PC - NNs ,model,/content/training-data/relation_extraction/8/triples/model.txt
novel model dubbed Piecewise Convolutional Neural Networks ( PC - NNs ),with,multi-instance learning,model,/content/training-data/relation_extraction/8/triples/model.txt
Contribution,has research problem,Distant Supervision for Relation Extraction,research-problem,/content/training-data/relation_extraction/8/triples/research-problem.txt
Contribution,has research problem,distant supervised relation extraction,research-problem,/content/training-data/relation_extraction/8/triples/research-problem.txt
Contribution,has research problem,relation extraction,research-problem,/content/training-data/relation_extraction/8/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/relation_extraction/8/triples/experimental-setup.txt
Experimental setup,In,dropout operation,experimental-setup,/content/training-data/relation_extraction/8/triples/experimental-setup.txt
dropout operation,randomly set,hidden unit activities,experimental-setup,/content/training-data/relation_extraction/8/triples/experimental-setup.txt
hidden unit activities,with,probability,experimental-setup,/content/training-data/relation_extraction/8/triples/experimental-setup.txt
probability,of,0.5,experimental-setup,/content/training-data/relation_extraction/8/triples/experimental-setup.txt
hidden unit activities,to,zero,experimental-setup,/content/training-data/relation_extraction/8/triples/experimental-setup.txt
hidden unit activities,during,training,experimental-setup,/content/training-data/relation_extraction/8/triples/experimental-setup.txt
Experimental setup,use,grid search,experimental-setup,/content/training-data/relation_extraction/8/triples/experimental-setup.txt
grid search,to determine,optimal parameters,experimental-setup,/content/training-data/relation_extraction/8/triples/experimental-setup.txt
grid search,manually specify,subsets,experimental-setup,/content/training-data/relation_extraction/8/triples/experimental-setup.txt
subsets,of,parameter spaces,experimental-setup,/content/training-data/relation_extraction/8/triples/experimental-setup.txt
parameter spaces,has,"w ? { 1 , 2 , 3 , , 7 }",experimental-setup,/content/training-data/relation_extraction/8/triples/experimental-setup.txt
parameter spaces,has,"n ? { 50 , 60 , , 300}",experimental-setup,/content/training-data/relation_extraction/8/triples/experimental-setup.txt
Experimental setup,use,Adadelta,experimental-setup,/content/training-data/relation_extraction/8/triples/experimental-setup.txt
Adadelta,in,update procedure,experimental-setup,/content/training-data/relation_extraction/8/triples/experimental-setup.txt
Experimental setup,use,Skip - gram model ( word2 vec ,experimental-setup,/content/training-data/relation_extraction/8/triples/experimental-setup.txt
Skip - gram model ( word2 vec ),to train,word embeddings,experimental-setup,/content/training-data/relation_extraction/8/triples/experimental-setup.txt
word embeddings,on,NYT corpus,experimental-setup,/content/training-data/relation_extraction/8/triples/experimental-setup.txt
Experimental setup,heuristically choose,d p,experimental-setup,/content/training-data/relation_extraction/8/triples/experimental-setup.txt
d p,=,5,experimental-setup,/content/training-data/relation_extraction/8/triples/experimental-setup.txt
Experimental setup,has,batch size,experimental-setup,/content/training-data/relation_extraction/8/triples/experimental-setup.txt
batch size,fixed to,50,experimental-setup,/content/training-data/relation_extraction/8/triples/experimental-setup.txt
Experimental setup,tune,models,experimental-setup,/content/training-data/relation_extraction/8/triples/experimental-setup.txt
models,using,three - fold validation,experimental-setup,/content/training-data/relation_extraction/8/triples/experimental-setup.txt
three - fold validation,on,training set,experimental-setup,/content/training-data/relation_extraction/8/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/relation_extraction/8/triples/results.txt
Results,demonstrates,PCNNs + MIL,results,/content/training-data/relation_extraction/8/triples/results.txt
PCNNs + MIL,achieves,higher precision,results,/content/training-data/relation_extraction/8/triples/results.txt
higher precision,over,entire range of recall,results,/content/training-data/relation_extraction/8/triples/results.txt
Results,In terms of both,precision and recall,results,/content/training-data/relation_extraction/8/triples/results.txt
precision and recall,has,PCNNs + MIL,results,/content/training-data/relation_extraction/8/triples/results.txt
PCNNs + MIL,outperforms,all other evaluated approaches,results,/content/training-data/relation_extraction/8/triples/results.txt
Results,has,PCNNs + MIL,results,/content/training-data/relation_extraction/8/triples/results.txt
PCNNs + MIL,enhances,recall,results,/content/training-data/relation_extraction/8/triples/results.txt
recall,to,ap - proximately 34 %,results,/content/training-data/relation_extraction/8/triples/results.txt
PCNNs + MIL,without any loss of,precision,results,/content/training-data/relation_extraction/8/triples/results.txt
Results,Incorporating,multi-instance learning,results,/content/training-data/relation_extraction/8/triples/results.txt
multi-instance learning,into,convolutional neural network,results,/content/training-data/relation_extraction/8/triples/results.txt
convolutional neural network,effective means of addressing,wrong label problem,results,/content/training-data/relation_extraction/8/triples/results.txt
Results,Automatically learning,features,results,/content/training-data/relation_extraction/8/triples/results.txt
features,via,PCNNs,results,/content/training-data/relation_extraction/8/triples/results.txt
PCNNs,alleviate,error propagation,results,/content/training-data/relation_extraction/8/triples/results.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
Hyperparameters,limit,spans,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
spans,to,max length,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
max length,of,L = 10,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
Hyperparameters,has,Learning,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
Learning,done with,"Adam ( Kingma and Ba , 2015 ",hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
"Adam ( Kingma and Ba , 2015 )",with,default parameters,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
Hyperparameters,has,Early Stopping,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
Early Stopping,of,20 evaluations,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
20 evaluations,on,dev set,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
Hyperparameters,has,learned character embeddings,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
learned character embeddings,of size,8,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
Hyperparameters,has,Regularization Dropout,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
Regularization Dropout,applied with,dropout rate 0.2,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
dropout rate 0.2,to,all hidden layers,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
all hidden layers,of,all MLPs and feature encodings,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
Regularization Dropout,applied with,dropout rate 0.4,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
dropout rate 0.4,to,all LSTM layer outputs,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
Regularization Dropout,applied with,dropout rate 0.5,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
dropout rate 0.5,to,all word and character embeddings,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
Hyperparameters,has,learning rate,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
learning rate,annealed by,1 %,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
1 %,every,100 iterations,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
Hyperparameters,has,1 - dimensional convolutions,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
1 - dimensional convolutions,of window size,3,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
Hyperparameters,has,All Multi Layer Perceptrons ( MLP ,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
All Multi Layer Perceptrons ( MLP ),has,two hidden layers,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
two hidden layers,with,500 dimensions,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
two hidden layers,followed by,ReLU activation,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
Hyperparameters,has,Minibatch Size,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
Minibatch Size,is,1,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
Hyperparameters,has,stacked bi - LSTMs,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
stacked bi - LSTMs,has,3 layers,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
3 layers,with,200 - dimensional hidden states,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
3 layers,with,highway connections,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
Hyperparameters,consider,spans,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
spans,entirely within,sentence,hyperparameters,/content/training-data/relation_extraction/10/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/relation_extraction/10/triples/model.txt
Model,has,span representations,model,/content/training-data/relation_extraction/10/triples/model.txt
span representations,to perform,entity mention detection,model,/content/training-data/relation_extraction/10/triples/model.txt
entity mention detection,on,all spans,model,/content/training-data/relation_extraction/10/triples/model.txt
all spans,in,parallel,model,/content/training-data/relation_extraction/10/triples/model.txt
span representations,to perform,relation extraction,model,/content/training-data/relation_extraction/10/triples/model.txt
relation extraction,on,all pairs,model,/content/training-data/relation_extraction/10/triples/model.txt
all pairs,of,detected entity mentions,model,/content/training-data/relation_extraction/10/triples/model.txt
Model,propose,simple bi - LSTM based model,model,/content/training-data/relation_extraction/10/triples/model.txt
simple bi - LSTM based model,which generates,span representations,model,/content/training-data/relation_extraction/10/triples/model.txt
span representations,for,each possible span,model,/content/training-data/relation_extraction/10/triples/model.txt
Contribution,has research problem,Relation Extraction,research-problem,/content/training-data/relation_extraction/10/triples/research-problem.txt
Contribution,has research problem,Relation Extraction ( RE ,research-problem,/content/training-data/relation_extraction/10/triples/research-problem.txt
Contribution,has research problem,RE,research-problem,/content/training-data/relation_extraction/10/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/relation_extraction/10/triples/results.txt
Results,For,both tasks,results,/content/training-data/relation_extraction/10/triples/results.txt
both tasks,than,previous works,results,/content/training-data/relation_extraction/10/triples/results.txt
previous works,close to,our model 's Precision,results,/content/training-data/relation_extraction/10/triples/results.txt
previous works,significantly higher,Recall,results,/content/training-data/relation_extraction/10/triples/results.txt
Results,has,our large gains,results,/content/training-data/relation_extraction/10/triples/results.txt
our large gains,in,RE Recall ( and F 1 ,results,/content/training-data/relation_extraction/10/triples/results.txt
RE Recall ( and F 1 ),showcase,effectiveness,results,/content/training-data/relation_extraction/10/triples/results.txt
effectiveness,of,our simple modeling of ordered span pairs,results,/content/training-data/relation_extraction/10/triples/results.txt
our simple modeling of ordered span pairs,for,relation extraction,results,/content/training-data/relation_extraction/10/triples/results.txt
Results,has,Our proposed model,results,/content/training-data/relation_extraction/10/triples/results.txt
Our proposed model,achieves,new SOTA,results,/content/training-data/relation_extraction/10/triples/results.txt
new SOTA,with,F 1,results,/content/training-data/relation_extraction/10/triples/results.txt
F 1,of,62. 83,results,/content/training-data/relation_extraction/10/triples/results.txt
new SOTA,on,RE,results,/content/training-data/relation_extraction/10/triples/results.txt
Our proposed model,also beats,multitask model,results,/content/training-data/relation_extraction/10/triples/results.txt
multitask model,by more than,1.5 F 1 points,results,/content/training-data/relation_extraction/10/triples/results.txt
multitask model,which uses,signals,results,/content/training-data/relation_extraction/10/triples/results.txt
signals,from,additional tasks,results,/content/training-data/relation_extraction/10/triples/results.txt
Results,has,Recall gains,results,/content/training-data/relation_extraction/10/triples/results.txt
Recall gains,much higher than for,EMD,results,/content/training-data/relation_extraction/10/triples/results.txt
EMD,has,0.6 absolute points,results,/content/training-data/relation_extraction/10/triples/results.txt
Recall gains,for,RE,results,/content/training-data/relation_extraction/10/triples/results.txt
RE,has,4.3 absolute points,results,/content/training-data/relation_extraction/10/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/relation_extraction/3/triples/baselines.txt
Baselines,has,BERT SP with position embedding on the final attention layer,baselines,/content/training-data/relation_extraction/3/triples/baselines.txt
BERT SP with position embedding on the final attention layer,has,more straightforward way,baselines,/content/training-data/relation_extraction/3/triples/baselines.txt
more straightforward way,to achieve,MRE,baselines,/content/training-data/relation_extraction/3/triples/baselines.txt
MRE,using,position embeddings,baselines,/content/training-data/relation_extraction/3/triples/baselines.txt
MRE,in,one - pass,baselines,/content/training-data/relation_extraction/3/triples/baselines.txt
Baselines,has,Entity - Aware BERT SP,baselines,/content/training-data/relation_extraction/3/triples/baselines.txt
Entity - Aware BERT SP,is,our full model,baselines,/content/training-data/relation_extraction/3/triples/baselines.txt
Baselines,has,BERT SP,baselines,/content/training-data/relation_extraction/3/triples/baselines.txt
BERT SP,has,BERT with structured prediction only,baselines,/content/training-data/relation_extraction/3/triples/baselines.txt
Baselines,has,BERT SP with entity indicators on input layer,baselines,/content/training-data/relation_extraction/3/triples/baselines.txt
BERT SP with entity indicators on input layer,replaces,our structured attention layer,baselines,/content/training-data/relation_extraction/3/triples/baselines.txt
BERT SP with entity indicators on input layer,adds,indicators,baselines,/content/training-data/relation_extraction/3/triples/baselines.txt
indicators,of,entities ( transformed to embeddings ,baselines,/content/training-data/relation_extraction/3/triples/baselines.txt
Contribution,has,Model,model,/content/training-data/relation_extraction/3/triples/model.txt
Model,use,Bidirectional Encoder Representations from Transformers ( BERT ,model,/content/training-data/relation_extraction/3/triples/model.txt
Bidirectional Encoder Representations from Transformers ( BERT ),as,transformer - based encoder,model,/content/training-data/relation_extraction/3/triples/model.txt
Model,introduce,structured prediction layer,model,/content/training-data/relation_extraction/3/triples/model.txt
structured prediction layer,for predicting,multiple relations,model,/content/training-data/relation_extraction/3/triples/model.txt
multiple relations,for,different entity pairs,model,/content/training-data/relation_extraction/3/triples/model.txt
Model,has,proposed solution,model,/content/training-data/relation_extraction/3/triples/model.txt
proposed solution,built on top of,"existing transformer - based , pretrained general - purposed language encoders",model,/content/training-data/relation_extraction/3/triples/model.txt
Model,make,selfattention layers,model,/content/training-data/relation_extraction/3/triples/model.txt
selfattention layers,aware of,positions,model,/content/training-data/relation_extraction/3/triples/model.txt
positions,of,all en-tities,model,/content/training-data/relation_extraction/3/triples/model.txt
all en-tities,in,input paragraph,model,/content/training-data/relation_extraction/3/triples/model.txt
Model,presents,solution,model,/content/training-data/relation_extraction/3/triples/model.txt
solution,resolve,inefficient multiple - passes issue,model,/content/training-data/relation_extraction/3/triples/model.txt
inefficient multiple - passes issue,of,existing solutions,model,/content/training-data/relation_extraction/3/triples/model.txt
existing solutions,for,MRE,model,/content/training-data/relation_extraction/3/triples/model.txt
MRE,by encoding,input only once,model,/content/training-data/relation_extraction/3/triples/model.txt
Contribution,has research problem,Extracting Multiple - Relations,research-problem,/content/training-data/relation_extraction/3/triples/research-problem.txt
Contribution,has research problem,extracting multiple entity - relations,research-problem,/content/training-data/relation_extraction/3/triples/research-problem.txt
Contribution,has research problem,multiple entityrelations extraction,research-problem,/content/training-data/relation_extraction/3/triples/research-problem.txt
Contribution,has research problem,Relation extraction ( RE ,research-problem,/content/training-data/relation_extraction/3/triples/research-problem.txt
Contribution,has research problem,RE,research-problem,/content/training-data/relation_extraction/3/triples/research-problem.txt
Contribution,has research problem,multiplerelations extraction ( MRE ,research-problem,/content/training-data/relation_extraction/3/triples/research-problem.txt
Contribution,has research problem,MRE,research-problem,/content/training-data/relation_extraction/3/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/relation_extraction/3/triples/results.txt
Results,When predicting,multiple relations,results,/content/training-data/relation_extraction/3/triples/results.txt
multiple relations,in,one - pass,results,/content/training-data/relation_extraction/3/triples/results.txt
multiple relations,have,further 0.8 % improvement,results,/content/training-data/relation_extraction/3/triples/results.txt
further 0.8 % improvement,on,Micro - F1,results,/content/training-data/relation_extraction/3/triples/results.txt
multiple relations,have,0.9 % drop,results,/content/training-data/relation_extraction/3/triples/results.txt
0.9 % drop,on,Macro - F1,results,/content/training-data/relation_extraction/3/triples/results.txt
Results,Note,our method,results,/content/training-data/relation_extraction/3/triples/results.txt
our method,not designed for,domain adaptation,results,/content/training-data/relation_extraction/3/triples/results.txt
our method,still outperforms,methods,results,/content/training-data/relation_extraction/3/triples/results.txt
methods,with,domain adaptation,results,/content/training-data/relation_extraction/3/triples/results.txt
Results,works for,singlerelation per pass setting,results,/content/training-data/relation_extraction/3/triples/results.txt
singlerelation per pass setting,has,performance,results,/content/training-data/relation_extraction/3/triples/results.txt
performance,lags behind using,only indicators,results,/content/training-data/relation_extraction/3/triples/results.txt
only indicators,of,two target entities,results,/content/training-data/relation_extraction/3/triples/results.txt
Results,For,BERT SP with position embeddings,results,/content/training-data/relation_extraction/3/triples/results.txt
BERT SP with position embeddings,test with,two different settings,results,/content/training-data/relation_extraction/3/triples/results.txt
two different settings,so,results,results,/content/training-data/relation_extraction/3/triples/results.txt
results,are,same,results,/content/training-data/relation_extraction/3/triples/results.txt
BERT SP with position embeddings,on,final attention layer,results,/content/training-data/relation_extraction/3/triples/results.txt
BERT SP with position embeddings,train,model,results,/content/training-data/relation_extraction/3/triples/results.txt
model,in,single - relation setting,results,/content/training-data/relation_extraction/3/triples/results.txt
Results,For,BERT SP,results,/content/training-data/relation_extraction/3/triples/results.txt
BERT SP,with,entity indicators,results,/content/training-data/relation_extraction/3/triples/results.txt
entity indicators,on,inputs,results,/content/training-data/relation_extraction/3/triples/results.txt
BERT SP,observed,2 % gap,results,/content/training-data/relation_extraction/3/triples/results.txt
Results,has,Our full model,results,/content/training-data/relation_extraction/3/triples/results.txt
Our full model,with,structured fine - tuning,results,/content/training-data/relation_extraction/3/triples/results.txt
structured fine - tuning,of,attention layers,results,/content/training-data/relation_extraction/3/triples/results.txt
Our full model,brings,further improvement,results,/content/training-data/relation_extraction/3/triples/results.txt
further improvement,in,MRE one - pass setting,results,/content/training-data/relation_extraction/3/triples/results.txt
further improvement,of,about 5.5 %,results,/content/training-data/relation_extraction/3/triples/results.txt
further improvement,achieves,new state - of - the - art performance,results,/content/training-data/relation_extraction/3/triples/results.txt
new state - of - the - art performance,compared to,methods with domain adaptation,results,/content/training-data/relation_extraction/3/triples/results.txt
Results,has,Our Entity - Aware BERT SP,results,/content/training-data/relation_extraction/3/triples/results.txt
Our Entity - Aware BERT SP,gives,comparable results,results,/content/training-data/relation_extraction/3/triples/results.txt
comparable results,with,slightly lower Macro - F1,results,/content/training-data/relation_extraction/3/triples/results.txt
comparable results,with,slightly higher Micro - F1,results,/content/training-data/relation_extraction/3/triples/results.txt
comparable results,to,top - ranked system,results,/content/training-data/relation_extraction/3/triples/results.txt
top - ranked system,in,shared task,results,/content/training-data/relation_extraction/3/triples/results.txt
Results,has,first observation,results,/content/training-data/relation_extraction/3/triples/results.txt
first observation,is that,our model architecture,results,/content/training-data/relation_extraction/3/triples/results.txt
our model architecture,achieves,much better results,results,/content/training-data/relation_extraction/3/triples/results.txt
much better results,compared to,previous state - of - the - art methods,results,/content/training-data/relation_extraction/3/triples/results.txt
Results,has,BERT SP,results,/content/training-data/relation_extraction/3/triples/results.txt
BERT SP,successfully adapt,pre-trained BERT,results,/content/training-data/relation_extraction/3/triples/results.txt
pre-trained BERT,to,MRE task,results,/content/training-data/relation_extraction/3/triples/results.txt
BERT SP,achieves,comparable performance,results,/content/training-data/relation_extraction/3/triples/results.txt
Results,Among,all the BERT - based approaches,results,/content/training-data/relation_extraction/3/triples/results.txt
all the BERT - based approaches,finetuning,off - the - shelf BERT,results,/content/training-data/relation_extraction/3/triples/results.txt
off - the - shelf BERT,does not give,satisfying result,results,/content/training-data/relation_extraction/3/triples/results.txt
Results,compared to,top singlemodel result,results,/content/training-data/relation_extraction/3/triples/results.txt
top singlemodel result,which makes use of,additional word and entity embeddings,results,/content/training-data/relation_extraction/3/triples/results.txt
additional word and entity embeddings,pretrained on,in - domain data,results,/content/training-data/relation_extraction/3/triples/results.txt
additional word and entity embeddings,has,our methods,results,/content/training-data/relation_extraction/3/triples/results.txt
our methods,demonstrate,clear advantage,results,/content/training-data/relation_extraction/3/triples/results.txt
clear advantage,as,single model,results,/content/training-data/relation_extraction/3/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/relation_extraction/11/triples/baselines.txt
Baselines,has,MIMLRE,baselines,/content/training-data/relation_extraction/11/triples/baselines.txt
MIMLRE,has,graphical model,baselines,/content/training-data/relation_extraction/11/triples/baselines.txt
graphical model,jointly models,multiple instances and multiple labels,baselines,/content/training-data/relation_extraction/11/triples/baselines.txt
Baselines,has,BGWA,baselines,/content/training-data/relation_extraction/11/triples/baselines.txt
BGWA,has,Bi - GRU based relation extraction model,baselines,/content/training-data/relation_extraction/11/triples/baselines.txt
Bi - GRU based relation extraction model,with,word and sentence level attention,baselines,/content/training-data/relation_extraction/11/triples/baselines.txt
Baselines,has,MultiR,baselines,/content/training-data/relation_extraction/11/triples/baselines.txt
MultiR,has,Probabilistic graphical model,baselines,/content/training-data/relation_extraction/11/triples/baselines.txt
Probabilistic graphical model,for,multi instance learning,baselines,/content/training-data/relation_extraction/11/triples/baselines.txt
Baselines,has,Mintz,baselines,/content/training-data/relation_extraction/11/triples/baselines.txt
Mintz,has,Multi-class logistic regression model,baselines,/content/training-data/relation_extraction/11/triples/baselines.txt
Multi-class logistic regression model,for,distant supervision paradigm,baselines,/content/training-data/relation_extraction/11/triples/baselines.txt
Baselines,has,PCNN,baselines,/content/training-data/relation_extraction/11/triples/baselines.txt
PCNN,has,CNN based relation extraction model,baselines,/content/training-data/relation_extraction/11/triples/baselines.txt
CNN based relation extraction model,uses,piecewise max - pooling,baselines,/content/training-data/relation_extraction/11/triples/baselines.txt
piecewise max - pooling,for,sentence representation,baselines,/content/training-data/relation_extraction/11/triples/baselines.txt
Baselines,has,PCNN + ATT,baselines,/content/training-data/relation_extraction/11/triples/baselines.txt
PCNN + ATT,has,piecewise max - pooling,baselines,/content/training-data/relation_extraction/11/triples/baselines.txt
piecewise max - pooling,over,CNN based model,baselines,/content/training-data/relation_extraction/11/triples/baselines.txt
CNN based model,to get,sentence representation,baselines,/content/training-data/relation_extraction/11/triples/baselines.txt
sentence representation,followed by,attention over sentences,baselines,/content/training-data/relation_extraction/11/triples/baselines.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/relation_extraction/11/triples/ablation-analysis.txt
Ablation analysis,validate that,GCNs,ablation-analysis,/content/training-data/relation_extraction/11/triples/ablation-analysis.txt
GCNs,effective at encoding,syntactic information,ablation-analysis,/content/training-data/relation_extraction/11/triples/ablation-analysis.txt
Ablation analysis,inducing,side information,ablation-analysis,/content/training-data/relation_extraction/11/triples/ablation-analysis.txt
side information,leads to,improved relation extraction,ablation-analysis,/content/training-data/relation_extraction/11/triples/ablation-analysis.txt
Ablation analysis,has,model,ablation-analysis,/content/training-data/relation_extraction/11/triples/ablation-analysis.txt
model,performs,best,ablation-analysis,/content/training-data/relation_extraction/11/triples/ablation-analysis.txt
best,when,aliases,ablation-analysis,/content/training-data/relation_extraction/11/triples/ablation-analysis.txt
aliases,provided by,KB,ablation-analysis,/content/training-data/relation_extraction/11/triples/ablation-analysis.txt
Ablation analysis,has,RESIDE,ablation-analysis,/content/training-data/relation_extraction/11/triples/ablation-analysis.txt
RESIDE,gives,competitive performance,ablation-analysis,/content/training-data/relation_extraction/11/triples/ablation-analysis.txt
competitive performance,when,very limited amount of relation alias information,ablation-analysis,/content/training-data/relation_extraction/11/triples/ablation-analysis.txt
very limited amount of relation alias information,is,available,ablation-analysis,/content/training-data/relation_extraction/11/triples/ablation-analysis.txt
Ablation analysis,observe,performance,ablation-analysis,/content/training-data/relation_extraction/11/triples/ablation-analysis.txt
performance,improves further with,availability of more alias information,ablation-analysis,/content/training-data/relation_extraction/11/triples/ablation-analysis.txt
Contribution,Code,http://github.com / malllabiisc / RESIDE,code,/content/training-data/relation_extraction/11/triples/code.txt
Contribution,has,Model,model,/content/training-data/relation_extraction/11/triples/model.txt
Model,uses,encoded syntactic information,model,/content/training-data/relation_extraction/11/triples/model.txt
encoded syntactic information,obtained from,Graph Convolution Networks ( GCN ,model,/content/training-data/relation_extraction/11/triples/model.txt
encoded syntactic information,to improve,neural relation extraction,model,/content/training-data/relation_extraction/11/triples/model.txt
encoded syntactic information,along with,embedded side information,model,/content/training-data/relation_extraction/11/triples/model.txt
Model,has,RESIDE,model,/content/training-data/relation_extraction/11/triples/model.txt
RESIDE,makes,principled use,model,/content/training-data/relation_extraction/11/triples/model.txt
principled use,of,entity type and relation alias information,model,/content/training-data/relation_extraction/11/triples/model.txt
entity type and relation alias information,to impose,soft constraints,model,/content/training-data/relation_extraction/11/triples/model.txt
soft constraints,while predicting,relation,model,/content/training-data/relation_extraction/11/triples/model.txt
entity type and relation alias information,from,KBs,model,/content/training-data/relation_extraction/11/triples/model.txt
Model,propose,RESIDE,model,/content/training-data/relation_extraction/11/triples/model.txt
RESIDE,has,novel distant supervised relation extraction method,model,/content/training-data/relation_extraction/11/triples/model.txt
novel distant supervised relation extraction method,utilizes,additional supervision,model,/content/training-data/relation_extraction/11/triples/model.txt
additional supervision,through,neural network based architecture,model,/content/training-data/relation_extraction/11/triples/model.txt
additional supervision,from,KB,model,/content/training-data/relation_extraction/11/triples/model.txt
Contribution,has research problem,Distantly - Supervised Neural Relation Extraction,research-problem,/content/training-data/relation_extraction/11/triples/research-problem.txt
Contribution,has research problem,Distantly - supervised Relation Extraction ( RE ,research-problem,/content/training-data/relation_extraction/11/triples/research-problem.txt
Contribution,has research problem,Relation Extraction ( RE ,research-problem,/content/training-data/relation_extraction/11/triples/research-problem.txt
Contribution,has research problem,RE,research-problem,/content/training-data/relation_extraction/11/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/relation_extraction/11/triples/results.txt
Results,find that,RESIDE,results,/content/training-data/relation_extraction/11/triples/results.txt
RESIDE,achieves,higher precision,results,/content/training-data/relation_extraction/11/triples/results.txt
higher precision,over,entire recall range,results,/content/training-data/relation_extraction/11/triples/results.txt
entire recall range,on,both the datasets,results,/content/training-data/relation_extraction/11/triples/results.txt
Results,has,RESIDE,results,/content/training-data/relation_extraction/11/triples/results.txt
RESIDE,outperforms,PCNN + ATT and BGWA,results,/content/training-data/relation_extraction/11/triples/results.txt
Results,has,higher performance,results,/content/training-data/relation_extraction/11/triples/results.txt
higher performance,of,BGWA and PCNN + ATT,results,/content/training-data/relation_extraction/11/triples/results.txt
BGWA and PCNN + ATT,over,PCNN,results,/content/training-data/relation_extraction/11/triples/results.txt
higher performance,shows that,attention,results,/content/training-data/relation_extraction/11/triples/results.txt
attention,helps in,distant supervised RE,results,/content/training-data/relation_extraction/11/triples/results.txt
Results,has,non-neural baselines,results,/content/training-data/relation_extraction/11/triples/results.txt
non-neural baselines,could not perform,well,results,/content/training-data/relation_extraction/11/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/relation_extraction/12/triples/baselines.txt
Baselines,For,cross - sentence n- ary relation extraction task,baselines,/content/training-data/relation_extraction/12/triples/baselines.txt
cross - sentence n- ary relation extraction task,has,feature - based classifier,baselines,/content/training-data/relation_extraction/12/triples/baselines.txt
feature - based classifier,based on,shortest dependency paths,baselines,/content/training-data/relation_extraction/12/triples/baselines.txt
shortest dependency paths,between,all entity pairs,baselines,/content/training-data/relation_extraction/12/triples/baselines.txt
cross - sentence n- ary relation extraction task,has,Graph - structured LSTM methods,baselines,/content/training-data/relation_extraction/12/triples/baselines.txt
Graph - structured LSTM methods,including,Graph LSTM,baselines,/content/training-data/relation_extraction/12/triples/baselines.txt
Graph - structured LSTM methods,including,bidirectional DAG LSTM ( Bidir DAG LSTM ,baselines,/content/training-data/relation_extraction/12/triples/baselines.txt
Graph - structured LSTM methods,including,Graph State LSTM ( GS GLSTM ,baselines,/content/training-data/relation_extraction/12/triples/baselines.txt
cross - sentence n- ary relation extraction task,has,Graph convolutional networks ( GCN ,baselines,/content/training-data/relation_extraction/12/triples/baselines.txt
Graph convolutional networks ( GCN ),with,pruned trees,baselines,/content/training-data/relation_extraction/12/triples/baselines.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
Ablation analysis,using,80 %,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
80 %,of,training data,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
training data,has,C - AGGCN model,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
C - AGGCN model,able to achieve,F 1 score,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
F 1 score,of,66.5,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
66.5,higher than,C - GCN,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
C - GCN,trained on,whole dataset,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
Ablation analysis,observe that,all the C - AGGCN models,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
all the C - AGGCN models,with,varied values of K,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
varied values of K,outperform,state - of - the - art C - GCN model,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
Ablation analysis,observe that,adding,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
adding,has,either attention guided layers or densely connected layers,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
either attention guided layers or densely connected layers,improves,performance,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
performance,of,model,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
Ablation analysis,When,size,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
size,of,training data,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
training data,observe that,performance gap,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
performance gap,becomes,more obvious,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
training data,has,increases,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
Ablation analysis,suggests that,C - AGGCN,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
C - AGGCN,benefit more from,larger graphs ( full tree ,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
Ablation analysis,Without,feed - forward layer,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
feed - forward layer,has,result,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
result,drops to,F1 score of 67.8,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
Ablation analysis,notice that,performance,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
performance,of,C - AGGCN with full trees,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
C - AGGCN with full trees,outperforms,all C - AGGCNs with pruned trees,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
Ablation analysis,notice that,feed - forward layer,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
feed - forward layer,effective in,our model,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
Ablation analysis,has,C - AGGCN with full trees,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
C - AGGCN with full trees,outperforms,C - AGGCN with pruned trees and C - GCN,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
C - AGGCN with pruned trees and C - GCN,against,various sentence lengths,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
Ablation analysis,has,C - AGGCN,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
C - AGGCN,consistently outperforms,C - GCN,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
C - GCN,under,same amount of training data,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
Ablation analysis,has,improvement,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
improvement,achieved by,C - AGGCN with pruned trees,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
C - AGGCN with pruned trees,has,decays,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
decays,when,sentence length,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
sentence length,has,increases,ablation-analysis,/content/training-data/relation_extraction/12/triples/ablation-analysis.txt
Contribution,Code,https://github.com/Cartus / AGGCN_TACRED,code,/content/training-data/relation_extraction/12/triples/code.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/relation_extraction/12/triples/hyperparameters.txt
Hyperparameters,choose,number of heads N,hyperparameters,/content/training-data/relation_extraction/12/triples/hyperparameters.txt
number of heads N,for,attention guided layer,hyperparameters,/content/training-data/relation_extraction/12/triples/hyperparameters.txt
attention guided layer,from,"{ 1 , 2 , 3 , 4 }",hyperparameters,/content/training-data/relation_extraction/12/triples/hyperparameters.txt
Hyperparameters,choose,number of sub - layers L,hyperparameters,/content/training-data/relation_extraction/12/triples/hyperparameters.txt
number of sub - layers L,in,each densely connected layer,hyperparameters,/content/training-data/relation_extraction/12/triples/hyperparameters.txt
each densely connected layer,from,"{ 2 , 3 , 4 }",hyperparameters,/content/training-data/relation_extraction/12/triples/hyperparameters.txt
Hyperparameters,choose,block number M,hyperparameters,/content/training-data/relation_extraction/12/triples/hyperparameters.txt
block number M,from,"{ 1 , 2 , 3 }",hyperparameters,/content/training-data/relation_extraction/12/triples/hyperparameters.txt
Hyperparameters,has,Glo Ve vectors,hyperparameters,/content/training-data/relation_extraction/12/triples/hyperparameters.txt
Glo Ve vectors,used as,initialization,hyperparameters,/content/training-data/relation_extraction/12/triples/hyperparameters.txt
initialization,for,word embeddings,hyperparameters,/content/training-data/relation_extraction/12/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/relation_extraction/12/triples/model.txt
Model,introduce,dense connections,model,/content/training-data/relation_extraction/12/triples/model.txt
dense connections,to,GCN model,model,/content/training-data/relation_extraction/12/triples/model.txt
Model,For,GCNs,model,/content/training-data/relation_extraction/12/triples/model.txt
GCNs,has,L layers,model,/content/training-data/relation_extraction/12/triples/model.txt
L layers,to capture,neighborhood information,model,/content/training-data/relation_extraction/12/triples/model.txt
neighborhood information,that is,L hops away,model,/content/training-data/relation_extraction/12/triples/model.txt
Model,develop,""" soft pruning "" strategy",model,/content/training-data/relation_extraction/12/triples/model.txt
""" soft pruning "" strategy",that transforms,original dependency tree,model,/content/training-data/relation_extraction/12/triples/model.txt
original dependency tree,into,fully connected edgeweighted graph,model,/content/training-data/relation_extraction/12/triples/model.txt
Model,has,weights,model,/content/training-data/relation_extraction/12/triples/model.txt
weights,viewed as,strength of relatedness,model,/content/training-data/relation_extraction/12/triples/model.txt
strength of relatedness,can be learned in,end - to - end fashion,model,/content/training-data/relation_extraction/12/triples/model.txt
end - to - end fashion,by using,self - attention mechanism,model,/content/training-data/relation_extraction/12/triples/model.txt
strength of relatedness,between,nodes,model,/content/training-data/relation_extraction/12/triples/model.txt
Model,propose,novel Attention Guided Graph Convolutional Networks ( AGGCNs ,model,/content/training-data/relation_extraction/12/triples/model.txt
novel Attention Guided Graph Convolutional Networks ( AGGCNs ),operate directly on,full tree,model,/content/training-data/relation_extraction/12/triples/model.txt
Model,With the help of,dense connections,model,/content/training-data/relation_extraction/12/triples/model.txt
dense connections,able to,train,model,/content/training-data/relation_extraction/12/triples/model.txt
train,has,AGGCN model,model,/content/training-data/relation_extraction/12/triples/model.txt
AGGCN model,with,large depth,model,/content/training-data/relation_extraction/12/triples/model.txt
Contribution,has research problem,Relation Extraction,research-problem,/content/training-data/relation_extraction/12/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/relation_extraction/12/triples/results.txt
Results,Compared to,GCN models,results,/content/training-data/relation_extraction/12/triples/results.txt
GCN models,has,our model,results,/content/training-data/relation_extraction/12/triples/results.txt
our model,obtains,1.3 and 1.2 points higher,results,/content/training-data/relation_extraction/12/triples/results.txt
1.3 and 1.2 points higher,than,best performing model,results,/content/training-data/relation_extraction/12/triples/results.txt
best performing model,with,pruned tree ( K=1 ,results,/content/training-data/relation_extraction/12/triples/results.txt
Results,For,ternary relation extraction,results,/content/training-data/relation_extraction/12/triples/results.txt
ternary relation extraction,has,our AGGCN model,results,/content/training-data/relation_extraction/12/triples/results.txt
our AGGCN model,achieves,accuracies,results,/content/training-data/relation_extraction/12/triples/results.txt
accuracies,of,87.1 and 87.0,results,/content/training-data/relation_extraction/12/triples/results.txt
87.1 and 87.0,outperform,all the baselines,results,/content/training-data/relation_extraction/12/triples/results.txt
87.1 and 87.0,on,instances,results,/content/training-data/relation_extraction/12/triples/results.txt
instances,within,single sentence ( Single ,results,/content/training-data/relation_extraction/12/triples/results.txt
87.1 and 87.0,on,all instances ( Cross ,results,/content/training-data/relation_extraction/12/triples/results.txt
Results,For,binary relation extraction,results,/content/training-data/relation_extraction/12/triples/results.txt
binary relation extraction,has,AGGCN,results,/content/training-data/relation_extraction/12/triples/results.txt
AGGCN,consistently outperforms,GS GLSTM,results,/content/training-data/relation_extraction/12/triples/results.txt
AGGCN,consistently outperforms,GCN,results,/content/training-data/relation_extraction/12/triples/results.txt
Results,has,our AGGCN,results,/content/training-data/relation_extraction/12/triples/results.txt
our AGGCN,achieves,better test accuracy,results,/content/training-data/relation_extraction/12/triples/results.txt
better test accuracy,than,all GCN models,results,/content/training-data/relation_extraction/12/triples/results.txt
Results,has,C - AGGCN model,results,/content/training-data/relation_extraction/12/triples/results.txt
C - AGGCN model,achieves,F1 score,results,/content/training-data/relation_extraction/12/triples/results.txt
F1 score,of,68.2,results,/content/training-data/relation_extraction/12/triples/results.txt
68.2,outperforms,state - ofart C - GCN model,results,/content/training-data/relation_extraction/12/triples/results.txt
state - ofart C - GCN model,by,1.8 points,results,/content/training-data/relation_extraction/12/triples/results.txt
Results,has,AG - GCN model,results,/content/training-data/relation_extraction/12/triples/results.txt
AG - GCN model,surpasses,state - of - the - art Graphstructured LSTM model ( GS GLSTM ,results,/content/training-data/relation_extraction/12/triples/results.txt
state - of - the - art Graphstructured LSTM model ( GS GLSTM ),by,6.8 and 3.8 points,results,/content/training-data/relation_extraction/12/triples/results.txt
state - of - the - art Graphstructured LSTM model ( GS GLSTM ),for,Single and Cross settings,results,/content/training-data/relation_extraction/12/triples/results.txt
Results,has,AGGCN,results,/content/training-data/relation_extraction/12/triples/results.txt
AGGCN,performs,better,results,/content/training-data/relation_extraction/12/triples/results.txt
better,than,GCNs,results,/content/training-data/relation_extraction/12/triples/results.txt
Results,has,performance gap,results,/content/training-data/relation_extraction/12/triples/results.txt
performance gap,between,GCNs with pruned trees and AGGCNs with full trees,results,/content/training-data/relation_extraction/12/triples/results.txt
GCNs with pruned trees and AGGCNs with full trees,empirically show,AGGCN model,results,/content/training-data/relation_extraction/12/triples/results.txt
AGGCN model,better at distinguishing,relevant from irrelevant information,results,/content/training-data/relation_extraction/12/triples/results.txt
relevant from irrelevant information,for learning,better graph representation,results,/content/training-data/relation_extraction/12/triples/results.txt
Results,has,our AGGCN model,results,/content/training-data/relation_extraction/12/triples/results.txt
our AGGCN model,obtains,8.0 and 5.7 points,results,/content/training-data/relation_extraction/12/triples/results.txt
8.0 and 5.7 points,higher than,GS GLSTM model,results,/content/training-data/relation_extraction/12/triples/results.txt
GS GLSTM model,for,ternary and binary relations,results,/content/training-data/relation_extraction/12/triples/results.txt
Results,notice,AGGCN and C - AGGCN,results,/content/training-data/relation_extraction/12/triples/results.txt
AGGCN and C - AGGCN,achieve,better precision and recall scores,results,/content/training-data/relation_extraction/12/triples/results.txt
better precision and recall scores,than,GCN and C - GCN,results,/content/training-data/relation_extraction/12/triples/results.txt
Results,on,SemEval dataset,results,/content/training-data/relation_extraction/12/triples/results.txt
SemEval dataset,has,Our C - AGGCN model ( 85.7 ,results,/content/training-data/relation_extraction/12/triples/results.txt
Our C - AGGCN model ( 85.7 ),consistently outperforms,C - GCN model ( 84.8 ,results,/content/training-data/relation_extraction/12/triples/results.txt
Our C - AGGCN model ( 85.7 ),showing,good generalizability,results,/content/training-data/relation_extraction/12/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/relation_extraction/5/triples/baselines.txt
Baselines,has,Neural sequence model,baselines,/content/training-data/relation_extraction/5/triples/baselines.txt
Neural sequence model,has,competitive sequence model,baselines,/content/training-data/relation_extraction/5/triples/baselines.txt
competitive sequence model,employs,position - aware attention mechanism,baselines,/content/training-data/relation_extraction/5/triples/baselines.txt
position - aware attention mechanism,over,LSTM outputs ( PA - LSTM ,baselines,/content/training-data/relation_extraction/5/triples/baselines.txt
Baselines,has,Dependency - based models,baselines,/content/training-data/relation_extraction/5/triples/baselines.txt
Dependency - based models,has,Tree - LSTM,baselines,/content/training-data/relation_extraction/5/triples/baselines.txt
Tree - LSTM,is,recursive model,baselines,/content/training-data/relation_extraction/5/triples/baselines.txt
recursive model,generalizes,LSTM,baselines,/content/training-data/relation_extraction/5/triples/baselines.txt
LSTM,to,arbitrary tree structures,baselines,/content/training-data/relation_extraction/5/triples/baselines.txt
Dependency - based models,has,A logistic regression ( LR ) classifier,baselines,/content/training-data/relation_extraction/5/triples/baselines.txt
A logistic regression ( LR ) classifier,combines,dependencybased features,baselines,/content/training-data/relation_extraction/5/triples/baselines.txt
dependencybased features,with,other lexical features,baselines,/content/training-data/relation_extraction/5/triples/baselines.txt
Dependency - based models,has,Shortest Dependency Path LSTM ( SDP - LSTM ,baselines,/content/training-data/relation_extraction/5/triples/baselines.txt
Shortest Dependency Path LSTM ( SDP - LSTM ),applies,neural sequence model,baselines,/content/training-data/relation_extraction/5/triples/baselines.txt
neural sequence model,on,shortest path,baselines,/content/training-data/relation_extraction/5/triples/baselines.txt
shortest path,between,subject and object entities,baselines,/content/training-data/relation_extraction/5/triples/baselines.txt
subject and object entities,in,dependency tree,baselines,/content/training-data/relation_extraction/5/triples/baselines.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/relation_extraction/5/triples/ablation-analysis.txt
Ablation analysis,Removing,pruning,ablation-analysis,/content/training-data/relation_extraction/5/triples/ablation-analysis.txt
pruning,hurts,result,ablation-analysis,/content/training-data/relation_extraction/5/triples/ablation-analysis.txt
result,by another,9.7 F 1,ablation-analysis,/content/training-data/relation_extraction/5/triples/ablation-analysis.txt
Ablation analysis,has,entity representations and feedforward layers,ablation-analysis,/content/training-data/relation_extraction/5/triples/ablation-analysis.txt
entity representations and feedforward layers,contribute,1.0 F 1,ablation-analysis,/content/training-data/relation_extraction/5/triples/ablation-analysis.txt
Ablation analysis,has,F 1,ablation-analysis,/content/training-data/relation_extraction/5/triples/ablation-analysis.txt
F 1,drops by,10.3,ablation-analysis,/content/training-data/relation_extraction/5/triples/ablation-analysis.txt
10.3,when we remove,feedforward layers,ablation-analysis,/content/training-data/relation_extraction/5/triples/ablation-analysis.txt
10.3,when we remove,LSTM component,ablation-analysis,/content/training-data/relation_extraction/5/triples/ablation-analysis.txt
10.3,when we remove,dependency structure,ablation-analysis,/content/training-data/relation_extraction/5/triples/ablation-analysis.txt
Ablation analysis,remove,dependency structure,ablation-analysis,/content/training-data/relation_extraction/5/triples/ablation-analysis.txt
dependency structure,has,score,ablation-analysis,/content/training-data/relation_extraction/5/triples/ablation-analysis.txt
score,drops by,3.2 F 1,ablation-analysis,/content/training-data/relation_extraction/5/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/relation_extraction/5/triples/model.txt
Model,apply,novel path - centric pruning technique,model,/content/training-data/relation_extraction/5/triples/model.txt
novel path - centric pruning technique,to remove,irrelevant information,model,/content/training-data/relation_extraction/5/triples/model.txt
irrelevant information,from,the tree,model,/content/training-data/relation_extraction/5/triples/model.txt
irrelevant information,maximally keeping,relevant content,model,/content/training-data/relation_extraction/5/triples/model.txt
Model,encodes,dependency structure,model,/content/training-data/relation_extraction/5/triples/model.txt
dependency structure,over,input sentence,model,/content/training-data/relation_extraction/5/triples/model.txt
input sentence,with,efficient graph convolution operations,model,/content/training-data/relation_extraction/5/triples/model.txt
dependency structure,extracts,entity - centric representations,model,/content/training-data/relation_extraction/5/triples/model.txt
entity - centric representations,to make,robust relation predictions,model,/content/training-data/relation_extraction/5/triples/model.txt
Model,propose,novel extension of the graph convolutional network,model,/content/training-data/relation_extraction/5/triples/model.txt
novel extension of the graph convolutional network,tailored for,relation extraction,model,/content/training-data/relation_extraction/5/triples/model.txt
Contribution,has research problem,Relation Extraction,research-problem,/content/training-data/relation_extraction/5/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/relation_extraction/5/triples/results.txt
Results,find that,all three models,results,/content/training-data/relation_extraction/5/triples/results.txt
all three models,are,less effective,results,/content/training-data/relation_extraction/5/triples/results.txt
less effective,when,entire dependency tree,results,/content/training-data/relation_extraction/5/triples/results.txt
entire dependency tree,is,present,results,/content/training-data/relation_extraction/5/triples/results.txt
Results,show,effectiveness,results,/content/training-data/relation_extraction/5/triples/results.txt
effectiveness,of,path - centric pruning,results,/content/training-data/relation_extraction/5/triples/results.txt
path - centric pruning,compare,two GCN models and the Tree - LSTM,results,/content/training-data/relation_extraction/5/triples/results.txt
two GCN models and the Tree - LSTM,performance,peaks,results,/content/training-data/relation_extraction/5/triples/results.txt
peaks,outperforming,respective dependency path - based counterpart ( K = 0 ,results,/content/training-data/relation_extraction/5/triples/results.txt
peaks,when,K = 1,results,/content/training-data/relation_extraction/5/triples/results.txt
Results,contextualizing,GCN,results,/content/training-data/relation_extraction/5/triples/results.txt
GCN,makes it,less sensitive,results,/content/training-data/relation_extraction/5/triples/results.txt
less sensitive,to,changes,results,/content/training-data/relation_extraction/5/triples/results.txt
changes,in,tree structures,results,/content/training-data/relation_extraction/5/triples/results.txt
Results,on,TACRED Dataset,results,/content/training-data/relation_extraction/5/triples/results.txt
TACRED Dataset,using,contextualized word representations,results,/content/training-data/relation_extraction/5/triples/results.txt
contextualized word representations,has,C - GCN model,results,/content/training-data/relation_extraction/5/triples/results.txt
C - GCN model,outperforms,strong PA - LSTM model,results,/content/training-data/relation_extraction/5/triples/results.txt
strong PA - LSTM model,by,1.3 F 1,results,/content/training-data/relation_extraction/5/triples/results.txt
strong PA - LSTM model,achieves,new state of the art,results,/content/training-data/relation_extraction/5/triples/results.txt
TACRED Dataset,find,our GCN models,results,/content/training-data/relation_extraction/5/triples/results.txt
our GCN models,have,complementary strengths,results,/content/training-data/relation_extraction/5/triples/results.txt
complementary strengths,when compared to,PA - LSTM,results,/content/training-data/relation_extraction/5/triples/results.txt
TACRED Dataset,find,our model,results,/content/training-data/relation_extraction/5/triples/results.txt
our model,improves upon,other dependencybased models,results,/content/training-data/relation_extraction/5/triples/results.txt
other dependencybased models,in,both precision and recall,results,/content/training-data/relation_extraction/5/triples/results.txt
TACRED Dataset,has,interpolation,results,/content/training-data/relation_extraction/5/triples/results.txt
interpolation,between,C - GCN and a PA - LSTM,results,/content/training-data/relation_extraction/5/triples/results.txt
C - GCN and a PA - LSTM,further improves,result,results,/content/training-data/relation_extraction/5/triples/results.txt
result,to,68.2,results,/content/training-data/relation_extraction/5/triples/results.txt
TACRED Dataset,has,simple interpolation,results,/content/training-data/relation_extraction/5/triples/results.txt
simple interpolation,between,GCN and a PA - LSTM,results,/content/training-data/relation_extraction/5/triples/results.txt
GCN and a PA - LSTM,achieves,F 1 score,results,/content/training-data/relation_extraction/5/triples/results.txt
F 1 score,of,67.1,results,/content/training-data/relation_extraction/5/triples/results.txt
GCN and a PA - LSTM,outperforming,each model alone,results,/content/training-data/relation_extraction/5/triples/results.txt
each model alone,by,at least 2.0 F 1,results,/content/training-data/relation_extraction/5/triples/results.txt
TACRED Dataset,observe,GCN model,results,/content/training-data/relation_extraction/5/triples/results.txt
GCN model,outperforms,all dependency - based models,results,/content/training-data/relation_extraction/5/triples/results.txt
all dependency - based models,by,at least 1.6 F 1,results,/content/training-data/relation_extraction/5/triples/results.txt
TACRED Dataset,Comparing,C - GCN model,results,/content/training-data/relation_extraction/5/triples/results.txt
C - GCN model,with,GCN model,results,/content/training-data/relation_extraction/5/triples/results.txt
GCN model,find that,gain,results,/content/training-data/relation_extraction/5/triples/results.txt
gain,mainly comes from,improved recall,results,/content/training-data/relation_extraction/5/triples/results.txt
Results,on,SemEval Dataset,results,/content/training-data/relation_extraction/5/triples/results.txt
SemEval Dataset,find that under,conventional with- entity evaluation,results,/content/training-data/relation_extraction/5/triples/results.txt
conventional with- entity evaluation,has,our C - GCN model,results,/content/training-data/relation_extraction/5/triples/results.txt
our C - GCN model,outperforms,all existing dependency - based neural models,results,/content/training-data/relation_extraction/5/triples/results.txt
SemEval Dataset,by properly incorporating,off - path information,results,/content/training-data/relation_extraction/5/triples/results.txt
off - path information,has,our model,results,/content/training-data/relation_extraction/5/triples/results.txt
our model,outperforms,previous shortest dependency path - based model ( SDP - LSTM ,results,/content/training-data/relation_extraction/5/triples/results.txt
SemEval Dataset,Under,mask - entity evaluation,results,/content/training-data/relation_extraction/5/triples/results.txt
mask - entity evaluation,has,our C - GCN model,results,/content/training-data/relation_extraction/5/triples/results.txt
our C - GCN model,outperforms,PA - LSTM,results,/content/training-data/relation_extraction/5/triples/results.txt
PA - LSTM,by,substantial margin,results,/content/training-data/relation_extraction/5/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/relation_extraction/0/triples/baselines.txt
Baselines,has,feature - based structured perceptron model,baselines,/content/training-data/relation_extraction/0/triples/baselines.txt
feature - based structured perceptron model,employ,segment - based decoder,baselines,/content/training-data/relation_extraction/0/triples/baselines.txt
segment - based decoder,instead of,token - based decoding,baselines,/content/training-data/relation_extraction/0/triples/baselines.txt
feature - based structured perceptron model,with,efficient beam - search,baselines,/content/training-data/relation_extraction/0/triples/baselines.txt
Baselines,has,SPTree,baselines,/content/training-data/relation_extraction/0/triples/baselines.txt
SPTree,proposed,LSTM - based model,baselines,/content/training-data/relation_extraction/0/triples/baselines.txt
LSTM - based model,with,sequence layer,baselines,/content/training-data/relation_extraction/0/triples/baselines.txt
sequence layer,for,entity identification,baselines,/content/training-data/relation_extraction/0/triples/baselines.txt
LSTM - based model,with,tree - based dependency layer,baselines,/content/training-data/relation_extraction/0/triples/baselines.txt
tree - based dependency layer,which identifies,relations between pairs of candidate entities,baselines,/content/training-data/relation_extraction/0/triples/baselines.txt
Baselines,employed,our previous approach,baselines,/content/training-data/relation_extraction/0/triples/baselines.txt
our previous approach,for,extraction,baselines,/content/training-data/relation_extraction/0/triples/baselines.txt
extraction,of,opinion entities and relations,baselines,/content/training-data/relation_extraction/0/triples/baselines.txt
opinion entities and relations,to,this task,baselines,/content/training-data/relation_extraction/0/triples/baselines.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/relation_extraction/0/triples/hyperparameters.txt
Hyperparameters,regularize,our network,hyperparameters,/content/training-data/relation_extraction/0/triples/hyperparameters.txt
our network,using,dropout,hyperparameters,/content/training-data/relation_extraction/0/triples/hyperparameters.txt
dropout,with,drop - out rate,hyperparameters,/content/training-data/relation_extraction/0/triples/hyperparameters.txt
drop - out rate,tuned using,development set,hyperparameters,/content/training-data/relation_extraction/0/triples/hyperparameters.txt
Hyperparameters,have,dimensionality,hyperparameters,/content/training-data/relation_extraction/0/triples/hyperparameters.txt
dimensionality,of,hidden units,hyperparameters,/content/training-data/relation_extraction/0/triples/hyperparameters.txt
hidden units,is,100,hyperparameters,/content/training-data/relation_extraction/0/triples/hyperparameters.txt
Hyperparameters,have,3 hidden layers,hyperparameters,/content/training-data/relation_extraction/0/triples/hyperparameters.txt
3 hidden layers,in,our network,hyperparameters,/content/training-data/relation_extraction/0/triples/hyperparameters.txt
Hyperparameters,has,weights,hyperparameters,/content/training-data/relation_extraction/0/triples/hyperparameters.txt
weights,in,network,hyperparameters,/content/training-data/relation_extraction/0/triples/hyperparameters.txt
network,initialized from,random uniform noise,hyperparameters,/content/training-data/relation_extraction/0/triples/hyperparameters.txt
Hyperparameters,tune,hyperparameters,hyperparameters,/content/training-data/relation_extraction/0/triples/hyperparameters.txt
hyperparameters,use them for,training,hyperparameters,/content/training-data/relation_extraction/0/triples/hyperparameters.txt
training,on,ACE04 dataset,hyperparameters,/content/training-data/relation_extraction/0/triples/hyperparameters.txt
hyperparameters,based on,ACE05 development set,hyperparameters,/content/training-data/relation_extraction/0/triples/hyperparameters.txt
Hyperparameters,train,our model,hyperparameters,/content/training-data/relation_extraction/0/triples/hyperparameters.txt
our model,using,Adadelta,hyperparameters,/content/training-data/relation_extraction/0/triples/hyperparameters.txt
Adadelta,with,gradient clipping,hyperparameters,/content/training-data/relation_extraction/0/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/relation_extraction/0/triples/model.txt
Model,encode,output sequence,model,/content/training-data/relation_extraction/0/triples/model.txt
output sequence,from,left - to - right,model,/content/training-data/relation_extraction/0/triples/model.txt
Model,add,additional layer,model,/content/training-data/relation_extraction/0/triples/model.txt
additional layer,to encode,output sequence,model,/content/training-data/relation_extraction/0/triples/model.txt
output sequence,from,right - to - left,model,/content/training-data/relation_extraction/0/triples/model.txt
additional layer,to,our network,model,/content/training-data/relation_extraction/0/triples/model.txt
Model,At,each time step,model,/content/training-data/relation_extraction/0/triples/model.txt
each time step,use,attention - like model,model,/content/training-data/relation_extraction/0/triples/model.txt
attention - like model,to identify,tokens,model,/content/training-data/relation_extraction/0/triples/model.txt
tokens,in,specified relation,model,/content/training-data/relation_extraction/0/triples/model.txt
specified relation,with,current token,model,/content/training-data/relation_extraction/0/triples/model.txt
attention - like model,on,previously decoded time steps,model,/content/training-data/relation_extraction/0/triples/model.txt
Model,does not,depend,model,/content/training-data/relation_extraction/0/triples/model.txt
depend,on,any dependency tree information,model,/content/training-data/relation_extraction/0/triples/model.txt
Model,has,Our RNN - based model,model,/content/training-data/relation_extraction/0/triples/model.txt
Our RNN - based model,is,multi - layer bidirectional LSTM,model,/content/training-data/relation_extraction/0/triples/model.txt
multi - layer bidirectional LSTM,over,sequence,model,/content/training-data/relation_extraction/0/triples/model.txt
Model,propose,novel RNN - based model,model,/content/training-data/relation_extraction/0/triples/model.txt
novel RNN - based model,for,joint extraction of entity mentions and relations,model,/content/training-data/relation_extraction/0/triples/model.txt
Contribution,has research problem,Joint Extraction of Entity Mentions and Relations,research-problem,/content/training-data/relation_extraction/0/triples/research-problem.txt
Contribution,has research problem,Extraction of entities and their relations from text,research-problem,/content/training-data/relation_extraction/0/triples/research-problem.txt
Contribution,has research problem,entity mention and relation extraction at the sentencelevel,research-problem,/content/training-data/relation_extraction/0/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/relation_extraction/0/triples/results.txt
Results,adding,bidirectional encoding,results,/content/training-data/relation_extraction/0/triples/results.txt
bidirectional encoding,find,significantly improve,results,/content/training-data/relation_extraction/0/triples/results.txt
significantly improve,has,performance,results,/content/training-data/relation_extraction/0/triples/results.txt
performance,of,our system,results,/content/training-data/relation_extraction/0/triples/results.txt
performance,compared to,left - to - right encoding,results,/content/training-data/relation_extraction/0/triples/results.txt
bidirectional encoding,to,our system,results,/content/training-data/relation_extraction/0/triples/results.txt
Results,improves,precision,results,/content/training-data/relation_extraction/0/triples/results.txt
precision,compared to,left - toright decoding,results,/content/training-data/relation_extraction/0/triples/results.txt
left - toright decoding,combined with,multiple relations objective,results,/content/training-data/relation_extraction/0/triples/results.txt
Results,find that for,some relations,results,/content/training-data/relation_extraction/0/triples/results.txt
some relations,is,easier,results,/content/training-data/relation_extraction/0/triples/results.txt
easier,to detect,with respect to,results,/content/training-data/relation_extraction/0/triples/results.txt
with respect to,has,one of the entities,results,/content/training-data/relation_extraction/0/triples/results.txt
one of the entities,in,entity pair,results,/content/training-data/relation_extraction/0/triples/results.txt
Results,has,PHYS relation,results,/content/training-data/relation_extraction/0/triples/results.txt
PHYS relation,is,easier identified,results,/content/training-data/relation_extraction/0/triples/results.txt
easier identified,with respect to,GPE entity,results,/content/training-data/relation_extraction/0/triples/results.txt
GPE entity,than,PER entity,results,/content/training-data/relation_extraction/0/triples/results.txt
Results,has,Multiple Relations,results,/content/training-data/relation_extraction/0/triples/results.txt
Multiple Relations,find that,modifying our objective,results,/content/training-data/relation_extraction/0/triples/results.txt
modifying our objective,to include,multiple relations,results,/content/training-data/relation_extraction/0/triples/results.txt
multiple relations,improves,recall,results,/content/training-data/relation_extraction/0/triples/results.txt
recall,leading to,slight improvement,results,/content/training-data/relation_extraction/0/triples/results.txt
slight improvement,on,over all performance on relations,results,/content/training-data/relation_extraction/0/triples/results.txt
recall,of,our system,results,/content/training-data/relation_extraction/0/triples/results.txt
recall,on,relations,results,/content/training-data/relation_extraction/0/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/relation_extraction/2/triples/baselines.txt
Baselines,has,SVM,baselines,/content/training-data/relation_extraction/2/triples/baselines.txt
Baselines,has,RNN,baselines,/content/training-data/relation_extraction/2/triples/baselines.txt
Baselines,has,MVRNN,baselines,/content/training-data/relation_extraction/2/triples/baselines.txt
Baselines,has,CNN + Softmax,baselines,/content/training-data/relation_extraction/2/triples/baselines.txt
Baselines,has,FCM,baselines,/content/training-data/relation_extraction/2/triples/baselines.txt
Baselines,has,CR - CNN,baselines,/content/training-data/relation_extraction/2/triples/baselines.txt
Baselines,has,Attention - CNN,baselines,/content/training-data/relation_extraction/2/triples/baselines.txt
Baselines,has,Entity Attention Bi-LSTM,baselines,/content/training-data/relation_extraction/2/triples/baselines.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/relation_extraction/2/triples/ablation-analysis.txt
Ablation analysis,demonstrates,special separate tokens and the hidden entity vectors,ablation-analysis,/content/training-data/relation_extraction/2/triples/ablation-analysis.txt
special separate tokens and the hidden entity vectors,make,important contributions,ablation-analysis,/content/training-data/relation_extraction/2/triples/ablation-analysis.txt
Ablation analysis,has,BERT,ablation-analysis,/content/training-data/relation_extraction/2/triples/ablation-analysis.txt
BERT,can not locate,target entities,ablation-analysis,/content/training-data/relation_extraction/2/triples/ablation-analysis.txt
BERT,without,special separate tokens,ablation-analysis,/content/training-data/relation_extraction/2/triples/ablation-analysis.txt
Ablation analysis,has,BERT - NO - SEP - NO - ENT,ablation-analysis,/content/training-data/relation_extraction/2/triples/ablation-analysis.txt
BERT - NO - SEP - NO - ENT,performs,worst,ablation-analysis,/content/training-data/relation_extraction/2/triples/ablation-analysis.txt
worst,with,F1 8.16 absolute points,ablation-analysis,/content/training-data/relation_extraction/2/triples/ablation-analysis.txt
F1 8.16 absolute points,worse than,R - BERT,ablation-analysis,/content/training-data/relation_extraction/2/triples/ablation-analysis.txt
Ablation analysis,observe,three methods,ablation-analysis,/content/training-data/relation_extraction/2/triples/ablation-analysis.txt
three methods,perform,worse,ablation-analysis,/content/training-data/relation_extraction/2/triples/ablation-analysis.txt
worse,than,R - BERT,ablation-analysis,/content/training-data/relation_extraction/2/triples/ablation-analysis.txt
Ablation analysis,incorporating,output,ablation-analysis,/content/training-data/relation_extraction/2/triples/ablation-analysis.txt
output,of,target entity vectors,ablation-analysis,/content/training-data/relation_extraction/2/triples/ablation-analysis.txt
target entity vectors,to make,more accurate prediction,ablation-analysis,/content/training-data/relation_extraction/2/triples/ablation-analysis.txt
target entity vectors,further enriches,information,ablation-analysis,/content/training-data/relation_extraction/2/triples/ablation-analysis.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/relation_extraction/2/triples/hyperparameters.txt
Hyperparameters,add,dropout,hyperparameters,/content/training-data/relation_extraction/2/triples/hyperparameters.txt
dropout,before,each add - on layer,hyperparameters,/content/training-data/relation_extraction/2/triples/hyperparameters.txt
Hyperparameters,For,pre-trained BERT model,hyperparameters,/content/training-data/relation_extraction/2/triples/hyperparameters.txt
pre-trained BERT model,use,uncased basic model,hyperparameters,/content/training-data/relation_extraction/2/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/relation_extraction/2/triples/model.txt
Model,locate,positions,model,/content/training-data/relation_extraction/2/triples/model.txt
positions,in,output embedding,model,/content/training-data/relation_extraction/2/triples/model.txt
output embedding,from,BERT model,model,/content/training-data/relation_extraction/2/triples/model.txt
positions,of,two target entities,model,/content/training-data/relation_extraction/2/triples/model.txt
Model,apply,pretrained BERT model,model,/content/training-data/relation_extraction/2/triples/model.txt
pretrained BERT model,for,relation classification,model,/content/training-data/relation_extraction/2/triples/model.txt
Model,insert,special tokens,model,/content/training-data/relation_extraction/2/triples/model.txt
special tokens,before feeding,text,model,/content/training-data/relation_extraction/2/triples/model.txt
text,to,BERT,model,/content/training-data/relation_extraction/2/triples/model.txt
BERT,for,fine - tuning,model,/content/training-data/relation_extraction/2/triples/model.txt
special tokens,before and after,target entities,model,/content/training-data/relation_extraction/2/triples/model.txt
target entities,to identify,locations,model,/content/training-data/relation_extraction/2/triples/model.txt
locations,transfer,information,model,/content/training-data/relation_extraction/2/triples/model.txt
information,into,BERT model,model,/content/training-data/relation_extraction/2/triples/model.txt
locations,of,two target entities,model,/content/training-data/relation_extraction/2/triples/model.txt
Model,as the input to,multi - layer neural network,model,/content/training-data/relation_extraction/2/triples/model.txt
multi - layer neural network,for,classification,model,/content/training-data/relation_extraction/2/triples/model.txt
classification,use,embeddings,model,/content/training-data/relation_extraction/2/triples/model.txt
classification,use,sentence encoding,model,/content/training-data/relation_extraction/2/triples/model.txt
sentence encoding,has,embedding,model,/content/training-data/relation_extraction/2/triples/model.txt
embedding,of,special first token,model,/content/training-data/relation_extraction/2/triples/model.txt
special first token,in the setting of,BERT,model,/content/training-data/relation_extraction/2/triples/model.txt
Contribution,has research problem,Relation Classification,research-problem,/content/training-data/relation_extraction/2/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/relation_extraction/2/triples/results.txt
Results,see that,R - BERT,results,/content/training-data/relation_extraction/2/triples/results.txt
R - BERT,significantly beats,baseline methods,results,/content/training-data/relation_extraction/2/triples/results.txt
Results,has,MACRO F1 value,results,/content/training-data/relation_extraction/2/triples/results.txt
MACRO F1 value,of,R - BERT,results,/content/training-data/relation_extraction/2/triples/results.txt
R - BERT,is,89. 25,results,/content/training-data/relation_extraction/2/triples/results.txt
89. 25,much better than,previous best solution,results,/content/training-data/relation_extraction/2/triples/results.txt
Contribution,has,Model,model,/content/training-data/relation_extraction/13/triples/model.txt
Model,present,method,model,/content/training-data/relation_extraction/13/triples/model.txt
method,of,training,model,/content/training-data/relation_extraction/13/triples/model.txt
training,by matching,blanks,model,/content/training-data/relation_extraction/13/triples/model.txt
training,has,relation representation,model,/content/training-data/relation_extraction/13/triples/model.txt
relation representation,without,any supervision,model,/content/training-data/relation_extraction/13/triples/model.txt
any supervision,from,knowledge graph or human annotators,model,/content/training-data/relation_extraction/13/triples/model.txt
Model,study the ability of,Transformer neural network architecture,model,/content/training-data/relation_extraction/13/triples/model.txt
Transformer neural network architecture,identify,representation,model,/content/training-data/relation_extraction/13/triples/model.txt
representation,that outperforms,previous work,model,/content/training-data/relation_extraction/13/triples/model.txt
previous work,in,supervised relation extraction,model,/content/training-data/relation_extraction/13/triples/model.txt
Transformer neural network architecture,to encode,relations,model,/content/training-data/relation_extraction/13/triples/model.txt
relations,between,entity pairs,model,/content/training-data/relation_extraction/13/triples/model.txt
Contribution,has research problem,Relation Learning,research-problem,/content/training-data/relation_extraction/13/triples/research-problem.txt
Contribution,has research problem,identify and extract relations between entities,research-problem,/content/training-data/relation_extraction/13/triples/research-problem.txt
Contribution,has research problem,relation extraction,research-problem,/content/training-data/relation_extraction/13/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/relation_extraction/13/triples/results.txt
Results,given,access,results,/content/training-data/relation_extraction/13/triples/results.txt
access,to,all of the training data,results,/content/training-data/relation_extraction/13/triples/results.txt
all of the training data,has,BERT EM,results,/content/training-data/relation_extraction/13/triples/results.txt
BERT EM,approaches,BERT EM + MTB 's performance,results,/content/training-data/relation_extraction/13/triples/results.txt
Results,For,BERT EM + MTB,results,/content/training-data/relation_extraction/13/triples/results.txt
BERT EM + MTB,increase,very significant,results,/content/training-data/relation_extraction/13/triples/results.txt
very significant,has,12.7 %,results,/content/training-data/relation_extraction/13/triples/results.txt
12.7 %,on,10 - way - 1 - shot task,results,/content/training-data/relation_extraction/13/triples/results.txt
very significant,has,8.8 %,results,/content/training-data/relation_extraction/13/triples/results.txt
8.8 %,on,5 - way - 1 - shot task,results,/content/training-data/relation_extraction/13/triples/results.txt
BERT EM + MTB,significantly outperforms,BERT EM,results,/content/training-data/relation_extraction/13/triples/results.txt
Results,has,task agnostic BERT EM and BERT EM + MTB models,results,/content/training-data/relation_extraction/13/triples/results.txt
task agnostic BERT EM and BERT EM + MTB models,outperform,previous published state of the art,results,/content/training-data/relation_extraction/13/triples/results.txt
previous published state of the art,on,FewRel task,results,/content/training-data/relation_extraction/13/triples/results.txt
FewRel task,when they have not seen,any FewRel training data,results,/content/training-data/relation_extraction/13/triples/results.txt
Results,has,additional MTB based training,results,/content/training-data/relation_extraction/13/triples/results.txt
additional MTB based training,further increases,F 1 scores,results,/content/training-data/relation_extraction/13/triples/results.txt
F 1 scores,for,all tasks,results,/content/training-data/relation_extraction/13/triples/results.txt
Results,see that,MTB based training,results,/content/training-data/relation_extraction/13/triples/results.txt
MTB based training,even more effective for,low - resource cases,results,/content/training-data/relation_extraction/13/triples/results.txt
Results,training by,matching the blanks,results,/content/training-data/relation_extraction/13/triples/results.txt
matching the blanks,can,significantly reduce,results,/content/training-data/relation_extraction/13/triples/results.txt
significantly reduce,amount of,human input,results,/content/training-data/relation_extraction/13/triples/results.txt
human input,to create,relation extractors,results,/content/training-data/relation_extraction/13/triples/results.txt
human input,populate,knowledge base,results,/content/training-data/relation_extraction/13/triples/results.txt
Results,show that,MTB training,results,/content/training-data/relation_extraction/13/triples/results.txt
MTB training,could be used to,significantly reduce effort,results,/content/training-data/relation_extraction/13/triples/results.txt
significantly reduce effort,in implementing,exemplar based relation extraction system,results,/content/training-data/relation_extraction/13/triples/results.txt
Contribution,has,Approach,approach,/content/training-data/relation_extraction/7/triples/approach.txt
Approach,has,entity - wise attention,approach,/content/training-data/relation_extraction/7/triples/approach.txt
entity - wise attention,to alleviate,influence of noisy words,approach,/content/training-data/relation_extraction/7/triples/approach.txt
influence of noisy words,in,subtree,approach,/content/training-data/relation_extraction/7/triples/approach.txt
entity - wise attention,emphasize,task - relevant features,approach,/content/training-data/relation_extraction/7/triples/approach.txt
Approach,initialize,model parameters,approach,/content/training-data/relation_extraction/7/triples/approach.txt
model parameters,with,a priori knowledge,approach,/content/training-data/relation_extraction/7/triples/approach.txt
a priori knowledge,learned from,entity type classification task,approach,/content/training-data/relation_extraction/7/triples/approach.txt
entity type classification task,by,transfer learning,approach,/content/training-data/relation_extraction/7/triples/approach.txt
Approach,propose,novel word - level approach,approach,/content/training-data/relation_extraction/7/triples/approach.txt
novel word - level approach,improving,robustness,approach,/content/training-data/relation_extraction/7/triples/approach.txt
robustness,against,noisy words,approach,/content/training-data/relation_extraction/7/triples/approach.txt
novel word - level approach,for,distant supervised relation extraction,approach,/content/training-data/relation_extraction/7/triples/approach.txt
novel word - level approach,by reducing,inner-sentence noise,approach,/content/training-data/relation_extraction/7/triples/approach.txt
Approach,To reduce,innersentence noise,approach,/content/training-data/relation_extraction/7/triples/approach.txt
innersentence noise,utilize,novel Sub - Tree Parse ( STP ) method,approach,/content/training-data/relation_extraction/7/triples/approach.txt
novel Sub - Tree Parse ( STP ) method,by intercepting,subtree,approach,/content/training-data/relation_extraction/7/triples/approach.txt
subtree,under,parent of entities ' lowest common ancestor,approach,/content/training-data/relation_extraction/7/triples/approach.txt
novel Sub - Tree Parse ( STP ) method,to remove,irrelevant words,approach,/content/training-data/relation_extraction/7/triples/approach.txt
Contribution,has,Baselines,baselines,/content/training-data/relation_extraction/7/triples/baselines.txt
Baselines,proposes,BGRU,baselines,/content/training-data/relation_extraction/7/triples/baselines.txt
BGRU,with,word - level attention mechanism,baselines,/content/training-data/relation_extraction/7/triples/baselines.txt
Baselines,has,MultiR,baselines,/content/training-data/relation_extraction/7/triples/baselines.txt
MultiR,puts forward,graphical model,baselines,/content/training-data/relation_extraction/7/triples/baselines.txt
Baselines,has,Mintz,baselines,/content/training-data/relation_extraction/7/triples/baselines.txt
Mintz,proposes,humandesigned feature model,baselines,/content/training-data/relation_extraction/7/triples/baselines.txt
Baselines,has,PCNN,baselines,/content/training-data/relation_extraction/7/triples/baselines.txt
PCNN,puts forward,piecewise CNN for relation extraction,baselines,/content/training-data/relation_extraction/7/triples/baselines.txt
Baselines,has,PCNN + ATT,baselines,/content/training-data/relation_extraction/7/triples/baselines.txt
PCNN + ATT,proposes,selective attention mechanism,baselines,/content/training-data/relation_extraction/7/triples/baselines.txt
selective attention mechanism,with,PCNN,baselines,/content/training-data/relation_extraction/7/triples/baselines.txt
Baselines,has,MIML,baselines,/content/training-data/relation_extraction/7/triples/baselines.txt
MIML,proposes,multi -instance multi-label model,baselines,/content/training-data/relation_extraction/7/triples/baselines.txt
Contribution,has research problem,Neural Relation Extraction,research-problem,/content/training-data/relation_extraction/7/triples/research-problem.txt
Contribution,has research problem,Relation extraction,research-problem,/content/training-data/relation_extraction/7/triples/research-problem.txt
Contribution,has research problem,distant supervised relation extraction,research-problem,/content/training-data/relation_extraction/7/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/relation_extraction/7/triples/experimental-setup.txt
Experimental setup,Word embedding dimension k,50,experimental-setup,/content/training-data/relation_extraction/7/triples/experimental-setup.txt
Experimental setup,Entity - Relation Task weight,0.3,experimental-setup,/content/training-data/relation_extraction/7/triples/experimental-setup.txt
Experimental setup,utilize,word2vec,experimental-setup,/content/training-data/relation_extraction/7/triples/experimental-setup.txt
word2vec,to train,word embeddings,experimental-setup,/content/training-data/relation_extraction/7/triples/experimental-setup.txt
word embeddings,on,NYT corpus,experimental-setup,/content/training-data/relation_extraction/7/triples/experimental-setup.txt
Experimental setup,GRU size m,230,experimental-setup,/content/training-data/relation_extraction/7/triples/experimental-setup.txt
Experimental setup,POS embedding dimension l,5,experimental-setup,/content/training-data/relation_extraction/7/triples/experimental-setup.txt
Experimental setup,Batch size n,50,experimental-setup,/content/training-data/relation_extraction/7/triples/experimental-setup.txt
Experimental setup,Learning rate lr,0.001,experimental-setup,/content/training-data/relation_extraction/7/triples/experimental-setup.txt
Experimental setup,Entity - Task weights,0.5,experimental-setup,/content/training-data/relation_extraction/7/triples/experimental-setup.txt
Experimental setup,has,grid search approach,experimental-setup,/content/training-data/relation_extraction/7/triples/experimental-setup.txt
grid search approach,to select,position embedding size l ?,experimental-setup,/content/training-data/relation_extraction/7/triples/experimental-setup.txt
position embedding size l ?,has,"5 , 10 , 15 , 20",experimental-setup,/content/training-data/relation_extraction/7/triples/experimental-setup.txt
grid search approach,to select,GRU size m ?,experimental-setup,/content/training-data/relation_extraction/7/triples/experimental-setup.txt
GRU size m ?,has,"100 , 160 , 230 , 400",experimental-setup,/content/training-data/relation_extraction/7/triples/experimental-setup.txt
grid search approach,to select,optimal learning rate lr,experimental-setup,/content/training-data/relation_extraction/7/triples/experimental-setup.txt
optimal learning rate lr,among,"0.1 , 0.001 , 0.0005 , 0.0001",experimental-setup,/content/training-data/relation_extraction/7/triples/experimental-setup.txt
optimal learning rate lr,for,Adam optimizer,experimental-setup,/content/training-data/relation_extraction/7/triples/experimental-setup.txt
Experimental setup,l 2 penalty,0.0001,experimental-setup,/content/training-data/relation_extraction/7/triples/experimental-setup.txt
Experimental setup,Dropout probability p,0.5,experimental-setup,/content/training-data/relation_extraction/7/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/relation_extraction/7/triples/results.txt
Results,has,PR curve area,results,/content/training-data/relation_extraction/7/triples/results.txt
PR curve area,has,relative improvement,results,/content/training-data/relation_extraction/7/triples/results.txt
relative improvement,of,over 2.3 %,results,/content/training-data/relation_extraction/7/triples/results.txt
Results,has,BGRU + STP + TL,results,/content/training-data/relation_extraction/7/triples/results.txt
BGRU + STP + TL,increases,area,results,/content/training-data/relation_extraction/7/triples/results.txt
area,to,0.383,results,/content/training-data/relation_extraction/7/triples/results.txt
BGRU + STP + TL,achieves,best performance,results,/content/training-data/relation_extraction/7/triples/results.txt
Results,has,Our STP,results,/content/training-data/relation_extraction/7/triples/results.txt
Our STP,obtain,more precise sentence representation,results,/content/training-data/relation_extraction/7/triples/results.txt
Our STP,get rid of,irrelevant words,results,/content/training-data/relation_extraction/7/triples/results.txt
irrelevant words,in,each instance,results,/content/training-data/relation_extraction/7/triples/results.txt
Results,has,BGRU - WLA ( + STP ) + EWA,results,/content/training-data/relation_extraction/7/triples/results.txt
BGRU - WLA ( + STP ) + EWA,outperforms,BGRU (+ STP ,results,/content/training-data/relation_extraction/7/triples/results.txt
Results,has,SDP model,results,/content/training-data/relation_extraction/7/triples/results.txt
SDP model,obtains,even worse result,results,/content/training-data/relation_extraction/7/triples/results.txt
even worse result,than,pure one,results,/content/training-data/relation_extraction/7/triples/results.txt
Results,has,PR curve areas,results,/content/training-data/relation_extraction/7/triples/results.txt
PR curve areas,of,BGRU + SDP and BGRU,results,/content/training-data/relation_extraction/7/triples/results.txt
BGRU + SDP and BGRU,are about,0.332 and 0.337,results,/content/training-data/relation_extraction/7/triples/results.txt
PR curve areas,of,BGRU + STP,results,/content/training-data/relation_extraction/7/triples/results.txt
BGRU + STP,increases it to,0.366,results,/content/training-data/relation_extraction/7/triples/results.txt
Results,has,SDP method,results,/content/training-data/relation_extraction/7/triples/results.txt
SDP method,not appropriate to handle,low - quality sentences,results,/content/training-data/relation_extraction/7/triples/results.txt
low - quality sentences,where,key relation words,results,/content/training-data/relation_extraction/7/triples/results.txt
key relation words,not in,SDP,results,/content/training-data/relation_extraction/7/triples/results.txt
Results,has,models with TL,results,/content/training-data/relation_extraction/7/triples/results.txt
models with TL,achieve,better performance,results,/content/training-data/relation_extraction/7/triples/results.txt
models with TL,improve,PR curve area,results,/content/training-data/relation_extraction/7/triples/results.txt
PR curve area,by,over 4.7 %,results,/content/training-data/relation_extraction/7/triples/results.txt
Results,has,EWA,results,/content/training-data/relation_extraction/7/triples/results.txt
EWA,achieves,further improvements,results,/content/training-data/relation_extraction/7/triples/results.txt
EWA,outperforms,baseline,results,/content/training-data/relation_extraction/7/triples/results.txt
baseline,by over,4.6 %,results,/content/training-data/relation_extraction/7/triples/results.txt
Results,observe,model,results,/content/training-data/relation_extraction/7/triples/results.txt
model,with,STP,results,/content/training-data/relation_extraction/7/triples/results.txt
model,performs,best,results,/content/training-data/relation_extraction/7/triples/results.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/relation_extraction/1/triples/hyperparameters.txt
Hyperparameters,has,BERT base - cased and large - cased models,hyperparameters,/content/training-data/relation_extraction/1/triples/hyperparameters.txt
BERT base - cased and large - cased models,used in,our experiments,hyperparameters,/content/training-data/relation_extraction/1/triples/hyperparameters.txt
Hyperparameters,has,learning rate,hyperparameters,/content/training-data/relation_extraction/1/triples/hyperparameters.txt
learning rate,has,5 10 ?5,hyperparameters,/content/training-data/relation_extraction/1/triples/hyperparameters.txt
Hyperparameters,has,position embeddings,hyperparameters,/content/training-data/relation_extraction/1/triples/hyperparameters.txt
position embeddings,are,randomly initialized and fine - tuned,hyperparameters,/content/training-data/relation_extraction/1/triples/hyperparameters.txt
randomly initialized and fine - tuned,during,training process,hyperparameters,/content/training-data/relation_extraction/1/triples/hyperparameters.txt
Hyperparameters,has,predicate indicator embedding size,hyperparameters,/content/training-data/relation_extraction/1/triples/hyperparameters.txt
predicate indicator embedding size,is,10,hyperparameters,/content/training-data/relation_extraction/1/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/relation_extraction/1/triples/model.txt
Model,show,simple neural architectures,model,/content/training-data/relation_extraction/1/triples/model.txt
simple neural architectures,built on top of,BERT,model,/content/training-data/relation_extraction/1/triples/model.txt
simple neural architectures,yields,state - of - the - art performance,model,/content/training-data/relation_extraction/1/triples/model.txt
state - of - the - art performance,on,variety of benchmark datasets,model,/content/training-data/relation_extraction/1/triples/model.txt
Contribution,has research problem,Relation Extraction,research-problem,/content/training-data/relation_extraction/1/triples/research-problem.txt
Contribution,has research problem,Semantic Role Labeling,research-problem,/content/training-data/relation_extraction/1/triples/research-problem.txt
Contribution,has research problem,semantic role labeling ( SRL ,research-problem,/content/training-data/relation_extraction/1/triples/research-problem.txt
Contribution,has research problem,SRL,research-problem,/content/training-data/relation_extraction/1/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/relation_extraction/1/triples/results.txt
Results,see,BERT - LSTM - large model,results,/content/training-data/relation_extraction/1/triples/results.txt
BERT - LSTM - large model,achieves,state - of - the - art F 1 score,results,/content/training-data/relation_extraction/1/triples/results.txt
state - of - the - art F 1 score,among,single models,results,/content/training-data/relation_extraction/1/triples/results.txt
BERT - LSTM - large model,outperforms,ensemble model,results,/content/training-data/relation_extraction/1/triples/results.txt
ensemble model,has,in - domain and out - of - domain tests,results,/content/training-data/relation_extraction/1/triples/results.txt
ensemble model,on,CoNLL 2005,results,/content/training-data/relation_extraction/1/triples/results.txt
Results,falls short on,CoNLL 2012 benchmark,results,/content/training-data/relation_extraction/1/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/relation_extraction/9/triples/ablation-analysis.txt
Ablation analysis,observe that,all three of our components,ablation-analysis,/content/training-data/relation_extraction/9/triples/ablation-analysis.txt
all three of our components,lead to,noticeable improvements,ablation-analysis,/content/training-data/relation_extraction/9/triples/ablation-analysis.txt
noticeable improvements,over,baselines,ablation-analysis,/content/training-data/relation_extraction/9/triples/ablation-analysis.txt
Ablation analysis,evaluating,several simplified models,ablation-analysis,/content/training-data/relation_extraction/9/triples/ablation-analysis.txt
several simplified models,has,first simplification,ablation-analysis,/content/training-data/relation_extraction/9/triples/ablation-analysis.txt
first simplification,to use,model,ablation-analysis,/content/training-data/relation_extraction/9/triples/ablation-analysis.txt
model,with,pooling attention layer,ablation-analysis,/content/training-data/relation_extraction/9/triples/ablation-analysis.txt
model,without,input attention mechanism,ablation-analysis,/content/training-data/relation_extraction/9/triples/ablation-analysis.txt
several simplified models,has,third,ablation-analysis,/content/training-data/relation_extraction/9/triples/ablation-analysis.txt
third,uses,regular objective function,ablation-analysis,/content/training-data/relation_extraction/9/triples/ablation-analysis.txt
regular objective function,based on,inner product s = r w,ablation-analysis,/content/training-data/relation_extraction/9/triples/ablation-analysis.txt
inner product s = r w,for,sentence representation r,ablation-analysis,/content/training-data/relation_extraction/9/triples/ablation-analysis.txt
inner product s = r w,for,relation class embedding w,ablation-analysis,/content/training-data/relation_extraction/9/triples/ablation-analysis.txt
third,removes,both forms of attention,ablation-analysis,/content/training-data/relation_extraction/9/triples/ablation-analysis.txt
several simplified models,has,second,ablation-analysis,/content/training-data/relation_extraction/9/triples/ablation-analysis.txt
second,removes,both attention mechanisms,ablation-analysis,/content/training-data/relation_extraction/9/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/relation_extraction/9/triples/model.txt
Model,introduce,novel pair - wise margin - based objective function,model,/content/training-data/relation_extraction/9/triples/model.txt
novel pair - wise margin - based objective function,proves,superior,model,/content/training-data/relation_extraction/9/triples/model.txt
superior,to,standard loss functions,model,/content/training-data/relation_extraction/9/triples/model.txt
Model,has,CNN architecture,model,/content/training-data/relation_extraction/9/triples/model.txt
CNN architecture,relies on,novel multi-level attention mechanism,model,/content/training-data/relation_extraction/9/triples/model.txt
novel multi-level attention mechanism,to capture,entity - specific attention,model,/content/training-data/relation_extraction/9/triples/model.txt
entity - specific attention,has,primary attention,model,/content/training-data/relation_extraction/9/triples/model.txt
primary attention,at,input level,model,/content/training-data/relation_extraction/9/triples/model.txt
input level,with respect to,target entities,model,/content/training-data/relation_extraction/9/triples/model.txt
novel multi-level attention mechanism,to capture,relation - specific pooling attention,model,/content/training-data/relation_extraction/9/triples/model.txt
relation - specific pooling attention,has,secondary attention,model,/content/training-data/relation_extraction/9/triples/model.txt
secondary attention,with respect to,target relations,model,/content/training-data/relation_extraction/9/triples/model.txt
Model,propose,novel CNN architecture,model,/content/training-data/relation_extraction/9/triples/model.txt
Contribution,has research problem,Relation Classification,research-problem,/content/training-data/relation_extraction/9/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/relation_extraction/9/triples/experimental-setup.txt
Experimental setup,apply,cross-validation procedure,experimental-setup,/content/training-data/relation_extraction/9/triples/experimental-setup.txt
cross-validation procedure,to select,suitable hyperparameters,experimental-setup,/content/training-data/relation_extraction/9/triples/experimental-setup.txt
cross-validation procedure,on,training data,experimental-setup,/content/training-data/relation_extraction/9/triples/experimental-setup.txt
Experimental setup,use,word2 vec skip - gram model,experimental-setup,/content/training-data/relation_extraction/9/triples/experimental-setup.txt
word2 vec skip - gram model,to learn,initial word representations,experimental-setup,/content/training-data/relation_extraction/9/triples/experimental-setup.txt
initial word representations,on,Wikipedia,experimental-setup,/content/training-data/relation_extraction/9/triples/experimental-setup.txt
Experimental setup,has,Other matrices,experimental-setup,/content/training-data/relation_extraction/9/triples/experimental-setup.txt
Other matrices,initialized with,random values,experimental-setup,/content/training-data/relation_extraction/9/triples/experimental-setup.txt
random values,following,Gaussian distribution,experimental-setup,/content/training-data/relation_extraction/9/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/relation_extraction/9/triples/results.txt
Results,observe that,novel attentionbased architecture,results,/content/training-data/relation_extraction/9/triples/results.txt
novel attentionbased architecture,achieves,new state - of - the - art results,results,/content/training-data/relation_extraction/9/triples/results.txt
Results,has,Att - Input - CNN,results,/content/training-data/relation_extraction/9/triples/results.txt
Att - Input - CNN,relies only on,primal attention,results,/content/training-data/relation_extraction/9/triples/results.txt
primal attention,at,input level,results,/content/training-data/relation_extraction/9/triples/results.txt
Att - Input - CNN,performing,standard max - pooling,results,/content/training-data/relation_extraction/9/triples/results.txt
standard max - pooling,after,convolution layer,results,/content/training-data/relation_extraction/9/triples/results.txt
convolution layer,to generate,network output w O,results,/content/training-data/relation_extraction/9/triples/results.txt
Results,has,Our full dual attention model Att - Pooling - CNN,results,/content/training-data/relation_extraction/9/triples/results.txt
Our full dual attention model Att - Pooling - CNN,achieves,even more favorable F1- score,results,/content/training-data/relation_extraction/9/triples/results.txt
even more favorable F1- score,of,88 %,results,/content/training-data/relation_extraction/9/triples/results.txt
Results,With,Att - Input - CNN,results,/content/training-data/relation_extraction/9/triples/results.txt
Att - Input - CNN,achieve,F1-score,results,/content/training-data/relation_extraction/9/triples/results.txt
F1-score,of,87.5 %,results,/content/training-data/relation_extraction/9/triples/results.txt
87.5 %,outperforming,an SVM - based approach ( 82.2 % ,results,/content/training-data/relation_extraction/9/triples/results.txt
87.5 %,outperforming,wellknown CR - CNN model ( 84.1 % ,results,/content/training-data/relation_extraction/9/triples/results.txt
87.5 %,outperforming,newly released DRNNs ( 85.8 % ,results,/content/training-data/relation_extraction/9/triples/results.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/relation_extraction/6/triples/hyperparameters.txt
Hyperparameters,apply,Dropout,hyperparameters,/content/training-data/relation_extraction/6/triples/hyperparameters.txt
Dropout,on,penultimate layer,hyperparameters,/content/training-data/relation_extraction/6/triples/hyperparameters.txt
penultimate layer,as well as on,embeddings layer,hyperparameters,/content/training-data/relation_extraction/6/triples/hyperparameters.txt
embeddings layer,with,probability,hyperparameters,/content/training-data/relation_extraction/6/triples/hyperparameters.txt
probability,of,0.5,hyperparameters,/content/training-data/relation_extraction/6/triples/hyperparameters.txt
Hyperparameters,use,early stopping criterion,hyperparameters,/content/training-data/relation_extraction/6/triples/hyperparameters.txt
early stopping criterion,on,validation data,hyperparameters,/content/training-data/relation_extraction/6/triples/hyperparameters.txt
validation data,to determine,number of training epochs,hyperparameters,/content/training-data/relation_extraction/6/triples/hyperparameters.txt
Hyperparameters,choose,size,hyperparameters,/content/training-data/relation_extraction/6/triples/hyperparameters.txt
size,with,random search,hyperparameters,/content/training-data/relation_extraction/6/triples/hyperparameters.txt
random search,on,validation set,hyperparameters,/content/training-data/relation_extraction/6/triples/hyperparameters.txt
size,of,layers ( RNN layer size o = 256 ,hyperparameters,/content/training-data/relation_extraction/6/triples/hyperparameters.txt
Hyperparameters,trained using,Adam optimizer,hyperparameters,/content/training-data/relation_extraction/6/triples/hyperparameters.txt
Adam optimizer,with,categorical crossentropy,hyperparameters,/content/training-data/relation_extraction/6/triples/hyperparameters.txt
categorical crossentropy,as,loss function,hyperparameters,/content/training-data/relation_extraction/6/triples/hyperparameters.txt
Hyperparameters,has,learning rate,hyperparameters,/content/training-data/relation_extraction/6/triples/hyperparameters.txt
learning rate,fixed to,0.01,hyperparameters,/content/training-data/relation_extraction/6/triples/hyperparameters.txt
Hyperparameters,has,training,hyperparameters,/content/training-data/relation_extraction/6/triples/hyperparameters.txt
training,performed in,batches,hyperparameters,/content/training-data/relation_extraction/6/triples/hyperparameters.txt
batches,of,128 instances,hyperparameters,/content/training-data/relation_extraction/6/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/relation_extraction/6/triples/model.txt
Model,are,combined,model,/content/training-data/relation_extraction/6/triples/model.txt
combined,representations of,context relations,model,/content/training-data/relation_extraction/6/triples/model.txt
combined,to make,final prediction,model,/content/training-data/relation_extraction/6/triples/model.txt
combined,representation of,target relation,model,/content/training-data/relation_extraction/6/triples/model.txt
Model,has,architecture,model,/content/training-data/relation_extraction/6/triples/model.txt
architecture,uses,LSTM - based encoder,model,/content/training-data/relation_extraction/6/triples/model.txt
LSTM - based encoder,to jointly learn,representations,model,/content/training-data/relation_extraction/6/triples/model.txt
representations,for,all relations,model,/content/training-data/relation_extraction/6/triples/model.txt
all relations,in,single sentence,model,/content/training-data/relation_extraction/6/triples/model.txt
Model,present,novel architecture,model,/content/training-data/relation_extraction/6/triples/model.txt
novel architecture,considers,other relations,model,/content/training-data/relation_extraction/6/triples/model.txt
other relations,as,context,model,/content/training-data/relation_extraction/6/triples/model.txt
other relations,in,sentence,model,/content/training-data/relation_extraction/6/triples/model.txt
other relations,for predicting,label,model,/content/training-data/relation_extraction/6/triples/model.txt
label,of,target relation,model,/content/training-data/relation_extraction/6/triples/model.txt
Contribution,has research problem,Knowledge Base Relation Extraction,research-problem,/content/training-data/relation_extraction/6/triples/research-problem.txt
Contribution,has research problem,sentence - level relation extraction,research-problem,/content/training-data/relation_extraction/6/triples/research-problem.txt
Contribution,has research problem,relation extraction,research-problem,/content/training-data/relation_extraction/6/triples/research-problem.txt
Contribution,has research problem,sentential relation extraction,research-problem,/content/training-data/relation_extraction/6/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/relation_extraction/6/triples/results.txt
Results,using,attention,results,/content/training-data/relation_extraction/6/triples/results.txt
attention,crucial to extract,relevant information,results,/content/training-data/relation_extraction/6/triples/results.txt
relevant information,from,context relations,results,/content/training-data/relation_extraction/6/triples/results.txt
Results,shows,ContextAtt model,results,/content/training-data/relation_extraction/6/triples/results.txt
ContextAtt model,over,all relation types,results,/content/training-data/relation_extraction/6/triples/results.txt
ContextAtt model,performs,best,results,/content/training-data/relation_extraction/6/triples/results.txt
Results,Compared to,competitive LSTM - baseline,results,/content/training-data/relation_extraction/6/triples/results.txt
competitive LSTM - baseline,has,ContextAtt model,results,/content/training-data/relation_extraction/6/triples/results.txt
ContextAtt model,achieves,24 % reduction,results,/content/training-data/relation_extraction/6/triples/results.txt
24 % reduction,of,average error,results,/content/training-data/relation_extraction/6/triples/results.txt
Results,has,models,results,/content/training-data/relation_extraction/6/triples/results.txt
models,that take,context,results,/content/training-data/relation_extraction/6/triples/results.txt
context,into,account,results,/content/training-data/relation_extraction/6/triples/results.txt
models,perform,similar,results,/content/training-data/relation_extraction/6/triples/results.txt
similar,to,baselines,results,/content/training-data/relation_extraction/6/triples/results.txt
baselines,at,smallest recall numbers,results,/content/training-data/relation_extraction/6/triples/results.txt
models,perform,start,results,/content/training-data/relation_extraction/6/triples/results.txt
start,to,positively deviate,results,/content/training-data/relation_extraction/6/triples/results.txt
positively deviate,at,higher recall rates,results,/content/training-data/relation_extraction/6/triples/results.txt
Results,has,ContextAtt model,results,/content/training-data/relation_extraction/6/triples/results.txt
ContextAtt model,performs,better,results,/content/training-data/relation_extraction/6/triples/results.txt
better,over,entire recall range,results,/content/training-data/relation_extraction/6/triples/results.txt
better,than,any other system,results,/content/training-data/relation_extraction/6/triples/results.txt
Results,see that,ContextSum,results,/content/training-data/relation_extraction/6/triples/results.txt
ContextSum,does n't universally outperforms,LSTM - baseline,results,/content/training-data/relation_extraction/6/triples/results.txt
Results,observe,context - enabled model,results,/content/training-data/relation_extraction/6/triples/results.txt
context - enabled model,demonstrates,most improvement,results,/content/training-data/relation_extraction/6/triples/results.txt
most improvement,on,precision,results,/content/training-data/relation_extraction/6/triples/results.txt
context - enabled model,useful for,taxonomy relations,results,/content/training-data/relation_extraction/6/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/paraphrase_generation/0/triples/baselines.txt
Baselines,start with,baseline model,baselines,/content/training-data/paraphrase_generation/0/triples/baselines.txt
baseline model,take as,simple encoder and decoder network,baselines,/content/training-data/paraphrase_generation/0/triples/baselines.txt
simple encoder and decoder network,with,only the local loss ( ED - Local ,baselines,/content/training-data/paraphrase_generation/0/triples/baselines.txt
Baselines,has,variation of our model,baselines,/content/training-data/paraphrase_generation/0/triples/baselines.txt
variation of our model,used,both the global and local loss ( EDD - LG ,baselines,/content/training-data/paraphrase_generation/0/triples/baselines.txt
Baselines,experimented with,encoder - decoder and a discriminator network,baselines,/content/training-data/paraphrase_generation/0/triples/baselines.txt
encoder - decoder and a discriminator network,with,only global loss ( EDD - Global ,baselines,/content/training-data/paraphrase_generation/0/triples/baselines.txt
only global loss ( EDD - Global ),to distinguish,ground truth paraphrase,baselines,/content/training-data/paraphrase_generation/0/triples/baselines.txt
ground truth paraphrase,with,predicted one,baselines,/content/training-data/paraphrase_generation/0/triples/baselines.txt
Baselines,make,discriminator,baselines,/content/training-data/paraphrase_generation/0/triples/baselines.txt
discriminator,share,weights,baselines,/content/training-data/paraphrase_generation/0/triples/baselines.txt
weights,with,encoder,baselines,/content/training-data/paraphrase_generation/0/triples/baselines.txt
discriminator,train,network,baselines,/content/training-data/paraphrase_generation/0/triples/baselines.txt
network,with,both the losses ( EDD - LG ( shared ) ,baselines,/content/training-data/paraphrase_generation/0/triples/baselines.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/paraphrase_generation/0/triples/ablation-analysis.txt
Ablation analysis,has,proposed EDD - LG ( shared ) method,ablation-analysis,/content/training-data/paraphrase_generation/0/triples/ablation-analysis.txt
proposed EDD - LG ( shared ) method,works,way better,ablation-analysis,/content/training-data/paraphrase_generation/0/triples/ablation-analysis.txt
way better,achieving,improvement,ablation-analysis,/content/training-data/paraphrase_generation/0/triples/ablation-analysis.txt
improvement,of,10 % and 7 %,ablation-analysis,/content/training-data/paraphrase_generation/0/triples/ablation-analysis.txt
10 % and 7 %,in,scores,ablation-analysis,/content/training-data/paraphrase_generation/0/triples/ablation-analysis.txt
10 % and 7 %,for,100 K dataset,ablation-analysis,/content/training-data/paraphrase_generation/0/triples/ablation-analysis.txt
improvement,of,8 % and 6 %,ablation-analysis,/content/training-data/paraphrase_generation/0/triples/ablation-analysis.txt
8 % and 6 %,in,scores,ablation-analysis,/content/training-data/paraphrase_generation/0/triples/ablation-analysis.txt
8 % and 6 %,for,50 K dataset,ablation-analysis,/content/training-data/paraphrase_generation/0/triples/ablation-analysis.txt
way better,than,other variants,ablation-analysis,/content/training-data/paraphrase_generation/0/triples/ablation-analysis.txt
way better,in terms of,BLEU and METEOR metrics,ablation-analysis,/content/training-data/paraphrase_generation/0/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/paraphrase_generation/0/triples/model.txt
Model,ensures,' local ' loss,model,/content/training-data/paraphrase_generation/0/triples/model.txt
' local ' loss,incurred for,each recurrent unit cell,model,/content/training-data/paraphrase_generation/0/triples/model.txt
Model,consists of,sequential encoder - decoder,model,/content/training-data/paraphrase_generation/0/triples/model.txt
sequential encoder - decoder,is,further trained,model,/content/training-data/paraphrase_generation/0/triples/model.txt
further trained,using,pairwise discriminator,model,/content/training-data/paraphrase_generation/0/triples/model.txt
Model,provides,' global ' loss,model,/content/training-data/paraphrase_generation/0/triples/model.txt
' global ' loss,ensures,sentence embedding,model,/content/training-data/paraphrase_generation/0/triples/model.txt
sentence embedding,close to,other semantically related sentence embeddings,model,/content/training-data/paraphrase_generation/0/triples/model.txt
Model,To ensure,whole sentence,model,/content/training-data/paraphrase_generation/0/triples/model.txt
whole sentence,is,correctly encoded,model,/content/training-data/paraphrase_generation/0/triples/model.txt
correctly encoded,further ensure that,far,model,/content/training-data/paraphrase_generation/0/triples/model.txt
far,from,other ( sentences in the corpus ) embeddings,model,/content/training-data/paraphrase_generation/0/triples/model.txt
correctly encoded,further ensure that,close,model,/content/training-data/paraphrase_generation/0/triples/model.txt
close,to,desired ground - truth embeddings,model,/content/training-data/paraphrase_generation/0/triples/model.txt
correctly encoded,make further use of,pair - wise discriminator,model,/content/training-data/paraphrase_generation/0/triples/model.txt
pair - wise discriminator,that encodes,whole sentence,model,/content/training-data/paraphrase_generation/0/triples/model.txt
pair - wise discriminator,obtains,embedding,model,/content/training-data/paraphrase_generation/0/triples/model.txt
Model,has,encoder - decoder architecture,model,/content/training-data/paraphrase_generation/0/triples/model.txt
encoder - decoder architecture,widely used for,machine translation,model,/content/training-data/paraphrase_generation/0/triples/model.txt
encoder - decoder architecture,widely used for,machine comprehension,model,/content/training-data/paraphrase_generation/0/triples/model.txt
Contribution,has research problem,Learning Semantic Sentence Embeddings,research-problem,/content/training-data/paraphrase_generation/0/triples/research-problem.txt
Contribution,has research problem,obtaining sentence - level embeddings,research-problem,/content/training-data/paraphrase_generation/0/triples/research-problem.txt
Contribution,has,Baselines,baselines,/content/training-data/paraphrase_generation/1/triples/baselines.txt
Baselines,compare,standard VAE model,baselines,/content/training-data/paraphrase_generation/1/triples/baselines.txt
standard VAE model,i.e.,unsupervised version,baselines,/content/training-data/paraphrase_generation/1/triples/baselines.txt
Baselines,compare,""" supervised "" variant VAE - S",baselines,/content/training-data/paraphrase_generation/1/triples/baselines.txt
""" supervised "" variant VAE - S",of,unsupervised model,baselines,/content/training-data/paraphrase_generation/1/triples/baselines.txt
Baselines,has,Residual LSTM,baselines,/content/training-data/paraphrase_generation/1/triples/baselines.txt
Residual LSTM,is,current state - of - the - art,baselines,/content/training-data/paraphrase_generation/1/triples/baselines.txt
current state - of - the - art,on,MSCOCO dataset,baselines,/content/training-data/paraphrase_generation/1/triples/baselines.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/paraphrase_generation/1/triples/hyperparameters.txt
Hyperparameters,trained for,predefined number of iterations,hyperparameters,/content/training-data/paraphrase_generation/1/triples/hyperparameters.txt
predefined number of iterations,rather than,fixed number of epochs,hyperparameters,/content/training-data/paraphrase_generation/1/triples/hyperparameters.txt
Hyperparameters,has,Number of units,hyperparameters,/content/training-data/paraphrase_generation/1/triples/hyperparameters.txt
Number of units,in,LSTM,hyperparameters,/content/training-data/paraphrase_generation/1/triples/hyperparameters.txt
LSTM,set to be,maximum length,hyperparameters,/content/training-data/paraphrase_generation/1/triples/hyperparameters.txt
maximum length,of,sequence,hyperparameters,/content/training-data/paraphrase_generation/1/triples/hyperparameters.txt
sequence,in,training data,hyperparameters,/content/training-data/paraphrase_generation/1/triples/hyperparameters.txt
Hyperparameters,has,Batch size,hyperparameters,/content/training-data/paraphrase_generation/1/triples/hyperparameters.txt
Batch size,kept at,32,hyperparameters,/content/training-data/paraphrase_generation/1/triples/hyperparameters.txt
Hyperparameters,has,number of layers,hyperparameters,/content/training-data/paraphrase_generation/1/triples/hyperparameters.txt
number of layers,in,decoder,hyperparameters,/content/training-data/paraphrase_generation/1/triples/hyperparameters.txt
decoder,has,2,hyperparameters,/content/training-data/paraphrase_generation/1/triples/hyperparameters.txt
number of layers,in,encoder,hyperparameters,/content/training-data/paraphrase_generation/1/triples/hyperparameters.txt
encoder,is,1,hyperparameters,/content/training-data/paraphrase_generation/1/triples/hyperparameters.txt
Hyperparameters,has,dimension,hyperparameters,/content/training-data/paraphrase_generation/1/triples/hyperparameters.txt
dimension,of,embedding vector,hyperparameters,/content/training-data/paraphrase_generation/1/triples/hyperparameters.txt
embedding vector,set to,300,hyperparameters,/content/training-data/paraphrase_generation/1/triples/hyperparameters.txt
dimension,of,both encoder and decoder,hyperparameters,/content/training-data/paraphrase_generation/1/triples/hyperparameters.txt
both encoder and decoder,is,600,hyperparameters,/content/training-data/paraphrase_generation/1/triples/hyperparameters.txt
Hyperparameters,has,latent space dimension,hyperparameters,/content/training-data/paraphrase_generation/1/triples/hyperparameters.txt
latent space dimension,is,1100,hyperparameters,/content/training-data/paraphrase_generation/1/triples/hyperparameters.txt
Hyperparameters,trained with,stochastic gradient descent,hyperparameters,/content/training-data/paraphrase_generation/1/triples/hyperparameters.txt
stochastic gradient descent,with,learning rate,hyperparameters,/content/training-data/paraphrase_generation/1/triples/hyperparameters.txt
learning rate,fixed at,value of 5 10 ? 5,hyperparameters,/content/training-data/paraphrase_generation/1/triples/hyperparameters.txt
stochastic gradient descent,with,dropout rate,hyperparameters,/content/training-data/paraphrase_generation/1/triples/hyperparameters.txt
dropout rate,of,30 %,hyperparameters,/content/training-data/paraphrase_generation/1/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/paraphrase_generation/1/triples/model.txt
Model,has,framework,model,/content/training-data/paraphrase_generation/1/triples/model.txt
framework,combines,power,model,/content/training-data/paraphrase_generation/1/triples/model.txt
power,of,sequenceto - sequence models,model,/content/training-data/paraphrase_generation/1/triples/model.txt
sequenceto - sequence models,specifically,long short - term memory ( LSTM ,model,/content/training-data/paraphrase_generation/1/triples/model.txt
power,of,deep generative models,model,/content/training-data/paraphrase_generation/1/triples/model.txt
deep generative models,specifically,variational autoencoder ( VAE ,model,/content/training-data/paraphrase_generation/1/triples/model.txt
power,to develop,"novel , end - to - end deep learning architecture",model,/content/training-data/paraphrase_generation/1/triples/model.txt
"novel , end - to - end deep learning architecture",for,task,model,/content/training-data/paraphrase_generation/1/triples/model.txt
task,of,paraphrase generation,model,/content/training-data/paraphrase_generation/1/triples/model.txt
Model,has,our method,model,/content/training-data/paraphrase_generation/1/triples/model.txt
our method,conditions,both the sides,model,/content/training-data/paraphrase_generation/1/triples/model.txt
both the sides,of,VAE,model,/content/training-data/paraphrase_generation/1/triples/model.txt
VAE,on,intermediate representation,model,/content/training-data/paraphrase_generation/1/triples/model.txt
intermediate representation,of,input question,model,/content/training-data/paraphrase_generation/1/triples/model.txt
input question,obtained through,LSTM,model,/content/training-data/paraphrase_generation/1/triples/model.txt
both the sides,i.e.,encoder and decoder,model,/content/training-data/paraphrase_generation/1/triples/model.txt
Model,has,deep generative model,model,/content/training-data/paraphrase_generation/1/triples/model.txt
deep generative model,enjoys,"simple , modular architecture",model,/content/training-data/paraphrase_generation/1/triples/model.txt
deep generative model,can generate,single,model,/content/training-data/paraphrase_generation/1/triples/model.txt
deep generative model,can generate,"multiple , semantically sensible , paraphrases",model,/content/training-data/paraphrase_generation/1/triples/model.txt
Model,has,proposed method,model,/content/training-data/paraphrase_generation/1/triples/model.txt
proposed method,where,all variations,model,/content/training-data/paraphrase_generation/1/triples/model.txt
all variations,of,relatively better quality,model,/content/training-data/paraphrase_generation/1/triples/model.txt
relatively better quality,generated based on,different z,model,/content/training-data/paraphrase_generation/1/triples/model.txt
different z,sampled from,latent space,model,/content/training-data/paraphrase_generation/1/triples/model.txt
relatively better quality,are,top beam - search result,model,/content/training-data/paraphrase_generation/1/triples/model.txt
Model,present,mechanism,model,/content/training-data/paraphrase_generation/1/triples/model.txt
mechanism,to condition,our VAE model,model,/content/training-data/paraphrase_generation/1/triples/model.txt
our VAE model,to generate,paraphrases,model,/content/training-data/paraphrase_generation/1/triples/model.txt
our VAE model,on,original sentence,model,/content/training-data/paraphrase_generation/1/triples/model.txt
Model,present,deep generative framework,model,/content/training-data/paraphrase_generation/1/triples/model.txt
deep generative framework,for automatically generating,paraphrases,model,/content/training-data/paraphrase_generation/1/triples/model.txt
paraphrases,given,sentence,model,/content/training-data/paraphrase_generation/1/triples/model.txt
Contribution,has research problem,Paraphrase Generation,research-problem,/content/training-data/paraphrase_generation/1/triples/research-problem.txt
Contribution,has research problem,generating paraphrases automatically,research-problem,/content/training-data/paraphrase_generation/1/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/paraphrase_generation/1/triples/results.txt
Results,for,MSCOCO dataset,results,/content/training-data/paraphrase_generation/1/triples/results.txt
MSCOCO dataset,have,significant improvement,results,/content/training-data/paraphrase_generation/1/triples/results.txt
significant improvement,w.r.t.,baselines,results,/content/training-data/paraphrase_generation/1/triples/results.txt
MSCOCO dataset,For,BLEU and METEOR,results,/content/training-data/paraphrase_generation/1/triples/results.txt
BLEU and METEOR,has,best results,results,/content/training-data/paraphrase_generation/1/triples/results.txt
best results,are,4.7 % and 4 % absolute point improvement,results,/content/training-data/paraphrase_generation/1/triples/results.txt
4.7 % and 4 % absolute point improvement,over,state - of - the - art,results,/content/training-data/paraphrase_generation/1/triples/results.txt
MSCOCO dataset,has,Both variations,results,/content/training-data/paraphrase_generation/1/triples/results.txt
Both variations,of,supervised model,results,/content/training-data/paraphrase_generation/1/triples/results.txt
supervised model,perform,better,results,/content/training-data/paraphrase_generation/1/triples/results.txt
better,with,VAE - SVG,results,/content/training-data/paraphrase_generation/1/triples/results.txt
VAE - SVG,performing,slightly better,results,/content/training-data/paraphrase_generation/1/triples/results.txt
slightly better,than,VAE - SVG - eq,results,/content/training-data/paraphrase_generation/1/triples/results.txt
better,than,state - of - the - art,results,/content/training-data/paraphrase_generation/1/triples/results.txt
supervised model,i.e.,VAE - SVG,results,/content/training-data/paraphrase_generation/1/triples/results.txt
supervised model,i.e.,VAE - SVG - eq,results,/content/training-data/paraphrase_generation/1/triples/results.txt
Results,for,Quora dataset,results,/content/training-data/paraphrase_generation/1/triples/results.txt
Quora dataset,comparing,best variant of our model,results,/content/training-data/paraphrase_generation/1/triples/results.txt
best variant of our model,with,unsupervised model ( VAE ,results,/content/training-data/paraphrase_generation/1/triples/results.txt
best variant of our model,able to get,more than 19 % absolute point ( more than 2 times ) boost,results,/content/training-data/paraphrase_generation/1/triples/results.txt
more than 19 % absolute point ( more than 2 times ) boost,in,METEOR,results,/content/training-data/paraphrase_generation/1/triples/results.txt
best variant of our model,able to get,more than 27 % absolute point ( more than 3 times ) boost,results,/content/training-data/paraphrase_generation/1/triples/results.txt
more than 27 % absolute point ( more than 3 times ) boost,in,BLEU score,results,/content/training-data/paraphrase_generation/1/triples/results.txt
Quora dataset,comparing,VAE - S,results,/content/training-data/paraphrase_generation/1/triples/results.txt
VAE - S,able to get,more than 10 % absolute points,results,/content/training-data/paraphrase_generation/1/triples/results.txt
more than 10 % absolute points,in,METEOR ( 1.5 times ,results,/content/training-data/paraphrase_generation/1/triples/results.txt
VAE - S,able to get,boost of almost 19 % absolute points,results,/content/training-data/paraphrase_generation/1/triples/results.txt
boost of almost 19 % absolute points,in,BLEU ( 2 times ,results,/content/training-data/paraphrase_generation/1/triples/results.txt
Quora dataset,has,both variations,results,/content/training-data/paraphrase_generation/1/triples/results.txt
both variations,of,model,results,/content/training-data/paraphrase_generation/1/triples/results.txt
both variations,perform,significantly better,results,/content/training-data/paraphrase_generation/1/triples/results.txt
significantly better,than,unsupervised VAE,results,/content/training-data/paraphrase_generation/1/triples/results.txt
significantly better,than,VAE - S,results,/content/training-data/paraphrase_generation/1/triples/results.txt
Quora dataset,increase,training data size,results,/content/training-data/paraphrase_generation/1/triples/results.txt
training data size,has,results,results,/content/training-data/paraphrase_generation/1/triples/results.txt
results,has,improve,results,/content/training-data/paraphrase_generation/1/triples/results.txt
Quora dataset,experimented with,generating paraphrases,results,/content/training-data/paraphrase_generation/1/triples/results.txt
generating paraphrases,through,beam - search,results,/content/training-data/paraphrase_generation/1/triples/results.txt
generating paraphrases,turns out that,beam search,results,/content/training-data/paraphrase_generation/1/triples/results.txt
beam search,improves,results,results,/content/training-data/paraphrase_generation/1/triples/results.txt
results,has,significantly,results,/content/training-data/paraphrase_generation/1/triples/results.txt
Quora dataset,Comparing,results,results,/content/training-data/paraphrase_generation/1/triples/results.txt
results,across,different variants,results,/content/training-data/paraphrase_generation/1/triples/results.txt
different variants,of,supervised model,results,/content/training-data/paraphrase_generation/1/triples/results.txt
different variants,has,VAE - SVG - eq,results,/content/training-data/paraphrase_generation/1/triples/results.txt
VAE - SVG - eq,performs,best,results,/content/training-data/paraphrase_generation/1/triples/results.txt
Results,has,paraphrases,results,/content/training-data/paraphrase_generation/1/triples/results.txt
paraphrases,are,well - formed,results,/content/training-data/paraphrase_generation/1/triples/results.txt
paraphrases,are,semantically sensible,results,/content/training-data/paraphrase_generation/1/triples/results.txt
paraphrases,are,grammatically correct,results,/content/training-data/paraphrase_generation/1/triples/results.txt
paraphrases,generated by,our system,results,/content/training-data/paraphrase_generation/1/triples/results.txt
Results,has,average metric,results,/content/training-data/paraphrase_generation/1/triples/results.txt
average metric,of,VAE - SVG model,results,/content/training-data/paraphrase_generation/1/triples/results.txt
VAE - SVG model,able to give,10 % absolute point performance improvement,results,/content/training-data/paraphrase_generation/1/triples/results.txt
10 % absolute point performance improvement,for,TER metric,results,/content/training-data/paraphrase_generation/1/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/question_answering/4/triples/baselines.txt
Baselines,has,NewsQA,baselines,/content/training-data/question_answering/4/triples/baselines.txt
NewsQA,has,key competitors,baselines,/content/training-data/question_answering/4/triples/baselines.txt
key competitors,are,BiDAF,baselines,/content/training-data/question_answering/4/triples/baselines.txt
key competitors,are,Match - LSTM,baselines,/content/training-data/question_answering/4/triples/baselines.txt
key competitors,are,FastQA / Fast QA - Ext,baselines,/content/training-data/question_answering/4/triples/baselines.txt
key competitors,are,R2-BiLSTM,baselines,/content/training-data/question_answering/4/triples/baselines.txt
key competitors,are,AMANDA,baselines,/content/training-data/question_answering/4/triples/baselines.txt
Baselines,has,Quasar -T,baselines,/content/training-data/question_answering/4/triples/baselines.txt
Quasar -T,has,key competitors,baselines,/content/training-data/question_answering/4/triples/baselines.txt
key competitors,are,BiDAF,baselines,/content/training-data/question_answering/4/triples/baselines.txt
key competitors,are,Reinforced Ranker - Reader ( R 3 ,baselines,/content/training-data/question_answering/4/triples/baselines.txt
Baselines,has,SearchQA,baselines,/content/training-data/question_answering/4/triples/baselines.txt
SearchQA,has,competitor baselines,baselines,/content/training-data/question_answering/4/triples/baselines.txt
competitor baselines,are,Attention Sum Reader ( ASR ,baselines,/content/training-data/question_answering/4/triples/baselines.txt
competitor baselines,are,Focused Hierarchical RNNs ( FH - RNN ,baselines,/content/training-data/question_answering/4/triples/baselines.txt
competitor baselines,are,AMANDA,baselines,/content/training-data/question_answering/4/triples/baselines.txt
competitor baselines,are,BiDAF,baselines,/content/training-data/question_answering/4/triples/baselines.txt
competitor baselines,are,AQA,baselines,/content/training-data/question_answering/4/triples/baselines.txt
competitor baselines,are,Reinforced Ranker - Reader ( R 3 ,baselines,/content/training-data/question_answering/4/triples/baselines.txt
Baselines,has,Narrative QA,baselines,/content/training-data/question_answering/4/triples/baselines.txt
Narrative QA,compare with,baselines,baselines,/content/training-data/question_answering/4/triples/baselines.txt
baselines,namely,Seq2Seq,baselines,/content/training-data/question_answering/4/triples/baselines.txt
baselines,namely,Attention Sum Reader,baselines,/content/training-data/question_answering/4/triples/baselines.txt
baselines,namely,BiDAF,baselines,/content/training-data/question_answering/4/triples/baselines.txt
Narrative QA,compare with,recent BiAttention + MRU model,baselines,/content/training-data/question_answering/4/triples/baselines.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/question_answering/4/triples/ablation-analysis.txt
Ablation analysis,has,key insight,ablation-analysis,/content/training-data/question_answering/4/triples/ablation-analysis.txt
key insight,is,all model components,ablation-analysis,/content/training-data/question_answering/4/triples/ablation-analysis.txt
all model components,are crucial to,DECAPROP,ablation-analysis,/content/training-data/question_answering/4/triples/ablation-analysis.txt
Ablation analysis,has,DECAENC,ablation-analysis,/content/training-data/question_answering/4/triples/ablation-analysis.txt
DECAENC,seems to contribute,most,ablation-analysis,/content/training-data/question_answering/4/triples/ablation-analysis.txt
most,to,over all performance,ablation-analysis,/content/training-data/question_answering/4/triples/ablation-analysis.txt
Ablation analysis,observe,superiority,ablation-analysis,/content/training-data/question_answering/4/triples/ablation-analysis.txt
superiority,of,DECAPROP,ablation-analysis,/content/training-data/question_answering/4/triples/ablation-analysis.txt
DECAPROP,over,R - NET,ablation-analysis,/content/training-data/question_answering/4/triples/ablation-analysis.txt
DECAPROP,is,consistent and relatively stable,ablation-analysis,/content/training-data/question_answering/4/triples/ablation-analysis.txt
Ablation analysis,observe,significant gap,ablation-analysis,/content/training-data/question_answering/4/triples/ablation-analysis.txt
significant gap,in,performance,ablation-analysis,/content/training-data/question_answering/4/triples/ablation-analysis.txt
performance,between,DECAPROP and R - NET,ablation-analysis,/content/training-data/question_answering/4/triples/ablation-analysis.txt
Ablation analysis,on,New s QA development set,ablation-analysis,/content/training-data/question_answering/4/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/question_answering/4/triples/model.txt
Model,compress,attention outputs,model,/content/training-data/question_answering/4/triples/model.txt
attention outputs,be,small,model,/content/training-data/question_answering/4/triples/model.txt
small,to,propagate,model,/content/training-data/question_answering/4/triples/model.txt
Model,has,propagated features,model,/content/training-data/question_answering/4/triples/model.txt
propagated features,collectively passed into,prediction layers,model,/content/training-data/question_answering/4/triples/model.txt
propagated features,effectively connect,shallow layers,model,/content/training-data/question_answering/4/triples/model.txt
shallow layers,to,deeper layers,model,/content/training-data/question_answering/4/triples/model.txt
Model,has,network,model,/content/training-data/question_answering/4/triples/model.txt
network,is,densely connected,model,/content/training-data/question_answering/4/triples/model.txt
Model,propose,efficient Bidirectional Attention Connectors ( BAC ,model,/content/training-data/question_answering/4/triples/model.txt
efficient Bidirectional Attention Connectors ( BAC ),as,base building block,model,/content/training-data/question_answering/4/triples/model.txt
base building block,to connect,two sequences,model,/content/training-data/question_answering/4/triples/model.txt
two sequences,at,arbitrary layers,model,/content/training-data/question_answering/4/triples/model.txt
Model,propose,DECAPROP ( Densely Connected Attention Propagation ,model,/content/training-data/question_answering/4/triples/model.txt
DECAPROP ( Densely Connected Attention Propagation ),is,novel architecture,model,/content/training-data/question_answering/4/triples/model.txt
novel architecture,for,reading comprehension,model,/content/training-data/question_answering/4/triples/model.txt
Contribution,has research problem,Reading Comprehension,research-problem,/content/training-data/question_answering/4/triples/research-problem.txt
Contribution,has research problem,reading comprehension ( RC ,research-problem,/content/training-data/question_answering/4/triples/research-problem.txt
Contribution,has research problem,RC,research-problem,/content/training-data/question_answering/4/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
Experimental setup,choice of,hidden size,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
hidden size,tuned amongst,"{ 32 , 50 , 64 , 75 }",experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
Experimental setup,choice of,RNN encoder,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
RNN encoder,tuned between,GRU and LSTM cells,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
Experimental setup,apply,variational dropout,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
variational dropout,in - between,RNN layers,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
Experimental setup,use,Adam,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
Adam,with,? = 0.001,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
? = 0.001,for,Search QA,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
? = 0.001,for,Quasar - T,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
? = 0.001,for,Narrative QA,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
Experimental setup,use,CUDNN implementation,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
CUDNN implementation,of,RNN encoder,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
Experimental setup,use,learning rate decay factor,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
learning rate decay factor,of,2,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
Experimental setup,use,Adadelta,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
Adadelta,with,? = 0.5,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
? = 0.5,for,News QA,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
Experimental setup,use,patience,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
patience,of,3 epochs,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
Experimental setup,has,sequence lengths,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
sequence lengths,capped at,800/700/1500/1100,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
800/700/1500/1100,for,"News QA , Search QA , Quasar - T and Narrative QA",experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
Experimental setup,has,maximum characters per word,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
maximum characters per word,set to,16,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
Experimental setup,has,size,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
size,of,character embeddings,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
character embeddings,set to,8,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
size,of,character RNN,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
character RNN,set to,word - level RNN encoders,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
Experimental setup,has,Dropout rate,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
Dropout rate,tuned amongst,"{ 0.1 , 0.2 , 0.3 }",experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
Dropout rate,applied to,all RNN and fully - connected layers,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
Experimental setup,has,Batch size,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
Batch size,tuned amongst,"{ 16 , 32 , 64 }",experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
Experimental setup,has,number of factors,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
number of factors,in,factorization kernel,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
factorization kernel,set to,64,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
Experimental setup,has,model,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
model,implemented in,Tensorflow,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
Experimental setup,has,number of layers,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
number of layers,in,DECAENC,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
DECAENC,set to,3,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
Experimental setup,initialize,word embeddings,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
word embeddings,with,300D Glo Ve embeddings,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
300D Glo Ve embeddings,fixed during,training,experimental-setup,/content/training-data/question_answering/4/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/question_answering/4/triples/results.txt
Results,has,our proposed model,results,/content/training-data/question_answering/4/triples/results.txt
our proposed model,outperforms,well - established baselines,results,/content/training-data/question_answering/4/triples/results.txt
well - established baselines,such as,Match - LSTM ( + 18 % EM / + 16.3 % F1 ,results,/content/training-data/question_answering/4/triples/results.txt
well - established baselines,such as,BiDAF ( + 16 % EM / + 14 % F1 ,results,/content/training-data/question_answering/4/triples/results.txt
Results,has,DECAPROP,results,/content/training-data/question_answering/4/triples/results.txt
DECAPROP,achieves,state - of - the - art performance,results,/content/training-data/question_answering/4/triples/results.txt
state - of - the - art performance,on,all four datasets,results,/content/training-data/question_answering/4/triples/results.txt
DECAPROP,outperforms,existing state - of - the - art,results,/content/training-data/question_answering/4/triples/results.txt
existing state - of - the - art,by,+ 4.7 % EM,results,/content/training-data/question_answering/4/triples/results.txt
existing state - of - the - art,by,+ 2.6 % F1,results,/content/training-data/question_answering/4/triples/results.txt
existing state - of - the - art,i.e.,recent AMANDA model,results,/content/training-data/question_answering/4/triples/results.txt
Results,on,Quasar - T,results,/content/training-data/question_answering/4/triples/results.txt
Quasar - T,has,Our model,results,/content/training-data/question_answering/4/triples/results.txt
Our model,achieves,state - of - the - art performance,results,/content/training-data/question_answering/4/triples/results.txt
state - of - the - art performance,outperforming,state - of - the - art R 3 ( Reinforced Ranker Reader ,results,/content/training-data/question_answering/4/triples/results.txt
state - of - the - art R 3 ( Reinforced Ranker Reader ),by,considerable margin,results,/content/training-data/question_answering/4/triples/results.txt
considerable margin,of,+ 4.4 % EM / + 6 % F1,results,/content/training-data/question_answering/4/triples/results.txt
Quasar - T,has,our model,results,/content/training-data/question_answering/4/triples/results.txt
our model,outperforms,AMANDA,results,/content/training-data/question_answering/4/triples/results.txt
AMANDA,by,+ 15.4 % EM and + 14.2 % in terms of F1 score,results,/content/training-data/question_answering/4/triples/results.txt
our model,outperforms,AQA,results,/content/training-data/question_answering/4/triples/results.txt
AQA,has,+ 18.1 % EM / + 18 % F1,results,/content/training-data/question_answering/4/triples/results.txt
our model,outperforms,Reinforced Reader Ranker,results,/content/training-data/question_answering/4/triples/results.txt
Reinforced Reader Ranker,has,+ 7.8 % EM / + 8.3 % F1,results,/content/training-data/question_answering/4/triples/results.txt
Results,on,popular SQuAD benchmark,results,/content/training-data/question_answering/4/triples/results.txt
popular SQuAD benchmark,has,our model,results,/content/training-data/question_answering/4/triples/results.txt
our model,can outperform,base R - NET,results,/content/training-data/question_answering/4/triples/results.txt
our model,does not achieve,state - of - the - art performance,results,/content/training-data/question_answering/4/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/question_answering/3/triples/baselines.txt
Baselines,has,RACE,baselines,/content/training-data/question_answering/3/triples/baselines.txt
RACE,has,key competitors,baselines,/content/training-data/question_answering/3/triples/baselines.txt
key competitors,are,Stanford Attention Reader ( Stanford AR ,baselines,/content/training-data/question_answering/3/triples/baselines.txt
key competitors,are,Gated Attention Reader ( GA ,baselines,/content/training-data/question_answering/3/triples/baselines.txt
key competitors,are,Dynamic Fusion Networks ( DFN ,baselines,/content/training-data/question_answering/3/triples/baselines.txt
Baselines,has,SearchQA,baselines,/content/training-data/question_answering/3/triples/baselines.txt
SearchQA,has,main competitor baseline,baselines,/content/training-data/question_answering/3/triples/baselines.txt
main competitor baseline,is,AMANDA model,baselines,/content/training-data/question_answering/3/triples/baselines.txt
Baselines,has,NarrativeQA,baselines,/content/training-data/question_answering/3/triples/baselines.txt
NarrativeQA,has,baselines,baselines,/content/training-data/question_answering/3/triples/baselines.txt
baselines,are,context - less sequence to sequence ( seq2seq ) model,baselines,/content/training-data/question_answering/3/triples/baselines.txt
baselines,are,ASR,baselines,/content/training-data/question_answering/3/triples/baselines.txt
baselines,are,BiDAF,baselines,/content/training-data/question_answering/3/triples/baselines.txt
Contribution,has,Model,model,/content/training-data/question_answering/3/triples/model.txt
Model,for,given word,model,/content/training-data/question_answering/3/triples/model.txt
given word,in,target sequence,model,/content/training-data/question_answering/3/triples/model.txt
given word,has,our encoder,model,/content/training-data/question_answering/3/triples/model.txt
our encoder,exploits,long - term ( far ) and short - term ( near ) information,model,/content/training-data/question_answering/3/triples/model.txt
long - term ( far ) and short - term ( near ) information,to decide,information,model,/content/training-data/question_answering/3/triples/model.txt
information,to,retain,model,/content/training-data/question_answering/3/triples/model.txt
Model,has,output,model,/content/training-data/question_answering/3/triples/model.txt
output,of,dilated composition mechanism,model,/content/training-data/question_answering/3/triples/model.txt
dilated composition mechanism,acts as,gating functions,model,/content/training-data/question_answering/3/triples/model.txt
dilated composition mechanism,used to learn,compositional representations,model,/content/training-data/question_answering/3/triples/model.txt
compositional representations,of,input sequence,model,/content/training-data/question_answering/3/triples/model.txt
Model,has,Our proposed encoder,model,/content/training-data/question_answering/3/triples/model.txt
Our proposed encoder,leverages,dilated compositions,model,/content/training-data/question_answering/3/triples/model.txt
dilated compositions,to model,relationships,model,/content/training-data/question_answering/3/triples/model.txt
relationships,across,multiple granularities,model,/content/training-data/question_answering/3/triples/model.txt
Model,propose,new compositional encoder,model,/content/training-data/question_answering/3/triples/model.txt
new compositional encoder,serve as,new module,model,/content/training-data/question_answering/3/triples/model.txt
new module,complementary to,existing neural architectures,model,/content/training-data/question_answering/3/triples/model.txt
new compositional encoder,be used,in place,model,/content/training-data/question_answering/3/triples/model.txt
in place,of,standard RNN encoders,model,/content/training-data/question_answering/3/triples/model.txt
Contribution,has research problem,Reading Comprehension,research-problem,/content/training-data/question_answering/3/triples/research-problem.txt
Contribution,has research problem,reading comprehension ( RC ,research-problem,/content/training-data/question_answering/3/triples/research-problem.txt
Contribution,has research problem,RC,research-problem,/content/training-data/question_answering/3/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/question_answering/3/triples/experimental-setup.txt
Experimental setup,adopt,Adam optimizer,experimental-setup,/content/training-data/question_answering/3/triples/experimental-setup.txt
Adam optimizer,with,learning rate,experimental-setup,/content/training-data/question_answering/3/triples/experimental-setup.txt
learning rate,of,0.0003/ 0.001/0.001,experimental-setup,/content/training-data/question_answering/3/triples/experimental-setup.txt
0.0003/ 0.001/0.001,for,RACE / SearchQA / Narrative QA,experimental-setup,/content/training-data/question_answering/3/triples/experimental-setup.txt
Experimental setup,For,Narrative QA,experimental-setup,/content/training-data/question_answering/3/triples/experimental-setup.txt
Narrative QA,use,Rouge - L score,experimental-setup,/content/training-data/question_answering/3/triples/experimental-setup.txt
Rouge - L score,to find,best approximate answer,experimental-setup,/content/training-data/question_answering/3/triples/experimental-setup.txt
best approximate answer,relative to,human written answer,experimental-setup,/content/training-data/question_answering/3/triples/experimental-setup.txt
human written answer,for training,span model,experimental-setup,/content/training-data/question_answering/3/triples/experimental-setup.txt
Experimental setup,implement,all models,experimental-setup,/content/training-data/question_answering/3/triples/experimental-setup.txt
all models,in,TensorFlow,experimental-setup,/content/training-data/question_answering/3/triples/experimental-setup.txt
Experimental setup,has,runtime benchmarks,experimental-setup,/content/training-data/question_answering/3/triples/experimental-setup.txt
runtime benchmarks,based on,TitanXP GPU,experimental-setup,/content/training-data/question_answering/3/triples/experimental-setup.txt
Experimental setup,has,Dropout rate,experimental-setup,/content/training-data/question_answering/3/triples/experimental-setup.txt
Dropout rate,tuned amongst,"{ 0.1 , 0.2 , 0.3 }",experimental-setup,/content/training-data/question_answering/3/triples/experimental-setup.txt
"{ 0.1 , 0.2 , 0.3 }",on,all layers,experimental-setup,/content/training-data/question_answering/3/triples/experimental-setup.txt
all layers,including,embedding layer,experimental-setup,/content/training-data/question_answering/3/triples/experimental-setup.txt
Experimental setup,has,batch size,experimental-setup,/content/training-data/question_answering/3/triples/experimental-setup.txt
batch size,set to,64/256/32,experimental-setup,/content/training-data/question_answering/3/triples/experimental-setup.txt
Experimental setup,has,Word embeddings,experimental-setup,/content/training-data/question_answering/3/triples/experimental-setup.txt
Word embeddings,initialized with,300d Glo Ve vectors,experimental-setup,/content/training-data/question_answering/3/triples/experimental-setup.txt
Word embeddings,not fine - tuned during,training,experimental-setup,/content/training-data/question_answering/3/triples/experimental-setup.txt
Experimental setup,has,maximum sequence lengths,experimental-setup,/content/training-data/question_answering/3/triples/experimental-setup.txt
maximum sequence lengths,are,500/200/1100,experimental-setup,/content/training-data/question_answering/3/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/question_answering/3/triples/results.txt
Results,on,NarrativeQA benchmark,results,/content/training-data/question_answering/3/triples/results.txt
NarrativeQA benchmark,observe that,300d DCU,results,/content/training-data/question_answering/3/triples/results.txt
300d DCU,achieve,comparable performance,results,/content/training-data/question_answering/3/triples/results.txt
comparable performance,with,BiDAF,results,/content/training-data/question_answering/3/triples/results.txt
NarrativeQA benchmark,has,DCU - LSTM,results,/content/training-data/question_answering/3/triples/results.txt
DCU - LSTM,significantly outperforms,all models,results,/content/training-data/question_answering/3/triples/results.txt
all models,including,BiDAF,results,/content/training-data/question_answering/3/triples/results.txt
all models,in terms of,ROUGE - L,results,/content/training-data/question_answering/3/triples/results.txt
NarrativeQA benchmark,has,Performance improvement,results,/content/training-data/question_answering/3/triples/results.txt
Performance improvement,over,vanilla BiLSTM model,results,/content/training-data/question_answering/3/triples/results.txt
vanilla BiLSTM model,ranges from,1 % ? 3 %,results,/content/training-data/question_answering/3/triples/results.txt
1 % ? 3 %,across,all metrics,results,/content/training-data/question_answering/3/triples/results.txt
Results,on,RACE benchmark dataset,results,/content/training-data/question_answering/3/triples/results.txt
RACE benchmark dataset,pull ahead of,other recent baselines,results,/content/training-data/question_answering/3/triples/results.txt
other recent baselines,by,at least 5 %,results,/content/training-data/question_answering/3/triples/results.txt
other recent baselines,such as,ElimiNet,results,/content/training-data/question_answering/3/triples/results.txt
other recent baselines,such as,GA,results,/content/training-data/question_answering/3/triples/results.txt
RACE benchmark dataset,outperform,highly complex models,results,/content/training-data/question_answering/3/triples/results.txt
highly complex models,such as,DFN,results,/content/training-data/question_answering/3/triples/results.txt
RACE benchmark dataset,has,Our proposed DCU model,results,/content/training-data/question_answering/3/triples/results.txt
Our proposed DCU model,achieves,best result,results,/content/training-data/question_answering/3/triples/results.txt
best result,for,single models,results,/content/training-data/question_answering/3/triples/results.txt
best result,for,ensemble models,results,/content/training-data/question_answering/3/triples/results.txt
RACE benchmark dataset,has,best single model score,results,/content/training-data/question_answering/3/triples/results.txt
best single model score,from,RACE - H,results,/content/training-data/question_answering/3/triples/results.txt
best single model score,from,RACE - M,results,/content/training-data/question_answering/3/triples/results.txt
best single model score,alternates between,Sim - DCU,results,/content/training-data/question_answering/3/triples/results.txt
best single model score,alternates between,DCU,results,/content/training-data/question_answering/3/triples/results.txt
Results,on,Search QA dataset,results,/content/training-data/question_answering/3/triples/results.txt
Search QA dataset,achieve,same accuracy,results,/content/training-data/question_answering/3/triples/results.txt
same accuracy,as,AMANDA,results,/content/training-data/question_answering/3/triples/results.txt
same accuracy,without using,LSTM or GRU encoder,results,/content/training-data/question_answering/3/triples/results.txt
Search QA dataset,has,"hybrid combination , DCU - LSTM",results,/content/training-data/question_answering/3/triples/results.txt
"hybrid combination , DCU - LSTM",significantly outperforms,AMANDA,results,/content/training-data/question_answering/3/triples/results.txt
AMANDA,by,3 %,results,/content/training-data/question_answering/3/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/question_answering/5/triples/baselines.txt
Baselines,include,BiDAF,baselines,/content/training-data/question_answering/5/triples/baselines.txt
BiDAF,has,RC model,baselines,/content/training-data/question_answering/5/triples/baselines.txt
RC model,with,bidirectional attention flow,baselines,/content/training-data/question_answering/5/triples/baselines.txt
Baselines,include,AQA,baselines,/content/training-data/question_answering/5/triples/baselines.txt
AQA,has,reinforced system learning,baselines,/content/training-data/question_answering/5/triples/baselines.txt
reinforced system learning,to aggregate,answers,baselines,/content/training-data/question_answering/5/triples/baselines.txt
answers,generated by,re-written questions,baselines,/content/training-data/question_answering/5/triples/baselines.txt
Baselines,include,R 3,baselines,/content/training-data/question_answering/5/triples/baselines.txt
R 3,has,reinforced model,baselines,/content/training-data/question_answering/5/triples/baselines.txt
reinforced model,making use of,ranker,baselines,/content/training-data/question_answering/5/triples/baselines.txt
ranker,for selecting,passages,baselines,/content/training-data/question_answering/5/triples/baselines.txt
passages,to train,RC model,baselines,/content/training-data/question_answering/5/triples/baselines.txt
Baselines,include,GA,baselines,/content/training-data/question_answering/5/triples/baselines.txt
GA,has,reading comprehension model,baselines,/content/training-data/question_answering/5/triples/baselines.txt
reading comprehension model,with,gated - attention,baselines,/content/training-data/question_answering/5/triples/baselines.txt
Contribution,Code,https://github.com/shuohangwang/mprc,code,/content/training-data/question_answering/5/triples/code.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/question_answering/5/triples/hyperparameters.txt
Hyperparameters,set,all the words,hyperparameters,/content/training-data/question_answering/5/triples/hyperparameters.txt
all the words,as,zero vectors,hyperparameters,/content/training-data/question_answering/5/triples/hyperparameters.txt
all the words,beyond,Glove,hyperparameters,/content/training-data/question_answering/5/triples/hyperparameters.txt
Hyperparameters,set,learning rate,hyperparameters,/content/training-data/question_answering/5/triples/hyperparameters.txt
learning rate,to,0.002,hyperparameters,/content/training-data/question_answering/5/triples/hyperparameters.txt
Hyperparameters,set,batch size,hyperparameters,/content/training-data/question_answering/5/triples/hyperparameters.txt
batch size,to,30,hyperparameters,/content/training-data/question_answering/5/triples/hyperparameters.txt
Hyperparameters,set,l,hyperparameters,/content/training-data/question_answering/5/triples/hyperparameters.txt
l,to,300,hyperparameters,/content/training-data/question_answering/5/triples/hyperparameters.txt
Hyperparameters,use,pre-trained R 3 model,hyperparameters,/content/training-data/question_answering/5/triples/hyperparameters.txt
pre-trained R 3 model,to generate,top 50 candidate spans,hyperparameters,/content/training-data/question_answering/5/triples/hyperparameters.txt
top 50 candidate spans,use them for,further ranking,hyperparameters,/content/training-data/question_answering/5/triples/hyperparameters.txt
top 50 candidate spans,for,"training , development and test datasets",hyperparameters,/content/training-data/question_answering/5/triples/hyperparameters.txt
Hyperparameters,For,coverage - based re-ranker,hyperparameters,/content/training-data/question_answering/5/triples/hyperparameters.txt
coverage - based re-ranker,use,Adam,hyperparameters,/content/training-data/question_answering/5/triples/hyperparameters.txt
Adam,to optimize,model,hyperparameters,/content/training-data/question_answering/5/triples/hyperparameters.txt
Hyperparameters,tune,dropout probability,hyperparameters,/content/training-data/question_answering/5/triples/hyperparameters.txt
dropout probability,from,0 to 0.5,hyperparameters,/content/training-data/question_answering/5/triples/hyperparameters.txt
Hyperparameters,tune,number of candidate answers,hyperparameters,/content/training-data/question_answering/5/triples/hyperparameters.txt
number of candidate answers,for,re-ranking ( K ,hyperparameters,/content/training-data/question_answering/5/triples/hyperparameters.txt
re-ranking ( K ),in,"[ 3 , 5 , 10 ]",hyperparameters,/content/training-data/question_answering/5/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/question_answering/5/triples/model.txt
Model,formulate,evidence aggregation,model,/content/training-data/question_answering/5/triples/model.txt
evidence aggregation,as,answer re-ranking problem,model,/content/training-data/question_answering/5/triples/model.txt
Model,for,each answer candidate,model,/content/training-data/question_answering/5/triples/model.txt
each answer candidate,efficiently incorporate,global information,model,/content/training-data/question_answering/5/triples/model.txt
global information,from,multiple pieces,model,/content/training-data/question_answering/5/triples/model.txt
multiple pieces,of,textual evidence,model,/content/training-data/question_answering/5/triples/model.txt
global information,without,significantly increasing,model,/content/training-data/question_answering/5/triples/model.txt
significantly increasing,has,complexity,model,/content/training-data/question_answering/5/triples/model.txt
complexity,of,prediction,model,/content/training-data/question_answering/5/triples/model.txt
prediction,of,RC model,model,/content/training-data/question_answering/5/triples/model.txt
Model,has,re-rankers,model,/content/training-data/question_answering/5/triples/model.txt
re-rankers,are,coverage - based re-ranker,model,/content/training-data/question_answering/5/triples/model.txt
coverage - based re-ranker,aims to rank,answer candidate,model,/content/training-data/question_answering/5/triples/model.txt
answer candidate,has,higher,model,/content/training-data/question_answering/5/triples/model.txt
higher,if,union,model,/content/training-data/question_answering/5/triples/model.txt
union,of,all its contexts,model,/content/training-data/question_answering/5/triples/model.txt
all its contexts,in,different passages,model,/content/training-data/question_answering/5/triples/model.txt
all its contexts,could cover,more aspects,model,/content/training-data/question_answering/5/triples/model.txt
more aspects,included in,question,model,/content/training-data/question_answering/5/triples/model.txt
re-rankers,are,strength - based re-ranker,model,/content/training-data/question_answering/5/triples/model.txt
strength - based re-ranker,ranks,answer candidates,model,/content/training-data/question_answering/5/triples/model.txt
answer candidates,according to how often,evidence,model,/content/training-data/question_answering/5/triples/model.txt
evidence,occurs in,different passages,model,/content/training-data/question_answering/5/triples/model.txt
Model,propose,method,model,/content/training-data/question_answering/5/triples/model.txt
method,by explicitly aggregating,evidence,model,/content/training-data/question_answering/5/triples/model.txt
evidence,from across,multiple passages,model,/content/training-data/question_answering/5/triples/model.txt
method,to improve,open - domain QA,model,/content/training-data/question_answering/5/triples/model.txt
Contribution,has research problem,OPEN - DOMAIN QUESTION ANSWERING,research-problem,/content/training-data/question_answering/5/triples/research-problem.txt
Contribution,has research problem,Open-domain question answering ( QA ,research-problem,/content/training-data/question_answering/5/triples/research-problem.txt
Contribution,has research problem,open - domain QA,research-problem,/content/training-data/question_answering/5/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/question_answering/5/triples/results.txt
Results,showed that,R 3,results,/content/training-data/question_answering/5/triples/results.txt
R 3,competitive to,state - of - the - arts,results,/content/training-data/question_answering/5/triples/results.txt
R 3,achieved,"F1 68.5 , EM 63.0",results,/content/training-data/question_answering/5/triples/results.txt
"F1 68.5 , EM 63.0",on,Web domain,results,/content/training-data/question_answering/5/triples/results.txt
R 3,achieved,"F1 56.0 , EM 50.9",results,/content/training-data/question_answering/5/triples/results.txt
"F1 56.0 , EM 50.9",on,Wiki domain,results,/content/training-data/question_answering/5/triples/results.txt
Results,see that,full re-ranker,results,/content/training-data/question_answering/5/triples/results.txt
full re-ranker,combination of,different re-rankers,results,/content/training-data/question_answering/5/triples/results.txt
full re-ranker,significantly outperforms,previous best performance,results,/content/training-data/question_answering/5/triples/results.txt
previous best performance,especially on,Quasar - T,results,/content/training-data/question_answering/5/triples/results.txt
previous best performance,especially on,Search QA,results,/content/training-data/question_answering/5/triples/results.txt
previous best performance,by,large margin,results,/content/training-data/question_answering/5/triples/results.txt
Results,see that,our coverage - based re-ranker,results,/content/training-data/question_answering/5/triples/results.txt
our coverage - based re-ranker,achieves,consistently good performance,results,/content/training-data/question_answering/5/triples/results.txt
consistently good performance,on,three datasets,results,/content/training-data/question_answering/5/triples/results.txt
Results,has,our model,results,/content/training-data/question_answering/5/triples/results.txt
our model,is,much better,results,/content/training-data/question_answering/5/triples/results.txt
much better,than,human performance,results,/content/training-data/question_answering/5/triples/results.txt
human performance,on,Search QA dataset,results,/content/training-data/question_answering/5/triples/results.txt
Contribution,has,Approach,approach,/content/training-data/question_answering/0/triples/approach.txt
Approach,adapt,Gated Graph Neural Networks ( GGNNs ,approach,/content/training-data/question_answering/0/triples/approach.txt
Gated Graph Neural Networks ( GGNNs ),to process and score,semantic parses,approach,/content/training-data/question_answering/0/triples/approach.txt
Approach,for,each input question,approach,/content/training-data/question_answering/0/triples/approach.txt
each input question,construct,explicit structural semantic parse ( semantic graph ,approach,/content/training-data/question_answering/0/triples/approach.txt
Approach,has,Semantic parses,approach,/content/training-data/question_answering/0/triples/approach.txt
Semantic parses,deterministically converted to,query,approach,/content/training-data/question_answering/0/triples/approach.txt
query,to extract,answers,approach,/content/training-data/question_answering/0/triples/approach.txt
answers,from,KB,approach,/content/training-data/question_answering/0/triples/approach.txt
Approach,has,semantic parsing,approach,/content/training-data/question_answering/0/triples/approach.txt
semantic parsing,to,problem,approach,/content/training-data/question_answering/0/triples/approach.txt
problem,of,KB QA,approach,/content/training-data/question_answering/0/triples/approach.txt
Contribution,has,Baselines,baselines,/content/training-data/question_answering/0/triples/baselines.txt
Baselines,has,Gated Graph Neural Network ( GGNN ,baselines,/content/training-data/question_answering/0/triples/baselines.txt
Gated Graph Neural Network ( GGNN ),to process,semantic parses,baselines,/content/training-data/question_answering/0/triples/baselines.txt
Baselines,has,Pooled Edges model,baselines,/content/training-data/question_answering/0/triples/baselines.txt
Pooled Edges model,use,DCNN,baselines,/content/training-data/question_answering/0/triples/baselines.txt
DCNN,to encode,question and the label of each edge,baselines,/content/training-data/question_answering/0/triples/baselines.txt
question and the label of each edge,in,semantic graph,baselines,/content/training-data/question_answering/0/triples/baselines.txt
Baselines,has,Graph Neural Network ( GNN ,baselines,/content/training-data/question_answering/0/triples/baselines.txt
Graph Neural Network ( GNN ),include,model variant,baselines,/content/training-data/question_answering/0/triples/baselines.txt
model variant,directly computes,hidden state,baselines,/content/training-data/question_answering/0/triples/baselines.txt
hidden state,as,combination,baselines,/content/training-data/question_answering/0/triples/baselines.txt
combination,of,activations ( Eq 1 ) and the previous state,baselines,/content/training-data/question_answering/0/triples/baselines.txt
model variant,that does not use,gating mechanism,baselines,/content/training-data/question_answering/0/triples/baselines.txt
Contribution,Code,https://github.com/UKPLab/coling2018-graph-neural-networks-question-answering,code,/content/training-data/question_answering/0/triples/code.txt
Contribution,has research problem,Knowledge Base Question Answering,research-problem,/content/training-data/question_answering/0/triples/research-problem.txt
Contribution,has research problem,Knowledge base question answering ( QA ,research-problem,/content/training-data/question_answering/0/triples/research-problem.txt
Contribution,has research problem,QA,research-problem,/content/training-data/question_answering/0/triples/research-problem.txt
Contribution,has research problem,KB QA,research-problem,/content/training-data/question_answering/0/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/question_answering/0/triples/results.txt
Results,for,STAGG and Single Edge baselines,results,/content/training-data/question_answering/0/triples/results.txt
STAGG and Single Edge baselines,has,performance,results,/content/training-data/question_answering/0/triples/results.txt
performance,has,drops,results,/content/training-data/question_answering/0/triples/results.txt
performance,compared to,results,results,/content/training-data/question_answering/0/triples/results.txt
results,on,simpler questions,results,/content/training-data/question_answering/0/triples/results.txt
performance,on,more complex questions,results,/content/training-data/question_answering/0/triples/results.txt
Results,has,Single Edge baseline,results,/content/training-data/question_answering/0/triples/results.txt
Single Edge baseline,prefers,simple graphs,results,/content/training-data/question_answering/0/triples/results.txt
simple graphs,is,good strategy,results,/content/training-data/question_answering/0/triples/results.txt
good strategy,to achieve,higher recall values,results,/content/training-data/question_answering/0/triples/results.txt
simple graphs,that consist of,single edge,results,/content/training-data/question_answering/0/triples/results.txt
Results,has,Pooled Edges model,results,/content/training-data/question_answering/0/triples/results.txt
Pooled Edges model,maintains,better performance,results,/content/training-data/question_answering/0/triples/results.txt
better performance,across,questions of different complexity,results,/content/training-data/question_answering/0/triples/results.txt
better performance,shows,benefits,results,/content/training-data/question_answering/0/triples/results.txt
benefits,of encoding,all graph edges,results,/content/training-data/question_answering/0/triples/results.txt
Results,has,Single Edge model,results,/content/training-data/question_answering/0/triples/results.txt
Single Edge model,outperforms,more complex Pooled Edges model,results,/content/training-data/question_answering/0/triples/results.txt
more complex Pooled Edges model,by,noticeable margin,results,/content/training-data/question_answering/0/triples/results.txt
Results,has,STAGG architecture,results,/content/training-data/question_answering/0/triples/results.txt
STAGG architecture,delivers,worst results,results,/content/training-data/question_answering/0/triples/results.txt
Results,see that,GGNN model,results,/content/training-data/question_answering/0/triples/results.txt
GGNN model,offers,best results,results,/content/training-data/question_answering/0/triples/results.txt
best results,on,simple and complex questions,results,/content/training-data/question_answering/0/triples/results.txt
Results,on,WebQSP - WD data set,results,/content/training-data/question_answering/0/triples/results.txt
WebQSP - WD data set,has,graph models,results,/content/training-data/question_answering/0/triples/results.txt
graph models,outperform,all other models,results,/content/training-data/question_answering/0/triples/results.txt
all other models,across,"precision , recall and F-score",results,/content/training-data/question_answering/0/triples/results.txt
graph models,has,GGNN,results,/content/training-data/question_answering/0/triples/results.txt
GGNN,showing,best over all result,results,/content/training-data/question_answering/0/triples/results.txt
Contribution,has,Experiments,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Experiments,has,MovieQA dataset,experiments,/content/training-data/question_answering/2/triples/experiments.txt
MovieQA dataset,has,Experimental setup,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Experimental setup,set,maximum number of frames to consider,experiments,/content/training-data/question_answering/2/triples/experiments.txt
maximum number of frames to consider,to,F = 10,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Experimental setup,set,maximum number of subtitle sentences in a clip,experiments,/content/training-data/question_answering/2/triples/experiments.txt
maximum number of subtitle sentences in a clip,to,K = 100,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Experimental setup,set,maximum words,experiments,/content/training-data/question_answering/2/triples/experiments.txt
maximum words,to,V = 10,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Experimental setup,set,maximum number of movie clips per question,experiments,/content/training-data/question_answering/2/triples/experiments.txt
maximum number of movie clips per question,to,N = 20,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Experimental setup,use,AdaDelta optimizer,experiments,/content/training-data/question_answering/2/triples/experiments.txt
AdaDelta optimizer,with,initial learning rate,experiments,/content/training-data/question_answering/2/triples/experiments.txt
initial learning rate,of,0.5,experiments,/content/training-data/question_answering/2/triples/experiments.txt
initial learning rate,trained for,300 epochs,experiments,/content/training-data/question_answering/2/triples/experiments.txt
AdaDelta optimizer,with,minibatch,experiments,/content/training-data/question_answering/2/triples/experiments.txt
minibatch,of,16,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Experimental setup,implement,FVTA network,experiments,/content/training-data/question_answering/2/triples/experiments.txt
FVTA network,with,modality number,experiments,/content/training-data/question_answering/2/triples/experiments.txt
modality number,of,2 ( video & text ,experiments,/content/training-data/question_answering/2/triples/experiments.txt
FVTA network,for,Movie QA task,experiments,/content/training-data/question_answering/2/triples/experiments.txt
MovieQA dataset,has,Results,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Results,has,accuracy,experiments,/content/training-data/question_answering/2/triples/experiments.txt
accuracy,is,0.373,experiments,/content/training-data/question_answering/2/triples/experiments.txt
0.373,vs,0.363,experiments,/content/training-data/question_answering/2/triples/experiments.txt
0.373,on,test set,experiments,/content/training-data/question_answering/2/triples/experiments.txt
accuracy,is,0.410,experiments,/content/training-data/question_answering/2/triples/experiments.txt
0.410,vs,0.387,experiments,/content/training-data/question_answering/2/triples/experiments.txt
0.387,by,RWMN,experiments,/content/training-data/question_answering/2/triples/experiments.txt
0.410,on,validation set,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Results,has,FVTA,experiments,/content/training-data/question_answering/2/triples/experiments.txt
FVTA,consistently outperforms,classical attention models,experiments,/content/training-data/question_answering/2/triples/experiments.txt
classical attention models,including,soft attention,experiments,/content/training-data/question_answering/2/triples/experiments.txt
classical attention models,including,MCB,experiments,/content/training-data/question_answering/2/triples/experiments.txt
classical attention models,including,TGIF,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Results,has,FVTA model,experiments,/content/training-data/question_answering/2/triples/experiments.txt
FVTA model,achieves,comparable performance,experiments,/content/training-data/question_answering/2/triples/experiments.txt
comparable performance,to,state - of - the - art result,experiments,/content/training-data/question_answering/2/triples/experiments.txt
state - of - the - art result,on,MovieQA test server,experiments,/content/training-data/question_answering/2/triples/experiments.txt
FVTA model,outperforms,all baseline methods,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Experiments,has,Memex QA,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Memex QA,has,Experimental setup,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Experimental setup,encode,GPS locations,experiments,/content/training-data/question_answering/2/triples/experiments.txt
GPS locations,using,words,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Experimental setup,concatenate,output,experiments,/content/training-data/question_answering/2/triples/experiments.txt
output,of,both directions,experiments,/content/training-data/question_answering/2/triples/experiments.txt
both directions,of,LSTM,experiments,/content/training-data/question_answering/2/triples/experiments.txt
LSTM,Given,hidden state size,experiments,/content/training-data/question_answering/2/triples/experiments.txt
hidden state size,of,d,experiments,/content/training-data/question_answering/2/triples/experiments.txt
d,set to,50,experiments,/content/training-data/question_answering/2/triples/experiments.txt
LSTM,for,all media documents,experiments,/content/training-data/question_answering/2/triples/experiments.txt
all media documents,get,context tensor H,experiments,/content/training-data/question_answering/2/triples/experiments.txt
context tensor H,has,R 2dV KN 6,experiments,/content/training-data/question_answering/2/triples/experiments.txt
all media documents,get,question matrix Q,experiments,/content/training-data/question_answering/2/triples/experiments.txt
question matrix Q,has,R 2 d M,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Experimental setup,use,pre-trained Glo Ve word embeddings,experiments,/content/training-data/question_answering/2/triples/experiments.txt
pre-trained Glo Ve word embeddings,fixed during,training,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Experimental setup,use,linear transformation,experiments,/content/training-data/question_answering/2/triples/experiments.txt
linear transformation,to compress,image feature,experiments,/content/training-data/question_answering/2/triples/experiments.txt
image feature,into,100 dimensional,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Experimental setup,use,AdaDelta optimizer,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Experimental setup,use,initial learning rate,experiments,/content/training-data/question_answering/2/triples/experiments.txt
initial learning rate,of,0.5,experiments,/content/training-data/question_answering/2/triples/experiments.txt
0.5,to train for,200 epochs,experiments,/content/training-data/question_answering/2/triples/experiments.txt
200 epochs,with,dropout rate,experiments,/content/training-data/question_answering/2/triples/experiments.txt
dropout rate,of,0.3,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Experimental setup,For,image / video embedding,experiments,/content/training-data/question_answering/2/triples/experiments.txt
image / video embedding,extract,fixed - size features,experiments,/content/training-data/question_answering/2/triples/experiments.txt
fixed - size features,using,pre-trained CNN model,experiments,/content/training-data/question_answering/2/triples/experiments.txt
pre-trained CNN model,name,Inception - ResNet,experiments,/content/training-data/question_answering/2/triples/experiments.txt
pre-trained CNN model,by concatenating,pool5 layer and classification layer 's output,experiments,/content/training-data/question_answering/2/triples/experiments.txt
pool5 layer and classification layer 's output,before,softmax,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Experimental setup,has,bi-directional LSTM,experiments,/content/training-data/question_answering/2/triples/experiments.txt
bi-directional LSTM,used for,each modality,experiments,/content/training-data/question_answering/2/triples/experiments.txt
each modality,to obtain,contextual representations,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Experimental setup,has,"questions , textual context and answers",experiments,/content/training-data/question_answering/2/triples/experiments.txt
"questions , textual context and answers",are,tokenized,experiments,/content/training-data/question_answering/2/triples/experiments.txt
tokenized,using,Stanford word tokenizer,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Experimental setup,reshape,context tensor,experiments,/content/training-data/question_answering/2/triples/experiments.txt
context tensor,into,H ? R 2 d T 6,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Experimental setup,To select,best hyperparmeters,experiments,/content/training-data/question_answering/2/triples/experiments.txt
best hyperparmeters,randomly select,20 %,experiments,/content/training-data/question_answering/2/triples/experiments.txt
20 %,of,official training set,experiments,/content/training-data/question_answering/2/triples/experiments.txt
official training set,as,validation set,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Memex QA,has,Results,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Results,has,FVTA,experiments,/content/training-data/question_answering/2/triples/experiments.txt
FVTA,outperforms,other attention models,experiments,/content/training-data/question_answering/2/triples/experiments.txt
other attention models,on finding,relevant photos,experiments,/content/training-data/question_answering/2/triples/experiments.txt
relevant photos,for,question,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Memex QA,has,Ablation analysis,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Ablation analysis,compare,effectiveness,experiments,/content/training-data/question_answering/2/triples/experiments.txt
effectiveness,of,context - aware question attention,experiments,/content/training-data/question_answering/2/triples/experiments.txt
context - aware question attention,shows,question attention,experiments,/content/training-data/question_answering/2/triples/experiments.txt
question attention,provides,slight improvement,experiments,/content/training-data/question_answering/2/triples/experiments.txt
context - aware question attention,by removing,question attention,experiments,/content/training-data/question_answering/2/triples/experiments.txt
context - aware question attention,use,last timestep,experiments,/content/training-data/question_answering/2/triples/experiments.txt
last timestep,of,LSTM output,experiments,/content/training-data/question_answering/2/triples/experiments.txt
LSTM output,as,question representation,experiments,/content/training-data/question_answering/2/triples/experiments.txt
LSTM output,from,question,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Ablation analysis,For ablating,intra-sequence dependency,experiments,/content/training-data/question_answering/2/triples/experiments.txt
intra-sequence dependency,use,representations,experiments,/content/training-data/question_answering/2/triples/experiments.txt
representations,from,last timestep,experiments,/content/training-data/question_answering/2/triples/experiments.txt
last timestep,of,each context document,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Ablation analysis,For ablating,cross sequence interaction,experiments,/content/training-data/question_answering/2/triples/experiments.txt
cross sequence interaction,average,all attended context representation,experiments,/content/training-data/question_answering/2/triples/experiments.txt
all attended context representation,from,different modalities,experiments,/content/training-data/question_answering/2/triples/experiments.txt
different modalities,to get,final context vector,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Ablation analysis,has,Both aspects of correlation,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Both aspects of correlation,of,FVTA attention tensor,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Both aspects of correlation,while,intra-sequence dependency,experiments,/content/training-data/question_answering/2/triples/experiments.txt
intra-sequence dependency,shows,more importance,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Both aspects of correlation,contribute towards,model 's performance,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Ablation analysis,evaluate,FVTA attention mechanism,experiments,/content/training-data/question_answering/2/triples/experiments.txt
FVTA attention mechanism,first replace,our kernel tensor,experiments,/content/training-data/question_answering/2/triples/experiments.txt
our kernel tensor,with,simple cosine similarity function,experiments,/content/training-data/question_answering/2/triples/experiments.txt
our kernel tensor,has,Results,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Results,show that,standard cosine similarity,experiments,/content/training-data/question_answering/2/triples/experiments.txt
standard cosine similarity,inferior to,our similarity function,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Ablation analysis,train,FVTA without photos,experiments,/content/training-data/question_answering/2/triples/experiments.txt
FVTA without photos,to see,contribution,experiments,/content/training-data/question_answering/2/triples/experiments.txt
contribution,of,visual information,experiments,/content/training-data/question_answering/2/triples/experiments.txt
FVTA without photos,has,result,experiments,/content/training-data/question_answering/2/triples/experiments.txt
result,is,quite good,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Memex QA,has,Baselines,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Baselines,has,Embedding + LSTM,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Embedding + LSTM,utilizes,word embeddings and character embeddings,experiments,/content/training-data/question_answering/2/triples/experiments.txt
word embeddings and character embeddings,along with,same visual embeddings,experiments,/content/training-data/question_answering/2/triples/experiments.txt
same visual embeddings,used in,FVTA,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Baselines,has,Embedding + LSTM + Concat,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Embedding + LSTM + Concat,concatenates,last LSTM output,experiments,/content/training-data/question_answering/2/triples/experiments.txt
last LSTM output,from,different modalities,experiments,/content/training-data/question_answering/2/triples/experiments.txt
different modalities,to produce,final output,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Baselines,has,Classic Soft Attention,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Classic Soft Attention,uses,classic one dimensional question - to - context attention,experiments,/content/training-data/question_answering/2/triples/experiments.txt
classic one dimensional question - to - context attention,to summarize,context,experiments,/content/training-data/question_answering/2/triples/experiments.txt
context,for,question answering,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Baselines,has,DMN +,experiments,/content/training-data/question_answering/2/triples/experiments.txt
DMN +,is,improved dynamic memory networks,experiments,/content/training-data/question_answering/2/triples/experiments.txt
improved dynamic memory networks,which is one of,representative architectures,experiments,/content/training-data/question_answering/2/triples/experiments.txt
representative architectures,that achieve,good performance,experiments,/content/training-data/question_answering/2/triples/experiments.txt
good performance,on,VQA Task,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Baselines,has,Logistic Regression,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Logistic Regression,predicts,answer,experiments,/content/training-data/question_answering/2/triples/experiments.txt
answer,with,"concatenated image , question and metadata features",experiments,/content/training-data/question_answering/2/triples/experiments.txt
Baselines,has,TGIF Temporal Attention,experiments,/content/training-data/question_answering/2/triples/experiments.txt
TGIF Temporal Attention,is,spatial - temporal reasoning network,experiments,/content/training-data/question_answering/2/triples/experiments.txt
spatial - temporal reasoning network,on,sequential animated image QA,experiments,/content/training-data/question_answering/2/triples/experiments.txt
Contribution,has,Model,model,/content/training-data/question_answering/2/triples/model.txt
Model,has,FVTA attention,model,/content/training-data/question_answering/2/triples/model.txt
FVTA attention,allows for,collective reasoning,model,/content/training-data/question_answering/2/triples/model.txt
collective reasoning,by,attention kernel,model,/content/training-data/question_answering/2/triples/model.txt
attention kernel,learned over,"few , small , consecutive sub-sequences",model,/content/training-data/question_answering/2/triples/model.txt
"few , small , consecutive sub-sequences",of,text and image,model,/content/training-data/question_answering/2/triples/model.txt
Model,has,FVTA,model,/content/training-data/question_answering/2/triples/model.txt
FVTA,learns to,localize,model,/content/training-data/question_answering/2/triples/model.txt
localize,has,relevant information,model,/content/training-data/question_answering/2/triples/model.txt
relevant information,within,"few , small , temporally consecutive regions",model,/content/training-data/question_answering/2/triples/model.txt
"few , small , temporally consecutive regions",over,input sequences,model,/content/training-data/question_answering/2/triples/model.txt
FVTA,learns to,infer,model,/content/training-data/question_answering/2/triples/model.txt
infer,has,answer,model,/content/training-data/question_answering/2/triples/model.txt
answer,based on,cross-modal statistics,model,/content/training-data/question_answering/2/triples/model.txt
cross-modal statistics,pooled from,these regions,model,/content/training-data/question_answering/2/triples/model.txt
FVTA,proposes,novel kernel,model,/content/training-data/question_answering/2/triples/model.txt
novel kernel,to compute,attention tensor,model,/content/training-data/question_answering/2/triples/model.txt
attention tensor,jointly models,latent information,model,/content/training-data/question_answering/2/triples/model.txt
latent information,in,three sources,model,/content/training-data/question_answering/2/triples/model.txt
three sources,name,answer - signaling words,model,/content/training-data/question_answering/2/triples/model.txt
answer - signaling words,in,question,model,/content/training-data/question_answering/2/triples/model.txt
three sources,name,temporal correlation,model,/content/training-data/question_answering/2/triples/model.txt
temporal correlation,within,sequence,model,/content/training-data/question_answering/2/triples/model.txt
three sources,name,cross-modal interaction,model,/content/training-data/question_answering/2/triples/model.txt
cross-modal interaction,between,text and image,model,/content/training-data/question_answering/2/triples/model.txt
Model,propose,focal visual - text attention ( FVTA ) model,model,/content/training-data/question_answering/2/triples/model.txt
focal visual - text attention ( FVTA ) model,for,sequential data,model,/content/training-data/question_answering/2/triples/model.txt
Model,propose,novel attention kernel,model,/content/training-data/question_answering/2/triples/model.txt
novel attention kernel,for,VQA,model,/content/training-data/question_answering/2/triples/model.txt
VQA,on,visual - text data,model,/content/training-data/question_answering/2/triples/model.txt
Contribution,has research problem,Visual Question Answering,research-problem,/content/training-data/question_answering/2/triples/research-problem.txt
Contribution,has research problem,Visual question answering ( VQA ,research-problem,/content/training-data/question_answering/2/triples/research-problem.txt
Contribution,has research problem,VQA,research-problem,/content/training-data/question_answering/2/triples/research-problem.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/question_answering/1/triples/ablation-analysis.txt
Ablation analysis,At,word embedding layer,ablation-analysis,/content/training-data/question_answering/1/triples/ablation-analysis.txt
word embedding layer,has,query words,ablation-analysis,/content/training-data/question_answering/1/triples/ablation-analysis.txt
query words,not well aligned to,possible answers,ablation-analysis,/content/training-data/question_answering/1/triples/ablation-analysis.txt
possible answers,in,context,ablation-analysis,/content/training-data/question_answering/1/triples/ablation-analysis.txt
query words,such as,"When , Where and Who",ablation-analysis,/content/training-data/question_answering/1/triples/ablation-analysis.txt
Ablation analysis,has,char - level and word - level embeddings,ablation-analysis,/content/training-data/question_answering/1/triples/ablation-analysis.txt
char - level and word - level embeddings,contribute towards,model 's performance,ablation-analysis,/content/training-data/question_answering/1/triples/ablation-analysis.txt
Ablation analysis,has,proposed static attention,ablation-analysis,/content/training-data/question_answering/1/triples/ablation-analysis.txt
proposed static attention,outperforms,dynamically computed attention,ablation-analysis,/content/training-data/question_answering/1/triples/ablation-analysis.txt
dynamically computed attention,by,more than 3 points,ablation-analysis,/content/training-data/question_answering/1/triples/ablation-analysis.txt
Ablation analysis,has,C2Q attention,ablation-analysis,/content/training-data/question_answering/1/triples/ablation-analysis.txt
C2Q attention,proves to be,critical,ablation-analysis,/content/training-data/question_answering/1/triples/ablation-analysis.txt
critical,with,drop,ablation-analysis,/content/training-data/question_answering/1/triples/ablation-analysis.txt
drop,of,more than 10 points,ablation-analysis,/content/training-data/question_answering/1/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/question_answering/1/triples/model.txt
Model,introduce,Bi- Directional Attention Flow ( BIDAF ) network,model,/content/training-data/question_answering/1/triples/model.txt
Bi- Directional Attention Flow ( BIDAF ) network,has,hierarchical multi-stage architecture,model,/content/training-data/question_answering/1/triples/model.txt
hierarchical multi-stage architecture,for modeling,representations,model,/content/training-data/question_answering/1/triples/model.txt
representations,at,different levels of granularity,model,/content/training-data/question_answering/1/triples/model.txt
representations,of,context paragraph,model,/content/training-data/question_answering/1/triples/model.txt
Model,use,attention mechanisms,model,/content/training-data/question_answering/1/triples/model.txt
attention mechanisms,in,both directions,model,/content/training-data/question_answering/1/triples/model.txt
both directions,which provide,complimentary information,model,/content/training-data/question_answering/1/triples/model.txt
complimentary information,to,each other,model,/content/training-data/question_answering/1/triples/model.txt
both directions,name,query - to - context,model,/content/training-data/question_answering/1/triples/model.txt
both directions,name,context - to - query,model,/content/training-data/question_answering/1/triples/model.txt
Model,use,memory - less attention mechanism,model,/content/training-data/question_answering/1/triples/model.txt
memory - less attention mechanism,allows,attention,model,/content/training-data/question_answering/1/triples/model.txt
attention,at,each time step,model,/content/training-data/question_answering/1/triples/model.txt
each time step,to be,unaffected,model,/content/training-data/question_answering/1/triples/model.txt
unaffected,from,incorrect attendances,model,/content/training-data/question_answering/1/triples/model.txt
incorrect attendances,at,previous time steps,model,/content/training-data/question_answering/1/triples/model.txt
memory - less attention mechanism,forces,attention layer,model,/content/training-data/question_answering/1/triples/model.txt
attention layer,to focus on,learning,model,/content/training-data/question_answering/1/triples/model.txt
learning,has,attention,model,/content/training-data/question_answering/1/triples/model.txt
attention,between,query and the context,model,/content/training-data/question_answering/1/triples/model.txt
memory - less attention mechanism,enables,modeling layer,model,/content/training-data/question_answering/1/triples/model.txt
modeling layer,to focus on,learning,model,/content/training-data/question_answering/1/triples/model.txt
learning,has,interaction,model,/content/training-data/question_answering/1/triples/model.txt
interaction,within,query - aware context representation,model,/content/training-data/question_answering/1/triples/model.txt
Model,has,BIDAF,model,/content/training-data/question_answering/1/triples/model.txt
BIDAF,includes,"character - level , word - level , and contextual embeddings",model,/content/training-data/question_answering/1/triples/model.txt
BIDAF,uses,bi-directional attention flow,model,/content/training-data/question_answering/1/triples/model.txt
bi-directional attention flow,to obtain,query - aware context representation,model,/content/training-data/question_answering/1/triples/model.txt
Model,has,attention,model,/content/training-data/question_answering/1/triples/model.txt
attention,has,attended vector,model,/content/training-data/question_answering/1/triples/model.txt
attended vector,allowed to,flow through,model,/content/training-data/question_answering/1/triples/model.txt
flow through,to,subsequent modeling layer,model,/content/training-data/question_answering/1/triples/model.txt
attended vector,at,each time step,model,/content/training-data/question_answering/1/triples/model.txt
attention,computed for,every time step,model,/content/training-data/question_answering/1/triples/model.txt
Contribution,has research problem,MACHINE COMPREHENSION,research-problem,/content/training-data/question_answering/1/triples/research-problem.txt
Contribution,has research problem,Machine comprehension ( MC ,research-problem,/content/training-data/question_answering/1/triples/research-problem.txt
Contribution,has research problem,MC,research-problem,/content/training-data/question_answering/1/triples/research-problem.txt
Contribution,has research problem,question answering ( QA ,research-problem,/content/training-data/question_answering/1/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
Experimental setup,At,test time,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
test time,are used,moving averages,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
moving averages,instead of,raw weights,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
Experimental setup,use,100 1D filters,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
100 1D filters,with,width of 5,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
100 1D filters,for,CNN char embedding,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
Experimental setup,use,"AdaDelta ( Zeiler , 2012 ) optimizer",experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
"AdaDelta ( Zeiler , 2012 ) optimizer",with,minibatch size,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
minibatch size,of,60,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
"AdaDelta ( Zeiler , 2012 ) optimizer",with,initial learning rate,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
initial learning rate,of,0.5,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
initial learning rate,for,12 epochs,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
Experimental setup,has,Each paragraph and question,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
Each paragraph and question,tokenized by,regular - expression - based word tokenizer ( PTB Tokenizer ,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
Each paragraph and question,fed into,model,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
Experimental setup,has,dropout ) rate,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
dropout ) rate,of,0.2,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
0.2,used for,CNN,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
0.2,used for,all LSTM layers,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
0.2,used for,linear transformation,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
linear transformation,before,softmax,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
softmax,for,answers,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
Experimental setup,has,training process,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
training process,takes,roughly 20 hours,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
roughly 20 hours,on,single Titan X GPU,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
Experimental setup,has,The model,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
The model,has about,2.6 million parameters,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
Experimental setup,has,hidden state size ( d ,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
hidden state size ( d ),of,model,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
model,is,100,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
Experimental setup,During,training,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
training,has,moving averages,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
moving averages,of,all weights,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
all weights,of,model,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
all weights,maintained with,exponential decay rate,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
exponential decay rate,of,0.999,experimental-setup,/content/training-data/question_answering/1/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/question_answering/1/triples/results.txt
Results,has,BIDAF ( ensemble ,results,/content/training-data/question_answering/1/triples/results.txt
BIDAF ( ensemble ),achieves,F 1 score,results,/content/training-data/question_answering/1/triples/results.txt
F 1 score,of,81.1,results,/content/training-data/question_answering/1/triples/results.txt
BIDAF ( ensemble ),achieves,EM score,results,/content/training-data/question_answering/1/triples/results.txt
EM score,of,73.3,results,/content/training-data/question_answering/1/triples/results.txt
Contribution,Code,https://github.com/enigmaeth/skip-thought-gan,code,/content/training-data/text_generation/4/triples/code.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/text_generation/4/triples/hyperparameters.txt
Hyperparameters,has,combine - skip vectors,hyperparameters,/content/training-data/text_generation/4/triples/hyperparameters.txt
combine - skip vectors,with,last 2400 bi-skip model,hyperparameters,/content/training-data/text_generation/4/triples/hyperparameters.txt
last 2400 bi-skip model,found to be,best performing in the experiments,hyperparameters,/content/training-data/text_generation/4/triples/hyperparameters.txt
combine - skip vectors,with,first 2400 dimensions,hyperparameters,/content/training-data/text_generation/4/triples/hyperparameters.txt
first 2400 dimensions,being,uni-skip model,hyperparameters,/content/training-data/text_generation/4/triples/hyperparameters.txt
Hyperparameters,has,Skip - Thought encoder,hyperparameters,/content/training-data/text_generation/4/triples/hyperparameters.txt
Skip - Thought encoder,encodes,sentences,hyperparameters,/content/training-data/text_generation/4/triples/hyperparameters.txt
sentences,with,length,hyperparameters,/content/training-data/text_generation/4/triples/hyperparameters.txt
length,less than,30 words,hyperparameters,/content/training-data/text_generation/4/triples/hyperparameters.txt
sentences,using,2400 GRU units,hyperparameters,/content/training-data/text_generation/4/triples/hyperparameters.txt
2400 GRU units,with,word vector,hyperparameters,/content/training-data/text_generation/4/triples/hyperparameters.txt
word vector,dimensionality of,620,hyperparameters,/content/training-data/text_generation/4/triples/hyperparameters.txt
620,to produce,4800 - dimensional combineskip vectors,hyperparameters,/content/training-data/text_generation/4/triples/hyperparameters.txt
Contribution,has research problem,Generating Text,research-problem,/content/training-data/text_generation/4/triples/research-problem.txt
Contribution,has research problem,text generation,research-problem,/content/training-data/text_generation/4/triples/research-problem.txt
Contribution,has research problem,natural language text generation,research-problem,/content/training-data/text_generation/4/triples/research-problem.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/text_generation/3/triples/hyperparameters.txt
Hyperparameters,set,hidden size,hyperparameters,/content/training-data/text_generation/3/triples/hyperparameters.txt
hidden size,has,512,hyperparameters,/content/training-data/text_generation/3/triples/hyperparameters.txt
Hyperparameters,set,vocabulary size,hyperparameters,/content/training-data/text_generation/3/triples/hyperparameters.txt
vocabulary size,has,40 K,hyperparameters,/content/training-data/text_generation/3/triples/hyperparameters.txt
Hyperparameters,set,embedding size,hyperparameters,/content/training-data/text_generation/3/triples/hyperparameters.txt
embedding size,has,64,hyperparameters,/content/training-data/text_generation/3/triples/hyperparameters.txt
Hyperparameters,set,maximum length,hyperparameters,/content/training-data/text_generation/3/triples/hyperparameters.txt
maximum length,to,15 words,hyperparameters,/content/training-data/text_generation/3/triples/hyperparameters.txt
15 words,for,each generated sentence,hyperparameters,/content/training-data/text_generation/3/triples/hyperparameters.txt
Hyperparameters,has,initial learning rate,hyperparameters,/content/training-data/text_generation/3/triples/hyperparameters.txt
initial learning rate,has,0.002,hyperparameters,/content/training-data/text_generation/3/triples/hyperparameters.txt
Hyperparameters,has,model,hyperparameters,/content/training-data/text_generation/3/triples/hyperparameters.txt
model,trained in,minibatches,hyperparameters,/content/training-data/text_generation/3/triples/hyperparameters.txt
minibatches,with,batch size,hyperparameters,/content/training-data/text_generation/3/triples/hyperparameters.txt
batch size,of,256,hyperparameters,/content/training-data/text_generation/3/triples/hyperparameters.txt
Hyperparameters,has,parameters,hyperparameters,/content/training-data/text_generation/3/triples/hyperparameters.txt
parameters,initialized by,sampling,hyperparameters,/content/training-data/text_generation/3/triples/hyperparameters.txt
sampling,from,"uniform distribution ( [? 0.1 , 0.1 ] ",hyperparameters,/content/training-data/text_generation/3/triples/hyperparameters.txt
parameters,updated by,"Adam algorithm ( Kingma and Ba , 2014 ",hyperparameters,/content/training-data/text_generation/3/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/text_generation/3/triples/model.txt
Model,given,utterance - level representations,model,/content/training-data/text_generation/3/triples/model.txt
utterance - level representations,has,mapping module,model,/content/training-data/text_generation/3/triples/model.txt
mapping module,taught to learn,utterance - level dependency,model,/content/training-data/text_generation/3/triples/model.txt
Model,use,two auto- encoders,model,/content/training-data/text_generation/3/triples/model.txt
two auto- encoders,to learn,semantic representations,model,/content/training-data/text_generation/3/triples/model.txt
semantic representations,of,inputs and responses,model,/content/training-data/text_generation/3/triples/model.txt
inputs and responses,in,unsupervised style,model,/content/training-data/text_generation/3/triples/model.txt
Model,propose,novel Auto - Encoder Matching model,model,/content/training-data/text_generation/3/triples/model.txt
novel Auto - Encoder Matching model,to learn,utterance - level dependency,model,/content/training-data/text_generation/3/triples/model.txt
Contribution,has research problem,Dialogue Generation,research-problem,/content/training-data/text_generation/3/triples/research-problem.txt
Contribution,has research problem,Automatic dialogue generation,research-problem,/content/training-data/text_generation/3/triples/research-problem.txt
Contribution,has research problem,conversation generation,research-problem,/content/training-data/text_generation/3/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/text_generation/3/triples/results.txt
Results,demonstrates,effectiveness of utterance - level dependency,results,/content/training-data/text_generation/3/triples/results.txt
effectiveness of utterance - level dependency,on improving,quality of generated text,results,/content/training-data/text_generation/3/triples/results.txt
Results,noticed that,attention mechanism,results,/content/training-data/text_generation/3/triples/results.txt
attention mechanism,performs,almost the same,results,/content/training-data/text_generation/3/triples/results.txt
almost the same,compared to,AEM model,results,/content/training-data/text_generation/3/triples/results.txt
Results,find,AEM model,results,/content/training-data/text_generation/3/triples/results.txt
AEM model,achieves,significant improvement,results,/content/training-data/text_generation/3/triples/results.txt
significant improvement,on,diversity of generated text,results,/content/training-data/text_generation/3/triples/results.txt
Results,of,human evaluation,results,/content/training-data/text_generation/3/triples/results.txt
human evaluation,clear that,AEM model,results,/content/training-data/text_generation/3/triples/results.txt
AEM model,outperforms,Seq2Seq model,results,/content/training-data/text_generation/3/triples/results.txt
Seq2Seq model,with,large margin,results,/content/training-data/text_generation/3/triples/results.txt
human evaluation,interesting to note that with,attention mechanism,results,/content/training-data/text_generation/3/triples/results.txt
attention mechanism,has,coherence,results,/content/training-data/text_generation/3/triples/results.txt
coherence,decreased slightly in,Seq2Seq model,results,/content/training-data/text_generation/3/triples/results.txt
Seq2Seq model,increased significantly in,AEM model,results,/content/training-data/text_generation/3/triples/results.txt
human evaluation,has,Pearson 's correlation coefficient,results,/content/training-data/text_generation/3/triples/results.txt
Pearson 's correlation coefficient,is,0.57,results,/content/training-data/text_generation/3/triples/results.txt
0.57,on,fluency,results,/content/training-data/text_generation/3/triples/results.txt
Pearson 's correlation coefficient,is,0.69,results,/content/training-data/text_generation/3/triples/results.txt
0.69,on,coherence,results,/content/training-data/text_generation/3/triples/results.txt
human evaluation,has,inter-annotator agreement,results,/content/training-data/text_generation/3/triples/results.txt
inter-annotator agreement,is,satisfactory,results,/content/training-data/text_generation/3/triples/results.txt
human evaluation,expected that,AEM + Attention model,results,/content/training-data/text_generation/3/triples/results.txt
AEM + Attention model,achieves,best G-score,results,/content/training-data/text_generation/3/triples/results.txt
Results,combining,two dependencies,results,/content/training-data/text_generation/3/triples/results.txt
two dependencies,has,AEM + Attention model,results,/content/training-data/text_generation/3/triples/results.txt
AEM + Attention model,achieves,best results,results,/content/training-data/text_generation/3/triples/results.txt
Results,has,proposed AEM model,results,/content/training-data/text_generation/3/triples/results.txt
proposed AEM model,significantly outperforms,Seq2Seq model,results,/content/training-data/text_generation/3/triples/results.txt
Results,improvement from,AEM model,results,/content/training-data/text_generation/3/triples/results.txt
AEM model,to,AEM + Attention model,results,/content/training-data/text_generation/3/triples/results.txt
AEM + Attention model,is,0.68 BLEU - 4 point,results,/content/training-data/text_generation/3/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/text_generation/5/triples/ablation-analysis.txt
Ablation analysis,see,SCNN - VAE - Semi,ablation-analysis,/content/training-data/text_generation/5/triples/ablation-analysis.txt
SCNN - VAE - Semi,has,best classification accuracy,ablation-analysis,/content/training-data/text_generation/5/triples/ablation-analysis.txt
best classification accuracy,of,65.5,ablation-analysis,/content/training-data/text_generation/5/triples/ablation-analysis.txt
Ablation analysis,has,LCNN - VAE - Semi,ablation-analysis,/content/training-data/text_generation/5/triples/ablation-analysis.txt
LCNN - VAE - Semi,has,best NLL result,ablation-analysis,/content/training-data/text_generation/5/triples/ablation-analysis.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
Hyperparameters,set,word embedding dimension,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
word embedding dimension,to be,512,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
Hyperparameters,select,dropout ratio,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
dropout ratio,of,LSTMs ( both encoder and decoder ,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
LSTMs ( both encoder and decoder ),from,"0.3 , 0.5",hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
Hyperparameters,explore,LSTMs and CNNs,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
LSTMs and CNNs,as,decoders,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
Hyperparameters,in,CNN decoders,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
CNN decoders,has,number of channels,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
number of channels,for,convolutions,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
convolutions,is,512 internally,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
convolutions,is,1024 externally,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
Hyperparameters,use,KL cost annealing strategy,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
KL cost annealing strategy,set,initial weight,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
initial weight,of,KL cost term,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
KL cost term,increase,linearly,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
linearly,until,given iteration T,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
KL cost term,to be,0.01,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
Hyperparameters,use,Adam,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
Adam,to optimize,all models,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
Hyperparameters,use,LSTM,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
LSTM,as,encoder,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
encoder,for,VAE,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
Hyperparameters,use,drop word,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
drop word,for,LSTM decoder,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
Hyperparameters,use,learning rate,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
learning rate,selected from,"[ 2e - 3 , 1 e - 3 , 7.5 e - 4 ]",hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
Hyperparameters,use,batch size,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
batch size,of,32,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
Hyperparameters,use,drop word ratio,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
drop word ratio,selected from,"0 , 0.3 , 0.5 , 0.7",hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
Hyperparameters,use,Gumbel - softmax,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
Gumbel - softmax,to sample,y from q ( y|x ,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
Hyperparameters,use,vocabulary size,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
vocabulary size,of,20 k,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
20 k,for,both data sets,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
Hyperparameters,find,learning rate,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
learning rate,has,1e - 3 and ?1 = 0.5,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
1e - 3 and ?1 = 0.5,to perform,best,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
Hyperparameters,For,CNN decoder,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
CNN decoder,use,dropout ratio,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
dropout ratio,of,0.1,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
0.1,at,each layer,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
Hyperparameters,For,CNNs,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
CNNs,set,convolution filter size,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
convolution filter size,to be,3,hyperparameters,/content/training-data/text_generation/5/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/text_generation/5/triples/model.txt
Model,exploit,dilated CNN,model,/content/training-data/text_generation/5/triples/model.txt
dilated CNN,for,flexibility,model,/content/training-data/text_generation/5/triples/model.txt
flexibility,in varying the amount of,conditioning context,model,/content/training-data/text_generation/5/triples/model.txt
Model,propose,dilated CNN,model,/content/training-data/text_generation/5/triples/model.txt
dilated CNN,as,decoder,model,/content/training-data/text_generation/5/triples/model.txt
decoder,in,VAE,model,/content/training-data/text_generation/5/triples/model.txt
Contribution,has research problem,Text Modeling,research-problem,/content/training-data/text_generation/5/triples/research-problem.txt
Contribution,has research problem,generative text modeling,research-problem,/content/training-data/text_generation/5/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/text_generation/5/triples/results.txt
Results,for,language modeling,results,/content/training-data/text_generation/5/triples/results.txt
language modeling,When,LCNN,results,/content/training-data/text_generation/5/triples/results.txt
LCNN,used as,decoder,results,/content/training-data/text_generation/5/triples/results.txt
decoder,obtain,optimal trade off,results,/content/training-data/text_generation/5/triples/results.txt
optimal trade off,between using,contextual information and latent representation,results,/content/training-data/text_generation/5/triples/results.txt
language modeling,For,"SCNN , MCNN and LCNN",results,/content/training-data/text_generation/5/triples/results.txt
"SCNN , MCNN and LCNN",has,VAE results,results,/content/training-data/text_generation/5/triples/results.txt
VAE results,improve over,LM results,results,/content/training-data/text_generation/5/triples/results.txt
LM results,from,338.3,results,/content/training-data/text_generation/5/triples/results.txt
338.3,to,336.2,results,/content/training-data/text_generation/5/triples/results.txt
LM results,from,335.4,results,/content/training-data/text_generation/5/triples/results.txt
335.4,to,333.9,results,/content/training-data/text_generation/5/triples/results.txt
LM results,from,345.3,results,/content/training-data/text_generation/5/triples/results.txt
345.3,to,337.8,results,/content/training-data/text_generation/5/triples/results.txt
language modeling,has,LCNN - VAE,results,/content/training-data/text_generation/5/triples/results.txt
LCNN - VAE,achieves,NLL,results,/content/training-data/text_generation/5/triples/results.txt
NLL,of,333.9,results,/content/training-data/text_generation/5/triples/results.txt
LCNN - VAE,improves over,LSTM - LM,results,/content/training-data/text_generation/5/triples/results.txt
LSTM - LM,with,NLL,results,/content/training-data/text_generation/5/triples/results.txt
NLL,of,334.9,results,/content/training-data/text_generation/5/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/text_generation/0/triples/baselines.txt
Baselines,has,random token generation,baselines,/content/training-data/text_generation/0/triples/baselines.txt
Baselines,has,MLE trained LSTM G,baselines,/content/training-data/text_generation/0/triples/baselines.txt
Baselines,has,scheduled sampling,baselines,/content/training-data/text_generation/0/triples/baselines.txt
Baselines,has,Policy Gradient with BLEU ( PG - BLEU ,baselines,/content/training-data/text_generation/0/triples/baselines.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
Hyperparameters,In,SeqGAN algorithm,hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
SeqGAN algorithm,has,training set,hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
training set,for,discriminator,hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
discriminator,is comprised by,generated examples,hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
generated examples,with,label 0,hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
generated examples,with,instances,hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
instances,from,S,hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
S,with,label,hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
Hyperparameters,In,scheduled sampling,hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
scheduled sampling,training process,gradually changes,hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
gradually changes,from,fully guided scheme feeding the true previous tokens,hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
fully guided scheme feeding the true previous tokens,into,LSTM,hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
LSTM,towards,less guided scheme which mostly feeds the LSTM,hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
less guided scheme which mostly feeds the LSTM,with,generated tokens,hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
Hyperparameters,in,our synthetic data experiments,hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
our synthetic data experiments,has,kernel size,hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
kernel size,from,1 to T,hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
our synthetic data experiments,has,number,hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
number,of,each kernel size,hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
each kernel size,between,100 to 200,hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
Hyperparameters,has,Dropout ) and L2 regularization,hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
Dropout ) and L2 regularization,to avoid,over-fitting,hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
Hyperparameters,has,curriculum rate ?,hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
curriculum rate ?,to control,probability,hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
probability,of,replacing the true tokens,hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
replacing the true tokens,with,generated ones,hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
Hyperparameters,first initialize,parameters,hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
parameters,of an,LSTM network,hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
LSTM network,following,"normal distribution N ( 0 , 1 ",hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
"normal distribution N ( 0 , 1 )",as,oracle,hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
oracle,describing,"real data distribution G oracle ( x t |x 1 , . . . , x t?1 ",hyperparameters,/content/training-data/text_generation/0/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/text_generation/0/triples/model.txt
Model,employ,discriminator,model,/content/training-data/text_generation/0/triples/model.txt
discriminator,feedback,evaluation,model,/content/training-data/text_generation/0/triples/model.txt
evaluation,to guide,learning,model,/content/training-data/text_generation/0/triples/model.txt
learning,of,generative model,model,/content/training-data/text_generation/0/triples/model.txt
discriminator,to evaluate,sequence,model,/content/training-data/text_generation/0/triples/model.txt
Model,In,policy gradient,model,/content/training-data/text_generation/0/triples/model.txt
policy gradient,employ,Monte Carlo ( MC ) search,model,/content/training-data/text_generation/0/triples/model.txt
Monte Carlo ( MC ) search,to approximate,state - action value,model,/content/training-data/text_generation/0/triples/model.txt
Model,regard,generative model,model,/content/training-data/text_generation/0/triples/model.txt
generative model,as a,stochastic parametrized policy,model,/content/training-data/text_generation/0/triples/model.txt
Model,consider,sequence generation procedure,model,/content/training-data/text_generation/0/triples/model.txt
sequence generation procedure,as,sequential decision making process,model,/content/training-data/text_generation/0/triples/model.txt
Model,has,generative model,model,/content/training-data/text_generation/0/triples/model.txt
generative model,treated as,agent of reinforcement learning ( RL ,model,/content/training-data/text_generation/0/triples/model.txt
generative model,has,action,model,/content/training-data/text_generation/0/triples/model.txt
action,is,next token to be generated,model,/content/training-data/text_generation/0/triples/model.txt
generative model,has,state,model,/content/training-data/text_generation/0/triples/model.txt
state,is,generated tokens so far,model,/content/training-data/text_generation/0/triples/model.txt
Model,directly train,policy ( generative model ,model,/content/training-data/text_generation/0/triples/model.txt
policy ( generative model ),via,policy gradient,model,/content/training-data/text_generation/0/triples/model.txt
policy gradient,naturally avoids,differentiation difficulty,model,/content/training-data/text_generation/0/triples/model.txt
differentiation difficulty,for,discrete data,model,/content/training-data/text_generation/0/triples/model.txt
discrete data,in a,conventional GAN,model,/content/training-data/text_generation/0/triples/model.txt
Contribution,has research problem,generating real - valued data,research-problem,/content/training-data/text_generation/0/triples/research-problem.txt
Contribution,has research problem,generating sequences of discrete tokens,research-problem,/content/training-data/text_generation/0/triples/research-problem.txt
Contribution,has research problem,Generating sequential synthetic data,research-problem,/content/training-data/text_generation/0/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/text_generation/0/triples/results.txt
Results,see,impact of SeqGAN,results,/content/training-data/text_generation/0/triples/results.txt
impact of SeqGAN,outperforms,other baselines significantly,results,/content/training-data/text_generation/0/triples/results.txt
Results,indicates,prospect of applying adversarial training strategies,results,/content/training-data/text_generation/0/triples/results.txt
prospect of applying adversarial training strategies,to,discrete sequence generative models,results,/content/training-data/text_generation/0/triples/results.txt
discrete sequence generative models,to breakthrough,limitations of MLE,results,/content/training-data/text_generation/0/triples/results.txt
Results,has,SeqGAN,results,/content/training-data/text_generation/0/triples/results.txt
SeqGAN,outperforms,PG - BLEU,results,/content/training-data/text_generation/0/triples/results.txt
Results,has,significance T - test,results,/content/training-data/text_generation/0/triples/results.txt
significance T - test,on,NLL oracle score distribution,results,/content/training-data/text_generation/0/triples/results.txt
NLL oracle score distribution,of,generated sequences,results,/content/training-data/text_generation/0/triples/results.txt
generated sequences,from,compared models,results,/content/training-data/text_generation/0/triples/results.txt
compared models,demonstrates,significant improvement,results,/content/training-data/text_generation/0/triples/results.txt
significant improvement,of,SeqGAN,results,/content/training-data/text_generation/0/triples/results.txt
SeqGAN,over all,compared models,results,/content/training-data/text_generation/0/triples/results.txt
Results,After about,150 training epochs,results,/content/training-data/text_generation/0/triples/results.txt
150 training epochs,both,maximum likelihood estimation and the schedule sampling methods,results,/content/training-data/text_generation/0/triples/results.txt
maximum likelihood estimation and the schedule sampling methods,converge to,relatively high NLL oracle score,results,/content/training-data/text_generation/0/triples/results.txt
relatively high NLL oracle score,has,SeqGAN,results,/content/training-data/text_generation/0/triples/results.txt
SeqGAN,improve the limit of,generator,results,/content/training-data/text_generation/0/triples/results.txt
generator,with,same structure,results,/content/training-data/text_generation/0/triples/results.txt
same structure,as,baselines,results,/content/training-data/text_generation/0/triples/results.txt
Contribution,has,Approach,approach,/content/training-data/text_generation/2/triples/approach.txt
Approach,given,goal embedding,approach,/content/training-data/text_generation/2/triples/approach.txt
goal embedding,produced by,MAN - AGER,approach,/content/training-data/text_generation/2/triples/approach.txt
MAN - AGER,has,WORKER,approach,/content/training-data/text_generation/2/triples/approach.txt
WORKER,then combines,output,approach,/content/training-data/text_generation/2/triples/approach.txt
output,of,the LSTM and the goal embedding,approach,/content/training-data/text_generation/2/triples/approach.txt
output,to take,final action,approach,/content/training-data/text_generation/2/triples/approach.txt
final action,at,current state,approach,/content/training-data/text_generation/2/triples/approach.txt
WORKER,first encodes,current generated words,approach,/content/training-data/text_generation/2/triples/approach.txt
current generated words,with,another LSTM,approach,/content/training-data/text_generation/2/triples/approach.txt
Approach,called,Leak GAN,approach,/content/training-data/text_generation/2/triples/approach.txt
Leak GAN,to address,both the non-informativeness and the sparsity issues,approach,/content/training-data/text_generation/2/triples/approach.txt
Approach,introduce,hierarchical generator G,approach,/content/training-data/text_generation/2/triples/approach.txt
hierarchical generator G,consists of,high - level MANAGER module,approach,/content/training-data/text_generation/2/triples/approach.txt
hierarchical generator G,consists of,low - level WORKER module,approach,/content/training-data/text_generation/2/triples/approach.txt
Approach,has,MANAGER,approach,/content/training-data/text_generation/2/triples/approach.txt
MANAGER,along,shortterm memory network ( LSTM ,approach,/content/training-data/text_generation/2/triples/approach.txt
MANAGER,serves as,mediator,approach,/content/training-data/text_generation/2/triples/approach.txt
MANAGER,receives,generator D 's high - level feature representation,approach,/content/training-data/text_generation/2/triples/approach.txt
generator D 's high - level feature representation,form,guiding goal,approach,/content/training-data/text_generation/2/triples/approach.txt
guiding goal,for,WORKER module,approach,/content/training-data/text_generation/2/triples/approach.txt
Approach,has,LeakGAN,approach,/content/training-data/text_generation/2/triples/approach.txt
LeakGAN,providing,richer information,approach,/content/training-data/text_generation/2/triples/approach.txt
richer information,from,discriminator,approach,/content/training-data/text_generation/2/triples/approach.txt
discriminator,to,generator,approach,/content/training-data/text_generation/2/triples/approach.txt
generator,by borrowing,recent advances,approach,/content/training-data/text_generation/2/triples/approach.txt
recent advances,in,hierarchical reinforcement learning,approach,/content/training-data/text_generation/2/triples/approach.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/text_generation/2/triples/hyperparameters.txt
Hyperparameters,has,GAN Setting,hyperparameters,/content/training-data/text_generation/2/triples/hyperparameters.txt
GAN Setting,For,synthetic data experiment,hyperparameters,/content/training-data/text_generation/2/triples/hyperparameters.txt
synthetic data experiment,has,CNN kernel size,hyperparameters,/content/training-data/text_generation/2/triples/hyperparameters.txt
CNN kernel size,ranges,from 1 to T,hyperparameters,/content/training-data/text_generation/2/triples/hyperparameters.txt
GAN Setting,For,generator,hyperparameters,/content/training-data/text_generation/2/triples/hyperparameters.txt
generator,adopt,LSTM,hyperparameters,/content/training-data/text_generation/2/triples/hyperparameters.txt
LSTM,as,architectures of MANAGER and WORKER,hyperparameters,/content/training-data/text_generation/2/triples/hyperparameters.txt
architectures of MANAGER and WORKER,to capture,sequence context information,hyperparameters,/content/training-data/text_generation/2/triples/hyperparameters.txt
GAN Setting,number of,each kernel,hyperparameters,/content/training-data/text_generation/2/triples/hyperparameters.txt
each kernel,between,100 and 200,hyperparameters,/content/training-data/text_generation/2/triples/hyperparameters.txt
GAN Setting,choose,CNN architecture,hyperparameters,/content/training-data/text_generation/2/triples/hyperparameters.txt
CNN architecture,as,feature extractor and the binary classifier,hyperparameters,/content/training-data/text_generation/2/triples/hyperparameters.txt
GAN Setting,has,feature of text,hyperparameters,/content/training-data/text_generation/2/triples/hyperparameters.txt
feature of text,is,"1,720 dimensional vector",hyperparameters,/content/training-data/text_generation/2/triples/hyperparameters.txt
GAN Setting,has,MANAGER,hyperparameters,/content/training-data/text_generation/2/triples/hyperparameters.txt
MANAGER,produces,16 - dimensional goal embedding feature vector,hyperparameters,/content/training-data/text_generation/2/triples/hyperparameters.txt
16 - dimensional goal embedding feature vector,using,feature map,hyperparameters,/content/training-data/text_generation/2/triples/hyperparameters.txt
feature map,extracted by,CNN,hyperparameters,/content/training-data/text_generation/2/triples/hyperparameters.txt
GAN Setting,has,goal duration time c,hyperparameters,/content/training-data/text_generation/2/triples/hyperparameters.txt
goal duration time c,is a,hyperparameter,hyperparameters,/content/training-data/text_generation/2/triples/hyperparameters.txt
hyperparameter,set as,4,hyperparameters,/content/training-data/text_generation/2/triples/hyperparameters.txt
4,after,some preliminary experiments,hyperparameters,/content/training-data/text_generation/2/triples/hyperparameters.txt
GAN Setting,has,Dropout,hyperparameters,/content/training-data/text_generation/2/triples/hyperparameters.txt
Dropout,performed to avoid,overfitting,hyperparameters,/content/training-data/text_generation/2/triples/hyperparameters.txt
overfitting,with,keep rate 0.75,hyperparameters,/content/training-data/text_generation/2/triples/hyperparameters.txt
overfitting,with,L2 regularization,hyperparameters,/content/training-data/text_generation/2/triples/hyperparameters.txt
Contribution,has research problem,Text Generation,research-problem,/content/training-data/text_generation/2/triples/research-problem.txt
Contribution,has research problem,Automatically generating coherent and semantically meaningful text,research-problem,/content/training-data/text_generation/2/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/text_generation/2/triples/results.txt
Results,has,Short Text Generation : Chinese Poems,results,/content/training-data/text_generation/2/triples/results.txt
Short Text Generation : Chinese Poems,indicate,LeakGAN,results,/content/training-data/text_generation/2/triples/results.txt
LeakGAN,successfully handles,short text generation tasks,results,/content/training-data/text_generation/2/triples/results.txt
Results,has,Turing Test and Generated Samples,results,/content/training-data/text_generation/2/triples/results.txt
Turing Test and Generated Samples,has,performance,results,/content/training-data/text_generation/2/triples/results.txt
performance,on,two datasets,results,/content/training-data/text_generation/2/triples/results.txt
two datasets,indicates that,generated sentences,results,/content/training-data/text_generation/2/triples/results.txt
generated sentences,of,Leak GAN,results,/content/training-data/text_generation/2/triples/results.txt
Leak GAN,than those of,SeqGAN,results,/content/training-data/text_generation/2/triples/results.txt
SeqGAN,of,higher global consistency,results,/content/training-data/text_generation/2/triples/results.txt
SeqGAN,of,better readability,results,/content/training-data/text_generation/2/triples/results.txt
Results,has,Long Text Generation : EMNLP2017 WMT News,results,/content/training-data/text_generation/2/triples/results.txt
Long Text Generation : EMNLP2017 WMT News,In,all measured metrics,results,/content/training-data/text_generation/2/triples/results.txt
all measured metrics,has,LeakGAN,results,/content/training-data/text_generation/2/triples/results.txt
LeakGAN,shows,significant performance gain,results,/content/training-data/text_generation/2/triples/results.txt
significant performance gain,compared to,baseline models,results,/content/training-data/text_generation/2/triples/results.txt
Results,has,Synthetic Data Experiments,results,/content/training-data/text_generation/2/triples/results.txt
Synthetic Data Experiments,In,adversarial training stage,results,/content/training-data/text_generation/2/triples/results.txt
adversarial training stage,has,Leak GAN,results,/content/training-data/text_generation/2/triples/results.txt
Leak GAN,shows,better speed of convergence,results,/content/training-data/text_generation/2/triples/results.txt
Leak GAN,explores,local minimum,results,/content/training-data/text_generation/2/triples/results.txt
local minimum,is,significantly better,results,/content/training-data/text_generation/2/triples/results.txt
significantly better,than,previous results,results,/content/training-data/text_generation/2/triples/results.txt
Synthetic Data Experiments,In,pre-training stage,results,/content/training-data/text_generation/2/triples/results.txt
pre-training stage,has,LeakGAN,results,/content/training-data/text_generation/2/triples/results.txt
LeakGAN,already shown,observable performance superiority,results,/content/training-data/text_generation/2/triples/results.txt
observable performance superiority,compared to,other models,results,/content/training-data/text_generation/2/triples/results.txt
Results,has,Middle Text Generation : COCO Image Captions,results,/content/training-data/text_generation/2/triples/results.txt
Middle Text Generation : COCO Image Captions,of,BLEU scores,results,/content/training-data/text_generation/2/triples/results.txt
BLEU scores,on,COCO dataset,results,/content/training-data/text_generation/2/triples/results.txt
COCO dataset,indicate,Leak GAN,results,/content/training-data/text_generation/2/triples/results.txt
Leak GAN,performs,significantly better,results,/content/training-data/text_generation/2/triples/results.txt
significantly better,than,baseline models,results,/content/training-data/text_generation/2/triples/results.txt
baseline models,in,mid-length text generation task,results,/content/training-data/text_generation/2/triples/results.txt
Contribution,has,Model,model,/content/training-data/text_generation/1/triples/model.txt
Model,proposed,new adversarial network,model,/content/training-data/text_generation/1/triples/model.txt
new adversarial network,consists of,two neural network models,model,/content/training-data/text_generation/1/triples/model.txt
two neural network models,name,a generator and a ranker,model,/content/training-data/text_generation/1/triples/model.txt
Model,suitable for,language learning,model,/content/training-data/text_generation/1/triples/model.txt
language learning,in comparison to,conventional GANs,model,/content/training-data/text_generation/1/triples/model.txt
Model,relax,training,model,/content/training-data/text_generation/1/triples/model.txt
training,of,discriminator,model,/content/training-data/text_generation/1/triples/model.txt
training,to,learning - to - rank optimization problem,model,/content/training-data/text_generation/1/triples/model.txt
Model,learns,model,model,/content/training-data/text_generation/1/triples/model.txt
model,from,relative ranking information,model,/content/training-data/text_generation/1/triples/model.txt
relative ranking information,between,the machine - written and the human - written sentences,model,/content/training-data/text_generation/1/triples/model.txt
the machine - written and the human - written sentences,in,adversarial framework,model,/content/training-data/text_generation/1/triples/model.txt
Model,propose,novel adversarial learning framework,model,/content/training-data/text_generation/1/triples/model.txt
novel adversarial learning framework,name,RankGAN,model,/content/training-data/text_generation/1/triples/model.txt
Model,propose,train,model,/content/training-data/text_generation/1/triples/model.txt
train,has,ranker,model,/content/training-data/text_generation/1/triples/model.txt
ranker,to rank,machine - written sentences,model,/content/training-data/text_generation/1/triples/model.txt
machine - written sentences,with respect to,reference sentence,model,/content/training-data/text_generation/1/triples/model.txt
reference sentence,which is,human-written,model,/content/training-data/text_generation/1/triples/model.txt
machine - written sentences,lower than,human - written sentences,model,/content/training-data/text_generation/1/triples/model.txt
Model,During,learning,model,/content/training-data/text_generation/1/triples/model.txt
learning,adopt,policy gradient technique,model,/content/training-data/text_generation/1/triples/model.txt
policy gradient technique,to overcome,non-differentiable problem,model,/content/training-data/text_generation/1/triples/model.txt
Model,viewing,set of data samples collectively,model,/content/training-data/text_generation/1/triples/model.txt
set of data samples collectively,evaluating,quality,model,/content/training-data/text_generation/1/triples/model.txt
quality,through,relative ranking,model,/content/training-data/text_generation/1/triples/model.txt
quality,of,samples,model,/content/training-data/text_generation/1/triples/model.txt
samples,has,discriminator,model,/content/training-data/text_generation/1/triples/model.txt
discriminator,able to make,better assessment,model,/content/training-data/text_generation/1/triples/model.txt
better assessment,helps,generator,model,/content/training-data/text_generation/1/triples/model.txt
generator,to learn,better,model,/content/training-data/text_generation/1/triples/model.txt
Model,train,generator,model,/content/training-data/text_generation/1/triples/model.txt
generator,so that,machine - written sentences,model,/content/training-data/text_generation/1/triples/model.txt
machine - written sentences,ranked higher than,human - written sentences,model,/content/training-data/text_generation/1/triples/model.txt
generator,to synthesize,sentences,model,/content/training-data/text_generation/1/triples/model.txt
sentences,which confuse,ranker,model,/content/training-data/text_generation/1/triples/model.txt
Contribution,has research problem,Language Generation,research-problem,/content/training-data/text_generation/1/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/text_generation/1/triples/results.txt
Results,on,synthetic data,results,/content/training-data/text_generation/1/triples/results.txt
synthetic data,has,"MLE , PG - BLEU and SeqGAN",results,/content/training-data/text_generation/1/triples/results.txt
"MLE , PG - BLEU and SeqGAN",tend to,converge,results,/content/training-data/text_generation/1/triples/results.txt
converge,after,200 training epochs,results,/content/training-data/text_generation/1/triples/results.txt
synthetic data,has,proposed RankGAN,results,/content/training-data/text_generation/1/triples/results.txt
proposed RankGAN,consistently improves,language generator,results,/content/training-data/text_generation/1/triples/results.txt
proposed RankGAN,achieves,relatively lower,results,/content/training-data/text_generation/1/triples/results.txt
relatively lower,has,NLL score,results,/content/training-data/text_generation/1/triples/results.txt
synthetic data,has,RankGAN,results,/content/training-data/text_generation/1/triples/results.txt
RankGAN,performs,more favourably,results,/content/training-data/text_generation/1/triples/results.txt
more favourably,against,compared methods,results,/content/training-data/text_generation/1/triples/results.txt
synthetic data,worth noting,proposed RankGAN,results,/content/training-data/text_generation/1/triples/results.txt
proposed RankGAN,achieves,better performance,results,/content/training-data/text_generation/1/triples/results.txt
better performance,than,PG - BLEU,results,/content/training-data/text_generation/1/triples/results.txt
Results,on,Chinese poems composition,results,/content/training-data/text_generation/1/triples/results.txt
Chinese poems composition,seen that,proposed Rank GAN,results,/content/training-data/text_generation/1/triples/results.txt
proposed Rank GAN,performs,more favourably,results,/content/training-data/text_generation/1/triples/results.txt
more favourably,compared to,state - of - the - art methods,results,/content/training-data/text_generation/1/triples/results.txt
state - of - the - art methods,in terms of,BLEU - 2 score,results,/content/training-data/text_generation/1/triples/results.txt
Chinese poems composition,estimate,similarity,results,/content/training-data/text_generation/1/triples/results.txt
similarity,between,human - written poem and the machine - created one,results,/content/training-data/text_generation/1/triples/results.txt
Chinese poems composition,in terms of,human evaluation score,results,/content/training-data/text_generation/1/triples/results.txt
human evaluation score,has,RankGAN,results,/content/training-data/text_generation/1/triples/results.txt
RankGAN,outperforms,compared method,results,/content/training-data/text_generation/1/triples/results.txt
Results,on,Shakespeare 's plays,results,/content/training-data/text_generation/1/triples/results.txt
Shakespeare 's plays,has,proposed RankGAN,results,/content/training-data/text_generation/1/triples/results.txt
proposed RankGAN,is able to capture,transition pattern,results,/content/training-data/text_generation/1/triples/results.txt
transition pattern,among,words,results,/content/training-data/text_generation/1/triples/results.txt
Shakespeare 's plays,has,proposed method,results,/content/training-data/text_generation/1/triples/results.txt
proposed method,achieves,consistently higher BLEU score,results,/content/training-data/text_generation/1/triples/results.txt
consistently higher BLEU score,than,other methods,results,/content/training-data/text_generation/1/triples/results.txt
other methods,in terms of,different n-grams criteria,results,/content/training-data/text_generation/1/triples/results.txt
Results,on,COCO image captions,results,/content/training-data/text_generation/1/triples/results.txt
COCO image captions,has,human - written sentences,results,/content/training-data/text_generation/1/triples/results.txt
human - written sentences,get,highest score,results,/content/training-data/text_generation/1/triples/results.txt
highest score,comparing to,language models,results,/content/training-data/text_generation/1/triples/results.txt
human - written sentences,Among,GANs approaches,results,/content/training-data/text_generation/1/triples/results.txt
GANs approaches,has,RankGAN,results,/content/training-data/text_generation/1/triples/results.txt
RankGAN,receives,better score,results,/content/training-data/text_generation/1/triples/results.txt
better score,than,SeqGAN,results,/content/training-data/text_generation/1/triples/results.txt
COCO image captions,has,RankGAN,results,/content/training-data/text_generation/1/triples/results.txt
RankGAN,achieves,better performance,results,/content/training-data/text_generation/1/triples/results.txt
better performance,than,other methods,results,/content/training-data/text_generation/1/triples/results.txt
other methods,in terms of,different BLEU scores,results,/content/training-data/text_generation/1/triples/results.txt
COCO image captions,has,our model,results,/content/training-data/text_generation/1/triples/results.txt
our model,able to generate,"fluent , novel sentences",results,/content/training-data/text_generation/1/triples/results.txt
"fluent , novel sentences",not existing in,training set,results,/content/training-data/text_generation/1/triples/results.txt
Contribution,has,Approach,approach,/content/training-data/negation_scope_resolution/0/triples/approach.txt
Approach,explore,design choices,approach,/content/training-data/negation_scope_resolution/0/triples/approach.txt
Approach,apply,BERT,approach,/content/training-data/negation_scope_resolution/0/triples/approach.txt
BERT,to,negation detection and scope resolution,approach,/content/training-data/negation_scope_resolution/0/triples/approach.txt
Approach,experiment on,3 public datasets available,approach,/content/training-data/negation_scope_resolution/0/triples/approach.txt
3 public datasets available,name,BioScope Corpus ( Abstracts and Full Papers ,approach,/content/training-data/negation_scope_resolution/0/triples/approach.txt
3 public datasets available,name,Sherlock Dataset,approach,/content/training-data/negation_scope_resolution/0/triples/approach.txt
3 public datasets available,name,SFU Review Corpus,approach,/content/training-data/negation_scope_resolution/0/triples/approach.txt
Contribution,has research problem,Negation Detection and Scope Resolution,research-problem,/content/training-data/negation_scope_resolution/0/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
Experimental setup,trained,models,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
models,on,free GPUs,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
free GPUs,available via,Google Colaboratory,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
Experimental setup,use,Google 's BERT,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
Google 's BERT,as,base model,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
base model,to generate,contextual embeddings,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
contextual embeddings,for,sentence,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
Experimental setup,use,F 1 score,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
F 1 score,as,early stopping metric,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
Experimental setup,use,early stopping,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
early stopping,on,dev data,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
dev data,for,6 epochs,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
6 epochs,as,tolerance,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
Experimental setup,use,Adam optimizer,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
Adam optimizer,with,initial learning rate,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
initial learning rate,of,3 e - 5,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
Experimental setup,use,vector,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
vector,of,dimension R H x N_C,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
dimension R H x N_C,to compute,scores per token,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
Experimental setup,use,default 70 - 15 - 15 split,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
default 70 - 15 - 15 split,for,train - dev - test data,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
Experimental setup,use,Categorical Cross Entropy Loss,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
Categorical Cross Entropy Loss,to avoid,training,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
training,on,padded label outputs,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
Experimental setup,perform,cue detection and scope resolution,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
cue detection and scope resolution,for,all 3 datasets,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
cue detection and scope resolution,train on,1,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
cue detection and scope resolution,test on,all datasets,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
Experimental setup,has,BERT,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
BERT,outputs,vector,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
vector,of,size R H per token of the input,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
vector,feed to,common classification layer,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
common classification layer,of,dimen-sion R Hx5,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
dimen-sion R Hx5,for,cue detection,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
common classification layer,of,R Hx2,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
R Hx2,for,scope resolution,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
Experimental setup,input to,BERT model,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
BERT model,is,sequence of tokenized and encoded tokens,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
sequence of tokenized and encoded tokens,of,sentence,experimental-setup,/content/training-data/negation_scope_resolution/0/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
Results,For,negation cue detection,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
negation cue detection,outperform,baseline systems,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
negation cue detection,trained on,BioScope Abstracts,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
BioScope Abstracts,tested on,BioScope Full Papers,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
BioScope Full Papers,observed,stateof - the - art result,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
stateof - the - art result,of,91.24,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
negation cue detection,observe,significant gap,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
significant gap,between,"our model , NegBERT , and the current state - of the - art systems",results,/content/training-data/negation_scope_resolution/0/triples/results.txt
Results,For,cue detection,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
cue detection,on,Sherlock dataset test data,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
Sherlock dataset test data,see that,outperform,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
outperform,by,0.6 F1 measure,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
outperform,has,best system,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
cue detection,On,BioScope Full papers,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
BioScope Full papers,achieve,90.43 F1,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
90.43 F1,when,training,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
training,on,same data,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
cue detection,On,SFU Review Corpus,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
SFU Review Corpus,achieve,F1,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
F1,of,87.08,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
cue detection,On,BioScope Abstracts,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
BioScope Abstracts,perform,reasonably well,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
Results,For,scope resolution,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
scope resolution,On,Bioscope Full Papers,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
Bioscope Full Papers,outperform,best architecture,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
best architecture,training on,same dataset,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
best architecture,by,2.64 F1,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
scope resolution,On,SFU Review Corpus,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
SFU Review Corpus,outperform,best system,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
best system,by,1.02 F1,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
scope resolution,On,Sherlock dataset,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
Sherlock dataset,achieve,F1,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
F1,of,92.36,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
92.36,outperforming,previous State of the Art,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
previous State of the Art,by,significant margin ( almost 3.0 F1 ,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
scope resolution,On,BioScope Abstracts,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
BioScope Abstracts,achieve,F1,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
F1,of,95.68,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
95.68,outperforming,best architecture,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
best architecture,by,3.57 F1,results,/content/training-data/negation_scope_resolution/0/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/sentence_compression/3/triples/baselines.txt
Baselines,has,dependency - tree - based method,baselines,/content/training-data/sentence_compression/3/triples/baselines.txt
dependency - tree - based method,considers,sentence compression task,baselines,/content/training-data/sentence_compression/3/triples/baselines.txt
sentence compression task,as,optimization problem,baselines,/content/training-data/sentence_compression/3/triples/baselines.txt
optimization problem,by using,integer linear programming,baselines,/content/training-data/sentence_compression/3/triples/baselines.txt
Baselines,has,long short - term memory networks ( LSTMs ,baselines,/content/training-data/sentence_compression/3/triples/baselines.txt
long short - term memory networks ( LSTMs ),showed,strong promise,baselines,/content/training-data/sentence_compression/3/triples/baselines.txt
strong promise,in,sentence compression,baselines,/content/training-data/sentence_compression/3/triples/baselines.txt
Contribution,Code,https://github.com/code4conference/code4sc,code,/content/training-data/sentence_compression/3/triples/code.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentence_compression/3/triples/hyperparameters.txt
Hyperparameters,For,policy learning,hyperparameters,/content/training-data/sentence_compression/3/triples/hyperparameters.txt
policy learning,find,policy,hyperparameters,/content/training-data/sentence_compression/3/triples/hyperparameters.txt
policy,maximizes,reward,hyperparameters,/content/training-data/sentence_compression/3/triples/hyperparameters.txt
policy learning,used,REINFORCE algorithm,hyperparameters,/content/training-data/sentence_compression/3/triples/hyperparameters.txt
REINFORCE algorithm,to update,parameters,hyperparameters,/content/training-data/sentence_compression/3/triples/hyperparameters.txt
parameters,of,policy network,hyperparameters,/content/training-data/sentence_compression/3/triples/hyperparameters.txt
Hyperparameters,has,mini - batch size,hyperparameters,/content/training-data/sentence_compression/3/triples/hyperparameters.txt
mini - batch size,chosen from,"[ 5 , 50 , 100 ]",hyperparameters,/content/training-data/sentence_compression/3/triples/hyperparameters.txt
Hyperparameters,has,learning rate,hyperparameters,/content/training-data/sentence_compression/3/triples/hyperparameters.txt
learning rate,for,neural language model,hyperparameters,/content/training-data/sentence_compression/3/triples/hyperparameters.txt
neural language model,is,2.5 e - 4,hyperparameters,/content/training-data/sentence_compression/3/triples/hyperparameters.txt
learning rate,has,1e - 05,hyperparameters,/content/training-data/sentence_compression/3/triples/hyperparameters.txt
1e - 05,for,policy network,hyperparameters,/content/training-data/sentence_compression/3/triples/hyperparameters.txt
Hyperparameters,has,Vocabulary size,hyperparameters,/content/training-data/sentence_compression/3/triples/hyperparameters.txt
Vocabulary size,was,"50,000",hyperparameters,/content/training-data/sentence_compression/3/triples/hyperparameters.txt
Hyperparameters,has,embedding size,hyperparameters,/content/training-data/sentence_compression/3/triples/hyperparameters.txt
embedding size,for,word,hyperparameters,/content/training-data/sentence_compression/3/triples/hyperparameters.txt
embedding size,for,part - of - speech tag,hyperparameters,/content/training-data/sentence_compression/3/triples/hyperparameters.txt
embedding size,for,dependency relation,hyperparameters,/content/training-data/sentence_compression/3/triples/hyperparameters.txt
embedding size,is,128,hyperparameters,/content/training-data/sentence_compression/3/triples/hyperparameters.txt
Hyperparameters,employed,vanilla RNN,hyperparameters,/content/training-data/sentence_compression/3/triples/hyperparameters.txt
vanilla RNN,with,hidden size,hyperparameters,/content/training-data/sentence_compression/3/triples/hyperparameters.txt
hidden size,of,512,hyperparameters,/content/training-data/sentence_compression/3/triples/hyperparameters.txt
hidden size,for,policy network,hyperparameters,/content/training-data/sentence_compression/3/triples/hyperparameters.txt
hidden size,for,neural language model,hyperparameters,/content/training-data/sentence_compression/3/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentence_compression/3/triples/model.txt
Model,formulate,deletionbased sentence compression,model,/content/training-data/sentence_compression/3/triples/model.txt
deletionbased sentence compression,series of,trialand - error deletion operations,model,/content/training-data/sentence_compression/3/triples/model.txt
trialand - error deletion operations,through,reinforcement learning framework,model,/content/training-data/sentence_compression/3/triples/model.txt
Model,has,neural language model,model,/content/training-data/sentence_compression/3/triples/model.txt
neural language model,to learn,correct word collocations,model,/content/training-data/sentence_compression/3/triples/model.txt
correct word collocations,in terms of,syntax,model,/content/training-data/sentence_compression/3/triples/model.txt
correct word collocations,in terms of,semantics,model,/content/training-data/sentence_compression/3/triples/model.txt
Model,has,syntax - based neural language model,model,/content/training-data/sentence_compression/3/triples/model.txt
syntax - based neural language model,trained on,large - scale datasets,model,/content/training-data/sentence_compression/3/triples/model.txt
large - scale datasets,as,readability evaluator,model,/content/training-data/sentence_compression/3/triples/model.txt
Model,has,policy network,model,/content/training-data/sentence_compression/3/triples/model.txt
policy network,to form,compression,model,/content/training-data/sentence_compression/3/triples/model.txt
compression,performs either,RETAIN,model,/content/training-data/sentence_compression/3/triples/model.txt
compression,performs either,REMOVE,model,/content/training-data/sentence_compression/3/triples/model.txt
policy network,receives,"reward ( e.g. , readability score ",model,/content/training-data/sentence_compression/3/triples/model.txt
"reward ( e.g. , readability score )",to update,network,model,/content/training-data/sentence_compression/3/triples/model.txt
Contribution,has research problem,Sentence Compression,research-problem,/content/training-data/sentence_compression/3/triples/research-problem.txt
Contribution,has research problem,deletion - based sentence compression,research-problem,/content/training-data/sentence_compression/3/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentence_compression/3/triples/results.txt
Results,applying,Evaluator - SLM,results,/content/training-data/sentence_compression/3/triples/results.txt
Evaluator - SLM,observed,only a tiny improvement,results,/content/training-data/sentence_compression/3/triples/results.txt
Results,for,Google news dataset,results,/content/training-data/sentence_compression/3/triples/results.txt
Google news dataset,with,0.2 million instances,results,/content/training-data/sentence_compression/3/triples/results.txt
Google news dataset,has,perplexity,results,/content/training-data/sentence_compression/3/triples/results.txt
perplexity,is,76.5,results,/content/training-data/sentence_compression/3/triples/results.txt
Google news dataset,has,LSTMs ( LSTM + pos+dep ,results,/content/training-data/sentence_compression/3/triples/results.txt
LSTMs ( LSTM + pos+dep ),is,relatively strong baseline,results,/content/training-data/sentence_compression/3/triples/results.txt
relatively strong baseline,incorporating,dependency relations,results,/content/training-data/sentence_compression/3/triples/results.txt
relatively strong baseline,incorporating,part - of - speech tags,results,/content/training-data/sentence_compression/3/triples/results.txt
Results,For,Gigaword dataset,results,/content/training-data/sentence_compression/3/triples/results.txt
Gigaword dataset,with,1.02 million instances,results,/content/training-data/sentence_compression/3/triples/results.txt
Gigaword dataset,has,perplexity,results,/content/training-data/sentence_compression/3/triples/results.txt
perplexity,of,language model,results,/content/training-data/sentence_compression/3/triples/results.txt
language model,is,20.3,results,/content/training-data/sentence_compression/3/triples/results.txt
Results,shows that,small improvements,results,/content/training-data/sentence_compression/3/triples/results.txt
small improvements,observed on,two datasets,results,/content/training-data/sentence_compression/3/triples/results.txt
Results,has,Evaluator - SLMbased method,results,/content/training-data/sentence_compression/3/triples/results.txt
Evaluator - SLMbased method,yields,large improvement,results,/content/training-data/sentence_compression/3/triples/results.txt
large improvement,over,baselines,results,/content/training-data/sentence_compression/3/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/sentence_compression/0/triples/baselines.txt
Baselines,has,LSTM +,baselines,/content/training-data/sentence_compression/0/triples/baselines.txt
LSTM +,incorporated,dependency parse tree information,baselines,/content/training-data/sentence_compression/0/triples/baselines.txt
dependency parse tree information,into,LSTM model,baselines,/content/training-data/sentence_compression/0/triples/baselines.txt
LSTM +,used,prediction,baselines,/content/training-data/sentence_compression/0/triples/baselines.txt
prediction,on,previous word,baselines,/content/training-data/sentence_compression/0/triples/baselines.txt
previous word,to help,prediction,baselines,/content/training-data/sentence_compression/0/triples/baselines.txt
prediction,on,current word,baselines,/content/training-data/sentence_compression/0/triples/baselines.txt
Baselines,has,LSTM,baselines,/content/training-data/sentence_compression/0/triples/baselines.txt
LSTM,is,basic LSTM - based deletion method,baselines,/content/training-data/sentence_compression/0/triples/baselines.txt
Baselines,has,Abstractive seq2seq,baselines,/content/training-data/sentence_compression/0/triples/baselines.txt
Abstractive seq2seq,is,abstractive sequence - to - sequence model,baselines,/content/training-data/sentence_compression/0/triples/baselines.txt
abstractive sequence - to - sequence model,trained on,3.8 million Gigaword title - article pairs,baselines,/content/training-data/sentence_compression/0/triples/baselines.txt
Baselines,has,Traditional ILP,baselines,/content/training-data/sentence_compression/0/triples/baselines.txt
Traditional ILP,is,ILP - based method,baselines,/content/training-data/sentence_compression/0/triples/baselines.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentence_compression/0/triples/hyperparameters.txt
Hyperparameters,utilize,open source ILP solver,hyperparameters,/content/training-data/sentence_compression/0/triples/hyperparameters.txt
Hyperparameters,has,embeddings,hyperparameters,/content/training-data/sentence_compression/0/triples/hyperparameters.txt
embeddings,updated during,training,hyperparameters,/content/training-data/sentence_compression/0/triples/hyperparameters.txt
Hyperparameters,has,POS and dependency embeddings,hyperparameters,/content/training-data/sentence_compression/0/triples/hyperparameters.txt
POS and dependency embeddings,randomly initialized with,40 - dimensional vectors,hyperparameters,/content/training-data/sentence_compression/0/triples/hyperparameters.txt
Hyperparameters,has,batch size,hyperparameters,/content/training-data/sentence_compression/0/triples/hyperparameters.txt
batch size,set as,30,hyperparameters,/content/training-data/sentence_compression/0/triples/hyperparameters.txt
Hyperparameters,has,Dropping probability,hyperparameters,/content/training-data/sentence_compression/0/triples/hyperparameters.txt
Dropping probability,for,dropout layers,hyperparameters,/content/training-data/sentence_compression/0/triples/hyperparameters.txt
dropout layers,between,stacked LSTM layers,hyperparameters,/content/training-data/sentence_compression/0/triples/hyperparameters.txt
stacked LSTM layers,is,0.5,hyperparameters,/content/training-data/sentence_compression/0/triples/hyperparameters.txt
Hyperparameters,has,dimension,hyperparameters,/content/training-data/sentence_compression/0/triples/hyperparameters.txt
dimension,of,hidden layers,hyperparameters,/content/training-data/sentence_compression/0/triples/hyperparameters.txt
hidden layers,of,bi - LSTM,hyperparameters,/content/training-data/sentence_compression/0/triples/hyperparameters.txt
bi - LSTM,is,100,hyperparameters,/content/training-data/sentence_compression/0/triples/hyperparameters.txt
Hyperparameters,has,our model,hyperparameters,/content/training-data/sentence_compression/0/triples/hyperparameters.txt
our model,trained using,Adam algorithm,hyperparameters,/content/training-data/sentence_compression/0/triples/hyperparameters.txt
Adam algorithm,with,learning rate,hyperparameters,/content/training-data/sentence_compression/0/triples/hyperparameters.txt
learning rate,initialized at,0.001,hyperparameters,/content/training-data/sentence_compression/0/triples/hyperparameters.txt
Hyperparameters,has,Word embeddings,hyperparameters,/content/training-data/sentence_compression/0/triples/hyperparameters.txt
Word embeddings,initialized from,GloVe 100 dimensional pre-trained embeddings,hyperparameters,/content/training-data/sentence_compression/0/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentence_compression/0/triples/model.txt
Model,extend,deletionbased LSTM model,model,/content/training-data/sentence_compression/0/triples/model.txt
deletionbased LSTM model,for,sentence compression,model,/content/training-data/sentence_compression/0/triples/model.txt
Model,use,bi-directional LSTM,model,/content/training-data/sentence_compression/0/triples/model.txt
bi-directional LSTM,to include,contextual information,model,/content/training-data/sentence_compression/0/triples/model.txt
contextual information,from,both directions into the model,model,/content/training-data/sentence_compression/0/triples/model.txt
Model,propose,two major changes,model,/content/training-data/sentence_compression/0/triples/model.txt
two major changes,formulate,final predictions,model,/content/training-data/sentence_compression/0/triples/model.txt
final predictions,as,Integer Linear Programming problem,model,/content/training-data/sentence_compression/0/triples/model.txt
Integer Linear Programming problem,to incorporate,constraints,model,/content/training-data/sentence_compression/0/triples/model.txt
constraints,based on,syntactic relations,model,/content/training-data/sentence_compression/0/triples/model.txt
syntactic relations,between,words and expected lengths,model,/content/training-data/sentence_compression/0/triples/model.txt
words and expected lengths,of,compressed sentences,model,/content/training-data/sentence_compression/0/triples/model.txt
two major changes,explicitly introduce,POS embeddings and dependency relation embeddings,model,/content/training-data/sentence_compression/0/triples/model.txt
POS embeddings and dependency relation embeddings,into,neural network model,model,/content/training-data/sentence_compression/0/triples/model.txt
Contribution,has research problem,Sentence Compression,research-problem,/content/training-data/sentence_compression/0/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentence_compression/0/triples/results.txt
Results,In,in - domain setting,results,/content/training-data/sentence_compression/0/triples/results.txt
in - domain setting,has,our BiLSTM method,results,/content/training-data/sentence_compression/0/triples/results.txt
our BiLSTM method,with,syntactic features ( BiLSTM + SynFeat and BiL - STM + SynFeat + ILP ,results,/content/training-data/sentence_compression/0/triples/results.txt
our BiLSTM method,performs,similarly to or better,results,/content/training-data/sentence_compression/0/triples/results.txt
similarly to or better,than,LSTM + method,results,/content/training-data/sentence_compression/0/triples/results.txt
similarly to or better,in terms of,both F1 and accuracy,results,/content/training-data/sentence_compression/0/triples/results.txt
Results,In,out - of - domain setting,results,/content/training-data/sentence_compression/0/triples/results.txt
out - of - domain setting,has,our BiLSTM + SynFeat and BiLSTM+SynFeat+ILP methods,results,/content/training-data/sentence_compression/0/triples/results.txt
our BiLSTM + SynFeat and BiLSTM+SynFeat+ILP methods,clearly outperform,LSTM and LSTM + methods,results,/content/training-data/sentence_compression/0/triples/results.txt
Results,in,in - domain setting,results,/content/training-data/sentence_compression/0/triples/results.txt
in - domain setting,has,our method,results,/content/training-data/sentence_compression/0/triples/results.txt
our method,does not have,any advantage,results,/content/training-data/sentence_compression/0/triples/results.txt
any advantage,over,LSTM + method,results,/content/training-data/sentence_compression/0/triples/results.txt
Results,in,cross - domain setting,results,/content/training-data/sentence_compression/0/triples/results.txt
cross - domain setting,has,our method,results,/content/training-data/sentence_compression/0/triples/results.txt
our method,performs,better,results,/content/training-data/sentence_compression/0/triples/results.txt
better,than,LSTM +,results,/content/training-data/sentence_compression/0/triples/results.txt
better,when,amount of training data,results,/content/training-data/sentence_compression/0/triples/results.txt
amount of training data,is,relatively small,results,/content/training-data/sentence_compression/0/triples/results.txt
our method,uses,ILP,results,/content/training-data/sentence_compression/0/triples/results.txt
ILP,to impose,syntax - based constraints,results,/content/training-data/sentence_compression/0/triples/results.txt
Results,see that,abstractive method,results,/content/training-data/sentence_compression/0/triples/results.txt
abstractive method,performed,poorly,results,/content/training-data/sentence_compression/0/triples/results.txt
poorly,in,cross - domain settings,results,/content/training-data/sentence_compression/0/triples/results.txt
Results,has,Traditional ILP method,results,/content/training-data/sentence_compression/0/triples/results.txt
Traditional ILP method,performs,worse,results,/content/training-data/sentence_compression/0/triples/results.txt
worse,in,in - domain setting,results,/content/training-data/sentence_compression/0/triples/results.txt
worse,than both,LSTM and LSTM + methods and our methods,results,/content/training-data/sentence_compression/0/triples/results.txt
Traditional ILP method,works,better,results,/content/training-data/sentence_compression/0/triples/results.txt
better,than,LSTM and LSTM + methods,results,/content/training-data/sentence_compression/0/triples/results.txt
LSTM and LSTM + methods,in,out - of - domain setting,results,/content/training-data/sentence_compression/0/triples/results.txt
Results,has,our method,results,/content/training-data/sentence_compression/0/triples/results.txt
our method,works,reasonably well,results,/content/training-data/sentence_compression/0/triples/results.txt
reasonably well,for both,in - domain and out - ofdomain data,results,/content/training-data/sentence_compression/0/triples/results.txt
our method,comparable to,LSTM + method,results,/content/training-data/sentence_compression/0/triples/results.txt
LSTM + method,in,in - domain setting,results,/content/training-data/sentence_compression/0/triples/results.txt
Results,on,Google News,results,/content/training-data/sentence_compression/0/triples/results.txt
Google News,adding,ILP layer,results,/content/training-data/sentence_compression/0/triples/results.txt
ILP layer,decreased,sentence compression performance,results,/content/training-data/sentence_compression/0/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/sentence_compression/2/triples/baselines.txt
Baselines,has,BASELINE - LSTM,baselines,/content/training-data/sentence_compression/2/triples/baselines.txt
BASELINE - LSTM,is,multi - task learning,baselines,/content/training-data/sentence_compression/2/triples/baselines.txt
multi - task learning,predicting both,CCG supertags and sentence compression,baselines,/content/training-data/sentence_compression/2/triples/baselines.txt
CCG supertags and sentence compression,at,outer layer,baselines,/content/training-data/sentence_compression/2/triples/baselines.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentence_compression/2/triples/hyperparameters.txt
Hyperparameters,has,Both the baseline and our systems,hyperparameters,/content/training-data/sentence_compression/2/triples/hyperparameters.txt
Both the baseline and our systems,are,three - layer bi - LSTM models,hyperparameters,/content/training-data/sentence_compression/2/triples/hyperparameters.txt
three - layer bi - LSTM models,trained for,30 iterations,hyperparameters,/content/training-data/sentence_compression/2/triples/hyperparameters.txt
30 iterations,with,pretrained ( SENNA ) embeddings,hyperparameters,/content/training-data/sentence_compression/2/triples/hyperparameters.txt
Hyperparameters,has,input and hidden layers,hyperparameters,/content/training-data/sentence_compression/2/triples/hyperparameters.txt
input and hidden layers,are,50 dimensions,hyperparameters,/content/training-data/sentence_compression/2/triples/hyperparameters.txt
Hyperparameters,has,output layer,hyperparameters,/content/training-data/sentence_compression/2/triples/hyperparameters.txt
output layer,predict,sequences,hyperparameters,/content/training-data/sentence_compression/2/triples/hyperparameters.txt
sequences,of,two labels,hyperparameters,/content/training-data/sentence_compression/2/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentence_compression/2/triples/model.txt
Model,use,gaze data,model,/content/training-data/sentence_compression/2/triples/model.txt
gaze data,to improve,sentence compression results,model,/content/training-data/sentence_compression/2/triples/model.txt
sentence compression results,on,several datasets,model,/content/training-data/sentence_compression/2/triples/model.txt
gaze data,from,readers,model,/content/training-data/sentence_compression/2/triples/model.txt
readers,of,Dundee Corpus,model,/content/training-data/sentence_compression/2/triples/model.txt
Model,suggesting that,eye - tracking recordings,model,/content/training-data/sentence_compression/2/triples/model.txt
eye - tracking recordings,to induce,better models,model,/content/training-data/sentence_compression/2/triples/model.txt
better models,for,sentence compression,model,/content/training-data/sentence_compression/2/triples/model.txt
sentence compression,for,text simplification,model,/content/training-data/sentence_compression/2/triples/model.txt
Model,has,Our proposed model,model,/content/training-data/sentence_compression/2/triples/model.txt
Our proposed model,does not require,gaze data and the compression data,model,/content/training-data/sentence_compression/2/triples/model.txt
gaze data and the compression data,come from,same source,model,/content/training-data/sentence_compression/2/triples/model.txt
Model,has,intriguing potential,model,/content/training-data/sentence_compression/2/triples/model.txt
intriguing potential,of,this work,model,/content/training-data/sentence_compression/2/triples/model.txt
this work,in deriving,sentence simplification models,model,/content/training-data/sentence_compression/2/triples/model.txt
sentence simplification models,that are,personalized,model,/content/training-data/sentence_compression/2/triples/model.txt
personalized,based on,their reading behavior,model,/content/training-data/sentence_compression/2/triples/model.txt
personalized,for,individual users,model,/content/training-data/sentence_compression/2/triples/model.txt
Model,show how to use,existing eye - tracking recordings,model,/content/training-data/sentence_compression/2/triples/model.txt
existing eye - tracking recordings,to improve,induction,model,/content/training-data/sentence_compression/2/triples/model.txt
induction,of,Long Short - Term Memory models ( LSTMs ,model,/content/training-data/sentence_compression/2/triples/model.txt
Long Short - Term Memory models ( LSTMs ),for,sentence compression,model,/content/training-data/sentence_compression/2/triples/model.txt
Contribution,has research problem,sentence compression,research-problem,/content/training-data/sentence_compression/2/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentence_compression/2/triples/results.txt
Results,across,all three datasets,results,/content/training-data/sentence_compression/2/triples/results.txt
all three datasets,including,all three annotations of BROADCAST,results,/content/training-data/sentence_compression/2/triples/results.txt
all three datasets,has,gaze features,results,/content/training-data/sentence_compression/2/triples/results.txt
gaze features,lead to,improvements,results,/content/training-data/sentence_compression/2/triples/results.txt
improvements,over,baseline 3 - layer bi - LSTM,results,/content/training-data/sentence_compression/2/triples/results.txt
all three datasets,has,CASCADED - LSTM,results,/content/training-data/sentence_compression/2/triples/results.txt
CASCADED - LSTM,is,consistently better,results,/content/training-data/sentence_compression/2/triples/results.txt
consistently better,than,MULTITASK - LSTM,results,/content/training-data/sentence_compression/2/triples/results.txt
Results,For,all three datasets,results,/content/training-data/sentence_compression/2/triples/results.txt
all three datasets,has,inclusion,results,/content/training-data/sentence_compression/2/triples/results.txt
inclusion,of,gaze measures,results,/content/training-data/sentence_compression/2/triples/results.txt
gaze measures,leads to,improvements,results,/content/training-data/sentence_compression/2/triples/results.txt
improvements,over,baseline,results,/content/training-data/sentence_compression/2/triples/results.txt
gaze measures,name,first pass duration ( FP ,results,/content/training-data/sentence_compression/2/triples/results.txt
gaze measures,name,regression duration ( Regr. ,results,/content/training-data/sentence_compression/2/triples/results.txt
Results,With,harder datasets,results,/content/training-data/sentence_compression/2/triples/results.txt
harder datasets,impact of,gaze information,results,/content/training-data/sentence_compression/2/triples/results.txt
gaze information,with,improvements,results,/content/training-data/sentence_compression/2/triples/results.txt
improvements,using,first pass duration,results,/content/training-data/sentence_compression/2/triples/results.txt
improvements,using,regression duration,results,/content/training-data/sentence_compression/2/triples/results.txt
gaze information,favouring,cascaded architecture,results,/content/training-data/sentence_compression/2/triples/results.txt
gaze information,becomes,stronger,results,/content/training-data/sentence_compression/2/triples/results.txt
Contribution,has,Model,model,/content/training-data/sentence_compression/1/triples/model.txt
Model,benefits from,very recent advances in deep learning,model,/content/training-data/sentence_compression/1/triples/model.txt
Model,uses,word embeddings and Long Short Term Memory models ( LSTMs ,model,/content/training-data/sentence_compression/1/triples/model.txt
word embeddings and Long Short Term Memory models ( LSTMs ),to output,surprisingly readable and informative compressions,model,/content/training-data/sentence_compression/1/triples/model.txt
Model,Trained on,corpus,model,/content/training-data/sentence_compression/1/triples/model.txt
corpus,using,standard tool,model,/content/training-data/sentence_compression/1/triples/model.txt
standard tool,to obtain,word embeddings,model,/content/training-data/sentence_compression/1/triples/model.txt
corpus,of,less than two million,model,/content/training-data/sentence_compression/1/triples/model.txt
less than two million,automatically extracted,parallel sentences,model,/content/training-data/sentence_compression/1/triples/model.txt
Contribution,has research problem,Sentence Compression by Deletion,research-problem,/content/training-data/sentence_compression/1/triples/research-problem.txt
Contribution,has research problem,deletion - based sentence compression,research-problem,/content/training-data/sentence_compression/1/triples/research-problem.txt
Contribution,has research problem,Sentence compression,research-problem,/content/training-data/sentence_compression/1/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentence_compression/1/triples/results.txt
Results,has,More than 30 %,results,/content/training-data/sentence_compression/1/triples/results.txt
More than 30 %,which is in,sharp contrast,results,/content/training-data/sentence_compression/1/triples/results.txt
sharp contrast,with,20 %,results,/content/training-data/sentence_compression/1/triples/results.txt
20 %,of,MIRA,results,/content/training-data/sentence_compression/1/triples/results.txt
More than 30 %,of,golden compressions,results,/content/training-data/sentence_compression/1/triples/results.txt
golden compressions,could be,regenerated,results,/content/training-data/sentence_compression/1/triples/results.txt
regenerated,by,LSTM systems,results,/content/training-data/sentence_compression/1/triples/results.txt
Results,has,scores,results,/content/training-data/sentence_compression/1/triples/results.txt
scores,close to,0.81,results,/content/training-data/sentence_compression/1/triples/results.txt
Results,has,differences,results,/content/training-data/sentence_compression/1/triples/results.txt
differences,in,F- score,results,/content/training-data/sentence_compression/1/triples/results.txt
F- score,between,three versions of LSTM,results,/content/training-data/sentence_compression/1/triples/results.txt
three versions of LSTM,are,not significant,results,/content/training-data/sentence_compression/1/triples/results.txt
Results,has,significant difference,results,/content/training-data/sentence_compression/1/triples/results.txt
significant difference,in,performance,results,/content/training-data/sentence_compression/1/triples/results.txt
performance,of,MIRA baseline and the LSTM models,results,/content/training-data/sentence_compression/1/triples/results.txt
performance,in terms of,F1 - score and in accuracy,results,/content/training-data/sentence_compression/1/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/sentiment_analysis/32/triples/baselines.txt
Baselines,has,LSTM,baselines,/content/training-data/sentiment_analysis/32/triples/baselines.txt
LSTM,get,hidden state,baselines,/content/training-data/sentiment_analysis/32/triples/baselines.txt
hidden state,of,each word,baselines,/content/training-data/sentiment_analysis/32/triples/baselines.txt
LSTM,uses,one LSTM network,baselines,/content/training-data/sentiment_analysis/32/triples/baselines.txt
one LSTM network,to model,context,baselines,/content/training-data/sentiment_analysis/32/triples/baselines.txt
Baselines,has,AE - LSTM,baselines,/content/training-data/sentiment_analysis/32/triples/baselines.txt
AE - LSTM,represents,targets,baselines,/content/training-data/sentiment_analysis/32/triples/baselines.txt
targets,with,aspect embeddings,baselines,/content/training-data/sentiment_analysis/32/triples/baselines.txt
Baselines,has,ATAE - LSTM,baselines,/content/training-data/sentiment_analysis/32/triples/baselines.txt
ATAE - LSTM,based on,AE - LSTM,baselines,/content/training-data/sentiment_analysis/32/triples/baselines.txt
Baselines,has,Majority,baselines,/content/training-data/sentiment_analysis/32/triples/baselines.txt
Majority,is,basic baseline method,baselines,/content/training-data/sentiment_analysis/32/triples/baselines.txt
Majority,assigns,largest sentiment polarity,baselines,/content/training-data/sentiment_analysis/32/triples/baselines.txt
largest sentiment polarity,in,training set,baselines,/content/training-data/sentiment_analysis/32/triples/baselines.txt
training set,to,each sample,baselines,/content/training-data/sentiment_analysis/32/triples/baselines.txt
each sample,in,test set,baselines,/content/training-data/sentiment_analysis/32/triples/baselines.txt
Baselines,has,TD - LSTM,baselines,/content/training-data/sentiment_analysis/32/triples/baselines.txt
TD - LSTM,adopts,two long short - term memory ( LSTM ) networks,baselines,/content/training-data/sentiment_analysis/32/triples/baselines.txt
two long short - term memory ( LSTM ) networks,to model,right context,baselines,/content/training-data/sentiment_analysis/32/triples/baselines.txt
right context,with,target,baselines,/content/training-data/sentiment_analysis/32/triples/baselines.txt
two long short - term memory ( LSTM ) networks,to model,left context,baselines,/content/training-data/sentiment_analysis/32/triples/baselines.txt
left context,with,target,baselines,/content/training-data/sentiment_analysis/32/triples/baselines.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/32/triples/hyperparameters.txt
Hyperparameters,has,biases,hyperparameters,/content/training-data/sentiment_analysis/32/triples/hyperparameters.txt
biases,set to,zeros,hyperparameters,/content/training-data/sentiment_analysis/32/triples/hyperparameters.txt
Hyperparameters,has,weight matrices,hyperparameters,/content/training-data/sentiment_analysis/32/triples/hyperparameters.txt
weight matrices,given their,initial values,hyperparameters,/content/training-data/sentiment_analysis/32/triples/hyperparameters.txt
initial values,sampling from,"uniform distribution U ( ?0.1 , 0.1 ",hyperparameters,/content/training-data/sentiment_analysis/32/triples/hyperparameters.txt
Hyperparameters,has,coefficient,hyperparameters,/content/training-data/sentiment_analysis/32/triples/hyperparameters.txt
coefficient,of,L 2 normalization,hyperparameters,/content/training-data/sentiment_analysis/32/triples/hyperparameters.txt
L 2 normalization,in,objective function,hyperparameters,/content/training-data/sentiment_analysis/32/triples/hyperparameters.txt
L 2 normalization,set to,10 ?5,hyperparameters,/content/training-data/sentiment_analysis/32/triples/hyperparameters.txt
Hyperparameters,has,all word embeddings,hyperparameters,/content/training-data/sentiment_analysis/32/triples/hyperparameters.txt
all word embeddings,from,context and target,hyperparameters,/content/training-data/sentiment_analysis/32/triples/hyperparameters.txt
all word embeddings,initialized by,GloVe,hyperparameters,/content/training-data/sentiment_analysis/32/triples/hyperparameters.txt
Hyperparameters,has,dropout rate,hyperparameters,/content/training-data/sentiment_analysis/32/triples/hyperparameters.txt
dropout rate,set to,0.5,hyperparameters,/content/training-data/sentiment_analysis/32/triples/hyperparameters.txt
Hyperparameters,has,all out - of - vocabulary words,hyperparameters,/content/training-data/sentiment_analysis/32/triples/hyperparameters.txt
all out - of - vocabulary words,initialized by,sampling,hyperparameters,/content/training-data/sentiment_analysis/32/triples/hyperparameters.txt
sampling,from,"uniform distribution U ( ?0.1 , 0.1 ",hyperparameters,/content/training-data/sentiment_analysis/32/triples/hyperparameters.txt
Hyperparameters,has,dimensions,hyperparameters,/content/training-data/sentiment_analysis/32/triples/hyperparameters.txt
dimensions,of,"word embeddings , attention vectors and LSTM hidden states",hyperparameters,/content/training-data/sentiment_analysis/32/triples/hyperparameters.txt
"word embeddings , attention vectors and LSTM hidden states",set to,300,hyperparameters,/content/training-data/sentiment_analysis/32/triples/hyperparameters.txt
Hyperparameters,train,parameters,hyperparameters,/content/training-data/sentiment_analysis/32/triples/hyperparameters.txt
parameters,employ,Momentum,hyperparameters,/content/training-data/sentiment_analysis/32/triples/hyperparameters.txt
Momentum,adds,fraction,hyperparameters,/content/training-data/sentiment_analysis/32/triples/hyperparameters.txt
fraction,of,update vector,hyperparameters,/content/training-data/sentiment_analysis/32/triples/hyperparameters.txt
update vector,in,prior step,hyperparameters,/content/training-data/sentiment_analysis/32/triples/hyperparameters.txt
update vector,to,current update vector,hyperparameters,/content/training-data/sentiment_analysis/32/triples/hyperparameters.txt
parameters,of,IAN,hyperparameters,/content/training-data/sentiment_analysis/32/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/32/triples/model.txt
Model,has,IAN,model,/content/training-data/sentiment_analysis/32/triples/model.txt
IAN,compute,context representation,model,/content/training-data/sentiment_analysis/32/triples/model.txt
context representation,for,sentiment classification,model,/content/training-data/sentiment_analysis/32/triples/model.txt
IAN,utilizes,attention mechanism,model,/content/training-data/sentiment_analysis/32/triples/model.txt
attention mechanism,associated with,target,model,/content/training-data/sentiment_analysis/32/triples/model.txt
attention mechanism,to get,important information,model,/content/training-data/sentiment_analysis/32/triples/model.txt
important information,from,context,model,/content/training-data/sentiment_analysis/32/triples/model.txt
IAN,makes use of,interactive information,model,/content/training-data/sentiment_analysis/32/triples/model.txt
interactive information,to supervise,modeling,model,/content/training-data/sentiment_analysis/32/triples/model.txt
modeling,of,target,model,/content/training-data/sentiment_analysis/32/triples/model.txt
interactive information,from,context,model,/content/training-data/sentiment_analysis/32/triples/model.txt
IAN,predicts,sentiment polarity,model,/content/training-data/sentiment_analysis/32/triples/model.txt
sentiment polarity,with,both target representation and context representation concatenated,model,/content/training-data/sentiment_analysis/32/triples/model.txt
sentiment polarity,for,target,model,/content/training-data/sentiment_analysis/32/triples/model.txt
target,within,its context,model,/content/training-data/sentiment_analysis/32/triples/model.txt
Model,propose,interactive attention network ( IAN ) model,model,/content/training-data/sentiment_analysis/32/triples/model.txt
interactive attention network ( IAN ) model,based on,long - short term memory networks ( LSTM ,model,/content/training-data/sentiment_analysis/32/triples/model.txt
interactive attention network ( IAN ) model,based on,attention mechanism,model,/content/training-data/sentiment_analysis/32/triples/model.txt
Contribution,has research problem,Aspect - Level Sentiment Classification,research-problem,/content/training-data/sentiment_analysis/32/triples/research-problem.txt
Contribution,has research problem,sentiment classification,research-problem,/content/training-data/sentiment_analysis/32/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/32/triples/results.txt
Results,see,AE - LSTM and ATAE - LSTM,results,/content/training-data/sentiment_analysis/32/triples/results.txt
AE - LSTM and ATAE - LSTM,emphasize,modeling,results,/content/training-data/sentiment_analysis/32/triples/results.txt
modeling,of,targets,results,/content/training-data/sentiment_analysis/32/triples/results.txt
modeling,via,addition of the aspect embedding,results,/content/training-data/sentiment_analysis/32/triples/results.txt
Results,see,IAN,results,/content/training-data/sentiment_analysis/32/triples/results.txt
IAN,achieves,best performance,results,/content/training-data/sentiment_analysis/32/triples/results.txt
best performance,among,all baselines,results,/content/training-data/sentiment_analysis/32/triples/results.txt
Results,For,AE - LSTM and ATAE - LSTM,results,/content/training-data/sentiment_analysis/32/triples/results.txt
AE - LSTM and ATAE - LSTM,capture,important information,results,/content/training-data/sentiment_analysis/32/triples/results.txt
important information,in,context,results,/content/training-data/sentiment_analysis/32/triples/results.txt
context,with,supervision,results,/content/training-data/sentiment_analysis/32/triples/results.txt
supervision,of,target,results,/content/training-data/sentiment_analysis/32/triples/results.txt
AE - LSTM and ATAE - LSTM,generate,more reasonable representations,results,/content/training-data/sentiment_analysis/32/triples/results.txt
more reasonable representations,for,aspect - level sentiment classification,results,/content/training-data/sentiment_analysis/32/triples/results.txt
Results,has,both AE - LSTM and ATAE - LSTM,results,/content/training-data/sentiment_analysis/32/triples/results.txt
both AE - LSTM and ATAE - LSTM,stably exceed,TD - LSTM method,results,/content/training-data/sentiment_analysis/32/triples/results.txt
Results,has,LSTM method,results,/content/training-data/sentiment_analysis/32/triples/results.txt
LSTM method,gets,worst performance,results,/content/training-data/sentiment_analysis/32/triples/results.txt
worst performance,of,all the neural network baseline methods,results,/content/training-data/sentiment_analysis/32/triples/results.txt
Results,has,All the other methods,results,/content/training-data/sentiment_analysis/32/triples/results.txt
All the other methods,based on,LSTM models,results,/content/training-data/sentiment_analysis/32/triples/results.txt
LSTM models,better than,Majority method,results,/content/training-data/sentiment_analysis/32/triples/results.txt
Results,has,ATAE - LSTM,results,/content/training-data/sentiment_analysis/32/triples/results.txt
ATAE - LSTM,especially enhance,interaction,results,/content/training-data/sentiment_analysis/32/triples/results.txt
interaction,between,context words and target,results,/content/training-data/sentiment_analysis/32/triples/results.txt
ATAE - LSTM,has,better performance,results,/content/training-data/sentiment_analysis/32/triples/results.txt
better performance,than,AE - LSTM,results,/content/training-data/sentiment_analysis/32/triples/results.txt
ATAE - LSTM,Compared with,AE - LSTM,results,/content/training-data/sentiment_analysis/32/triples/results.txt
Results,has,more attentions,results,/content/training-data/sentiment_analysis/32/triples/results.txt
more attentions,achieves,higher accuracy,results,/content/training-data/sentiment_analysis/32/triples/results.txt
more attentions,paid to,targets,results,/content/training-data/sentiment_analysis/32/triples/results.txt
Results,has,TD - LSTM,results,/content/training-data/sentiment_analysis/32/triples/results.txt
TD - LSTM,outperforms,LSTM,results,/content/training-data/sentiment_analysis/32/triples/results.txt
LSTM,over,1 percent and 2 percent,results,/content/training-data/sentiment_analysis/32/triples/results.txt
1 percent and 2 percent,on,Restaurant and Laptop category,results,/content/training-data/sentiment_analysis/32/triples/results.txt
Results,Compared with,ATAE - LSTM model,results,/content/training-data/sentiment_analysis/32/triples/results.txt
ATAE - LSTM model,has,IAN,results,/content/training-data/sentiment_analysis/32/triples/results.txt
IAN,improves,performance,results,/content/training-data/sentiment_analysis/32/triples/results.txt
performance,about,1.4 % and 3.2 %,results,/content/training-data/sentiment_analysis/32/triples/results.txt
1.4 % and 3.2 %,on,Restaurant and Laptop categories,results,/content/training-data/sentiment_analysis/32/triples/results.txt
Contribution,has,Approach,approach,/content/training-data/sentiment_analysis/18/triples/approach.txt
Approach,has,first approach,approach,/content/training-data/sentiment_analysis/18/triples/approach.txt
first approach,use,autoencoder structure,approach,/content/training-data/sentiment_analysis/18/triples/approach.txt
autoencoder structure,to learn,both the aspect embeddings as well as the representation of the target,approach,/content/training-data/sentiment_analysis/18/triples/approach.txt
both the aspect embeddings as well as the representation of the target,as,weighted combination,approach,/content/training-data/sentiment_analysis/18/triples/approach.txt
weighted combination,of,aspect embeddings,approach,/content/training-data/sentiment_analysis/18/triples/approach.txt
first approach,model,each target,approach,/content/training-data/sentiment_analysis/18/triples/approach.txt
each target,as,mixture,approach,/content/training-data/sentiment_analysis/18/triples/approach.txt
mixture,of,K aspect embeddings,approach,/content/training-data/sentiment_analysis/18/triples/approach.txt
first approach,has,autoencoder structure,approach,/content/training-data/sentiment_analysis/18/triples/approach.txt
autoencoder structure,jointly trained with,neural attention - based sentiment classifier,approach,/content/training-data/sentiment_analysis/18/triples/approach.txt
neural attention - based sentiment classifier,to provide,good target representation,approach,/content/training-data/sentiment_analysis/18/triples/approach.txt
good target representation,as well as,high accuracy,approach,/content/training-data/sentiment_analysis/18/triples/approach.txt
high accuracy,on,predicted sentiment,approach,/content/training-data/sentiment_analysis/18/triples/approach.txt
Approach,has,second approach,approach,/content/training-data/sentiment_analysis/18/triples/approach.txt
second approach,has,syntax - based attention mechanism,approach,/content/training-data/sentiment_analysis/18/triples/approach.txt
syntax - based attention mechanism,selectively focuses on,small subset of context words,approach,/content/training-data/sentiment_analysis/18/triples/approach.txt
small subset of context words,close to,target,approach,/content/training-data/sentiment_analysis/18/triples/approach.txt
target,on,syntactic path,approach,/content/training-data/sentiment_analysis/18/triples/approach.txt
syntactic path,obtained by,applying a dependency parser,approach,/content/training-data/sentiment_analysis/18/triples/approach.txt
applying a dependency parser,on,review sentence,approach,/content/training-data/sentiment_analysis/18/triples/approach.txt
second approach,exploits,syntactic information,approach,/content/training-data/sentiment_analysis/18/triples/approach.txt
syntactic information,to construct,syntax - based attention model,approach,/content/training-data/sentiment_analysis/18/triples/approach.txt
Approach,propose,two novel approaches,approach,/content/training-data/sentiment_analysis/18/triples/approach.txt
two novel approaches,for improving,effectiveness of attention models,approach,/content/training-data/sentiment_analysis/18/triples/approach.txt
Contribution,has,Baselines,baselines,/content/training-data/sentiment_analysis/18/triples/baselines.txt
Baselines,has,LSTM,baselines,/content/training-data/sentiment_analysis/18/triples/baselines.txt
LSTM,built on,top of word embeddings,baselines,/content/training-data/sentiment_analysis/18/triples/baselines.txt
Baselines,has,Feature - based SVM,baselines,/content/training-data/sentiment_analysis/18/triples/baselines.txt
Feature - based SVM,compare with,reported results,baselines,/content/training-data/sentiment_analysis/18/triples/baselines.txt
reported results,of,top system,baselines,/content/training-data/sentiment_analysis/18/triples/baselines.txt
top system,in,SemEval 2014,baselines,/content/training-data/sentiment_analysis/18/triples/baselines.txt
Contribution,has research problem,Aspect - Level Sentiment Classification,research-problem,/content/training-data/sentiment_analysis/18/triples/research-problem.txt
Contribution,has research problem,fine - grained sentiment analysis,research-problem,/content/training-data/sentiment_analysis/18/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/18/triples/results.txt
Results,has,integrated full model over all,results,/content/training-data/sentiment_analysis/18/triples/results.txt
integrated full model over all,achieves,best performance,results,/content/training-data/sentiment_analysis/18/triples/results.txt
best performance,compared to using,only one of the two proposed approaches,results,/content/training-data/sentiment_analysis/18/triples/results.txt
Results,has,our best model,results,/content/training-data/sentiment_analysis/18/triples/results.txt
our best model,achieves,competitive results,results,/content/training-data/sentiment_analysis/18/triples/results.txt
competitive results,on,D1 and D2,results,/content/training-data/sentiment_analysis/18/triples/results.txt
D1 and D2,without relying on,so many manually - designed features and external resources,results,/content/training-data/sentiment_analysis/18/triples/results.txt
Results,has,proposed target representation,results,/content/training-data/sentiment_analysis/18/triples/results.txt
proposed target representation,is,more helpful,results,/content/training-data/sentiment_analysis/18/triples/results.txt
more helpful,than,laptop domain ( D2 ,results,/content/training-data/sentiment_analysis/18/triples/results.txt
more helpful,on,"restaurant domain ( D1 , D3 , and D4 ",results,/content/training-data/sentiment_analysis/18/triples/results.txt
Results,Compared with,LSTM + ATT,results,/content/training-data/sentiment_analysis/18/triples/results.txt
LSTM + ATT,has,all three settings,results,/content/training-data/sentiment_analysis/18/triples/results.txt
all three settings,of,our model,results,/content/training-data/sentiment_analysis/18/triples/results.txt
our model,are able to achieve,statistically significant improvements ( p < 0.05 ,results,/content/training-data/sentiment_analysis/18/triples/results.txt
statistically significant improvements ( p < 0.05 ),on,all datasets,results,/content/training-data/sentiment_analysis/18/triples/results.txt
Results,Compared with,all other neural baselines,results,/content/training-data/sentiment_analysis/18/triples/results.txt
all other neural baselines,has,our full model,results,/content/training-data/sentiment_analysis/18/triples/results.txt
our full model,achieves,statistically significant improvements ( p < 0.05 ,results,/content/training-data/sentiment_analysis/18/triples/results.txt
statistically significant improvements ( p < 0.05 ),on,both accuracies and macro - F1 scores,results,/content/training-data/sentiment_analysis/18/triples/results.txt
both accuracies and macro - F1 scores,for,"D1 , D3 , D4",results,/content/training-data/sentiment_analysis/18/triples/results.txt
Contribution,has,Dataset,datase,/content/training-data/sentiment_analysis/15/triples/dataset.txt
Dataset,name,Stanford Sentiment Treebank,datase,/content/training-data/sentiment_analysis/15/triples/dataset.txt
Stanford Sentiment Treebank,is,first corpus,datase,/content/training-data/sentiment_analysis/15/triples/dataset.txt
first corpus,with,fully labeled parse trees,datase,/content/training-data/sentiment_analysis/15/triples/dataset.txt
fully labeled parse trees,allows for,complete analysis,datase,/content/training-data/sentiment_analysis/15/triples/dataset.txt
complete analysis,of,compositional effects,datase,/content/training-data/sentiment_analysis/15/triples/dataset.txt
compositional effects,of,sentiment in language,datase,/content/training-data/sentiment_analysis/15/triples/dataset.txt
Dataset,has,new dataset,datase,/content/training-data/sentiment_analysis/15/triples/dataset.txt
new dataset,to analyze,intricacies,datase,/content/training-data/sentiment_analysis/15/triples/dataset.txt
intricacies,of,sentiment,datase,/content/training-data/sentiment_analysis/15/triples/dataset.txt
new dataset,to capture,complex linguistic phenomena,datase,/content/training-data/sentiment_analysis/15/triples/dataset.txt
Dataset,has,granularity and size,datase,/content/training-data/sentiment_analysis/15/triples/dataset.txt
granularity and size,enable,community,datase,/content/training-data/sentiment_analysis/15/triples/dataset.txt
community,to train,compositional models,datase,/content/training-data/sentiment_analysis/15/triples/dataset.txt
compositional models,based on,supervised and structured machine learning techniques,datase,/content/training-data/sentiment_analysis/15/triples/dataset.txt
Dataset,has,corpus,datase,/content/training-data/sentiment_analysis/15/triples/dataset.txt
corpus,based on,dataset,datase,/content/training-data/sentiment_analysis/15/triples/dataset.txt
dataset,consists of,"11,855 single sentences",datase,/content/training-data/sentiment_analysis/15/triples/dataset.txt
"11,855 single sentences",extracted from,movie reviews,datase,/content/training-data/sentiment_analysis/15/triples/dataset.txt
Dataset,has,parsed,datase,/content/training-data/sentiment_analysis/15/triples/dataset.txt
parsed,with,Stanford parser,datase,/content/training-data/sentiment_analysis/15/triples/dataset.txt
parsed,total of,"215,154 unique phrases",datase,/content/training-data/sentiment_analysis/15/triples/dataset.txt
"215,154 unique phrases",from,parse trees,datase,/content/training-data/sentiment_analysis/15/triples/dataset.txt
parse trees,annotated by,3 human judges,datase,/content/training-data/sentiment_analysis/15/triples/dataset.txt
Contribution,has,Baselines,baselines,/content/training-data/sentiment_analysis/15/triples/baselines.txt
Baselines,compare to,commonly used methods,baselines,/content/training-data/sentiment_analysis/15/triples/baselines.txt
commonly used methods,that use,bag of words features,baselines,/content/training-data/sentiment_analysis/15/triples/baselines.txt
bag of words features,with,Naive Bayes and SVMs,baselines,/content/training-data/sentiment_analysis/15/triples/baselines.txt
bag of words features,with,Naive Bayes,baselines,/content/training-data/sentiment_analysis/15/triples/baselines.txt
Naive Bayes,with,bag of bigram features,baselines,/content/training-data/sentiment_analysis/15/triples/baselines.txt
Baselines,compare to,model,baselines,/content/training-data/sentiment_analysis/15/triples/baselines.txt
model,ignores,word order ( VecAvg ,baselines,/content/training-data/sentiment_analysis/15/triples/baselines.txt
model,averages,neural word vectors,baselines,/content/training-data/sentiment_analysis/15/triples/baselines.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/sentiment_analysis/15/triples/ablation-analysis.txt
Ablation analysis,shows,typical case,ablation-analysis,/content/training-data/sentiment_analysis/15/triples/ablation-analysis.txt
typical case,in which,sentiment,ablation-analysis,/content/training-data/sentiment_analysis/15/triples/ablation-analysis.txt
sentiment,made,more positive,ablation-analysis,/content/training-data/sentiment_analysis/15/triples/ablation-analysis.txt
more positive,by switching,main class,ablation-analysis,/content/training-data/sentiment_analysis/15/triples/ablation-analysis.txt
main class,from,negative to neutral,ablation-analysis,/content/training-data/sentiment_analysis/15/triples/ablation-analysis.txt
Ablation analysis,conclude,RNTN,ablation-analysis,/content/training-data/sentiment_analysis/15/triples/ablation-analysis.txt
RNTN,best able to identify,effect of negations,ablation-analysis,/content/training-data/sentiment_analysis/15/triples/ablation-analysis.txt
effect of negations,upon,positive and negative sentiment sentences,ablation-analysis,/content/training-data/sentiment_analysis/15/triples/ablation-analysis.txt
Ablation analysis,has,RNTN,ablation-analysis,/content/training-data/sentiment_analysis/15/triples/ablation-analysis.txt
RNTN,has,highest reversal accuracy,ablation-analysis,/content/training-data/sentiment_analysis/15/triples/ablation-analysis.txt
highest reversal accuracy,showing its ability to,structurally learn negation of positive sentences,ablation-analysis,/content/training-data/sentiment_analysis/15/triples/ablation-analysis.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/15/triples/hyperparameters.txt
Hyperparameters,use,f = tanh,hyperparameters,/content/training-data/sentiment_analysis/15/triples/hyperparameters.txt
f = tanh,in,all experiments,hyperparameters,/content/training-data/sentiment_analysis/15/triples/hyperparameters.txt
Hyperparameters,has,RNTN,hyperparameters,/content/training-data/sentiment_analysis/15/triples/hyperparameters.txt
RNTN,usually achieve,best performance,hyperparameters,/content/training-data/sentiment_analysis/15/triples/hyperparameters.txt
best performance,on,dev set,hyperparameters,/content/training-data/sentiment_analysis/15/triples/hyperparameters.txt
dev set,after training for,3 - 5 hours,hyperparameters,/content/training-data/sentiment_analysis/15/triples/hyperparameters.txt
Hyperparameters,has,Optimal performance,hyperparameters,/content/training-data/sentiment_analysis/15/triples/hyperparameters.txt
Optimal performance,for,all models,hyperparameters,/content/training-data/sentiment_analysis/15/triples/hyperparameters.txt
all models,achieved at,word vector sizes,hyperparameters,/content/training-data/sentiment_analysis/15/triples/hyperparameters.txt
word vector sizes,between,25 and 35 dimensions,hyperparameters,/content/training-data/sentiment_analysis/15/triples/hyperparameters.txt
all models,achieved at,batch sizes,hyperparameters,/content/training-data/sentiment_analysis/15/triples/hyperparameters.txt
batch sizes,between,20 and 30,hyperparameters,/content/training-data/sentiment_analysis/15/triples/hyperparameters.txt
Hyperparameters,has,sentences,hyperparameters,/content/training-data/sentiment_analysis/15/triples/hyperparameters.txt
sentences,in,treebank,hyperparameters,/content/training-data/sentiment_analysis/15/triples/hyperparameters.txt
treebank,were split into,"train ( 8544 ) , dev ( 1101 ) and test splits ( 2210 ",hyperparameters,/content/training-data/sentiment_analysis/15/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/15/triples/model.txt
Model,called,Recursive Neural Tensor Network ( RNTN ,model,/content/training-data/sentiment_analysis/15/triples/model.txt
Model,has,Recursive Neural Tensor Networks,model,/content/training-data/sentiment_analysis/15/triples/model.txt
Recursive Neural Tensor Networks,compute,vectors,model,/content/training-data/sentiment_analysis/15/triples/model.txt
vectors,for,higher nodes,model,/content/training-data/sentiment_analysis/15/triples/model.txt
higher nodes,in,tree,model,/content/training-data/sentiment_analysis/15/triples/model.txt
tree,using,same tensor - based composition function,model,/content/training-data/sentiment_analysis/15/triples/model.txt
Recursive Neural Tensor Networks,take as input,phrases,model,/content/training-data/sentiment_analysis/15/triples/model.txt
phrases,of,any length,model,/content/training-data/sentiment_analysis/15/triples/model.txt
Recursive Neural Tensor Networks,represent,phrase,model,/content/training-data/sentiment_analysis/15/triples/model.txt
phrase,through,word vectors and a parse tree,model,/content/training-data/sentiment_analysis/15/triples/model.txt
Contribution,has research problem,Sentiment Treebank,research-problem,/content/training-data/sentiment_analysis/15/triples/research-problem.txt
Contribution,has research problem,richer supervised training and evaluation resources,research-problem,/content/training-data/sentiment_analysis/15/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/15/triples/results.txt
Results,combination of,new sentiment treebank and the RNTN,results,/content/training-data/sentiment_analysis/15/triples/results.txt
new sentiment treebank and the RNTN,pushes,state of the art,results,/content/training-data/sentiment_analysis/15/triples/results.txt
state of the art,on,short phrases,results,/content/training-data/sentiment_analysis/15/triples/results.txt
short phrases,up to,85.4 %,results,/content/training-data/sentiment_analysis/15/triples/results.txt
Results,showed,fine grained classification,results,/content/training-data/sentiment_analysis/15/triples/results.txt
fine grained classification,into,5 classes,results,/content/training-data/sentiment_analysis/15/triples/results.txt
5 classes,is,reasonable approximation,results,/content/training-data/sentiment_analysis/15/triples/results.txt
reasonable approximation,to capture,most of the data variation,results,/content/training-data/sentiment_analysis/15/triples/results.txt
Results,has,RNTN,results,/content/training-data/sentiment_analysis/15/triples/results.txt
RNTN,followed by,MV - RNN and RNN,results,/content/training-data/sentiment_analysis/15/triples/results.txt
RNTN,gets,highest performance,results,/content/training-data/sentiment_analysis/15/triples/results.txt
Results,has,recursive models,results,/content/training-data/sentiment_analysis/15/triples/results.txt
recursive models,work,very well,results,/content/training-data/sentiment_analysis/15/triples/results.txt
very well,on,shorter phrases,results,/content/training-data/sentiment_analysis/15/triples/results.txt
shorter phrases,where,negation and composition,results,/content/training-data/sentiment_analysis/15/triples/results.txt
negation and composition,are,important,results,/content/training-data/sentiment_analysis/15/triples/results.txt
Results,has,bag of features baselines,results,/content/training-data/sentiment_analysis/15/triples/results.txt
bag of features baselines,perform,well,results,/content/training-data/sentiment_analysis/15/triples/results.txt
well,with,longer sentences,results,/content/training-data/sentiment_analysis/15/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/sentiment_analysis/4/triples/baselines.txt
Baselines,has,MFN,baselines,/content/training-data/sentiment_analysis/4/triples/baselines.txt
MFN,performs,multi-view learning,baselines,/content/training-data/sentiment_analysis/4/triples/baselines.txt
multi-view learning,by using,Delta - memory Attention Network,baselines,/content/training-data/sentiment_analysis/4/triples/baselines.txt
Delta - memory Attention Network,has,fusion mechanism,baselines,/content/training-data/sentiment_analysis/4/triples/baselines.txt
fusion mechanism,to learn,cross - view interactions,baselines,/content/training-data/sentiment_analysis/4/triples/baselines.txt
Baselines,has,cLSTM,baselines,/content/training-data/sentiment_analysis/4/triples/baselines.txt
cLSTM,classifies,utterances,baselines,/content/training-data/sentiment_analysis/4/triples/baselines.txt
utterances,using,neighboring utterances ( of same speaker ,baselines,/content/training-data/sentiment_analysis/4/triples/baselines.txt
neighboring utterances ( of same speaker ),as,context,baselines,/content/training-data/sentiment_analysis/4/triples/baselines.txt
Baselines,has,memnet,baselines,/content/training-data/sentiment_analysis/4/triples/baselines.txt
memnet,is,end - toend memory network,baselines,/content/training-data/sentiment_analysis/4/triples/baselines.txt
Baselines,has,CMN models,baselines,/content/training-data/sentiment_analysis/4/triples/baselines.txt
CMN models,separate,contexts,baselines,/content/training-data/sentiment_analysis/4/triples/baselines.txt
contexts,to an,utterance,baselines,/content/training-data/sentiment_analysis/4/triples/baselines.txt
contexts,for,speaker and listener,baselines,/content/training-data/sentiment_analysis/4/triples/baselines.txt
Baselines,has,TFN,baselines,/content/training-data/sentiment_analysis/4/triples/baselines.txt
TFN,models,intra-and intermodality dynamics,baselines,/content/training-data/sentiment_analysis/4/triples/baselines.txt
intra-and intermodality dynamics,by explicitly aggregating,"uni - , bi- and trimodal interactions",baselines,/content/training-data/sentiment_analysis/4/triples/baselines.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/sentiment_analysis/4/triples/ablation-analysis.txt
Ablation analysis,has,Multi - hop vs No - hop,ablation-analysis,/content/training-data/sentiment_analysis/4/triples/ablation-analysis.txt
Multi - hop vs No - hop,removal of,multi-hop,ablation-analysis,/content/training-data/sentiment_analysis/4/triples/ablation-analysis.txt
multi-hop,leads to,worse performance,ablation-analysis,/content/training-data/sentiment_analysis/4/triples/ablation-analysis.txt
worse performance,than,removal of DGIM,ablation-analysis,/content/training-data/sentiment_analysis/4/triples/ablation-analysis.txt
Multi - hop vs No - hop,has,best performance,ablation-analysis,/content/training-data/sentiment_analysis/4/triples/ablation-analysis.txt
best performance,achieved by,variant 6,ablation-analysis,/content/training-data/sentiment_analysis/4/triples/ablation-analysis.txt
variant 6,which contains,all the proposed modules,ablation-analysis,/content/training-data/sentiment_analysis/4/triples/ablation-analysis.txt
all the proposed modules,in,its pipeline,ablation-analysis,/content/training-data/sentiment_analysis/4/triples/ablation-analysis.txt
Ablation analysis,has,DGIM,ablation-analysis,/content/training-data/sentiment_analysis/4/triples/ablation-analysis.txt
DGIM,prevents,storage of dynamic influences,ablation-analysis,/content/training-data/sentiment_analysis/4/triples/ablation-analysis.txt
storage of dynamic influences,between,speakers,ablation-analysis,/content/training-data/sentiment_analysis/4/triples/ablation-analysis.txt
speakers,leads to,performance deterioration,ablation-analysis,/content/training-data/sentiment_analysis/4/triples/ablation-analysis.txt
speakers,at each,historical time step,ablation-analysis,/content/training-data/sentiment_analysis/4/triples/ablation-analysis.txt
Ablation analysis,has,Self vs Dual History,ablation-analysis,/content/training-data/sentiment_analysis/4/triples/ablation-analysis.txt
Self vs Dual History,Compared to,"dual - history variants ( variants 3 , 5 , and 7 ",ablation-analysis,/content/training-data/sentiment_analysis/4/triples/ablation-analysis.txt
"dual - history variants ( variants 3 , 5 , and 7 )",has,these models,ablation-analysis,/content/training-data/sentiment_analysis/4/triples/ablation-analysis.txt
these models,provide,lesser performance,ablation-analysis,/content/training-data/sentiment_analysis/4/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/4/triples/model.txt
Model,given,test utterance,model,/content/training-data/sentiment_analysis/4/triples/model.txt
test utterance,has,ICON,model,/content/training-data/sentiment_analysis/4/triples/model.txt
ICON,considers,preceding utterances,model,/content/training-data/sentiment_analysis/4/triples/model.txt
preceding utterances,of,both speakers,model,/content/training-data/sentiment_analysis/4/triples/model.txt
both speakers,falling within,context - window,model,/content/training-data/sentiment_analysis/4/triples/model.txt
ICON,models,self - emotional influences,model,/content/training-data/sentiment_analysis/4/triples/model.txt
self - emotional influences,using,local gated recurrent units,model,/content/training-data/sentiment_analysis/4/triples/model.txt
test utterance,to be,classified,model,/content/training-data/sentiment_analysis/4/triples/model.txt
Model,At,each iteration,model,/content/training-data/sentiment_analysis/4/triples/model.txt
each iteration,has,representation of the test utterance,model,/content/training-data/sentiment_analysis/4/triples/model.txt
representation of the test utterance,improved with,summary representation,model,/content/training-data/sentiment_analysis/4/triples/model.txt
representation of the test utterance,used for,prediction,model,/content/training-data/sentiment_analysis/4/triples/model.txt
Model,to incorporate,inter -speaker influences,model,/content/training-data/sentiment_analysis/4/triples/model.txt
inter -speaker influences,has,global representation,model,/content/training-data/sentiment_analysis/4/triples/model.txt
global representation,is,generated,model,/content/training-data/sentiment_analysis/4/triples/model.txt
generated,using,GRU,model,/content/training-data/sentiment_analysis/4/triples/model.txt
GRU,intakes,output,model,/content/training-data/sentiment_analysis/4/triples/model.txt
output,of,local GRUs,model,/content/training-data/sentiment_analysis/4/triples/model.txt
Model,For,each instance,model,/content/training-data/sentiment_analysis/4/triples/model.txt
each instance,in,context - window,model,/content/training-data/sentiment_analysis/4/triples/model.txt
context - window,has,output,model,/content/training-data/sentiment_analysis/4/triples/model.txt
output,of,global GRU,model,/content/training-data/sentiment_analysis/4/triples/model.txt
global GRU,stored as,memory cell,model,/content/training-data/sentiment_analysis/4/triples/model.txt
Model,extracts,multimodal features,model,/content/training-data/sentiment_analysis/4/triples/model.txt
multimodal features,from,all utterancevideos,model,/content/training-data/sentiment_analysis/4/triples/model.txt
Model,has,memories,model,/content/training-data/sentiment_analysis/4/triples/model.txt
memories,subjected to,multiple read / write cycles,model,/content/training-data/sentiment_analysis/4/triples/model.txt
multiple read / write cycles,that include,attention mechanism,model,/content/training-data/sentiment_analysis/4/triples/model.txt
attention mechanism,for generating,contextual summaries,model,/content/training-data/sentiment_analysis/4/triples/model.txt
contextual summaries,of,conversational history,model,/content/training-data/sentiment_analysis/4/triples/model.txt
Model,propose,Interactive COnversational memory Network ( ICON ,model,/content/training-data/sentiment_analysis/4/triples/model.txt
Interactive COnversational memory Network ( ICON ),has,multimodal network,model,/content/training-data/sentiment_analysis/4/triples/model.txt
multimodal network,for identifying,emotions,model,/content/training-data/sentiment_analysis/4/triples/model.txt
emotions,in,utterance - videos,model,/content/training-data/sentiment_analysis/4/triples/model.txt
Contribution,has research problem,Multimodal Emotion Detection,research-problem,/content/training-data/sentiment_analysis/4/triples/research-problem.txt
Contribution,has research problem,Emotion recognition in conversations,research-problem,/content/training-data/sentiment_analysis/4/triples/research-problem.txt
Contribution,has research problem,Analyzing emotional dynamics in conversations,research-problem,/content/training-data/sentiment_analysis/4/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
Experimental setup,use,"Adam optimizer ( Kingma and Ba , 2014 ",experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
"Adam optimizer ( Kingma and Ba , 2014 )",for,training,experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
training,has,parameters,experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
parameters,starting with,initial learning rate,experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
initial learning rate,of,0.001,experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
Experimental setup,find,contextually conditioned features,experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
contextually conditioned features,perform better than,context - less features,experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
Experimental setup,For,multimodal feature extraction,experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
multimodal feature extraction,explore,different designs,experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
different designs,for,employed CNNs,experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
Experimental setup,For,visual features,experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
visual features,has,deeper CNN,experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
deeper CNN,provides,better representations,experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
Experimental setup,For,text,experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
text,find,single layer CNN,experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
single layer CNN,to perform at par with,deeper variants,experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
Experimental setup,has,20 % of the training set,experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
20 % of the training set,used as,validation set,experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
validation set,for,hyper - parameter tuning,experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
Experimental setup,has,Termination,experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
Termination,of,training - phase,experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
training - phase,decided by,early - stopping,experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
early - stopping,with,patience,experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
patience,of,10 d = 100 dv = 512 dem = 100 K = 40 R = 3 epochs,experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
Experimental setup,has,best hyper - parameters,experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
best hyper - parameters,decided using,gridsearch,experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
Experimental setup,has,network,experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
network,subjected to,regularization,experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
regularization,for,norm,experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
norm,of,40,experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
regularization,in the form of,Dropout and Gradient - clipping,experimental-setup,/content/training-data/sentiment_analysis/4/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/4/triples/results.txt
Results,In,labels,results,/content/training-data/sentiment_analysis/4/triples/results.txt
labels,has,ICON,results,/content/training-data/sentiment_analysis/4/triples/results.txt
ICON,attains,improved performance,results,/content/training-data/sentiment_analysis/4/triples/results.txt
improved performance,over,counterparts,results,/content/training-data/sentiment_analysis/4/triples/results.txt
Results,For,each emotion,results,/content/training-data/sentiment_analysis/4/triples/results.txt
each emotion,has,ICON,results,/content/training-data/sentiment_analysis/4/triples/results.txt
ICON,has,performance,results,/content/training-data/sentiment_analysis/4/triples/results.txt
performance,at par with,c LSTM,results,/content/training-data/sentiment_analysis/4/triples/results.txt
performance,without,significant gap,results,/content/training-data/sentiment_analysis/4/triples/results.txt
ICON,outperforms,all the compared models,results,/content/training-data/sentiment_analysis/4/triples/results.txt
all the compared models,except for,happiness emotion,results,/content/training-data/sentiment_analysis/4/triples/results.txt
Results,has,trimodal network,results,/content/training-data/sentiment_analysis/4/triples/results.txt
trimodal network,provides,best performance,results,/content/training-data/sentiment_analysis/4/triples/results.txt
best performance,preceded by,bimodal variants,results,/content/training-data/sentiment_analysis/4/triples/results.txt
Results,has,audio and visual modality,results,/content/training-data/sentiment_analysis/4/triples/results.txt
audio and visual modality,when used with,text,results,/content/training-data/sentiment_analysis/4/triples/results.txt
text,shared,complementary data,results,/content/training-data/sentiment_analysis/4/triples/results.txt
complementary data,to improve,over all performance,results,/content/training-data/sentiment_analysis/4/triples/results.txt
Results,has,ICON,results,/content/training-data/sentiment_analysis/4/triples/results.txt
ICON,performs better than,compared models,results,/content/training-data/sentiment_analysis/4/triples/results.txt
compared models,with,significant performance increase,results,/content/training-data/sentiment_analysis/4/triples/results.txt
significant performance increase,in,emotions ( ? 2.1 % acc. ,results,/content/training-data/sentiment_analysis/4/triples/results.txt
ICON,manages to,identify,results,/content/training-data/sentiment_analysis/4/triples/results.txt
identify,correctly,relatively similar excitement emotion,results,/content/training-data/sentiment_analysis/4/triples/results.txt
relatively similar excitement emotion,by,large margin,results,/content/training-data/sentiment_analysis/4/triples/results.txt
Results,Among,unimodals,results,/content/training-data/sentiment_analysis/4/triples/results.txt
unimodals,has,language modality,results,/content/training-data/sentiment_analysis/4/triples/results.txt
language modality,performs,best,results,/content/training-data/sentiment_analysis/4/triples/results.txt
Results,presents,different combinations of modes,results,/content/training-data/sentiment_analysis/4/triples/results.txt
different combinations of modes,used by,ICON,results,/content/training-data/sentiment_analysis/4/triples/results.txt
ICON,on,IEMOCAP,results,/content/training-data/sentiment_analysis/4/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/sentiment_analysis/48/triples/baselines.txt
Baselines,has,TNet - AS,baselines,/content/training-data/sentiment_analysis/48/triples/baselines.txt
TNet - AS,Without using,attention module,baselines,/content/training-data/sentiment_analysis/48/triples/baselines.txt
Baselines,has,IAN,baselines,/content/training-data/sentiment_analysis/48/triples/baselines.txt
IAN,adopts,two LSTMs,baselines,/content/training-data/sentiment_analysis/48/triples/baselines.txt
two LSTMs,to derive,representations,baselines,/content/training-data/sentiment_analysis/48/triples/baselines.txt
representations,of,context and the target phrase,baselines,/content/training-data/sentiment_analysis/48/triples/baselines.txt
IAN,has,concatenation,baselines,/content/training-data/sentiment_analysis/48/triples/baselines.txt
concatenation,fed to,softmax layer,baselines,/content/training-data/sentiment_analysis/48/triples/baselines.txt
Baselines,has,BILSTM - ATT -G,baselines,/content/training-data/sentiment_analysis/48/triples/baselines.txt
BILSTM - ATT -G,models,left and right contexts,baselines,/content/training-data/sentiment_analysis/48/triples/baselines.txt
left and right contexts,using,two attention - based LSTMs,baselines,/content/training-data/sentiment_analysis/48/triples/baselines.txt
BILSTM - ATT -G,makes use of,special gate layer,baselines,/content/training-data/sentiment_analysis/48/triples/baselines.txt
special gate layer,to combine,two representations,baselines,/content/training-data/sentiment_analysis/48/triples/baselines.txt
Baselines,has,TC - LSTM,baselines,/content/training-data/sentiment_analysis/48/triples/baselines.txt
TC - LSTM,has,concatenation,baselines,/content/training-data/sentiment_analysis/48/triples/baselines.txt
concatenation,of,two representations,baselines,/content/training-data/sentiment_analysis/48/triples/baselines.txt
two representations,used to,predict,baselines,/content/training-data/sentiment_analysis/48/triples/baselines.txt
predict,has,label,baselines,/content/training-data/sentiment_analysis/48/triples/baselines.txt
TC - LSTM,has,Two LSTMs,baselines,/content/training-data/sentiment_analysis/48/triples/baselines.txt
Two LSTMs,used to,model,baselines,/content/training-data/sentiment_analysis/48/triples/baselines.txt
model,has,left and right context,baselines,/content/training-data/sentiment_analysis/48/triples/baselines.txt
left and right context,of,target,baselines,/content/training-data/sentiment_analysis/48/triples/baselines.txt
Baselines,has,MemNet,baselines,/content/training-data/sentiment_analysis/48/triples/baselines.txt
MemNet,uses,attention mechanism,baselines,/content/training-data/sentiment_analysis/48/triples/baselines.txt
attention mechanism,over,word embedding,baselines,/content/training-data/sentiment_analysis/48/triples/baselines.txt
word embedding,over,multiple rounds,baselines,/content/training-data/sentiment_analysis/48/triples/baselines.txt
attention mechanism,to aggregate,information,baselines,/content/training-data/sentiment_analysis/48/triples/baselines.txt
information,in,sentence,baselines,/content/training-data/sentiment_analysis/48/triples/baselines.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/48/triples/hyperparameters.txt
Hyperparameters,has,number of selfattention heads,hyperparameters,/content/training-data/sentiment_analysis/48/triples/hyperparameters.txt
number of selfattention heads,is,8,hyperparameters,/content/training-data/sentiment_analysis/48/triples/hyperparameters.txt
Hyperparameters,has,number of units,hyperparameters,/content/training-data/sentiment_analysis/48/triples/hyperparameters.txt
number of units,in,encoder and the decoder,hyperparameters,/content/training-data/sentiment_analysis/48/triples/hyperparameters.txt
number of units,is,100,hyperparameters,/content/training-data/sentiment_analysis/48/triples/hyperparameters.txt
Hyperparameters,has,KL weight,hyperparameters,/content/training-data/sentiment_analysis/48/triples/hyperparameters.txt
KL weight,set to be,1e - 4,hyperparameters,/content/training-data/sentiment_analysis/48/triples/hyperparameters.txt
Hyperparameters,has,number of layers,hyperparameters,/content/training-data/sentiment_analysis/48/triples/hyperparameters.txt
number of layers,of both,Transformer blocks,hyperparameters,/content/training-data/sentiment_analysis/48/triples/hyperparameters.txt
Transformer blocks,is,2,hyperparameters,/content/training-data/sentiment_analysis/48/triples/hyperparameters.txt
Hyperparameters,has,latent variable,hyperparameters,/content/training-data/sentiment_analysis/48/triples/hyperparameters.txt
latent variable,of,size 50,hyperparameters,/content/training-data/sentiment_analysis/48/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/48/triples/model.txt
Model,by separating,representation,model,/content/training-data/sentiment_analysis/48/triples/model.txt
representation,of,input sentence,model,/content/training-data/sentiment_analysis/48/triples/model.txt
representation,has,classifier,model,/content/training-data/sentiment_analysis/48/triples/model.txt
classifier,becomes,independent module,model,/content/training-data/sentiment_analysis/48/triples/model.txt
independent module,in,our framework,model,/content/training-data/sentiment_analysis/48/triples/model.txt
Model,proposed,classifier - agnostic framework,model,/content/training-data/sentiment_analysis/48/triples/model.txt
classifier - agnostic framework,named,"Aspect - term Semi-supervised Variational Autoencoder ( Kingma and Welling , 2014 ) based on Transformer ( ASVAET ",model,/content/training-data/sentiment_analysis/48/triples/model.txt
Model,By regarding,aspect sentiment polarity,model,/content/training-data/sentiment_analysis/48/triples/model.txt
aspect sentiment polarity,as,discrete latent variable,model,/content/training-data/sentiment_analysis/48/triples/model.txt
aspect sentiment polarity,of,unlabeled data,model,/content/training-data/sentiment_analysis/48/triples/model.txt
aspect sentiment polarity,has,model,model,/content/training-data/sentiment_analysis/48/triples/model.txt
model,implicitly induces,sentiment polarity,model,/content/training-data/sentiment_analysis/48/triples/model.txt
sentiment polarity,via,variational inference,model,/content/training-data/sentiment_analysis/48/triples/model.txt
Model,has,aspect - term sentiment polarity,model,/content/training-data/sentiment_analysis/48/triples/model.txt
aspect - term sentiment polarity,inferred from,specific ATSA classifier,model,/content/training-data/sentiment_analysis/48/triples/model.txt
Model,has,variational autoencoder,model,/content/training-data/sentiment_analysis/48/triples/model.txt
variational autoencoder,offers,flexibility,model,/content/training-data/sentiment_analysis/48/triples/model.txt
flexibility,to customize,model structure,model,/content/training-data/sentiment_analysis/48/triples/model.txt
Model,has,representation,model,/content/training-data/sentiment_analysis/48/triples/model.txt
representation,of,lexical context,model,/content/training-data/sentiment_analysis/48/triples/model.txt
lexical context,extracted by,encoder,model,/content/training-data/sentiment_analysis/48/triples/model.txt
Contribution,has research problem,Aspect - term Sentiment Analysis,research-problem,/content/training-data/sentiment_analysis/48/triples/research-problem.txt
Contribution,has research problem,aspect - term sentiment analysis ( ATSA ,research-problem,/content/training-data/sentiment_analysis/48/triples/research-problem.txt
Contribution,has research problem,aspect - category sentiment analysis ( ACSA ,research-problem,/content/training-data/sentiment_analysis/48/triples/research-problem.txt
Contribution,has research problem,ACSA,research-problem,/content/training-data/sentiment_analysis/48/triples/research-problem.txt
Contribution,has research problem,ATSA,research-problem,/content/training-data/sentiment_analysis/48/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/48/triples/results.txt
Results,For,MemNet,results,/content/training-data/sentiment_analysis/48/triples/results.txt
MemNet,has,test accuracy,results,/content/training-data/sentiment_analysis/48/triples/results.txt
test accuracy,improved by,about 2 %,results,/content/training-data/sentiment_analysis/48/triples/results.txt
about 2 %,by,TSSVAE,results,/content/training-data/sentiment_analysis/48/triples/results.txt
Results,adoption of,indomain pre-trained word vectors,results,/content/training-data/sentiment_analysis/48/triples/results.txt
indomain pre-trained word vectors,beneficial for,performance,results,/content/training-data/sentiment_analysis/48/triples/results.txt
performance,compared with,Glove vectors,results,/content/training-data/sentiment_analysis/48/triples/results.txt
Results,has,TNet - AS,results,/content/training-data/sentiment_analysis/48/triples/results.txt
TNet - AS,outperforms,other three models,results,/content/training-data/sentiment_analysis/48/triples/results.txt
Results,has,ASVAET,results,/content/training-data/sentiment_analysis/48/triples/results.txt
ASVAET,able to improve,supervised performance,results,/content/training-data/sentiment_analysis/48/triples/results.txt
supervised performance,for,all classifiers,results,/content/training-data/sentiment_analysis/48/triples/results.txt
ASVAET,outperforms,compared semisupervised methods,results,/content/training-data/sentiment_analysis/48/triples/results.txt
Results,Compared with,other two semi-supervised methods,results,/content/training-data/sentiment_analysis/48/triples/results.txt
other two semi-supervised methods,has,ASVAET,results,/content/training-data/sentiment_analysis/48/triples/results.txt
ASVAET,shows,better results,results,/content/training-data/sentiment_analysis/48/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/sentiment_analysis/35/triples/baselines.txt
Baselines,has,Non-Transfer,baselines,/content/training-data/sentiment_analysis/35/triples/baselines.txt
Non-Transfer,has,Target Network ( TN ,baselines,/content/training-data/sentiment_analysis/35/triples/baselines.txt
Target Network ( TN ),is,proposed base model ( BiLSTM + C2A + Pas ,baselines,/content/training-data/sentiment_analysis/35/triples/baselines.txt
proposed base model ( BiLSTM + C2A + Pas ),trained on,D t,baselines,/content/training-data/sentiment_analysis/35/triples/baselines.txt
D t,for,target task,baselines,/content/training-data/sentiment_analysis/35/triples/baselines.txt
Baselines,has,Transfer,baselines,/content/training-data/sentiment_analysis/35/triples/baselines.txt
Transfer,has,Source- only ( SO ,baselines,/content/training-data/sentiment_analysis/35/triples/baselines.txt
Source- only ( SO ),uses,source network,baselines,/content/training-data/sentiment_analysis/35/triples/baselines.txt
source network,tests it on,D t,baselines,/content/training-data/sentiment_analysis/35/triples/baselines.txt
source network,trained on,D s,baselines,/content/training-data/sentiment_analysis/35/triples/baselines.txt
D s,to initialize,target network,baselines,/content/training-data/sentiment_analysis/35/triples/baselines.txt
Transfer,has,M - MMD,baselines,/content/training-data/sentiment_analysis/35/triples/baselines.txt
M - MMD,aligns,different class distributions,baselines,/content/training-data/sentiment_analysis/35/triples/baselines.txt
different class distributions,between,domains,baselines,/content/training-data/sentiment_analysis/35/triples/baselines.txt
domains,based on,multiple Maximum Mean Discrepancy ( MMD ,baselines,/content/training-data/sentiment_analysis/35/triples/baselines.txt
Transfer,has,Fine-tuning ( FT ,baselines,/content/training-data/sentiment_analysis/35/triples/baselines.txt
Fine-tuning ( FT ),advances,SO,baselines,/content/training-data/sentiment_analysis/35/triples/baselines.txt
SO,with further finetuning,target network,baselines,/content/training-data/sentiment_analysis/35/triples/baselines.txt
target network,on,D t,baselines,/content/training-data/sentiment_analysis/35/triples/baselines.txt
Transfer,has,M- DAN,baselines,/content/training-data/sentiment_analysis/35/triples/baselines.txt
M- DAN,is,multi-adversarial version,baselines,/content/training-data/sentiment_analysis/35/triples/baselines.txt
multi-adversarial version,based on,multiple domain discriminators,baselines,/content/training-data/sentiment_analysis/35/triples/baselines.txt
multi-adversarial version,of,Domain Adversarial Network ( DAN ,baselines,/content/training-data/sentiment_analysis/35/triples/baselines.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
Hyperparameters,tuned on,10 % randomly held - out training data,hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
10 % randomly held - out training data,fixed to be used in all,transfer pairs,hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
10 % randomly held - out training data,of,target domain,hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
target domain,in,R1?L task,hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
Hyperparameters,perform,early stopping,hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
early stopping,on,validation set,hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
validation set,during,training process,hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
Hyperparameters,has,All weights,hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
All weights,randomly initialized from,"uniform distribution U ( ? 0.01 , 0.01 ",hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
All weights,in,networks,hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
Hyperparameters,has,Gradients,hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
Gradients,with,2 norm,hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
2 norm,larger than,40,hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
Gradients,are,normalized,hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
normalized,to be,40,hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
Hyperparameters,has,word embeddings,hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
word embeddings,fine - tuned during,training,hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
word embeddings,initialized with,200 - dimension GloVE vectors,hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
Hyperparameters,has,Adam ( Kingma and Ba 2014 ,hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
Adam ( Kingma and Ba 2014 ),used as,optimizer,hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
optimizer,with,initial learning rate 10 ? 4,hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
Hyperparameters,has,batch sizes,hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
batch sizes,are,64 and 32,hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
64 and 32,for,source and target domains,hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
Hyperparameters,has,fc layer size,hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
fc layer size,is,300,hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
Hyperparameters,To alleviate,overfitting,hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
overfitting,apply,dropout,hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
dropout,on,word embeddings,hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
word embeddings,with,dropout rate 0.5,hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
word embeddings,of,context,hyperparameters,/content/training-data/sentiment_analysis/35/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/35/triples/model.txt
Model,To prevent,false alignment,model,/content/training-data/sentiment_analysis/35/triples/model.txt
false alignment,adopt,Contrastive Feature Alignment ( CFA ,model,/content/training-data/sentiment_analysis/35/triples/model.txt
Contrastive Feature Alignment ( CFA ),to semantically align,aspect - specific representations,model,/content/training-data/sentiment_analysis/35/triples/model.txt
Model,to reduce,task discrepancy,model,/content/training-data/sentiment_analysis/35/triples/model.txt
task discrepancy,modeling,two tasks,model,/content/training-data/sentiment_analysis/35/triples/model.txt
two tasks,at,same fine - grained level,model,/content/training-data/sentiment_analysis/35/triples/model.txt
task discrepancy,propose,novel Coarse2 Fine ( C2F ) attention module,model,/content/training-data/sentiment_analysis/35/triples/model.txt
novel Coarse2 Fine ( C2F ) attention module,to help,source task,model,/content/training-data/sentiment_analysis/35/triples/model.txt
source task,automatically capture,corresponding aspect term,model,/content/training-data/sentiment_analysis/35/triples/model.txt
corresponding aspect term,in,context,model,/content/training-data/sentiment_analysis/35/triples/model.txt
corresponding aspect term,towards,given aspect category,model,/content/training-data/sentiment_analysis/35/triples/model.txt
task discrepancy,between,domains,model,/content/training-data/sentiment_analysis/35/triples/model.txt
Model,has,MGAN,model,/content/training-data/sentiment_analysis/35/triples/model.txt
MGAN,consists of,two networks,model,/content/training-data/sentiment_analysis/35/triples/model.txt
two networks,for learning,aspect - specific representations,model,/content/training-data/sentiment_analysis/35/triples/model.txt
aspect - specific representations,for,two domains,model,/content/training-data/sentiment_analysis/35/triples/model.txt
Model,has,CFA,model,/content/training-data/sentiment_analysis/35/triples/model.txt
CFA,considers,semantic separation,model,/content/training-data/sentiment_analysis/35/triples/model.txt
semantic separation,by guaranteeing,distributions,model,/content/training-data/sentiment_analysis/35/triples/model.txt
distributions,from both,different classes and domains,model,/content/training-data/sentiment_analysis/35/triples/model.txt
distributions,to be,as dissimilar as possible,model,/content/training-data/sentiment_analysis/35/triples/model.txt
CFA,considers,semantic alignment,model,/content/training-data/sentiment_analysis/35/triples/model.txt
semantic alignment,by maximally ensuring,equivalent distributions,model,/content/training-data/sentiment_analysis/35/triples/model.txt
equivalent distributions,from,different domains,model,/content/training-data/sentiment_analysis/35/triples/model.txt
different domains,same,class,model,/content/training-data/sentiment_analysis/35/triples/model.txt
Model,propose,novel framework named Multi - Granularity Alignment Network ( MGAN ,model,/content/training-data/sentiment_analysis/35/triples/model.txt
novel framework named Multi - Granularity Alignment Network ( MGAN ),simultaneously align,aspect granularity and aspect- specific feature representations,model,/content/training-data/sentiment_analysis/35/triples/model.txt
aspect granularity and aspect- specific feature representations,across,domains,model,/content/training-data/sentiment_analysis/35/triples/model.txt
Contribution,has research problem,Aspect - level Sentiment Classification,research-problem,/content/training-data/sentiment_analysis/35/triples/research-problem.txt
Contribution,has research problem,Aspect - level sentiment classification ( ASC ,research-problem,/content/training-data/sentiment_analysis/35/triples/research-problem.txt
Contribution,has research problem,aspect - oriented sentiment analysis,research-problem,/content/training-data/sentiment_analysis/35/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/35/triples/results.txt
Results,has,Comparison with Transfer,results,/content/training-data/sentiment_analysis/35/triples/results.txt
Comparison with Transfer,has,popular technique FT,results,/content/training-data/sentiment_analysis/35/triples/results.txt
popular technique FT,can not achieve,satisfactory results,results,/content/training-data/sentiment_analysis/35/triples/results.txt
Comparison with Transfer,has,MGAN,results,/content/training-data/sentiment_analysis/35/triples/results.txt
MGAN,considers both of them in,point - wise surrogate,results,/content/training-data/sentiment_analysis/35/triples/results.txt
point - wise surrogate,improves,performance,results,/content/training-data/sentiment_analysis/35/triples/results.txt
performance,of,our method,results,/content/training-data/sentiment_analysis/35/triples/results.txt
MGAN,outperforms,ablation MGAN w/ o SS,results,/content/training-data/sentiment_analysis/35/triples/results.txt
ablation MGAN w/ o SS,removing,semantic separation loss,results,/content/training-data/sentiment_analysis/35/triples/results.txt
semantic separation loss,of,CFA,results,/content/training-data/sentiment_analysis/35/triples/results.txt
CFA,by,0.81 %,results,/content/training-data/sentiment_analysis/35/triples/results.txt
0.81 %,for,accuracy,results,/content/training-data/sentiment_analysis/35/triples/results.txt
CFA,by,1.00 %,results,/content/training-data/sentiment_analysis/35/triples/results.txt
1.00 %,for,Macro - F1 on average,results,/content/training-data/sentiment_analysis/35/triples/results.txt
Comparison with Transfer,has,full model MGAN,results,/content/training-data/sentiment_analysis/35/triples/results.txt
full model MGAN,outperforms,M - DAN and M - MMD,results,/content/training-data/sentiment_analysis/35/triples/results.txt
M - DAN and M - MMD,by,1.90 % and 1.66 %,results,/content/training-data/sentiment_analysis/35/triples/results.txt
1.90 % and 1.66 %,for,Marco - F1 on average,results,/content/training-data/sentiment_analysis/35/triples/results.txt
M - DAN and M - MMD,by,1.80 % and 1.33 %,results,/content/training-data/sentiment_analysis/35/triples/results.txt
1.80 % and 1.33 %,for,accuracy,results,/content/training-data/sentiment_analysis/35/triples/results.txt
Comparison with Transfer,has,SO,results,/content/training-data/sentiment_analysis/35/triples/results.txt
SO,performs,poorly,results,/content/training-data/sentiment_analysis/35/triples/results.txt
Results,has,Comparison with Non - Transfer,results,/content/training-data/sentiment_analysis/35/triples/results.txt
Comparison with Non - Transfer,has,MGAN,results,/content/training-data/sentiment_analysis/35/triples/results.txt
MGAN,consistently outperforms,MGAN w / o C2 F,results,/content/training-data/sentiment_analysis/35/triples/results.txt
MGAN w / o C2 F,where,source position information,results,/content/training-data/sentiment_analysis/35/triples/results.txt
source position information,is,missed,results,/content/training-data/sentiment_analysis/35/triples/results.txt
missed,by,"1.41 % , 1.03 % , 1.09 %",results,/content/training-data/sentiment_analysis/35/triples/results.txt
"1.41 % , 1.03 % , 1.09 %",for,accuracy,results,/content/training-data/sentiment_analysis/35/triples/results.txt
missed,by,"1.79 % , 3.62 % and 1.16 %",results,/content/training-data/sentiment_analysis/35/triples/results.txt
"1.79 % , 3.62 % and 1.16 %",for,Macro - F1,results,/content/training-data/sentiment_analysis/35/triples/results.txt
MGAN w / o C2 F,where,C2F module,results,/content/training-data/sentiment_analysis/35/triples/results.txt
C2F module,of,source network,results,/content/training-data/sentiment_analysis/35/triples/results.txt
C2F module,is,removed,results,/content/training-data/sentiment_analysis/35/triples/results.txt
Comparison with Non - Transfer,has,MGAN w / o PI,results,/content/training-data/sentiment_analysis/35/triples/results.txt
MGAN w / o PI,performs,very poorly,results,/content/training-data/sentiment_analysis/35/triples/results.txt
MGAN w / o PI,does not utilize,position information,results,/content/training-data/sentiment_analysis/35/triples/results.txt
Results,has,Effect of C2F Attention Module,results,/content/training-data/sentiment_analysis/35/triples/results.txt
Effect of C2F Attention Module,benefited from,distilled knowledge,results,/content/training-data/sentiment_analysis/35/triples/results.txt
distilled knowledge,from,source task,results,/content/training-data/sentiment_analysis/35/triples/results.txt
distilled knowledge,has,MGAN,results,/content/training-data/sentiment_analysis/35/triples/results.txt
MGAN,can better model,complicated relatedness,results,/content/training-data/sentiment_analysis/35/triples/results.txt
complicated relatedness,between,context and aspect term,results,/content/training-data/sentiment_analysis/35/triples/results.txt
context and aspect term,for,target domain L,results,/content/training-data/sentiment_analysis/35/triples/results.txt
distilled knowledge,has,MGAN w / o C2F,results,/content/training-data/sentiment_analysis/35/triples/results.txt
MGAN w / o C2F,performs,poorly,results,/content/training-data/sentiment_analysis/35/triples/results.txt
Effect of C2F Attention Module,has,MGAN,results,/content/training-data/sentiment_analysis/35/triples/results.txt
MGAN,uses,C2F,results,/content/training-data/sentiment_analysis/35/triples/results.txt
C2F,to capture,more specific aspect terms,results,/content/training-data/sentiment_analysis/35/triples/results.txt
more specific aspect terms,from,context,results,/content/training-data/sentiment_analysis/35/triples/results.txt
context,towards,aspect category,results,/content/training-data/sentiment_analysis/35/triples/results.txt
more specific aspect terms,helps,source task,results,/content/training-data/sentiment_analysis/35/triples/results.txt
source task,capture,more fine - grained semantics,results,/content/training-data/sentiment_analysis/35/triples/results.txt
more fine - grained semantics,of,aspect category,results,/content/training-data/sentiment_analysis/35/triples/results.txt
source task,capture,detailed position information,results,/content/training-data/sentiment_analysis/35/triples/results.txt
detailed position information,like,target task,results,/content/training-data/sentiment_analysis/35/triples/results.txt
MGAN,compared with,MGAN w / o C2F,results,/content/training-data/sentiment_analysis/35/triples/results.txt
MGAN w / o C2F,locates,wrong sentiment contexts,results,/content/training-data/sentiment_analysis/35/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/sentiment_analysis/37/triples/baselines.txt
Baselines,has,LSTM,baselines,/content/training-data/sentiment_analysis/37/triples/baselines.txt
LSTM,has,sentence,baselines,/content/training-data/sentiment_analysis/37/triples/baselines.txt
sentence,fed to,short - term memory ( LSTM ) network,baselines,/content/training-data/sentiment_analysis/37/triples/baselines.txt
short - term memory ( LSTM ) network,to propagate,context,baselines,/content/training-data/sentiment_analysis/37/triples/baselines.txt
context,among,constituent words,baselines,/content/training-data/sentiment_analysis/37/triples/baselines.txt
Baselines,has,TD- LSTM,baselines,/content/training-data/sentiment_analysis/37/triples/baselines.txt
TD- LSTM,has,sequence,baselines,/content/training-data/sentiment_analysis/37/triples/baselines.txt
sequence,of,words preceding ( left context ) and succeeding ( right context ) target aspect term,baselines,/content/training-data/sentiment_analysis/37/triples/baselines.txt
words preceding ( left context ) and succeeding ( right context ) target aspect term,fed to,two different LSTMs,baselines,/content/training-data/sentiment_analysis/37/triples/baselines.txt
Baselines,has,ATAE - LSTM,baselines,/content/training-data/sentiment_analysis/37/triples/baselines.txt
ATAE - LSTM,identical to,AE - LSTM,baselines,/content/training-data/sentiment_analysis/37/triples/baselines.txt
AE - LSTM,except,LSTM,baselines,/content/training-data/sentiment_analysis/37/triples/baselines.txt
LSTM,fed with,concatenation,baselines,/content/training-data/sentiment_analysis/37/triples/baselines.txt
concatenation,of,aspect - term representation and word representation,baselines,/content/training-data/sentiment_analysis/37/triples/baselines.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/37/triples/model.txt
Model,employ,memory networks,model,/content/training-data/sentiment_analysis/37/triples/model.txt
memory networks,to repeatedly match,target aspect representation,model,/content/training-data/sentiment_analysis/37/triples/model.txt
target aspect representation,with,other aspects,model,/content/training-data/sentiment_analysis/37/triples/model.txt
target aspect representation,to generate,more accurate representation,model,/content/training-data/sentiment_analysis/37/triples/model.txt
more accurate representation,of,target aspect,model,/content/training-data/sentiment_analysis/37/triples/model.txt
more accurate representation,name,refined representation,model,/content/training-data/sentiment_analysis/37/triples/model.txt
refined representation,fed to,softmax classifier,model,/content/training-data/sentiment_analysis/37/triples/model.txt
softmax classifier,for,final classification,model,/content/training-data/sentiment_analysis/37/triples/model.txt
Model,independently generate,aspect - aware sentence representations,model,/content/training-data/sentiment_analysis/37/triples/model.txt
aspect - aware sentence representations,using,gated recurrent unit ( GRU ,model,/content/training-data/sentiment_analysis/37/triples/model.txt
aspect - aware sentence representations,using,attention mechanism,model,/content/training-data/sentiment_analysis/37/triples/model.txt
aspect - aware sentence representations,for,all the aspects,model,/content/training-data/sentiment_analysis/37/triples/model.txt
Contribution,has research problem,Aspect - Based Sentiment Analysis,research-problem,/content/training-data/sentiment_analysis/37/triples/research-problem.txt
Contribution,has research problem,Sentiment analysis,research-problem,/content/training-data/sentiment_analysis/37/triples/research-problem.txt
Contribution,has research problem,Aspect - based sentiment analysis ( ABSA ,research-problem,/content/training-data/sentiment_analysis/37/triples/research-problem.txt
Contribution,has research problem,ABSA,research-problem,/content/training-data/sentiment_analysis/37/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/37/triples/results.txt
Results,evident,our IARM model,results,/content/training-data/sentiment_analysis/37/triples/results.txt
our IARM model,outperforms,all the baseline models,results,/content/training-data/sentiment_analysis/37/triples/results.txt
all the baseline models,in both of,domains,results,/content/training-data/sentiment_analysis/37/triples/results.txt
all the baseline models,including,state of the art,results,/content/training-data/sentiment_analysis/37/triples/results.txt
Results,obtained,bigger improvement,results,/content/training-data/sentiment_analysis/37/triples/results.txt
bigger improvement,in,laptop domain,results,/content/training-data/sentiment_analysis/37/triples/results.txt
laptop domain,of,1.7 %,results,/content/training-data/sentiment_analysis/37/triples/results.txt
laptop domain,compared to,restaurant domain,results,/content/training-data/sentiment_analysis/37/triples/results.txt
restaurant domain,of,1.4 %,results,/content/training-data/sentiment_analysis/37/triples/results.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/8/triples/model.txt
Model,compare,performance,model,/content/training-data/sentiment_analysis/8/triples/model.txt
performance,of,lighter machine learning models,model,/content/training-data/sentiment_analysis/8/triples/model.txt
lighter machine learning models,with,heavily data - reliant deep learning models,model,/content/training-data/sentiment_analysis/8/triples/model.txt
Model,extract,handcrafted features,model,/content/training-data/sentiment_analysis/8/triples/model.txt
handcrafted features,from,time domain,model,/content/training-data/sentiment_analysis/8/triples/model.txt
time domain,of,audio signal,model,/content/training-data/sentiment_analysis/8/triples/model.txt
handcrafted features,train,respective models,model,/content/training-data/sentiment_analysis/8/triples/model.txt
Model,explore,implication of hand - crafted features,model,/content/training-data/sentiment_analysis/8/triples/model.txt
implication of hand - crafted features,for,SER,model,/content/training-data/sentiment_analysis/8/triples/model.txt
Model,In,first approach,model,/content/training-data/sentiment_analysis/8/triples/model.txt
first approach,train,traditional machine learning classifiers,model,/content/training-data/sentiment_analysis/8/triples/model.txt
traditional machine learning classifiers,namely,Random Forests,model,/content/training-data/sentiment_analysis/8/triples/model.txt
traditional machine learning classifiers,namely,Gradient Boosting,model,/content/training-data/sentiment_analysis/8/triples/model.txt
traditional machine learning classifiers,namely,Support Vector Machines,model,/content/training-data/sentiment_analysis/8/triples/model.txt
traditional machine learning classifiers,namely,Naive - Bayes,model,/content/training-data/sentiment_analysis/8/triples/model.txt
traditional machine learning classifiers,namely,Logistic Regression,model,/content/training-data/sentiment_analysis/8/triples/model.txt
Model,In,second approach,model,/content/training-data/sentiment_analysis/8/triples/model.txt
second approach,build,Multi - Layer Perceptron and an LSTM classifier,model,/content/training-data/sentiment_analysis/8/triples/model.txt
Multi - Layer Perceptron and an LSTM classifier,to recognize,emotion,model,/content/training-data/sentiment_analysis/8/triples/model.txt
emotion,given,speech signal,model,/content/training-data/sentiment_analysis/8/triples/model.txt
Model,combine,features,model,/content/training-data/sentiment_analysis/8/triples/model.txt
features,from,textual modality,model,/content/training-data/sentiment_analysis/8/triples/model.txt
textual modality,to understand,correlation between different modalities,model,/content/training-data/sentiment_analysis/8/triples/model.txt
textual modality,aid,ambiguity resolution,model,/content/training-data/sentiment_analysis/8/triples/model.txt
Contribution,has research problem,Multimodal Speech Emotion Recognition and Ambiguity Resolution,research-problem,/content/training-data/sentiment_analysis/8/triples/research-problem.txt
Contribution,has research problem,Identifying emotion from speech,research-problem,/content/training-data/sentiment_analysis/8/triples/research-problem.txt
Contribution,has research problem,Speech Emotion Recognition ( SER ,research-problem,/content/training-data/sentiment_analysis/8/triples/research-problem.txt
Contribution,has research problem,SER,research-problem,/content/training-data/sentiment_analysis/8/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/sentiment_analysis/8/triples/experimental-setup.txt
Experimental setup,stop,training,experimental-setup,/content/training-data/sentiment_analysis/8/triples/experimental-setup.txt
training,when,do not see any improvement,experimental-setup,/content/training-data/sentiment_analysis/8/triples/experimental-setup.txt
do not see any improvement,in,validation performance,experimental-setup,/content/training-data/sentiment_analysis/8/triples/experimental-setup.txt
validation performance,for,> 10 epochs,experimental-setup,/content/training-data/sentiment_analysis/8/triples/experimental-setup.txt
Experimental setup,use,PyTorch,experimental-setup,/content/training-data/sentiment_analysis/8/triples/experimental-setup.txt
PyTorch,to implement,LSTM classifiers,experimental-setup,/content/training-data/sentiment_analysis/8/triples/experimental-setup.txt
Experimental setup,use,scikit - learn and xgboost,experimental-setup,/content/training-data/sentiment_analysis/8/triples/experimental-setup.txt
scikit - learn and xgboost,to implement,"all the ML classifiers ( RF , XGB , SVM , MNB , and LR ) and the MLP",experimental-setup,/content/training-data/sentiment_analysis/8/triples/experimental-setup.txt
Experimental setup,use,"librosa , a Python library",experimental-setup,/content/training-data/sentiment_analysis/8/triples/experimental-setup.txt
"librosa , a Python library",to process,audio files,experimental-setup,/content/training-data/sentiment_analysis/8/triples/experimental-setup.txt
Experimental setup,to regularize,hidden space,experimental-setup,/content/training-data/sentiment_analysis/8/triples/experimental-setup.txt
hidden space,use,shut - off mechanism,experimental-setup,/content/training-data/sentiment_analysis/8/triples/experimental-setup.txt
shut - off mechanism,called,dropout,experimental-setup,/content/training-data/sentiment_analysis/8/triples/experimental-setup.txt
shut - off mechanism,where,fraction of neurons are not used,experimental-setup,/content/training-data/sentiment_analysis/8/triples/experimental-setup.txt
fraction of neurons are not used,for,final prediction,experimental-setup,/content/training-data/sentiment_analysis/8/triples/experimental-setup.txt
hidden space,of,LSTM classifiers,experimental-setup,/content/training-data/sentiment_analysis/8/triples/experimental-setup.txt
Experimental setup,has,LSTM classifiers,experimental-setup,/content/training-data/sentiment_analysis/8/triples/experimental-setup.txt
LSTM classifiers,trained on,NVIDIA Titan X GPU,experimental-setup,/content/training-data/sentiment_analysis/8/triples/experimental-setup.txt
Experimental setup,randomly split,our dataset,experimental-setup,/content/training-data/sentiment_analysis/8/triples/experimental-setup.txt
our dataset,into,train ( 80 % ) and test ( 20 % ) set,experimental-setup,/content/training-data/sentiment_analysis/8/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/8/triples/results.txt
Results,has,Text - only results,results,/content/training-data/sentiment_analysis/8/triples/results.txt
Text - only results,observe,performance of all the models,results,/content/training-data/sentiment_analysis/8/triples/results.txt
performance of all the models,is,similar,results,/content/training-data/sentiment_analysis/8/triples/results.txt
Results,has,Audio + Text results,results,/content/training-data/sentiment_analysis/8/triples/results.txt
Audio + Text results,combining,audio and text features,results,/content/training-data/sentiment_analysis/8/triples/results.txt
audio and text features,gives,boost,results,/content/training-data/sentiment_analysis/8/triples/results.txt
boost,of,? 14 % for all the metrics,results,/content/training-data/sentiment_analysis/8/triples/results.txt
Results,has,Audio - only results,results,/content/training-data/sentiment_analysis/8/triples/results.txt
Audio - only results,has,Performance of LSTM and ARE,results,/content/training-data/sentiment_analysis/8/triples/results.txt
Performance of LSTM and ARE,reveals,deep models indeed need a lot of information,results,/content/training-data/sentiment_analysis/8/triples/results.txt
deep models indeed need a lot of information,to learn,features,results,/content/training-data/sentiment_analysis/8/triples/results.txt
features,as,LSTM classifier,results,/content/training-data/sentiment_analysis/8/triples/results.txt
LSTM classifier,achieves,very low accuracy,results,/content/training-data/sentiment_analysis/8/triples/results.txt
very low accuracy,as compared to,end - to - end trained ARE,results,/content/training-data/sentiment_analysis/8/triples/results.txt
LSTM classifier,trained on,eight - dimensional features,results,/content/training-data/sentiment_analysis/8/triples/results.txt
Results,has,our simpler and lighter ML models,results,/content/training-data/sentiment_analysis/8/triples/results.txt
our simpler and lighter ML models,either outperform or are comparable to,much heavier current state - of - the art,results,/content/training-data/sentiment_analysis/8/triples/results.txt
Results,conclude that,our simple ML methods,results,/content/training-data/sentiment_analysis/8/triples/results.txt
our simple ML methods,are,very robust,results,/content/training-data/sentiment_analysis/8/triples/results.txt
very robust,to have achieved,comparable performance,results,/content/training-data/sentiment_analysis/8/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/sentiment_analysis/25/triples/baselines.txt
Baselines,has,NRC - Canada,baselines,/content/training-data/sentiment_analysis/25/triples/baselines.txt
NRC - Canada,is,top method,baselines,/content/training-data/sentiment_analysis/25/triples/baselines.txt
top method,in,SemEval 2014 Task 4,baselines,/content/training-data/sentiment_analysis/25/triples/baselines.txt
top method,for,ACSA and ATSA task,baselines,/content/training-data/sentiment_analysis/25/triples/baselines.txt
Baselines,has,CNN,baselines,/content/training-data/sentiment_analysis/25/triples/baselines.txt
CNN,widely used on,text classification task,baselines,/content/training-data/sentiment_analysis/25/triples/baselines.txt
Baselines,has,GCN,baselines,/content/training-data/sentiment_analysis/25/triples/baselines.txt
GCN,in which,GTRU,baselines,/content/training-data/sentiment_analysis/25/triples/baselines.txt
GTRU,does not have,aspect embedding,baselines,/content/training-data/sentiment_analysis/25/triples/baselines.txt
aspect embedding,as,additional input,baselines,/content/training-data/sentiment_analysis/25/triples/baselines.txt
GCN,stands for,gated convolutional neural network,baselines,/content/training-data/sentiment_analysis/25/triples/baselines.txt
Baselines,has,IAN,baselines,/content/training-data/sentiment_analysis/25/triples/baselines.txt
IAN,based on,LSTM and attention mechanisms,baselines,/content/training-data/sentiment_analysis/25/triples/baselines.txt
IAN,stands for,interactive attention network,baselines,/content/training-data/sentiment_analysis/25/triples/baselines.txt
interactive attention network,for,ATSA task,baselines,/content/training-data/sentiment_analysis/25/triples/baselines.txt
Baselines,has,ATAE - LSTM,baselines,/content/training-data/sentiment_analysis/25/triples/baselines.txt
ATAE - LSTM,is,attention - based LSTM,baselines,/content/training-data/sentiment_analysis/25/triples/baselines.txt
attention - based LSTM,for,ACSA task,baselines,/content/training-data/sentiment_analysis/25/triples/baselines.txt
Baselines,has,TD - LSTM,baselines,/content/training-data/sentiment_analysis/25/triples/baselines.txt
TD - LSTM,uses,two LSTM networks,baselines,/content/training-data/sentiment_analysis/25/triples/baselines.txt
two LSTM networks,to generate,target - dependent representation,baselines,/content/training-data/sentiment_analysis/25/triples/baselines.txt
target - dependent representation,for,sentiment prediction,baselines,/content/training-data/sentiment_analysis/25/triples/baselines.txt
two LSTM networks,to model,preceding and following contexts,baselines,/content/training-data/sentiment_analysis/25/triples/baselines.txt
preceding and following contexts,of,target,baselines,/content/training-data/sentiment_analysis/25/triples/baselines.txt
Baselines,has,RAM,baselines,/content/training-data/sentiment_analysis/25/triples/baselines.txt
RAM,is,recurrent attention network,baselines,/content/training-data/sentiment_analysis/25/triples/baselines.txt
recurrent attention network,for,ATSA task,baselines,/content/training-data/sentiment_analysis/25/triples/baselines.txt
RAM,uses,LSTM and multiple attention mechanisms,baselines,/content/training-data/sentiment_analysis/25/triples/baselines.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/25/triples/hyperparameters.txt
Hyperparameters,fine tune,early stopping,hyperparameters,/content/training-data/sentiment_analysis/25/triples/hyperparameters.txt
early stopping,with,5 - fold cross validation,hyperparameters,/content/training-data/sentiment_analysis/25/triples/hyperparameters.txt
5 - fold cross validation,on,training datasets,hyperparameters,/content/training-data/sentiment_analysis/25/triples/hyperparameters.txt
Hyperparameters,use,Adagrad,hyperparameters,/content/training-data/sentiment_analysis/25/triples/hyperparameters.txt
Adagrad,with,batch size,hyperparameters,/content/training-data/sentiment_analysis/25/triples/hyperparameters.txt
batch size,of,32 instances,hyperparameters,/content/training-data/sentiment_analysis/25/triples/hyperparameters.txt
Adagrad,with,default learning rate,hyperparameters,/content/training-data/sentiment_analysis/25/triples/hyperparameters.txt
default learning rate,of,1 e ? 2,hyperparameters,/content/training-data/sentiment_analysis/25/triples/hyperparameters.txt
Adagrad,with,maximal epochs,hyperparameters,/content/training-data/sentiment_analysis/25/triples/hyperparameters.txt
maximal epochs,of,30,hyperparameters,/content/training-data/sentiment_analysis/25/triples/hyperparameters.txt
Hyperparameters,has,word embedding vectors,hyperparameters,/content/training-data/sentiment_analysis/25/triples/hyperparameters.txt
word embedding vectors,initialized with,300 - dimension GloVe vectors,hyperparameters,/content/training-data/sentiment_analysis/25/triples/hyperparameters.txt
300 - dimension GloVe vectors,pre-trained on,unlabeled data,hyperparameters,/content/training-data/sentiment_analysis/25/triples/hyperparameters.txt
unlabeled data,of,840 billion tokens,hyperparameters,/content/training-data/sentiment_analysis/25/triples/hyperparameters.txt
Hyperparameters,has,neural models,hyperparameters,/content/training-data/sentiment_analysis/25/triples/hyperparameters.txt
neural models,implemented in,PyTorch,hyperparameters,/content/training-data/sentiment_analysis/25/triples/hyperparameters.txt
Hyperparameters,has,Words out of the vocabulary,hyperparameters,/content/training-data/sentiment_analysis/25/triples/hyperparameters.txt
Words out of the vocabulary,of,Glo Ve,hyperparameters,/content/training-data/sentiment_analysis/25/triples/hyperparameters.txt
Glo Ve,are,randomly initialized,hyperparameters,/content/training-data/sentiment_analysis/25/triples/hyperparameters.txt
randomly initialized,with,"uniform distribution U ( ? 0.25 , 0.25 ",hyperparameters,/content/training-data/sentiment_analysis/25/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/25/triples/model.txt
Model,proposed,gating units,model,/content/training-data/sentiment_analysis/25/triples/model.txt
gating units,have,two nonlinear gates,model,/content/training-data/sentiment_analysis/25/triples/model.txt
two nonlinear gates,connected to,one convolutional layer,model,/content/training-data/sentiment_analysis/25/triples/model.txt
Model,For,ATSA task,model,/content/training-data/sentiment_analysis/25/triples/model.txt
ATSA task,extend,our model,model,/content/training-data/sentiment_analysis/25/triples/model.txt
our model,to include,another convolutional layer,model,/content/training-data/sentiment_analysis/25/triples/model.txt
another convolutional layer,for,target expressions,model,/content/training-data/sentiment_analysis/25/triples/model.txt
Model,For,ACSA task,model,/content/training-data/sentiment_analysis/25/triples/model.txt
ACSA task,has,our model,model,/content/training-data/sentiment_analysis/25/triples/model.txt
our model,has,two separate convolutional layers,model,/content/training-data/sentiment_analysis/25/triples/model.txt
two separate convolutional layers,on the top of,embedding layer,model,/content/training-data/sentiment_analysis/25/triples/model.txt
embedding layer,whose,outputs,model,/content/training-data/sentiment_analysis/25/triples/model.txt
outputs,are combined by,novel gating units,model,/content/training-data/sentiment_analysis/25/triples/model.txt
Model,propose,fast and effective neural network,model,/content/training-data/sentiment_analysis/25/triples/model.txt
fast and effective neural network,for,ACSA and ATSA,model,/content/training-data/sentiment_analysis/25/triples/model.txt
ACSA and ATSA,based on,convolutions and gating mechanisms,model,/content/training-data/sentiment_analysis/25/triples/model.txt
Contribution,has research problem,Aspect Based Sentiment Analysis,research-problem,/content/training-data/sentiment_analysis/25/triples/research-problem.txt
Contribution,has research problem,Aspect based sentiment analysis ( ABSA ,research-problem,/content/training-data/sentiment_analysis/25/triples/research-problem.txt
Contribution,has research problem,aspect - category sentiment analysis ( ACSA ,research-problem,/content/training-data/sentiment_analysis/25/triples/research-problem.txt
Contribution,has research problem,aspect - term sentiment analysis ( ATSA ,research-problem,/content/training-data/sentiment_analysis/25/triples/research-problem.txt
Contribution,has research problem,ABSA,research-problem,/content/training-data/sentiment_analysis/25/triples/research-problem.txt
Contribution,has research problem,ACSA,research-problem,/content/training-data/sentiment_analysis/25/triples/research-problem.txt
Contribution,has research problem,ATSA,research-problem,/content/training-data/sentiment_analysis/25/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/25/triples/results.txt
Results,has,ACSA,results,/content/training-data/sentiment_analysis/25/triples/results.txt
ACSA,has,LSTM based model ATAE - LSTM,results,/content/training-data/sentiment_analysis/25/triples/results.txt
LSTM based model ATAE - LSTM,has,worst performance,results,/content/training-data/sentiment_analysis/25/triples/results.txt
worst performance,of,all neural networks,results,/content/training-data/sentiment_analysis/25/triples/results.txt
ACSA,has,GCAE,results,/content/training-data/sentiment_analysis/25/triples/results.txt
GCAE,improves,performance,results,/content/training-data/sentiment_analysis/25/triples/results.txt
performance,by,1.1 % to 2.5 %,results,/content/training-data/sentiment_analysis/25/triples/results.txt
1.1 % to 2.5 %,compared with,ATAE - LSTM,results,/content/training-data/sentiment_analysis/25/triples/results.txt
GCAE,achieves,5 % higher,results,/content/training-data/sentiment_analysis/25/triples/results.txt
5 % higher,on,SemEval - 2014 on ACSA task,results,/content/training-data/sentiment_analysis/25/triples/results.txt
GCAE,achieves,4 % higher accuracy,results,/content/training-data/sentiment_analysis/25/triples/results.txt
4 % higher accuracy,than,ATAE - LSTM,results,/content/training-data/sentiment_analysis/25/triples/results.txt
ATAE - LSTM,on,Restaurant - Large,results,/content/training-data/sentiment_analysis/25/triples/results.txt
ACSA,has,GCN,results,/content/training-data/sentiment_analysis/25/triples/results.txt
GCN,has,higher score,results,/content/training-data/sentiment_analysis/25/triples/results.txt
higher score,than,GCAE,results,/content/training-data/sentiment_analysis/25/triples/results.txt
GCAE,on,original restaurant dataset,results,/content/training-data/sentiment_analysis/25/triples/results.txt
ACSA,Without the large amount of,sentiment lexicons,results,/content/training-data/sentiment_analysis/25/triples/results.txt
sentiment lexicons,has,SVM,results,/content/training-data/sentiment_analysis/25/triples/results.txt
SVM,performance,increased,results,/content/training-data/sentiment_analysis/25/triples/results.txt
increased,by,7.6 %,results,/content/training-data/sentiment_analysis/25/triples/results.txt
SVM,perform,worse,results,/content/training-data/sentiment_analysis/25/triples/results.txt
worse,than,neural methods,results,/content/training-data/sentiment_analysis/25/triples/results.txt
Results,has,ATSA,results,/content/training-data/sentiment_analysis/25/triples/results.txt
ATSA,has,GCAE,results,/content/training-data/sentiment_analysis/25/triples/results.txt
GCAE,outperforms,other neural models and basic SVM,results,/content/training-data/sentiment_analysis/25/triples/results.txt
ATSA,has,IAN,results,/content/training-data/sentiment_analysis/25/triples/results.txt
IAN,has,better performance,results,/content/training-data/sentiment_analysis/25/triples/results.txt
better performance,than,TD - LSTM and ATAE - LSTM,results,/content/training-data/sentiment_analysis/25/triples/results.txt
ATSA,has,RAM,results,/content/training-data/sentiment_analysis/25/triples/results.txt
RAM,achieves,good accuracy,results,/content/training-data/sentiment_analysis/25/triples/results.txt
good accuracy,by combining,multiple attentions,results,/content/training-data/sentiment_analysis/25/triples/results.txt
multiple attentions,with,recurrent neural network,results,/content/training-data/sentiment_analysis/25/triples/results.txt
ATSA,On,hard test dataset,results,/content/training-data/sentiment_analysis/25/triples/results.txt
hard test dataset,has,GCAE,results,/content/training-data/sentiment_analysis/25/triples/results.txt
GCAE,has,1.7 % higher,results,/content/training-data/sentiment_analysis/25/triples/results.txt
1.7 % higher,on,laptop data,results,/content/training-data/sentiment_analysis/25/triples/results.txt
GCAE,has,1 % higher accuracy,results,/content/training-data/sentiment_analysis/25/triples/results.txt
1 % higher accuracy,than,RAM,results,/content/training-data/sentiment_analysis/25/triples/results.txt
1 % higher accuracy,on,restaurant data,results,/content/training-data/sentiment_analysis/25/triples/results.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/49/triples/hyperparameters.txt
Hyperparameters,employ,ReLu activation function,hyperparameters,/content/training-data/sentiment_analysis/49/triples/hyperparameters.txt
ReLu activation function,in,dense layers,hyperparameters,/content/training-data/sentiment_analysis/49/triples/hyperparameters.txt
Hyperparameters,employ,softmax activation,hyperparameters,/content/training-data/sentiment_analysis/49/triples/hyperparameters.txt
softmax activation,in,final classification layer,hyperparameters,/content/training-data/sentiment_analysis/49/triples/hyperparameters.txt
Hyperparameters,Utilizing,dense layer,hyperparameters,/content/training-data/sentiment_analysis/49/triples/hyperparameters.txt
dense layer,project,input features,hyperparameters,/content/training-data/sentiment_analysis/49/triples/hyperparameters.txt
input features,of,all the three modalities,hyperparameters,/content/training-data/sentiment_analysis/49/triples/hyperparameters.txt
input features,to,same dimensions,hyperparameters,/content/training-data/sentiment_analysis/49/triples/hyperparameters.txt
Hyperparameters,use,Bi-directional GRUs,hyperparameters,/content/training-data/sentiment_analysis/49/triples/hyperparameters.txt
Bi-directional GRUs,having,300 neurons,hyperparameters,/content/training-data/sentiment_analysis/49/triples/hyperparameters.txt
300 neurons,followed by,dense layer,hyperparameters,/content/training-data/sentiment_analysis/49/triples/hyperparameters.txt
dense layer,consisting of,100 neurons,hyperparameters,/content/training-data/sentiment_analysis/49/triples/hyperparameters.txt
Hyperparameters,for,Bi - GRU layers,hyperparameters,/content/training-data/sentiment_analysis/49/triples/hyperparameters.txt
Bi - GRU layers,use,dropout,hyperparameters,/content/training-data/sentiment_analysis/49/triples/hyperparameters.txt
dropout,=,0.4 ( MOSI ,hyperparameters,/content/training-data/sentiment_analysis/49/triples/hyperparameters.txt
dropout,=,0.3 ( MOSEI ,hyperparameters,/content/training-data/sentiment_analysis/49/triples/hyperparameters.txt
Hyperparameters,For,training the network,hyperparameters,/content/training-data/sentiment_analysis/49/triples/hyperparameters.txt
training the network,set,batch size,hyperparameters,/content/training-data/sentiment_analysis/49/triples/hyperparameters.txt
batch size,=,32,hyperparameters,/content/training-data/sentiment_analysis/49/triples/hyperparameters.txt
training the network,use,Adam optimizer,hyperparameters,/content/training-data/sentiment_analysis/49/triples/hyperparameters.txt
Adam optimizer,with,cross - entropy loss function,hyperparameters,/content/training-data/sentiment_analysis/49/triples/hyperparameters.txt
training the network,train for,50 epochs,hyperparameters,/content/training-data/sentiment_analysis/49/triples/hyperparameters.txt
Hyperparameters,as a measure of,regularization,hyperparameters,/content/training-data/sentiment_analysis/49/triples/hyperparameters.txt
regularization,set,dropout,hyperparameters,/content/training-data/sentiment_analysis/49/triples/hyperparameters.txt
dropout,=,0.5 ( MOSI ,hyperparameters,/content/training-data/sentiment_analysis/49/triples/hyperparameters.txt
dropout,=,0.3 ( MOSEI ,hyperparameters,/content/training-data/sentiment_analysis/49/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/49/triples/model.txt
Model,facilitates,modality selection,model,/content/training-data/sentiment_analysis/49/triples/model.txt
modality selection,attending over,contextual utterances,model,/content/training-data/sentiment_analysis/49/triples/model.txt
modality selection,generates,better multimodal feature representation,model,/content/training-data/sentiment_analysis/49/triples/model.txt
better multimodal feature representation,when,modalities from the context,model,/content/training-data/sentiment_analysis/49/triples/model.txt
modalities from the context,combined with,modalities of the target utterance,model,/content/training-data/sentiment_analysis/49/triples/model.txt
Model,has,attention mechanism,model,/content/training-data/sentiment_analysis/49/triples/model.txt
attention mechanism,attend to,important contextual utterances,model,/content/training-data/sentiment_analysis/49/triples/model.txt
important contextual utterances,having,higher relatedness or similarity,model,/content/training-data/sentiment_analysis/49/triples/model.txt
higher relatedness or similarity,with,target utterance,model,/content/training-data/sentiment_analysis/49/triples/model.txt
Model,attend over,contextual utterances,model,/content/training-data/sentiment_analysis/49/triples/model.txt
contextual utterances,by computing,correlations,model,/content/training-data/sentiment_analysis/49/triples/model.txt
correlations,among,modalities,model,/content/training-data/sentiment_analysis/49/triples/model.txt
modalities,of,target utterance and the context utterances,model,/content/training-data/sentiment_analysis/49/triples/model.txt
Model,propose,novel method,model,/content/training-data/sentiment_analysis/49/triples/model.txt
novel method,employs,recurrent neural network based multimodal multi-utterance attention framework,model,/content/training-data/sentiment_analysis/49/triples/model.txt
recurrent neural network based multimodal multi-utterance attention framework,for,sentiment prediction,model,/content/training-data/sentiment_analysis/49/triples/model.txt
Model,propose,novel fusion method,model,/content/training-data/sentiment_analysis/49/triples/model.txt
novel fusion method,focusing on,inter-modality relations,model,/content/training-data/sentiment_analysis/49/triples/model.txt
inter-modality relations,computed between,target utterance and its context,model,/content/training-data/sentiment_analysis/49/triples/model.txt
Contribution,has research problem,Multi-modal Sentiment Analysis,research-problem,/content/training-data/sentiment_analysis/49/triples/research-problem.txt
Contribution,has research problem,sentiment analysis,research-problem,/content/training-data/sentiment_analysis/49/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/49/triples/results.txt
Results,experiment with,tri-modal inputs,results,/content/training-data/sentiment_analysis/49/triples/results.txt
tri-modal inputs,observe,improved performance,results,/content/training-data/sentiment_analysis/49/triples/results.txt
improved performance,of,"79. 80 % , 79.76 % and 79.63 %",results,/content/training-data/sentiment_analysis/49/triples/results.txt
"79. 80 % , 79.76 % and 79.63 %",for,"MMMU - BA , MMUU - SA and MU - SA frameworks",results,/content/training-data/sentiment_analysis/49/triples/results.txt
Results,For,MOSEI dataset,results,/content/training-data/sentiment_analysis/49/triples/results.txt
MOSEI dataset,obtain,better performance,results,/content/training-data/sentiment_analysis/49/triples/results.txt
better performance,with,text,results,/content/training-data/sentiment_analysis/49/triples/results.txt
Results,For,text - acoustic input pairs,results,/content/training-data/sentiment_analysis/49/triples/results.txt
text - acoustic input pairs,obtain,highest accuracies,results,/content/training-data/sentiment_analysis/49/triples/results.txt
highest accuracies,with,"79. 74 % , 79.60 % and 79.32 %",results,/content/training-data/sentiment_analysis/49/triples/results.txt
"79. 74 % , 79.60 % and 79.32 %",for,"MMMU - BA , MMUU - SA and MU - SA frameworks",results,/content/training-data/sentiment_analysis/49/triples/results.txt
Results,has,performance improvement,results,/content/training-data/sentiment_analysis/49/triples/results.txt
performance improvement,found to be,statistically significant ( T-test ,results,/content/training-data/sentiment_analysis/49/triples/results.txt
statistically significant ( T-test ),than,bimodality and uni-modality inputs,results,/content/training-data/sentiment_analysis/49/triples/results.txt
Results,observe,MMMU - BA framework,results,/content/training-data/sentiment_analysis/49/triples/results.txt
MMMU - BA framework,reports,best accuracy,results,/content/training-data/sentiment_analysis/49/triples/results.txt
best accuracy,of,79 . 80 %,results,/content/training-data/sentiment_analysis/49/triples/results.txt
best accuracy,for,MOSEI dataset,results,/content/training-data/sentiment_analysis/49/triples/results.txt
Contribution,has,Approach,approach,/content/training-data/sentiment_analysis/42/triples/approach.txt
Approach,consists of,multiple computational layers,approach,/content/training-data/sentiment_analysis/42/triples/approach.txt
multiple computational layers,with,shared parameters,approach,/content/training-data/sentiment_analysis/42/triples/approach.txt
Approach,is,data - driven,approach,/content/training-data/sentiment_analysis/42/triples/approach.txt
Approach,is,computationally efficient,approach,/content/training-data/sentiment_analysis/42/triples/approach.txt
Approach,develop,deep memory network,approach,/content/training-data/sentiment_analysis/42/triples/approach.txt
deep memory network,for,aspect level sentiment classification,approach,/content/training-data/sentiment_analysis/42/triples/approach.txt
Approach,has,Each layer,approach,/content/training-data/sentiment_analysis/42/triples/approach.txt
Each layer,is,content - and location - based attention model,approach,/content/training-data/sentiment_analysis/42/triples/approach.txt
content - and location - based attention model,first learns,importance / weight,approach,/content/training-data/sentiment_analysis/42/triples/approach.txt
importance / weight,of,each context word,approach,/content/training-data/sentiment_analysis/42/triples/approach.txt
content - and location - based attention model,utilizes,information,approach,/content/training-data/sentiment_analysis/42/triples/approach.txt
information,to calculate,continuous text representation,approach,/content/training-data/sentiment_analysis/42/triples/approach.txt
Approach,has,every component,approach,/content/training-data/sentiment_analysis/42/triples/approach.txt
every component,is,differentiable,approach,/content/training-data/sentiment_analysis/42/triples/approach.txt
every component,has,entire model,approach,/content/training-data/sentiment_analysis/42/triples/approach.txt
entire model,could be efficiently trained,end - toend,approach,/content/training-data/sentiment_analysis/42/triples/approach.txt
end - toend,with,gradient descent,approach,/content/training-data/sentiment_analysis/42/triples/approach.txt
gradient descent,where,loss function,approach,/content/training-data/sentiment_analysis/42/triples/approach.txt
loss function,is,cross - entropy error,approach,/content/training-data/sentiment_analysis/42/triples/approach.txt
cross - entropy error,of,sentiment classification,approach,/content/training-data/sentiment_analysis/42/triples/approach.txt
Approach,has,text representation,approach,/content/training-data/sentiment_analysis/42/triples/approach.txt
text representation,in,last layer,approach,/content/training-data/sentiment_analysis/42/triples/approach.txt
last layer,regarded as,feature,approach,/content/training-data/sentiment_analysis/42/triples/approach.txt
feature,for,sentiment classification,approach,/content/training-data/sentiment_analysis/42/triples/approach.txt
Contribution,has,Baselines,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
Baselines,use,same Glove word vectors,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
same Glove word vectors,for,fair comparison,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
Baselines,implement,ContextAVG,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
ContextAVG,has,simplistic version,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
simplistic version,of,our approach,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
Baselines,has,three LSTM models,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
three LSTM models,In,LSTM,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
LSTM,has,LSTM based recurrent model,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
LSTM based recurrent model,applied from,start,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
start,to,end,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
end,of,sentence,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
LSTM,has,last hidden vector,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
last hidden vector,used as,sentence representation,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
three LSTM models,In,TDLSTM,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
TDLSTM,extends,LSTM,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
TDLSTM,by taking into account,aspect,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
TDLSTM,uses,two LSTM networks,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
two LSTM networks,has,forward one and a backward one,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
two LSTM networks,towards,aspect,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
three LSTM models,In,TDLSTM + ATT,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
TDLSTM + ATT,by incorporating,attention mechanism,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
attention mechanism,over,hidden vectors,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
TDLSTM + ATT,extends,TDLSTM,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
Baselines,has,Feature - based SVM,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
Feature - based SVM,performs,state - of - the - art,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
state - of - the - art,on,aspect level sentiment classification,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
Baselines,has,Majority,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
Majority,is,basic baseline method,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
Majority,assigns,majority sentiment label,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
majority sentiment label,in,training set,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
training set,to each,instance,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
instance,in,test set,baselines,/content/training-data/sentiment_analysis/42/triples/baselines.txt
Contribution,has research problem,Aspect Level Sentiment Classification,research-problem,/content/training-data/sentiment_analysis/42/triples/research-problem.txt
Contribution,has research problem,sentiment analysis,research-problem,/content/training-data/sentiment_analysis/42/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/42/triples/results.txt
Results,find that,performance,results,/content/training-data/sentiment_analysis/42/triples/results.txt
performance,of,Contex - tAVG,results,/content/training-data/sentiment_analysis/42/triples/results.txt
Contex - tAVG,is,very poor,results,/content/training-data/sentiment_analysis/42/triples/results.txt
Results,find that,feature - based SVM,results,/content/training-data/sentiment_analysis/42/triples/results.txt
feature - based SVM,substantially outperforms,other baseline methods,results,/content/training-data/sentiment_analysis/42/triples/results.txt
feature - based SVM,is,extremely strong performer,results,/content/training-data/sentiment_analysis/42/triples/results.txt
Results,find that,multiple computational layers,results,/content/training-data/sentiment_analysis/42/triples/results.txt
multiple computational layers,consistently improve,classification accuracy,results,/content/training-data/sentiment_analysis/42/triples/results.txt
classification accuracy,in,all these models,results,/content/training-data/sentiment_analysis/42/triples/results.txt
Results,Among,observe that,results,/content/training-data/sentiment_analysis/42/triples/results.txt
observe that,using,more computational layers,results,/content/training-data/sentiment_analysis/42/triples/results.txt
more computational layers,lead to,better performance,results,/content/training-data/sentiment_analysis/42/triples/results.txt
better performance,especially when,number of hops,results,/content/training-data/sentiment_analysis/42/triples/results.txt
number of hops,is,less than six,results,/content/training-data/sentiment_analysis/42/triples/results.txt
Results,Among,three recurrent models,results,/content/training-data/sentiment_analysis/42/triples/results.txt
three recurrent models,has,TDLSTM,results,/content/training-data/sentiment_analysis/42/triples/results.txt
TDLSTM,performs,better,results,/content/training-data/sentiment_analysis/42/triples/results.txt
better,than,LSTM,results,/content/training-data/sentiment_analysis/42/triples/results.txt
Results,consider,each hidden vector,results,/content/training-data/sentiment_analysis/42/triples/results.txt
each hidden vector,of,TDLSTM,results,/content/training-data/sentiment_analysis/42/triples/results.txt
each hidden vector,encodes,semantics,results,/content/training-data/sentiment_analysis/42/triples/results.txt
semantics,of,word sequence,results,/content/training-data/sentiment_analysis/42/triples/results.txt
word sequence,until,current position,results,/content/training-data/sentiment_analysis/42/triples/results.txt
Results,has,All these models,results,/content/training-data/sentiment_analysis/42/triples/results.txt
All these models,perform,comparably,results,/content/training-data/sentiment_analysis/42/triples/results.txt
comparably,when,number of hops,results,/content/training-data/sentiment_analysis/42/triples/results.txt
number of hops,is,larger than five,results,/content/training-data/sentiment_analysis/42/triples/results.txt
Results,has,best performances,results,/content/training-data/sentiment_analysis/42/triples/results.txt
best performances,achieved when,model,results,/content/training-data/sentiment_analysis/42/triples/results.txt
model,contains,seven and nine hops,results,/content/training-data/sentiment_analysis/42/triples/results.txt
Results,On,both datasets,results,/content/training-data/sentiment_analysis/42/triples/results.txt
both datasets,has,proposed approach,results,/content/training-data/sentiment_analysis/42/triples/results.txt
proposed approach,could obtain,comparable accuracy,results,/content/training-data/sentiment_analysis/42/triples/results.txt
comparable accuracy,compared to,state - of - art feature - based SVM system,results,/content/training-data/sentiment_analysis/42/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
Baselines,has,CNN,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
CNN,identical to,our textual feature extractor network,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
CNN,does not use,contextual information,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
Baselines,has,c- LSTM+ Att,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
c- LSTM+ Att,has,variant attention,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
variant attention,at,each timestamp,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
variant attention,applied to,c - LSTM output,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
Baselines,has,MFN,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
MFN,Specific to,multimodal scenario,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
MFN,utilizes,multi-view learning,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
multi-view learning,by modeling,view - specific and cross - view interactions,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
Baselines,has,CMN,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
CMN,has,state - of - the - art method,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
state - of - the - art method,models,utterance context,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
utterance context,from,dialogue history,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
dialogue history,using,two distinct GRUs,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
two distinct GRUs,for,two speakers,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
Baselines,has,c - LSTM,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
c - LSTM,is,Biredectional LSTM,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
Biredectional LSTM,used to capture,context,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
context,from,surrounding utterances,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
surrounding utterances,to generate,contextaware utterance representation,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
Baselines,has,TFN,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
TFN,specific to,multimodal scenario,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
TFN,has,Tensor outer product,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
Tensor outer product,used to capture,intermodality and intra-modality interactions,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
Baselines,has,Memnet,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
Memnet,has,output,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
output,from,memory network,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
memory network,used as,final utterance representation,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
final utterance representation,for,emotion classification,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
Memnet,has,current utterance,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
current utterance,fed to,memory network,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
memory network,where,memories,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
memories,correspond to,preceding utterances,baselines,/content/training-data/sentiment_analysis/10/triples/baselines.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/10/triples/model.txt
Model,At time t,emotion GRU cell,model,/content/training-data/sentiment_analysis/10/triples/model.txt
emotion GRU cell,gets,emotion representation of t ? 1 and the speaker state of t,model,/content/training-data/sentiment_analysis/10/triples/model.txt
Model,at time t,speaker state,model,/content/training-data/sentiment_analysis/10/triples/model.txt
speaker state,directly gets information from,speaker 's previous state,model,/content/training-data/sentiment_analysis/10/triples/model.txt
speaker state,directly gets information from,global GRU,model,/content/training-data/sentiment_analysis/10/triples/model.txt
global GRU,which has information on,preceding parties,model,/content/training-data/sentiment_analysis/10/triples/model.txt
Model,has,incoming utterance,model,/content/training-data/sentiment_analysis/10/triples/model.txt
incoming utterance,fed into,two GRUs,model,/content/training-data/sentiment_analysis/10/triples/model.txt
two GRUs,called,global GRU and party GRU,model,/content/training-data/sentiment_analysis/10/triples/model.txt
Model,has,global GRU,model,/content/training-data/sentiment_analysis/10/triples/model.txt
global GRU,Attending over,gives contextual representation,model,/content/training-data/sentiment_analysis/10/triples/model.txt
gives contextual representation,has information of,all preceding utterances,model,/content/training-data/sentiment_analysis/10/triples/model.txt
all preceding utterances,by,different parties,model,/content/training-data/sentiment_analysis/10/triples/model.txt
different parties,in,conversation,model,/content/training-data/sentiment_analysis/10/triples/model.txt
global GRU,encodes,corresponding party information,model,/content/training-data/sentiment_analysis/10/triples/model.txt
corresponding party information,while encoding,utterance,model,/content/training-data/sentiment_analysis/10/triples/model.txt
Model,has,updated speaker state,model,/content/training-data/sentiment_analysis/10/triples/model.txt
updated speaker state,fed into,emotion GRU,model,/content/training-data/sentiment_analysis/10/triples/model.txt
emotion GRU,to decode,emotion representation,model,/content/training-data/sentiment_analysis/10/triples/model.txt
emotion representation,of,given utterance,model,/content/training-data/sentiment_analysis/10/triples/model.txt
given utterance,used for,emotion classification,model,/content/training-data/sentiment_analysis/10/triples/model.txt
Model,has,DialogueRNN system,model,/content/training-data/sentiment_analysis/10/triples/model.txt
DialogueRNN system,employs,three gated recurrent units ( GRU ,model,/content/training-data/sentiment_analysis/10/triples/model.txt
Model,has,speaker state,model,/content/training-data/sentiment_analysis/10/triples/model.txt
speaker state,depends on,context,model,/content/training-data/sentiment_analysis/10/triples/model.txt
context,through,attention and the speaker 's previous state,model,/content/training-data/sentiment_analysis/10/triples/model.txt
Contribution,has research problem,Emotion Detection in Conversations,research-problem,/content/training-data/sentiment_analysis/10/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/10/triples/results.txt
Results,for,IEMOCAP dataset,results,/content/training-data/sentiment_analysis/10/triples/results.txt
IEMOCAP dataset,has,our model,results,/content/training-data/sentiment_analysis/10/triples/results.txt
our model,surpasses,state - of - the - art method CMN,results,/content/training-data/sentiment_analysis/10/triples/results.txt
state - of - the - art method CMN,by,2.77 % accuracy,results,/content/training-data/sentiment_analysis/10/triples/results.txt
state - of - the - art method CMN,by,3.76 % f 1 - score,results,/content/training-data/sentiment_analysis/10/triples/results.txt
Results,has,DialogueRNN vs. DialogueRNN Variants,results,/content/training-data/sentiment_analysis/10/triples/results.txt
DialogueRNN vs. DialogueRNN Variants,has,BiDialogueRNN,results,/content/training-data/sentiment_analysis/10/triples/results.txt
BiDialogueRNN,outperforms,Dialogue RNN,results,/content/training-data/sentiment_analysis/10/triples/results.txt
Dialogue RNN,on,both datasets,results,/content/training-data/sentiment_analysis/10/triples/results.txt
DialogueRNN vs. DialogueRNN Variants,has,DialogueRNN l,results,/content/training-data/sentiment_analysis/10/triples/results.txt
DialogueRNN l,using,explicit listener state update,results,/content/training-data/sentiment_analysis/10/triples/results.txt
explicit listener state update,yields,slightly worse performance,results,/content/training-data/sentiment_analysis/10/triples/results.txt
slightly worse performance,than,regular DialogueRNN,results,/content/training-data/sentiment_analysis/10/triples/results.txt
Results,has,AVEC,results,/content/training-data/sentiment_analysis/10/triples/results.txt
AVEC,has,DialogueRNN,results,/content/training-data/sentiment_analysis/10/triples/results.txt
DialogueRNN,outperforms,CMN,results,/content/training-data/sentiment_analysis/10/triples/results.txt
CMN,for,"valence , arousal , expectancy , and power attributes",results,/content/training-data/sentiment_analysis/10/triples/results.txt
Results,on average,Di - alogue RNN,results,/content/training-data/sentiment_analysis/10/triples/results.txt
Di - alogue RNN,outperforms,all the baseline methods,results,/content/training-data/sentiment_analysis/10/triples/results.txt
all the baseline methods,including,state - of - the - art CMN,results,/content/training-data/sentiment_analysis/10/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
Baselines,has,IAN,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
IAN,generates,representations,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
representations,for,aspect terms and contexts,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
aspect terms and contexts,with,two attention - based LSTM network,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
Baselines,has,ATAE - LSTM,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
ATAE - LSTM,employs,attention mechanism,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
attention mechanism,to get,sentence representation,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
sentence representation,for,final classification,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
ATAE - LSTM,attaches,aspect embedding,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
aspect embedding,to,each word embedding,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
each word embedding,to capture,aspect - dependent information,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
Baselines,has,AEN - BERT,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
AEN - BERT,is,AEN,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
AEN,with,BERT embedding,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
Baselines,has,PBAN,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
PBAN,appends,position embedding,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
position embedding,into,each word embedding,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
Baselines,has,AEN,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
AEN,consists of,embedding layer,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
AEN,consists of,attentional encoder layer,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
AEN,consists of,aspect - specific attention layer,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
AEN,consists of,output layer,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
Baselines,has,Mem Net,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
Mem Net,uses,deep memory network,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
deep memory network,on,context word embeddings,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
context word embeddings,for,sentence representation,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
sentence representation,to capture,relevance,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
relevance,between,each context word and the aspect,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
Baselines,has,TSN,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
TSN,is,two - stage framework,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
two - stage framework,for,aspect - level sentiment analysis,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
Baselines,has,TD - LSTM,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
TD - LSTM,employs,two LSTMs,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
TD - LSTM,has,last hidden states,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
last hidden states,of,two LSTMs,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
last hidden states,has,finally concatenated,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
finally concatenated,for predicting,sentiment polarity,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
sentiment polarity,of,aspect,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
TD - LSTM,constructs,aspect-specific representation,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
aspect-specific representation,by,right context,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
right context,with,aspect,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
aspect-specific representation,by,left context,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
left context,with,aspect,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
Baselines,has,RAM,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
RAM,captures,relevance,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
relevance,between,each context word and the aspect,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
RAM,employs,gated recurrent unit network,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
gated recurrent unit network,to model,multiple attention mechanism,baselines,/content/training-data/sentiment_analysis/27/triples/baselines.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
Hyperparameters,add,L2-regularization,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
L2-regularization,to,last fully connect layer,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
last fully connect layer,with,weight,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
weight,of,0.01,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
Hyperparameters,to initialize,word embeddings,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
word embeddings,use,GloVe 3 word vector,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
word embeddings,use,pre-trained language model word representation BERT,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
Hyperparameters,implement,proposed model,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
proposed model,using,Tensorflow,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
Hyperparameters,has,number,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
number,of,LSTM hidden units,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
LSTM hidden units,set to,300,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
Hyperparameters,has,output dimension,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
output dimension,of,GCN layer,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
GCN layer,set to,600,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
Hyperparameters,has,weight matrix,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
weight matrix,of,last fully connect layer,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
last fully connect layer,randomly initialized by,"normal distribution N ( 0 , 1 ",hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
Hyperparameters,has,dimension,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
dimension,of,each word vector,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
each word vector,is,300,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
300,for,GloVe,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
each word vector,is,768,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
768,for,BERT,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
Hyperparameters,has,all the weight matrices,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
all the weight matrices,randomly initialized by,"uniform distribution U ( ? 0.01 , 0.01 ",hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
all the weight matrices,Besides,last fully connect layer,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
Hyperparameters,During,training,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
training,set,dropout,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
dropout,to,0.5,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
training,has,optimizer,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
optimizer,is,Adam Optimizer,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
Adam Optimizer,with,learning rate,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
learning rate,of,0.001,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
training,has,batch size,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
batch size,set to,32,hyperparameters,/content/training-data/sentiment_analysis/27/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/27/triples/model.txt
Model,applies,bidirectional attention mechanism,model,/content/training-data/sentiment_analysis/27/triples/model.txt
bidirectional attention mechanism,with,position encoding,model,/content/training-data/sentiment_analysis/27/triples/model.txt
position encoding,before,GCN,model,/content/training-data/sentiment_analysis/27/triples/model.txt
Model,learns,sentiment dependencies,model,/content/training-data/sentiment_analysis/27/triples/model.txt
sentiment dependencies,of,aspects,model,/content/training-data/sentiment_analysis/27/triples/model.txt
aspects,via,graph structure,model,/content/training-data/sentiment_analysis/27/triples/model.txt
Model,For,every node,model,/content/training-data/sentiment_analysis/27/triples/model.txt
every node,in,graph,model,/content/training-data/sentiment_analysis/27/triples/model.txt
every node,has,GCN,model,/content/training-data/sentiment_analysis/27/triples/model.txt
GCN,encodes,relevant information,model,/content/training-data/sentiment_analysis/27/triples/model.txt
relevant information,as,new feature representation vector,model,/content/training-data/sentiment_analysis/27/triples/model.txt
relevant information,about,neighborhoods,model,/content/training-data/sentiment_analysis/27/triples/model.txt
Model,first to consider,sentiment dependencies,model,/content/training-data/sentiment_analysis/27/triples/model.txt
sentiment dependencies,between,aspects,model,/content/training-data/sentiment_analysis/27/triples/model.txt
aspects,in,one sentence,model,/content/training-data/sentiment_analysis/27/triples/model.txt
one sentence,for,aspect - level sentiment classification task,model,/content/training-data/sentiment_analysis/27/triples/model.txt
Model,has,edge,model,/content/training-data/sentiment_analysis/27/triples/model.txt
edge,represents,sentiment dependency relation,model,/content/training-data/sentiment_analysis/27/triples/model.txt
sentiment dependency relation,of,two nodes,model,/content/training-data/sentiment_analysis/27/triples/model.txt
Model,has,GCN,model,/content/training-data/sentiment_analysis/27/triples/model.txt
GCN,is,simple and effective convolutional neural network,model,/content/training-data/sentiment_analysis/27/triples/model.txt
simple and effective convolutional neural network,operating on,graphs,model,/content/training-data/sentiment_analysis/27/triples/model.txt
graphs,can catch,inter-dependent information,model,/content/training-data/sentiment_analysis/27/triples/model.txt
inter-dependent information,from,rich relational data,model,/content/training-data/sentiment_analysis/27/triples/model.txt
Model,has,aspect,model,/content/training-data/sentiment_analysis/27/triples/model.txt
aspect,treated as,node,model,/content/training-data/sentiment_analysis/27/triples/model.txt
Model,propose,novel method,model,/content/training-data/sentiment_analysis/27/triples/model.txt
novel method,to model,Sentiment Dependencies with Graph Convolutional Networks ( SDGCN ,model,/content/training-data/sentiment_analysis/27/triples/model.txt
Sentiment Dependencies with Graph Convolutional Networks ( SDGCN ),for,aspect - level sentiment classification,model,/content/training-data/sentiment_analysis/27/triples/model.txt
Contribution,has research problem,Aspect - level Sentiment Classification,research-problem,/content/training-data/sentiment_analysis/27/triples/research-problem.txt
Contribution,has research problem,sentiment analysis,research-problem,/content/training-data/sentiment_analysis/27/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/27/triples/results.txt
Results,After taking,importance,results,/content/training-data/sentiment_analysis/27/triples/results.txt
importance,with,attention mechanism,results,/content/training-data/sentiment_analysis/27/triples/results.txt
importance,achieve,stable improvement,results,/content/training-data/sentiment_analysis/27/triples/results.txt
stable improvement,comparing to,TD - LSTM,results,/content/training-data/sentiment_analysis/27/triples/results.txt
importance,of,aspect,results,/content/training-data/sentiment_analysis/27/triples/results.txt
aspect,into,account,results,/content/training-data/sentiment_analysis/27/triples/results.txt
Results,Benefits from,power,results,/content/training-data/sentiment_analysis/27/triples/results.txt
power,of,pre-trained BERT,results,/content/training-data/sentiment_analysis/27/triples/results.txt
Results,Benefits from,BERT - based models,results,/content/training-data/sentiment_analysis/27/triples/results.txt
BERT - based models,shown,huge superiority,results,/content/training-data/sentiment_analysis/27/triples/results.txt
huge superiority,over,GloVe - based models,results,/content/training-data/sentiment_analysis/27/triples/results.txt
Results,Among,all the GloVe - based methods,results,/content/training-data/sentiment_analysis/27/triples/results.txt
all the GloVe - based methods,has,TD - LSTM approach,results,/content/training-data/sentiment_analysis/27/triples/results.txt
TD - LSTM approach,performs,worst,results,/content/training-data/sentiment_analysis/27/triples/results.txt
Results,has,two models ( SDGCN - A and SDGCN - G ,results,/content/training-data/sentiment_analysis/27/triples/results.txt
two models ( SDGCN - A and SDGCN - G ),with,position information,results,/content/training-data/sentiment_analysis/27/triples/results.txt
position information,gain,significant improvement,results,/content/training-data/sentiment_analysis/27/triples/results.txt
significant improvement,compared to,two models,results,/content/training-data/sentiment_analysis/27/triples/results.txt
two models,without,position information,results,/content/training-data/sentiment_analysis/27/triples/results.txt
Results,has,PBAN,results,/content/training-data/sentiment_analysis/27/triples/results.txt
PBAN,worse than,RAN,results,/content/training-data/sentiment_analysis/27/triples/results.txt
RAN,on,Laptop dataset,results,/content/training-data/sentiment_analysis/27/triples/results.txt
PBAN,better than,RAM,results,/content/training-data/sentiment_analysis/27/triples/results.txt
RAM,on,Restaurant dataset,results,/content/training-data/sentiment_analysis/27/triples/results.txt
PBAN,achieves,similar performance,results,/content/training-data/sentiment_analysis/27/triples/results.txt
similar performance,as,RAM,results,/content/training-data/sentiment_analysis/27/triples/results.txt
RAM,by employing,position embedding,results,/content/training-data/sentiment_analysis/27/triples/results.txt
Results,has,AEN,results,/content/training-data/sentiment_analysis/27/triples/results.txt
AEN,still worse than,RAM and PBAN,results,/content/training-data/sentiment_analysis/27/triples/results.txt
AEN,slightly better than,TSN,results,/content/training-data/sentiment_analysis/27/triples/results.txt
Results,has,RAM,results,/content/training-data/sentiment_analysis/27/triples/results.txt
RAM,achieves,better performance,results,/content/training-data/sentiment_analysis/27/triples/results.txt
better performance,than,other basic attention - based models,results,/content/training-data/sentiment_analysis/27/triples/results.txt
Results,compared with,AEN - BERT,results,/content/training-data/sentiment_analysis/27/triples/results.txt
AEN - BERT,on,Restaurant dataset,results,/content/training-data/sentiment_analysis/27/triples/results.txt
Results,compared with,SDGCN - BERT,results,/content/training-data/sentiment_analysis/27/triples/results.txt
SDGCN - BERT,achieves,absolute increases,results,/content/training-data/sentiment_analysis/27/triples/results.txt
absolute increases,in,accuracy and Macro - F1 measure,results,/content/training-data/sentiment_analysis/27/triples/results.txt
absolute increases,of,1.09 % and 1.86 %,results,/content/training-data/sentiment_analysis/27/triples/results.txt
SDGCN - BERT,gains,absolute increases,results,/content/training-data/sentiment_analysis/27/triples/results.txt
absolute increases,in,accuracy and Macro - F1 measure,results,/content/training-data/sentiment_analysis/27/triples/results.txt
absolute increases,of,1.42 % and 2.03 %,results,/content/training-data/sentiment_analysis/27/triples/results.txt
absolute increases,on,Laptop dataset,results,/content/training-data/sentiment_analysis/27/triples/results.txt
Results,Compared with,RAM and PBAN,results,/content/training-data/sentiment_analysis/27/triples/results.txt
RAM and PBAN,has,over all performance,results,/content/training-data/sentiment_analysis/27/triples/results.txt
over all performance,of,TSN,results,/content/training-data/sentiment_analysis/27/triples/results.txt
over all performance,has,not perform well,results,/content/training-data/sentiment_analysis/27/triples/results.txt
not perform well,on both,Restaurant dataset and Laptop dataset,results,/content/training-data/sentiment_analysis/27/triples/results.txt
Results,Comparing,results,results,/content/training-data/sentiment_analysis/27/triples/results.txt
results,of,SDGCN - A w/o position and SDGCN - G w/o position,results,/content/training-data/sentiment_analysis/27/triples/results.txt
results,of,SDGCN - A and SDGCN - G,results,/content/training-data/sentiment_analysis/27/triples/results.txt
results,observe,GCN,results,/content/training-data/sentiment_analysis/27/triples/results.txt
GCN,slightly higher than,built with adjacent - relation,results,/content/training-data/sentiment_analysis/27/triples/results.txt
built with adjacent - relation,in both,accuracy and Macro - F1 measure,results,/content/training-data/sentiment_analysis/27/triples/results.txt
GCN,built with,global - relation,results,/content/training-data/sentiment_analysis/27/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/sentiment_analysis/24/triples/ablation-analysis.txt
Ablation analysis,has,Message passing -d,ablation-analysis,/content/training-data/sentiment_analysis/24/triples/ablation-analysis.txt
Message passing -d,still helpful with,considerable performance gains,ablation-analysis,/content/training-data/sentiment_analysis/24/triples/ablation-analysis.txt
Ablation analysis,observe,adding documentlevel tasks ( + DS / DD ,ablation-analysis,/content/training-data/sentiment_analysis/24/triples/ablation-analysis.txt
adding documentlevel tasks ( + DS / DD ),with,parameter sharing,ablation-analysis,/content/training-data/sentiment_analysis/24/triples/ablation-analysis.txt
parameter sharing,marginally improves,performance,ablation-analysis,/content/training-data/sentiment_analysis/24/triples/ablation-analysis.txt
performance,of,IMN ?d,ablation-analysis,/content/training-data/sentiment_analysis/24/triples/ablation-analysis.txt
Ablation analysis,observe,+ Message passing - a and + Message passing - d,ablation-analysis,/content/training-data/sentiment_analysis/24/triples/ablation-analysis.txt
+ Message passing - a and + Message passing - d,contribute to,performance gains,ablation-analysis,/content/training-data/sentiment_analysis/24/triples/ablation-analysis.txt
+ Message passing - a and + Message passing - d,demonstrates,effectiveness,ablation-analysis,/content/training-data/sentiment_analysis/24/triples/ablation-analysis.txt
effectiveness,of,proposed message passing mechanism,ablation-analysis,/content/training-data/sentiment_analysis/24/triples/ablation-analysis.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
Hyperparameters,At,training phase,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
training phase,randomly sample,20 %,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
20 %,as,development set,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
20 %,of,training data,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
training data,from,aspect - level dataset,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
training phase,use,remaining 80 %,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
remaining 80 %,for,training,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
Hyperparameters,adopt,multi - layer - CNN structure,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
multi - layer - CNN structure,from,CNN - based encoders,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
Hyperparameters,adopt,released domainspecific embeddings,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
released domainspecific embeddings,for,restaurant and laptop domains,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
restaurant and laptop domains,with,100 dimensions,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
released domainspecific embeddings,trained on,large domain - specific corpus,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
large domain - specific corpus,using,fast Text,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
Hyperparameters,use,Adam optimizer,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
Adam optimizer,with,learning rate,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
learning rate,set to,10 ? 4,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
Adam optimizer,set,batch size,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
batch size,to,32,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
Hyperparameters,For,word embedding initialization,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
word embedding initialization,concatenate,general - purpose embedding matrix,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
word embedding initialization,concatenate,domain - specific embedding matrix,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
Hyperparameters,has,Learning rate and batch size,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
Learning rate and batch size,set to,conventional values,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
conventional values,without specific tuning for,our task,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
Hyperparameters,has,general - purpose embeddings,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
general - purpose embeddings,are,pre-trained Glove vectors,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
pre-trained Glove vectors,with,300 dimensions,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
Hyperparameters,tune,maximum number of iterations T,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
maximum number of iterations T,in,message passing mechanism,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
message passing mechanism,by training,IMN ?d,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
IMN ?d,via,cross validation,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
maximum number of iterations T,set to,2,hyperparameters,/content/training-data/sentiment_analysis/24/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/24/triples/model.txt
Model,introduces,novel message passing mechanism,model,/content/training-data/sentiment_analysis/24/triples/model.txt
novel message passing mechanism,allows,informative interactions,model,/content/training-data/sentiment_analysis/24/triples/model.txt
informative interactions,between,tasks,model,/content/training-data/sentiment_analysis/24/triples/model.txt
Model,incorporated,two document - level classification tasks,model,/content/training-data/sentiment_analysis/24/triples/model.txt
two document - level classification tasks,to be jointly trained with,AE and AS,model,/content/training-data/sentiment_analysis/24/triples/model.txt
AE and AS,allowing,aspect - level tasks,model,/content/training-data/sentiment_analysis/24/triples/model.txt
aspect - level tasks,to benefit from,document - level information,model,/content/training-data/sentiment_analysis/24/triples/model.txt
two document - level classification tasks,name,sentiment classification ( DS ,model,/content/training-data/sentiment_analysis/24/triples/model.txt
two document - level classification tasks,name,domain classification ( DD ,model,/content/training-data/sentiment_analysis/24/triples/model.txt
Model,has,information,model,/content/training-data/sentiment_analysis/24/triples/model.txt
information,combined with,shared latent representation,model,/content/training-data/sentiment_analysis/24/triples/model.txt
information,made available to,all tasks,model,/content/training-data/sentiment_analysis/24/triples/model.txt
all tasks,for,further processing,model,/content/training-data/sentiment_analysis/24/triples/model.txt
Model,has,IMN,model,/content/training-data/sentiment_analysis/24/triples/model.txt
IMN,allows,fined - grained tokenlevel classification tasks,model,/content/training-data/sentiment_analysis/24/triples/model.txt
fined - grained tokenlevel classification tasks,to be trained together with,document - level classification tasks,model,/content/training-data/sentiment_analysis/24/triples/model.txt
IMN,allows,AE and AS,model,/content/training-data/sentiment_analysis/24/triples/model.txt
AE and AS,to be trained together with,related document - level tasks,model,/content/training-data/sentiment_analysis/24/triples/model.txt
related document - level tasks,exploiting,knowledge,model,/content/training-data/sentiment_analysis/24/triples/model.txt
knowledge,from,larger document - level corpora,model,/content/training-data/sentiment_analysis/24/triples/model.txt
IMN,explicitly models,interactions,model,/content/training-data/sentiment_analysis/24/triples/model.txt
interactions,through,message passing mechanism,model,/content/training-data/sentiment_analysis/24/triples/model.txt
message passing mechanism,allowing,different tasks,model,/content/training-data/sentiment_analysis/24/triples/model.txt
different tasks,to better influence,each other,model,/content/training-data/sentiment_analysis/24/triples/model.txt
interactions,between,tasks,model,/content/training-data/sentiment_analysis/24/triples/model.txt
Model,propose,interactive multitask learning network ( IMN ,model,/content/training-data/sentiment_analysis/24/triples/model.txt
Model,sends,useful information,model,/content/training-data/sentiment_analysis/24/triples/model.txt
useful information,from,different tasks,model,/content/training-data/sentiment_analysis/24/triples/model.txt
different tasks,back to,shared latent representation,model,/content/training-data/sentiment_analysis/24/triples/model.txt
Contribution,has research problem,Aspect - Based Sentiment Analysis,research-problem,/content/training-data/sentiment_analysis/24/triples/research-problem.txt
Contribution,has research problem,Aspect - based sentiment analysis ( ABSA ,research-problem,/content/training-data/sentiment_analysis/24/triples/research-problem.txt
Contribution,has research problem,aspect term extraction ( AE ,research-problem,/content/training-data/sentiment_analysis/24/triples/research-problem.txt
Contribution,has research problem,aspect - level sentiment classification ( AS ,research-problem,/content/training-data/sentiment_analysis/24/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/24/triples/results.txt
Results,observe that,IMN ?d,results,/content/training-data/sentiment_analysis/24/triples/results.txt
IMN ?d,able to,significantly outperform,results,/content/training-data/sentiment_analysis/24/triples/results.txt
significantly outperform,other,baselines,results,/content/training-data/sentiment_analysis/24/triples/results.txt
significantly outperform,on,F1,results,/content/training-data/sentiment_analysis/24/triples/results.txt
Results,for,AE ( F1 - a and F1 - o ,results,/content/training-data/sentiment_analysis/24/triples/results.txt
AE ( F1 - a and F1 - o ),has,IMN ?d,results,/content/training-data/sentiment_analysis/24/triples/results.txt
IMN ?d,performs,best,results,/content/training-data/sentiment_analysis/24/triples/results.txt
best,in,most cases,results,/content/training-data/sentiment_analysis/24/triples/results.txt
Results,For,AS ( acc - s and F1 - s ,results,/content/training-data/sentiment_analysis/24/triples/results.txt
AS ( acc - s and F1 - s ),has,IMN,results,/content/training-data/sentiment_analysis/24/triples/results.txt
IMN,outperforms,other methods,results,/content/training-data/sentiment_analysis/24/triples/results.txt
other methods,by,large margins,results,/content/training-data/sentiment_analysis/24/triples/results.txt
Results,has,IMN ?d wo DE,results,/content/training-data/sentiment_analysis/24/triples/results.txt
IMN ?d wo DE,competitive with,DECNN - dTrans,results,/content/training-data/sentiment_analysis/24/triples/results.txt
Results,has,IMN ?d,results,/content/training-data/sentiment_analysis/24/triples/results.txt
IMN ?d,more affected without,domain - specific embeddings,results,/content/training-data/sentiment_analysis/24/triples/results.txt
IMN ?d,outperforms,all other baselines,results,/content/training-data/sentiment_analysis/24/triples/results.txt
all other baselines,except,DECNN - d Trans,results,/content/training-data/sentiment_analysis/24/triples/results.txt
Results,has,IMN,results,/content/training-data/sentiment_analysis/24/triples/results.txt
IMN,boosts,performance,results,/content/training-data/sentiment_analysis/24/triples/results.txt
IMN,outperforms,best F1,results,/content/training-data/sentiment_analysis/24/triples/results.txt
best F1,from,baselines,results,/content/training-data/sentiment_analysis/24/triples/results.txt
baselines,by,"2.29 % , 1.77 % , and 2.61 %",results,/content/training-data/sentiment_analysis/24/triples/results.txt
"2.29 % , 1.77 % , and 2.61 %",on,"D1 , D2 , and D3",results,/content/training-data/sentiment_analysis/24/triples/results.txt
Results,has,IMN wo DE,results,/content/training-data/sentiment_analysis/24/triples/results.txt
IMN wo DE,performs,only marginally,results,/content/training-data/sentiment_analysis/24/triples/results.txt
only marginally,below,IMN,results,/content/training-data/sentiment_analysis/24/triples/results.txt
Results,has,DECNN - dTrans,results,/content/training-data/sentiment_analysis/24/triples/results.txt
DECNN - dTrans,is,very strong baseline,results,/content/training-data/sentiment_analysis/24/triples/results.txt
very strong baseline,exploits,additional knowledge,results,/content/training-data/sentiment_analysis/24/triples/results.txt
additional knowledge,from,larger corpora,results,/content/training-data/sentiment_analysis/24/triples/results.txt
larger corpora,for,both tasks,results,/content/training-data/sentiment_analysis/24/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
Baselines,design,basic BERT - based model,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
basic BERT - based model,to evaluate,performance,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
performance,of,AEN - BERT,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
Baselines,has,Non - RNN based baselines,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
Non - RNN based baselines,has,Feature - based SVM,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
Feature - based SVM,is,traditional support vector machine based model,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
traditional support vector machine based model,with,extensive feature engineering,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
Non - RNN based baselines,has,Rec - NN,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
Rec - NN,learns,sentence representation,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
sentence representation,toward,target,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
sentence representation,via,semantic composition,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
semantic composition,using,Recursive NNs,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
Rec - NN,uses,rules,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
rules,to transform,dependency tree,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
Rec - NN,put,opinion target,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
opinion target,at,root,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
Non - RNN based baselines,has,MemNet,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
MemNet,uses,multi-hops of attention layers,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
multi-hops of attention layers,to explicitly captures,importance,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
importance,of,each context word,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
multi-hops of attention layers,on,context word embeddings,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
context word embeddings,for,sentence representation,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
Baselines,has,Basic BERT - based model,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
Basic BERT - based model,has,BERT - SPC,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
BERT - SPC,feeds,"sequence "" [ CLS ] + context + [ SEP ] + target + [ SEP ] """,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
"sequence "" [ CLS ] + context + [ SEP ] + target + [ SEP ] """,into,basic BERT model,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
"sequence "" [ CLS ] + context + [ SEP ] + target + [ SEP ] """,for,sentence pair classification task,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
Baselines,has,AEN - Glo Ve ablations,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
AEN - Glo Ve ablations,has,AEN - GloVe w/ o LSR,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
AEN - GloVe w/ o LSR,ablates,label smoothing regularization,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
AEN - Glo Ve ablations,has,AEN-GloVe-BiLSTM,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
AEN-GloVe-BiLSTM,replaces,attentional encoder layer,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
attentional encoder layer,with,two bidirectional LSTM,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
AEN - Glo Ve ablations,has,AEN - GloVe w/ o MHA,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
AEN - GloVe w/ o MHA,ablates,MHA module,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
AEN - Glo Ve ablations,has,AEN - GloVe w/ o PCT,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
AEN - GloVe w/ o PCT,ablates,PCT module,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
Baselines,has,RNN based baselines,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
RNN based baselines,has,IAN,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
IAN,learns,representations,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
representations,of,target and context,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
target and context,with,two LSTMs and attentions,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
target and context,which generates,representations,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
representations,for,targets and contexts,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
targets and contexts,with respect to,each other,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
RNN based baselines,has,ATAE - LSTM,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
ATAE - LSTM,use,LSTM,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
LSTM,with,attention,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
attention,to get,final representation,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
final representation,for,classification,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
ATAE - LSTM,strengthens,effect of target embeddings,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
effect of target embeddings,which appends,target embeddings,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
target embeddings,with,each word embeddings,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
RNN based baselines,has,TD - LSTM,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
TD - LSTM,extends,LSTM,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
LSTM,by using,two LSTM networks,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
LSTM,to model,right context,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
right context,with,target,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
LSTM,to model,left context,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
left context,with,target,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
RNN based baselines,has,RAM,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
RAM,using,gated recurrent unit network,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
gated recurrent unit network,to combine,multiple attention outputs,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
multiple attention outputs,for,sentence representation,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
RAM,strengthens,Mem - Net,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
Mem - Net,by representing,memory,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
memory,with,bidirectional LSTM,baselines,/content/training-data/sentiment_analysis/28/triples/baselines.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/sentiment_analysis/28/triples/ablation-analysis.txt
Ablation analysis,Comparing the results of,AEN - GloVe and AEN - Glo Ve w / o LSR,ablation-analysis,/content/training-data/sentiment_analysis/28/triples/ablation-analysis.txt
AEN - GloVe and AEN - Glo Ve w / o LSR,observe,accuracy,ablation-analysis,/content/training-data/sentiment_analysis/28/triples/ablation-analysis.txt
accuracy,drops,significantly,ablation-analysis,/content/training-data/sentiment_analysis/28/triples/ablation-analysis.txt
significantly,on,all three datasets,ablation-analysis,/content/training-data/sentiment_analysis/28/triples/ablation-analysis.txt
accuracy,of,AEN - Glo Ve w / o LSR,ablation-analysis,/content/training-data/sentiment_analysis/28/triples/ablation-analysis.txt
Ablation analysis,has,over all performance,ablation-analysis,/content/training-data/sentiment_analysis/28/triples/ablation-analysis.txt
over all performance,of,AEN - Glo Ve,ablation-analysis,/content/training-data/sentiment_analysis/28/triples/ablation-analysis.txt
AEN - Glo Ve,performs,better,ablation-analysis,/content/training-data/sentiment_analysis/28/triples/ablation-analysis.txt
better,on,Restaurant dataset,ablation-analysis,/content/training-data/sentiment_analysis/28/triples/ablation-analysis.txt
over all performance,of,AEN - GloVe and AEN - Glo Ve - BiLSTM,ablation-analysis,/content/training-data/sentiment_analysis/28/triples/ablation-analysis.txt
AEN - GloVe and AEN - Glo Ve - BiLSTM,is,relatively close,ablation-analysis,/content/training-data/sentiment_analysis/28/triples/ablation-analysis.txt
Ablation analysis,has,AEN - Glo Ve,ablation-analysis,/content/training-data/sentiment_analysis/28/triples/ablation-analysis.txt
AEN - Glo Ve,easier to,parallelize,ablation-analysis,/content/training-data/sentiment_analysis/28/triples/ablation-analysis.txt
AEN - Glo Ve,has,fewer parameters,ablation-analysis,/content/training-data/sentiment_analysis/28/triples/ablation-analysis.txt
Ablation analysis,has,model size,ablation-analysis,/content/training-data/sentiment_analysis/28/triples/ablation-analysis.txt
model size,of,AEN - Glo Ve - BiLSTM,ablation-analysis,/content/training-data/sentiment_analysis/28/triples/ablation-analysis.txt
AEN - Glo Ve - BiLSTM,more than,twice,ablation-analysis,/content/training-data/sentiment_analysis/28/triples/ablation-analysis.txt
twice,of,AEN - GloVe,ablation-analysis,/content/training-data/sentiment_analysis/28/triples/ablation-analysis.txt
AEN - Glo Ve - BiLSTM,does not bring,any performance improvements,ablation-analysis,/content/training-data/sentiment_analysis/28/triples/ablation-analysis.txt
Ablation analysis,has,AEN - Glo Ve 's lightweight level,ablation-analysis,/content/training-data/sentiment_analysis/28/triples/ablation-analysis.txt
AEN - Glo Ve 's lightweight level,ranks,second,ablation-analysis,/content/training-data/sentiment_analysis/28/triples/ablation-analysis.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/28/triples/hyperparameters.txt
Hyperparameters,shows,number of training and test instances,hyperparameters,/content/training-data/sentiment_analysis/28/triples/hyperparameters.txt
number of training and test instances,in,each category,hyperparameters,/content/training-data/sentiment_analysis/28/triples/hyperparameters.txt
Hyperparameters,has,Embedding dimension d dim,hyperparameters,/content/training-data/sentiment_analysis/28/triples/hyperparameters.txt
Embedding dimension d dim,is,300,hyperparameters,/content/training-data/sentiment_analysis/28/triples/hyperparameters.txt
300,for,GloVe,hyperparameters,/content/training-data/sentiment_analysis/28/triples/hyperparameters.txt
Embedding dimension d dim,is,768,hyperparameters,/content/training-data/sentiment_analysis/28/triples/hyperparameters.txt
768,for,pretrained BERT,hyperparameters,/content/training-data/sentiment_analysis/28/triples/hyperparameters.txt
Hyperparameters,has,Dimension,hyperparameters,/content/training-data/sentiment_analysis/28/triples/hyperparameters.txt
Dimension,of,hidden states d hid,hyperparameters,/content/training-data/sentiment_analysis/28/triples/hyperparameters.txt
hidden states d hid,set to,300,hyperparameters,/content/training-data/sentiment_analysis/28/triples/hyperparameters.txt
Hyperparameters,has,"Adam optimizer ( Kingma and Ba , 2014 ",hyperparameters,/content/training-data/sentiment_analysis/28/triples/hyperparameters.txt
"Adam optimizer ( Kingma and Ba , 2014 )",applied to,update,hyperparameters,/content/training-data/sentiment_analysis/28/triples/hyperparameters.txt
update,has,all the parameters,hyperparameters,/content/training-data/sentiment_analysis/28/triples/hyperparameters.txt
Hyperparameters,has,weights,hyperparameters,/content/training-data/sentiment_analysis/28/triples/hyperparameters.txt
weights,of,our model,hyperparameters,/content/training-data/sentiment_analysis/28/triples/hyperparameters.txt
weights,initialized with,Glorot initialization,hyperparameters,/content/training-data/sentiment_analysis/28/triples/hyperparameters.txt
Hyperparameters,has,Word embeddings,hyperparameters,/content/training-data/sentiment_analysis/28/triples/hyperparameters.txt
Word embeddings,in,AEN - Glo Ve,hyperparameters,/content/training-data/sentiment_analysis/28/triples/hyperparameters.txt
AEN - Glo Ve,do not get updated in,learning process,hyperparameters,/content/training-data/sentiment_analysis/28/triples/hyperparameters.txt
Word embeddings,fine - tune,pre-trained BERT,hyperparameters,/content/training-data/sentiment_analysis/28/triples/hyperparameters.txt
pre-trained BERT,in,AEN - BERT,hyperparameters,/content/training-data/sentiment_analysis/28/triples/hyperparameters.txt
Hyperparameters,During,training,hyperparameters,/content/training-data/sentiment_analysis/28/triples/hyperparameters.txt
training,set,label smoothing parameter,hyperparameters,/content/training-data/sentiment_analysis/28/triples/hyperparameters.txt
label smoothing parameter,to,0.2,hyperparameters,/content/training-data/sentiment_analysis/28/triples/hyperparameters.txt
training,set,dropout rate,hyperparameters,/content/training-data/sentiment_analysis/28/triples/hyperparameters.txt
dropout rate,is,0.1,hyperparameters,/content/training-data/sentiment_analysis/28/triples/hyperparameters.txt
training,set,coefficient ? of L 2 regularization item,hyperparameters,/content/training-data/sentiment_analysis/28/triples/hyperparameters.txt
coefficient ? of L 2 regularization item,is,10 ? 5,hyperparameters,/content/training-data/sentiment_analysis/28/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/28/triples/model.txt
Model,To deal with,label unreliability issue,model,/content/training-data/sentiment_analysis/28/triples/model.txt
label unreliability issue,employ,label smoothing regularization,model,/content/training-data/sentiment_analysis/28/triples/model.txt
label smoothing regularization,to encourage,model,model,/content/training-data/sentiment_analysis/28/triples/model.txt
model,with,fuzzy labels,model,/content/training-data/sentiment_analysis/28/triples/model.txt
model,to be,less confident,model,/content/training-data/sentiment_analysis/28/triples/model.txt
Model,apply,pre-trained BERT,model,/content/training-data/sentiment_analysis/28/triples/model.txt
Model,has,our model,model,/content/training-data/sentiment_analysis/28/triples/model.txt
our model,employs,attention,model,/content/training-data/sentiment_analysis/28/triples/model.txt
attention,as,competitive alternative,model,/content/training-data/sentiment_analysis/28/triples/model.txt
competitive alternative,to draw,introspective and interactive semantics,model,/content/training-data/sentiment_analysis/28/triples/model.txt
introspective and interactive semantics,between,target and context words,model,/content/training-data/sentiment_analysis/28/triples/model.txt
our model,eschews,recurrence,model,/content/training-data/sentiment_analysis/28/triples/model.txt
Model,propose,attention based model,model,/content/training-data/sentiment_analysis/28/triples/model.txt
Contribution,has research problem,Targeted Sentiment Classification,research-problem,/content/training-data/sentiment_analysis/28/triples/research-problem.txt
Contribution,has research problem,fine - grained sentiment analysis,research-problem,/content/training-data/sentiment_analysis/28/triples/research-problem.txt
Contribution,has research problem,fine - grained targeted sentiment classification,research-problem,/content/training-data/sentiment_analysis/28/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/28/triples/results.txt
Results,Like,AEN,results,/content/training-data/sentiment_analysis/28/triples/results.txt
AEN,has,Mem Net,results,/content/training-data/sentiment_analysis/28/triples/results.txt
Mem Net,eschews,recurrence,results,/content/training-data/sentiment_analysis/28/triples/results.txt
Mem Net,has,over all performance,results,/content/training-data/sentiment_analysis/28/triples/results.txt
over all performance,is,not good,results,/content/training-data/sentiment_analysis/28/triples/results.txt
Results,has,over all performance,results,/content/training-data/sentiment_analysis/28/triples/results.txt
over all performance,of,TD - LSTM,results,/content/training-data/sentiment_analysis/28/triples/results.txt
TD - LSTM,makes,rough treatment,results,/content/training-data/sentiment_analysis/28/triples/results.txt
rough treatment,of,target words,results,/content/training-data/sentiment_analysis/28/triples/results.txt
TD - LSTM,is,not good,results,/content/training-data/sentiment_analysis/28/triples/results.txt
Results,has,"ATAE - LSTM , IAN and RAM",results,/content/training-data/sentiment_analysis/28/triples/results.txt
"ATAE - LSTM , IAN and RAM",stably exceed,TD - LSTM method,results,/content/training-data/sentiment_analysis/28/triples/results.txt
TD - LSTM method,on,Restaurant and Laptop datasets,results,/content/training-data/sentiment_analysis/28/triples/results.txt
"ATAE - LSTM , IAN and RAM",are,attention based models,results,/content/training-data/sentiment_analysis/28/triples/results.txt
Results,has,Feature - based SVM,results,/content/training-data/sentiment_analysis/28/triples/results.txt
Feature - based SVM,still,competitive baseline,results,/content/training-data/sentiment_analysis/28/triples/results.txt
Feature - based SVM,relying on,manually - designed features,results,/content/training-data/sentiment_analysis/28/triples/results.txt
Results,has,Rec - NN,results,/content/training-data/sentiment_analysis/28/triples/results.txt
Rec - NN,gets,worst performances,results,/content/training-data/sentiment_analysis/28/triples/results.txt
worst performances,among,all neural network baselines,results,/content/training-data/sentiment_analysis/28/triples/results.txt
Results,has,RAM,results,/content/training-data/sentiment_analysis/28/triples/results.txt
RAM,does not perform,well,results,/content/training-data/sentiment_analysis/28/triples/results.txt
well,on,Twitter dataset,results,/content/training-data/sentiment_analysis/28/triples/results.txt
RAM,better than,other RNN based models,results,/content/training-data/sentiment_analysis/28/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
Baselines,has,IAN,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
IAN,generates,representations,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
representations,for,targets and contexts,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
IAN,learns,attentions,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
attentions,in,contexts and targets,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
Baselines,has,AOA - LSTM,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
AOA - LSTM,introduces,attention - over- attention ( AOA ) based network,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
attention - over- attention ( AOA ) based network,explicitly capture,interaction,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
interaction,between,aspects and context sentences,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
attention - over- attention ( AOA ) based network,to model,aspects and sentences,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
aspects and sentences,in,joint way,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
Baselines,has,Bi - LSTM and Bi - GRU,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
Bi - LSTM and Bi - GRU,adopt,Bi - LSTM and a Bi - GRU network,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
Bi - LSTM and a Bi - GRU network,use,hidden state,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
hidden state,of,final word,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
final word,for,prediction,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
Bi - LSTM and a Bi - GRU network,to model,sentence,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
Baselines,has,LCR - Rot,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
LCR - Rot,employs,three Bi- LSTMs,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
three Bi- LSTMs,to model,left context,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
three Bi- LSTMs,to model,target,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
three Bi- LSTMs,to model,right context,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
Baselines,has,Majority,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
Majority,assigns,sentiment polarity,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
sentiment polarity,with,most frequent occurrences,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
most frequent occurrences,in,training set,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
training set,to,each sample,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
each sample,in,test set,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
Baselines,has,TD - LSTM,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
TD - LSTM,adopts,two LSTMs,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
two LSTMs,to model,right context,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
right context,with,target,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
two LSTMs,to model,left context,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
left context,with,target,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
TD - LSTM,takes,hidden states,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
hidden states,of,LSTM,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
LSTM,at,last time - step,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
last time - step,to represent,sentence,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
sentence,for,prediction,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
Baselines,has,MemNet,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
MemNet,applies,attention,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
attention,has,multiple times,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
multiple times,on,word embeddings,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
MemNet,has,output,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
output,of,last attention,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
output,fed to,softmax,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
softmax,for,prediction,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
Baselines,has,RAM,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
RAM,is,multilayer architecture,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
multilayer architecture,where,each layer,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
each layer,consists of,GRU cell,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
GRU cell,to learn,sentence representation,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
each layer,consists of,attention - based aggregation,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
attention - based aggregation,of,word features,baselines,/content/training-data/sentiment_analysis/22/triples/baselines.txt
Contribution,Code,https://github.com/DUT-LiuYang/Aspect-Sentiment-Analysis,code,/content/training-data/sentiment_analysis/22/triples/code.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/22/triples/hyperparameters.txt
Hyperparameters,In,model training,hyperparameters,/content/training-data/sentiment_analysis/22/triples/hyperparameters.txt
model training,set,learning rate,hyperparameters,/content/training-data/sentiment_analysis/22/triples/hyperparameters.txt
learning rate,to,0.001,hyperparameters,/content/training-data/sentiment_analysis/22/triples/hyperparameters.txt
model training,set,batch size,hyperparameters,/content/training-data/sentiment_analysis/22/triples/hyperparameters.txt
batch size,to,64,hyperparameters,/content/training-data/sentiment_analysis/22/triples/hyperparameters.txt
model training,set,dropout rate,hyperparameters,/content/training-data/sentiment_analysis/22/triples/hyperparameters.txt
dropout rate,to,0.5,hyperparameters,/content/training-data/sentiment_analysis/22/triples/hyperparameters.txt
Hyperparameters,use,300 - dimension word vectors,hyperparameters,/content/training-data/sentiment_analysis/22/triples/hyperparameters.txt
300 - dimension word vectors,pre-trained by,GloVe,hyperparameters,/content/training-data/sentiment_analysis/22/triples/hyperparameters.txt
Hyperparameters,has,paired t- test,hyperparameters,/content/training-data/sentiment_analysis/22/triples/hyperparameters.txt
paired t- test,used for,significance testing,hyperparameters,/content/training-data/sentiment_analysis/22/triples/hyperparameters.txt
Hyperparameters,has,All out - of - vocabulary words,hyperparameters,/content/training-data/sentiment_analysis/22/triples/hyperparameters.txt
All out - of - vocabulary words,initialized as,zero vectors,hyperparameters,/content/training-data/sentiment_analysis/22/triples/hyperparameters.txt
Hyperparameters,has,Keras,hyperparameters,/content/training-data/sentiment_analysis/22/triples/hyperparameters.txt
Keras,for implementing,neural network model,hyperparameters,/content/training-data/sentiment_analysis/22/triples/hyperparameters.txt
Hyperparameters,has,dimension,hyperparameters,/content/training-data/sentiment_analysis/22/triples/hyperparameters.txt
dimension,of,position embeddings,hyperparameters,/content/training-data/sentiment_analysis/22/triples/hyperparameters.txt
position embeddings,set to,50,hyperparameters,/content/training-data/sentiment_analysis/22/triples/hyperparameters.txt
Hyperparameters,has,all biases,hyperparameters,/content/training-data/sentiment_analysis/22/triples/hyperparameters.txt
all biases,set to,zero,hyperparameters,/content/training-data/sentiment_analysis/22/triples/hyperparameters.txt
Hyperparameters,has,dimensions,hyperparameters,/content/training-data/sentiment_analysis/22/triples/hyperparameters.txt
dimensions,set to,300,hyperparameters,/content/training-data/sentiment_analysis/22/triples/hyperparameters.txt
300,of,hidden states and fused embeddings,hyperparameters,/content/training-data/sentiment_analysis/22/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/22/triples/model.txt
Model,feed,achieved sentence representation,model,/content/training-data/sentiment_analysis/22/triples/model.txt
achieved sentence representation,into,softmax layer,model,/content/training-data/sentiment_analysis/22/triples/model.txt
softmax layer,to predict,sentiment polarity,model,/content/training-data/sentiment_analysis/22/triples/model.txt
Model,has,succinct fusion mechanism,model,/content/training-data/sentiment_analysis/22/triples/model.txt
succinct fusion mechanism,proposed to,fuse,model,/content/training-data/sentiment_analysis/22/triples/model.txt
fuse,information of,aspects and the contexts,model,/content/training-data/sentiment_analysis/22/triples/model.txt
aspects and the contexts,achieving,final sentence representation,model,/content/training-data/sentiment_analysis/22/triples/model.txt
Model,has,position - aware encoding layer,model,/content/training-data/sentiment_analysis/22/triples/model.txt
position - aware encoding layer,for modelling,sentence,model,/content/training-data/sentiment_analysis/22/triples/model.txt
position - aware encoding layer,to achieve,position - aware abstract representation,model,/content/training-data/sentiment_analysis/22/triples/model.txt
position - aware abstract representation,of,each word,model,/content/training-data/sentiment_analysis/22/triples/model.txt
Model,propose,hierarchical attention based positionaware network ( HAPN ,model,/content/training-data/sentiment_analysis/22/triples/model.txt
hierarchical attention based positionaware network ( HAPN ),for,aspect - level sentiment classification,model,/content/training-data/sentiment_analysis/22/triples/model.txt
Contribution,has research problem,Aspect-level Sentiment Analysis,research-problem,/content/training-data/sentiment_analysis/22/triples/research-problem.txt
Contribution,has research problem,sentiment analysis,research-problem,/content/training-data/sentiment_analysis/22/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/22/triples/results.txt
Results,introducing,position embeddings,results,/content/training-data/sentiment_analysis/22/triples/results.txt
position embeddings,has,accuracy,results,/content/training-data/sentiment_analysis/22/triples/results.txt
accuracy,has an,increase,results,/content/training-data/sentiment_analysis/22/triples/results.txt
increase,of,0.62 % and 2.67 %,results,/content/training-data/sentiment_analysis/22/triples/results.txt
0.62 % and 2.67 %,on,two datasets,results,/content/training-data/sentiment_analysis/22/triples/results.txt
Results,has,output,results,/content/training-data/sentiment_analysis/22/triples/results.txt
output,of,Source2aspect attention,results,/content/training-data/sentiment_analysis/22/triples/results.txt
Source2aspect attention,used for,information fusion,results,/content/training-data/sentiment_analysis/22/triples/results.txt
Results,has,TD - LSTM model,results,/content/training-data/sentiment_analysis/22/triples/results.txt
TD - LSTM model,has,accuracy,results,/content/training-data/sentiment_analysis/22/triples/results.txt
accuracy,achieved by,TD - LSTM,results,/content/training-data/sentiment_analysis/22/triples/results.txt
TD - LSTM,is,2.94 % and 2.4 %,results,/content/training-data/sentiment_analysis/22/triples/results.txt
2.94 % and 2.4 %,lower than,Bi - LSTM,results,/content/training-data/sentiment_analysis/22/triples/results.txt
TD - LSTM model,gets,worst performance,results,/content/training-data/sentiment_analysis/22/triples/results.txt
worst performance,of,all RNN based models,results,/content/training-data/sentiment_analysis/22/triples/results.txt
TD - LSTM model,shown to be better than,LSTM,results,/content/training-data/sentiment_analysis/22/triples/results.txt
Results,has,information fusion operation,results,/content/training-data/sentiment_analysis/22/triples/results.txt
information fusion operation,used to,calculate,results,/content/training-data/sentiment_analysis/22/triples/results.txt
calculate,has,Source2context attention value,results,/content/training-data/sentiment_analysis/22/triples/results.txt
Results,has,Our method,results,/content/training-data/sentiment_analysis/22/triples/results.txt
Our method,achieves,accuracies,results,/content/training-data/sentiment_analysis/22/triples/results.txt
accuracies,of,82.23 %,results,/content/training-data/sentiment_analysis/22/triples/results.txt
82.23 %,as well as,77 . 27 %,results,/content/training-data/sentiment_analysis/22/triples/results.txt
77 . 27 %,on,Restaurant and Laptop dataset,results,/content/training-data/sentiment_analysis/22/triples/results.txt
Results,has,HAPN,results,/content/training-data/sentiment_analysis/22/triples/results.txt
HAPN,achieves,improvement,results,/content/training-data/sentiment_analysis/22/triples/results.txt
improvement,of,0.35 % and 0.78 %,results,/content/training-data/sentiment_analysis/22/triples/results.txt
0.35 % and 0.78 %,on,accuracy,results,/content/training-data/sentiment_analysis/22/triples/results.txt
Results,has,accuracy,results,/content/training-data/sentiment_analysis/22/triples/results.txt
accuracy,achieved by,Bi - GRU - PW,results,/content/training-data/sentiment_analysis/22/triples/results.txt
Bi - GRU - PW,is,0.72 %,results,/content/training-data/sentiment_analysis/22/triples/results.txt
0.72 %,as well as,1.41 %,results,/content/training-data/sentiment_analysis/22/triples/results.txt
1.41 %,lower than,Bi - GRU,results,/content/training-data/sentiment_analysis/22/triples/results.txt
Bi - GRU,on,Restaurant and Laptop dataset,results,/content/training-data/sentiment_analysis/22/triples/results.txt
Results,has,Bi - GRU - PW,results,/content/training-data/sentiment_analysis/22/triples/results.txt
Bi - GRU - PW,performs,even worse,results,/content/training-data/sentiment_analysis/22/triples/results.txt
even worse,than,Bi - GRU,results,/content/training-data/sentiment_analysis/22/triples/results.txt
Results,has,Bi - GRU - PE,results,/content/training-data/sentiment_analysis/22/triples/results.txt
Bi - GRU - PE,achieving,accuracies,results,/content/training-data/sentiment_analysis/22/triples/results.txt
accuracies,of,80.89 % and 76.02 %,results,/content/training-data/sentiment_analysis/22/triples/results.txt
80.89 % and 76.02 %,on,two datasets,results,/content/training-data/sentiment_analysis/22/triples/results.txt
Results,Compared with,state - of - the - art methods,results,/content/training-data/sentiment_analysis/22/triples/results.txt
state - of - the - art methods,has,our model,results,/content/training-data/sentiment_analysis/22/triples/results.txt
our model,achieves,best performance,results,/content/training-data/sentiment_analysis/22/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/sentiment_analysis/19/triples/baselines.txt
Baselines,compare,SuBiLSTM and SuBiLSTM - Tied,baselines,/content/training-data/sentiment_analysis/19/triples/baselines.txt
SuBiLSTM and SuBiLSTM - Tied,with,single - layer BiLSTM and a 2 - layer BiLSTM encoder,baselines,/content/training-data/sentiment_analysis/19/triples/baselines.txt
single - layer BiLSTM and a 2 - layer BiLSTM encoder,with,same hidden dimension,baselines,/content/training-data/sentiment_analysis/19/triples/baselines.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/19/triples/model.txt
Model,call,Suffix BiLSTM,model,/content/training-data/sentiment_analysis/19/triples/model.txt
Model,call,SuBiLSTM,model,/content/training-data/sentiment_analysis/19/triples/model.txt
SuBiLSTM,in,short,model,/content/training-data/sentiment_analysis/19/triples/model.txt
Model,For,each token t,model,/content/training-data/sentiment_analysis/19/triples/model.txt
each token t,encode both,prefix and suffix,model,/content/training-data/sentiment_analysis/19/triples/model.txt
prefix and suffix,in both,forward and reverse direction,model,/content/training-data/sentiment_analysis/19/triples/model.txt
Model,propose,"simple , general and effective technique",model,/content/training-data/sentiment_analysis/19/triples/model.txt
"simple , general and effective technique",to compute,contextual representations,model,/content/training-data/sentiment_analysis/19/triples/model.txt
contextual representations,that capture,long range dependencies,model,/content/training-data/sentiment_analysis/19/triples/model.txt
Model,combine,prefix and suffix representations,model,/content/training-data/sentiment_analysis/19/triples/model.txt
prefix and suffix representations,by,simple max - pooling operation,model,/content/training-data/sentiment_analysis/19/triples/model.txt
simple max - pooling operation,to produce,richer contextual representation,model,/content/training-data/sentiment_analysis/19/triples/model.txt
richer contextual representation,in both,forward and reverse direction,model,/content/training-data/sentiment_analysis/19/triples/model.txt
Contribution,has research problem,Improved Sentence Modeling,research-problem,/content/training-data/sentiment_analysis/19/triples/research-problem.txt
Contribution,has research problem,computing representations of sequential data,research-problem,/content/training-data/sentiment_analysis/19/triples/research-problem.txt
Contribution,has research problem,fine - grained sentiment classification,research-problem,/content/training-data/sentiment_analysis/19/triples/research-problem.txt
Contribution,has research problem,question classification,research-problem,/content/training-data/sentiment_analysis/19/triples/research-problem.txt
Contribution,has research problem,modeling sequential data,research-problem,/content/training-data/sentiment_analysis/19/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/19/triples/results.txt
Results,For,larger datasets ( SNLI and QUORA ,results,/content/training-data/sentiment_analysis/19/triples/results.txt
larger datasets ( SNLI and QUORA ),has,SuBILSTM,results,/content/training-data/sentiment_analysis/19/triples/results.txt
SuBILSTM,edges out,tied version,results,/content/training-data/sentiment_analysis/19/triples/results.txt
SuBILSTM,owing to,larger capacity,results,/content/training-data/sentiment_analysis/19/triples/results.txt
Results,has,relative performance,results,/content/training-data/sentiment_analysis/19/triples/results.txt
relative performance,of,SuBiL - STM and SuBiLSTM - Tied,results,/content/training-data/sentiment_analysis/19/triples/results.txt
SuBiL - STM and SuBiLSTM - Tied,are,fairly close,results,/content/training-data/sentiment_analysis/19/triples/results.txt
Results,has,training complexity,results,/content/training-data/sentiment_analysis/19/triples/results.txt
training complexity,for both,models,results,/content/training-data/sentiment_analysis/19/triples/results.txt
models,is,similar,results,/content/training-data/sentiment_analysis/19/triples/results.txt
models,has,SuBILSTM - Tied,results,/content/training-data/sentiment_analysis/19/triples/results.txt
SuBILSTM - Tied,with,half the parameters,results,/content/training-data/sentiment_analysis/19/triples/results.txt
half the parameters,should be,more favored model,results,/content/training-data/sentiment_analysis/19/triples/results.txt
more favored model,for,sentence modeling tasks,results,/content/training-data/sentiment_analysis/19/triples/results.txt
Results,has,SuBiLSTM - Tied,results,/content/training-data/sentiment_analysis/19/triples/results.txt
SuBiLSTM - Tied,works,better,results,/content/training-data/sentiment_analysis/19/triples/results.txt
better,on,small datasets ( SST and TREC ,results,/content/training-data/sentiment_analysis/19/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
Baselines,has,RNTN,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
RNTN,name,Recursive Tensor Neural Network,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
RNTN,to model,correlations,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
correlations,between,different dimensions,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
different dimensions,of,child nodes vectors,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
Baselines,has,CNN,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
CNN,to generate,task - specific sentence representation,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
CNN,name,Convolutional Neural Networks,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
Baselines,has,ID - LSTM,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
ID - LSTM,uses,reinforcement learning,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
reinforcement learning,to learn,structured sentence representation,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
structured sentence representation,for,sentiment classification,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
Baselines,has,LSTM / Bi-LSTM,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
LSTM / Bi-LSTM,employs,Long Short - Term Memory,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
LSTM / Bi-LSTM,employs,bidirectional variant,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
bidirectional variant,to capture,sequential information,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
Baselines,has,LR - Bi-LSTM,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
LR - Bi-LSTM,imposes,linguistic roles,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
linguistic roles,into,neural networks,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
linguistic roles,by applying,linguistic regularization,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
linguistic regularization,with,KL divergence,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
linguistic regularization,on,intermediate outputs,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
Baselines,has,Tree-LSTM,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
Tree-LSTM,has,gates,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
gates,into,tree - structured neural network,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
Tree-LSTM,has,Memory cells,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
Memory cells,introduced by,Tree - Structured Long Short - Term Memory,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
Baselines,has,Self - attention,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
Self - attention,proposes,selfattention mechanism,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
selfattention mechanism,to learn,structured sentence embedding,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
Baselines,has,NCSL,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
NCSL,name,Neural Context - Sensitive Lexicon ( NSCL ,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
NCSL,to obtain,prior sentiment scores of words,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
prior sentiment scores of words,in,sentence,baselines,/content/training-data/sentiment_analysis/46/triples/baselines.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/46/triples/hyperparameters.txt
Hyperparameters,set,all the bias vectors,hyperparameters,/content/training-data/sentiment_analysis/46/triples/hyperparameters.txt
all the bias vectors,as,zero vectors,hyperparameters,/content/training-data/sentiment_analysis/46/triples/hyperparameters.txt
Hyperparameters,optimize,proposed model,hyperparameters,/content/training-data/sentiment_analysis/46/triples/hyperparameters.txt
proposed model,with,RMSprop algorithm,hyperparameters,/content/training-data/sentiment_analysis/46/triples/hyperparameters.txt
proposed model,using,mini-batch training,hyperparameters,/content/training-data/sentiment_analysis/46/triples/hyperparameters.txt
Hyperparameters,has,size,hyperparameters,/content/training-data/sentiment_analysis/46/triples/hyperparameters.txt
size,of,mini-batch,hyperparameters,/content/training-data/sentiment_analysis/46/triples/hyperparameters.txt
mini-batch,is,60,hyperparameters,/content/training-data/sentiment_analysis/46/triples/hyperparameters.txt
Hyperparameters,has,Kernel sizes,hyperparameters,/content/training-data/sentiment_analysis/46/triples/hyperparameters.txt
Kernel sizes,of,multi-gram convolution,hyperparameters,/content/training-data/sentiment_analysis/46/triples/hyperparameters.txt
multi-gram convolution,for,Char - CNN,hyperparameters,/content/training-data/sentiment_analysis/46/triples/hyperparameters.txt
Kernel sizes,set to,"2 , 3",hyperparameters,/content/training-data/sentiment_analysis/46/triples/hyperparameters.txt
Hyperparameters,has,coefficient,hyperparameters,/content/training-data/sentiment_analysis/46/triples/hyperparameters.txt
coefficient,of,L 2 normalization,hyperparameters,/content/training-data/sentiment_analysis/46/triples/hyperparameters.txt
L 2 normalization,set to,10 ?5,hyperparameters,/content/training-data/sentiment_analysis/46/triples/hyperparameters.txt
Hyperparameters,has,weight matrices,hyperparameters,/content/training-data/sentiment_analysis/46/triples/hyperparameters.txt
weight matrices,initialized as,random orthogonal matrices,hyperparameters,/content/training-data/sentiment_analysis/46/triples/hyperparameters.txt
Hyperparameters,has,dropout rate,hyperparameters,/content/training-data/sentiment_analysis/46/triples/hyperparameters.txt
dropout rate,is,0.5,hyperparameters,/content/training-data/sentiment_analysis/46/triples/hyperparameters.txt
Hyperparameters,has,dimensions,hyperparameters,/content/training-data/sentiment_analysis/46/triples/hyperparameters.txt
dimensions,of,characterlevel embedding and word embedding ( Glo Ve ,hyperparameters,/content/training-data/sentiment_analysis/46/triples/hyperparameters.txt
characterlevel embedding and word embedding ( Glo Ve ),set to,300,hyperparameters,/content/training-data/sentiment_analysis/46/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/46/triples/model.txt
Model,design,coupled word embedding module,model,/content/training-data/sentiment_analysis/46/triples/model.txt
coupled word embedding module,to model,word representation,model,/content/training-data/sentiment_analysis/46/triples/model.txt
word representation,from,character - level and word - level semantics,model,/content/training-data/sentiment_analysis/46/triples/model.txt
Model,propose,Multi- sentimentresource Enhanced Attention Network ( MEAN ,model,/content/training-data/sentiment_analysis/46/triples/model.txt
Multi- sentimentresource Enhanced Attention Network ( MEAN ),for,sentence - level sentiment classification,model,/content/training-data/sentiment_analysis/46/triples/model.txt
Multi- sentimentresource Enhanced Attention Network ( MEAN ),to integrate,many kinds of sentiment linguistic knowledge,model,/content/training-data/sentiment_analysis/46/triples/model.txt
many kinds of sentiment linguistic knowledge,into,deep neural networks,model,/content/training-data/sentiment_analysis/46/triples/model.txt
many kinds of sentiment linguistic knowledge,via,multi -path attention mechanism,model,/content/training-data/sentiment_analysis/46/triples/model.txt
Model,propose,multisentiment - resource attention module,model,/content/training-data/sentiment_analysis/46/triples/model.txt
multisentiment - resource attention module,to learn,more comprehensive and meaningful sentiment - specific sentence representation,model,/content/training-data/sentiment_analysis/46/triples/model.txt
more comprehensive and meaningful sentiment - specific sentence representation,by using,three types of sentiment resource words,model,/content/training-data/sentiment_analysis/46/triples/model.txt
three types of sentiment resource words,attending to,context words,model,/content/training-data/sentiment_analysis/46/triples/model.txt
three types of sentiment resource words,as,attention sources,model,/content/training-data/sentiment_analysis/46/triples/model.txt
multisentiment - resource attention module,attend to,different sentimentrelevant information,model,/content/training-data/sentiment_analysis/46/triples/model.txt
different sentimentrelevant information,from,different representation subspaces,model,/content/training-data/sentiment_analysis/46/triples/model.txt
different representation subspaces,capture,over all semantics,model,/content/training-data/sentiment_analysis/46/triples/model.txt
over all semantics,of,"sentiment , negation and intensity words",model,/content/training-data/sentiment_analysis/46/triples/model.txt
"sentiment , negation and intensity words",for,sentiment prediction,model,/content/training-data/sentiment_analysis/46/triples/model.txt
different representation subspaces,implied by,different types of sentiment sources,model,/content/training-data/sentiment_analysis/46/triples/model.txt
Contribution,has research problem,Sentiment Classification,research-problem,/content/training-data/sentiment_analysis/46/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/46/triples/results.txt
Results,has,our model,results,/content/training-data/sentiment_analysis/46/triples/results.txt
our model,consistently outperforms,LR - Bi - LSTM,results,/content/training-data/sentiment_analysis/46/triples/results.txt
LR - Bi - LSTM,which integrates,linguistic roles,results,/content/training-data/sentiment_analysis/46/triples/results.txt
linguistic roles,into,neural networks,results,/content/training-data/sentiment_analysis/46/triples/results.txt
neural networks,via,linguistic regularization,results,/content/training-data/sentiment_analysis/46/triples/results.txt
linguistic roles,of,"sentiment , negation and intensity words",results,/content/training-data/sentiment_analysis/46/triples/results.txt
our model,brings,substantial improvement,results,/content/training-data/sentiment_analysis/46/triples/results.txt
substantial improvement,over,methods,results,/content/training-data/sentiment_analysis/46/triples/results.txt
methods,that do not leverage,sentiment linguistic knowledge,results,/content/training-data/sentiment_analysis/46/triples/results.txt
our model,achieves,2.4 % improvements,results,/content/training-data/sentiment_analysis/46/triples/results.txt
2.4 % improvements,over,MR dataset,results,/content/training-data/sentiment_analysis/46/triples/results.txt
our model,achieves,0.8 % improvements,results,/content/training-data/sentiment_analysis/46/triples/results.txt
0.8 % improvements,over,SST dataset,results,/content/training-data/sentiment_analysis/46/triples/results.txt
our model,achieves,compared to LR - Bi - LSTM,results,/content/training-data/sentiment_analysis/46/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
Baselines,has,MGAN - F,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
MGAN - F,utilizes,proposed fine - grained attentions,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
proposed fine - grained attentions,for,prediction,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
Baselines,has,MGAN - CF,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
MGAN - CF,adopts both,coarse - grained and fine - grained attentions,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
Baselines,has,MGAN,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
MGAN,is,complete multi-grained attention network model,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
Baselines,has,IAN,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
IAN,interactively learns,coarse - grained attentions,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
coarse - grained attentions,concatenate,vectors,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
vectors,for,prediction,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
coarse - grained attentions,between,context and aspect,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
Baselines,has,MGAN - C,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
MGAN - C,employs,coarse - grained attentions,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
coarse - grained attentions,for,prediction,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
Baselines,has,BILSTM - ATT -G,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
BILSTM - ATT -G,models,left and right context,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
left and right context,with,two attention - based LSTMs,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
BILSTM - ATT -G,utilizes,gates,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
gates,to control,importance,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
importance,of,left context,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
importance,of,right context,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
importance,of,entire sentence,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
entire sentence,for,prediction,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
Baselines,has,Majority,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
Majority,is,basic baseline,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
Majority,chooses,largest sentiment polarity,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
largest sentiment polarity,in,training set,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
training set,to each,instance,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
instance,in,test set,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
Baselines,has,MemNet,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
MemNet,applys,multi-hop attentions,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
multi-hop attentions,on,word embeddings,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
MemNet,learns,attention weights,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
attention weights,on,context word vectors,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
context word vectors,with respect to,averaged query vector,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
Baselines,has,RAM,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
RAM,proposes to use,GRU network,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
GRU network,to get,aggregated vector,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
aggregated vector,from,attentions,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
RAM,learns,multi-hop attentions,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
multi-hop attentions,on,hidden states,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
hidden states,of,bidirectional LSTM networks,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
bidirectional LSTM networks,for,context words,baselines,/content/training-data/sentiment_analysis/43/triples/baselines.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/43/triples/hyperparameters.txt
Hyperparameters,has,coefficient,hyperparameters,/content/training-data/sentiment_analysis/43/triples/hyperparameters.txt
coefficient,of,L 2 regularization,hyperparameters,/content/training-data/sentiment_analysis/43/triples/hyperparameters.txt
L 2 regularization,is,10 ? 5,hyperparameters,/content/training-data/sentiment_analysis/43/triples/hyperparameters.txt
Hyperparameters,has,word embeddings,hyperparameters,/content/training-data/sentiment_analysis/43/triples/hyperparameters.txt
word embeddings,for both,context and aspect words,hyperparameters,/content/training-data/sentiment_analysis/43/triples/hyperparameters.txt
context and aspect words,initialized by,Glove,hyperparameters,/content/training-data/sentiment_analysis/43/triples/hyperparameters.txt
Hyperparameters,has,dropout rate,hyperparameters,/content/training-data/sentiment_analysis/43/triples/hyperparameters.txt
dropout rate,set to,0.5,hyperparameters,/content/training-data/sentiment_analysis/43/triples/hyperparameters.txt
Hyperparameters,has,dimension,hyperparameters,/content/training-data/sentiment_analysis/43/triples/hyperparameters.txt
dimension,set to,300,hyperparameters,/content/training-data/sentiment_analysis/43/triples/hyperparameters.txt
300,of,word embedding d v,hyperparameters,/content/training-data/sentiment_analysis/43/triples/hyperparameters.txt
300,of,hidden stated,hyperparameters,/content/training-data/sentiment_analysis/43/triples/hyperparameters.txt
Hyperparameters,has,weight matrix and bias,hyperparameters,/content/training-data/sentiment_analysis/43/triples/hyperparameters.txt
weight matrix and bias,are,initialized,hyperparameters,/content/training-data/sentiment_analysis/43/triples/hyperparameters.txt
initialized,by,sampling,hyperparameters,/content/training-data/sentiment_analysis/43/triples/hyperparameters.txt
sampling,from,"uniform distribution U ( 0.01 , 0.01 ",hyperparameters,/content/training-data/sentiment_analysis/43/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/43/triples/model.txt
Model,utilize,bidirectional coarsegrained attention,model,/content/training-data/sentiment_analysis/43/triples/model.txt
bidirectional coarsegrained attention,combine them with,finegrained attention vectors,model,/content/training-data/sentiment_analysis/43/triples/model.txt
finegrained attention vectors,to compose,multigrained attention network,model,/content/training-data/sentiment_analysis/43/triples/model.txt
multigrained attention network,for,final sentiment polarity prediction,model,/content/training-data/sentiment_analysis/43/triples/model.txt
Model,to make use of,valuable aspect - level interaction information,model,/content/training-data/sentiment_analysis/43/triples/model.txt
valuable aspect - level interaction information,design,aspect alignment loss,model,/content/training-data/sentiment_analysis/43/triples/model.txt
aspect alignment loss,to enhance,difference,model,/content/training-data/sentiment_analysis/43/triples/model.txt
difference,of,attention weights,model,/content/training-data/sentiment_analysis/43/triples/model.txt
difference,towards,aspects,model,/content/training-data/sentiment_analysis/43/triples/model.txt
aspects,which have,same context,model,/content/training-data/sentiment_analysis/43/triples/model.txt
aspects,which have,different sentiment polarities,model,/content/training-data/sentiment_analysis/43/triples/model.txt
aspect alignment loss,in,objective function,model,/content/training-data/sentiment_analysis/43/triples/model.txt
Model,propose,multi -grained attention network,model,/content/training-data/sentiment_analysis/43/triples/model.txt
Model,propose,fine - grained attention mechanism,model,/content/training-data/sentiment_analysis/43/triples/model.txt
fine - grained attention mechanism,to characterize,word - level interactions,model,/content/training-data/sentiment_analysis/43/triples/model.txt
word - level interactions,between,aspect and context words,model,/content/training-data/sentiment_analysis/43/triples/model.txt
fine - grained attention mechanism,relieve,information loss,model,/content/training-data/sentiment_analysis/43/triples/model.txt
information loss,occurred in,coarse - grained attention mechanism,model,/content/training-data/sentiment_analysis/43/triples/model.txt
Contribution,has research problem,Aspect - Level Sentiment Classification,research-problem,/content/training-data/sentiment_analysis/43/triples/research-problem.txt
Contribution,has research problem,aspect level sentiment classification,research-problem,/content/training-data/sentiment_analysis/43/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/43/triples/results.txt
Results,has,BILSTM - ATT - G,results,/content/training-data/sentiment_analysis/43/triples/results.txt
BILSTM - ATT - G,models,left context and right context,results,/content/training-data/sentiment_analysis/43/triples/results.txt
left context and right context,using,attention - based LSTMs,results,/content/training-data/sentiment_analysis/43/triples/results.txt
BILSTM - ATT - G,achieves,better performance,results,/content/training-data/sentiment_analysis/43/triples/results.txt
better performance,than,MemNet,results,/content/training-data/sentiment_analysis/43/triples/results.txt
Results,has,MGAN,results,/content/training-data/sentiment_analysis/43/triples/results.txt
MGAN,outperforms,Majority and Feature + SVM,results,/content/training-data/sentiment_analysis/43/triples/results.txt
Results,has,Our method,results,/content/training-data/sentiment_analysis/43/triples/results.txt
Our method,consistently performs,better,results,/content/training-data/sentiment_analysis/43/triples/results.txt
better,than,IAN,results,/content/training-data/sentiment_analysis/43/triples/results.txt
Results,has,IAN,results,/content/training-data/sentiment_analysis/43/triples/results.txt
IAN,achieves,slightly better results,results,/content/training-data/sentiment_analysis/43/triples/results.txt
slightly better results,with,previous LSTM - based methods,results,/content/training-data/sentiment_analysis/43/triples/results.txt
Results,has,ATAE - LSTM,results,/content/training-data/sentiment_analysis/43/triples/results.txt
ATAE - LSTM,is,better,results,/content/training-data/sentiment_analysis/43/triples/results.txt
better,than,LSTM,results,/content/training-data/sentiment_analysis/43/triples/results.txt
Results,has,Majority,results,/content/training-data/sentiment_analysis/43/triples/results.txt
Majority,performs,worst,results,/content/training-data/sentiment_analysis/43/triples/results.txt
Results,has,TD - LSTM,results,/content/training-data/sentiment_analysis/43/triples/results.txt
TD - LSTM,performs,slightly better,results,/content/training-data/sentiment_analysis/43/triples/results.txt
slightly better,than,ATAE - LSTM,results,/content/training-data/sentiment_analysis/43/triples/results.txt
TD - LSTM,performs,worse,results,/content/training-data/sentiment_analysis/43/triples/results.txt
worse,than,our method MGAN,results,/content/training-data/sentiment_analysis/43/triples/results.txt
Results,has,RAM,results,/content/training-data/sentiment_analysis/43/triples/results.txt
RAM,performs,better,results,/content/training-data/sentiment_analysis/43/triples/results.txt
better,than,other baselines,results,/content/training-data/sentiment_analysis/43/triples/results.txt
Results,has,Our proposed MGAN,results,/content/training-data/sentiment_analysis/43/triples/results.txt
Our proposed MGAN,consistently performs,better,results,/content/training-data/sentiment_analysis/43/triples/results.txt
better,than,MemNet,results,/content/training-data/sentiment_analysis/43/triples/results.txt
better,than,BILSTM - ATT - G,results,/content/training-data/sentiment_analysis/43/triples/results.txt
better,than,RAM,results,/content/training-data/sentiment_analysis/43/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/sentiment_analysis/16/triples/baselines.txt
Baselines,has,AMN,baselines,/content/training-data/sentiment_analysis/16/triples/baselines.txt
AMN,has,state - of - the - art memory network,baselines,/content/training-data/sentiment_analysis/16/triples/baselines.txt
state - of - the - art memory network,used for,ASC,baselines,/content/training-data/sentiment_analysis/16/triples/baselines.txt
Baselines,has,AE - LSTM,baselines,/content/training-data/sentiment_analysis/16/triples/baselines.txt
AE - LSTM,compare with,state - of - the - art attention - based LSTM,baselines,/content/training-data/sentiment_analysis/16/triples/baselines.txt
state - of - the - art attention - based LSTM,for,ASC,baselines,/content/training-data/sentiment_analysis/16/triples/baselines.txt
Baselines,has,ATAE - LSTM,baselines,/content/training-data/sentiment_analysis/16/triples/baselines.txt
ATAE - LSTM,has,attention - based LSTM,baselines,/content/training-data/sentiment_analysis/16/triples/baselines.txt
attention - based LSTM,for,ASC,baselines,/content/training-data/sentiment_analysis/16/triples/baselines.txt
Baselines,has,BL - MN,baselines,/content/training-data/sentiment_analysis/16/triples/baselines.txt
BL - MN,has,Our basic memory network,baselines,/content/training-data/sentiment_analysis/16/triples/baselines.txt
Our basic memory network,does not use,proposed techniques,baselines,/content/training-data/sentiment_analysis/16/triples/baselines.txt
proposed techniques,for capturing,target - sensitive sentiments,baselines,/content/training-data/sentiment_analysis/16/triples/baselines.txt
Baselines,has,Target - sensitive Memory Networks ( TMNs ,baselines,/content/training-data/sentiment_analysis/16/triples/baselines.txt
Target - sensitive Memory Networks ( TMNs ),has,six proposed techniques,baselines,/content/training-data/sentiment_analysis/16/triples/baselines.txt
six proposed techniques,give,six target - sensitive memory networks,baselines,/content/training-data/sentiment_analysis/16/triples/baselines.txt
six proposed techniques,name,NP,baselines,/content/training-data/sentiment_analysis/16/triples/baselines.txt
six proposed techniques,name,CNP,baselines,/content/training-data/sentiment_analysis/16/triples/baselines.txt
six proposed techniques,name,IT,baselines,/content/training-data/sentiment_analysis/16/triples/baselines.txt
six proposed techniques,name,CI,baselines,/content/training-data/sentiment_analysis/16/triples/baselines.txt
six proposed techniques,name,JCI,baselines,/content/training-data/sentiment_analysis/16/triples/baselines.txt
six proposed techniques,name,JPI,baselines,/content/training-data/sentiment_analysis/16/triples/baselines.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/16/triples/hyperparameters.txt
Hyperparameters,compare,memory networks,hyperparameters,/content/training-data/sentiment_analysis/16/triples/hyperparameters.txt
memory networks,in,"multiple computational layers version ( i.e. , multiple hops ",hyperparameters,/content/training-data/sentiment_analysis/16/triples/hyperparameters.txt
memory networks,has,number of hops,hyperparameters,/content/training-data/sentiment_analysis/16/triples/hyperparameters.txt
number of hops,set to,3,hyperparameters,/content/training-data/sentiment_analysis/16/triples/hyperparameters.txt
Hyperparameters,use,open - domain word embeddings,hyperparameters,/content/training-data/sentiment_analysis/16/triples/hyperparameters.txt
open - domain word embeddings,for,initialization,hyperparameters,/content/training-data/sentiment_analysis/16/triples/hyperparameters.txt
initialization,of,word vectors,hyperparameters,/content/training-data/sentiment_analysis/16/triples/hyperparameters.txt
Hyperparameters,implemented,all models,hyperparameters,/content/training-data/sentiment_analysis/16/triples/hyperparameters.txt
all models,in,TensorFlow environment,hyperparameters,/content/training-data/sentiment_analysis/16/triples/hyperparameters.txt
TensorFlow environment,using,"same input , embedding size , dropout rate , optimizer , etc.",hyperparameters,/content/training-data/sentiment_analysis/16/triples/hyperparameters.txt
Hyperparameters,initialize,other model parameters,hyperparameters,/content/training-data/sentiment_analysis/16/triples/hyperparameters.txt
other model parameters,from,"uniform distribution U ( - 0.05 , 0.05 ",hyperparameters,/content/training-data/sentiment_analysis/16/triples/hyperparameters.txt
Hyperparameters,has,learning rate,hyperparameters,/content/training-data/sentiment_analysis/16/triples/hyperparameters.txt
learning rate,set to,0.01,hyperparameters,/content/training-data/sentiment_analysis/16/triples/hyperparameters.txt
Hyperparameters,has,dropout rate,hyperparameters,/content/training-data/sentiment_analysis/16/triples/hyperparameters.txt
dropout rate,set to,0.1,hyperparameters,/content/training-data/sentiment_analysis/16/triples/hyperparameters.txt
Hyperparameters,has,Stochastic gradient descent,hyperparameters,/content/training-data/sentiment_analysis/16/triples/hyperparameters.txt
Stochastic gradient descent,used as,our optimizer,hyperparameters,/content/training-data/sentiment_analysis/16/triples/hyperparameters.txt
Hyperparameters,has,dimension,hyperparameters,/content/training-data/sentiment_analysis/16/triples/hyperparameters.txt
dimension,are,300,hyperparameters,/content/training-data/sentiment_analysis/16/triples/hyperparameters.txt
300,of,word embedding,hyperparameters,/content/training-data/sentiment_analysis/16/triples/hyperparameters.txt
300,of,size of the hidden layers,hyperparameters,/content/training-data/sentiment_analysis/16/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/16/triples/model.txt
Model,propose,target - sensitive memory networks ( TMNs ,model,/content/training-data/sentiment_analysis/16/triples/model.txt
target - sensitive memory networks ( TMNs ),can capture,sentiment interaction,model,/content/training-data/sentiment_analysis/16/triples/model.txt
sentiment interaction,between,targets and contexts,model,/content/training-data/sentiment_analysis/16/triples/model.txt
Contribution,has research problem,Aspect Sentiment Classification,research-problem,/content/training-data/sentiment_analysis/16/triples/research-problem.txt
Contribution,has research problem,Aspect sentiment classification ( ASC ,research-problem,/content/training-data/sentiment_analysis/16/triples/research-problem.txt
Contribution,has research problem,sentiment analysis,research-problem,/content/training-data/sentiment_analysis/16/triples/research-problem.txt
Contribution,has research problem,ASC,research-problem,/content/training-data/sentiment_analysis/16/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/16/triples/results.txt
Results,In,3 - hop setting,results,/content/training-data/sentiment_analysis/16/triples/results.txt
3 - hop setting,has,"JCI , IT , and CI",results,/content/training-data/sentiment_analysis/16/triples/results.txt
"JCI , IT , and CI",achieve,best scores,results,/content/training-data/sentiment_analysis/16/triples/results.txt
"JCI , IT , and CI",outperforming,strongest baseline AMN,results,/content/training-data/sentiment_analysis/16/triples/results.txt
strongest baseline AMN,by,"2.38 % , 2.18 % , and 2.03 %",results,/content/training-data/sentiment_analysis/16/triples/results.txt
3 - hop setting,has,TMNs,results,/content/training-data/sentiment_analysis/16/triples/results.txt
TMNs,achieve,much better results,results,/content/training-data/sentiment_analysis/16/triples/results.txt
much better results,on,Restaurant,results,/content/training-data/sentiment_analysis/16/triples/results.txt
Results,has,CI and JPI,results,/content/training-data/sentiment_analysis/16/triples/results.txt
CI and JPI,perform,well,results,/content/training-data/sentiment_analysis/16/triples/results.txt
well,in,most cases,results,/content/training-data/sentiment_analysis/16/triples/results.txt
Results,has,"IT , NP , and CNP",results,/content/training-data/sentiment_analysis/16/triples/results.txt
"IT , NP , and CNP",achieve,very good scores,results,/content/training-data/sentiment_analysis/16/triples/results.txt
very good scores,in,some cases,results,/content/training-data/sentiment_analysis/16/triples/results.txt
very good scores,are,less stable,results,/content/training-data/sentiment_analysis/16/triples/results.txt
Results,Comparing,all TMNs,results,/content/training-data/sentiment_analysis/16/triples/results.txt
all TMNs,see that,JCI,results,/content/training-data/sentiment_analysis/16/triples/results.txt
JCI,works,best,results,/content/training-data/sentiment_analysis/16/triples/results.txt
Results,Comparing,1 - hop memory networks,results,/content/training-data/sentiment_analysis/16/triples/results.txt
1 - hop memory networks,see,significant performance gains,results,/content/training-data/sentiment_analysis/16/triples/results.txt
significant performance gains,achieved by,"CNP , CI , JCI , and JPI",results,/content/training-data/sentiment_analysis/16/triples/results.txt
significant performance gains,on,both datasets,results,/content/training-data/sentiment_analysis/16/triples/results.txt
both datasets,each of them has,p < 0.01,results,/content/training-data/sentiment_analysis/16/triples/results.txt
p < 0.01,over,strongest baseline ( BL - MN ,results,/content/training-data/sentiment_analysis/16/triples/results.txt
strongest baseline ( BL - MN ),from,paired t- test,results,/content/training-data/sentiment_analysis/16/triples/results.txt
p < 0.01,using,F1 - Macro,results,/content/training-data/sentiment_analysis/16/triples/results.txt
Results,On,Laptop,results,/content/training-data/sentiment_analysis/16/triples/results.txt
Laptop,has,BL - MN and most TMNs,results,/content/training-data/sentiment_analysis/16/triples/results.txt
BL - MN and most TMNs,perform,similarly,results,/content/training-data/sentiment_analysis/16/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
Baselines,has,BERT BASE,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
BERT BASE,treat,each utterance,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
each utterance,with,context,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
context,as,single document,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
BERT BASE,limit,document length,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
document length,to,last 100 tokens,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
last 100 tokens,to allow,larger batch size,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
Baselines,has,KET StdAttn,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
KET StdAttn,replace,dynamic contextaware affective graph attention,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
dynamic contextaware affective graph attention,by,standard graph attention,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
Baselines,has,DialogueRNN,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
DialogueRNN,has,stateof - the - art model,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
stateof - the - art model,for,emotion detection,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
emotion detection,in,textual conversations,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
DialogueRNN,models both,context and speakers information,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
Baselines,has,CNN+cLSTM,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
CNN+cLSTM,has,CNN,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
CNN,to extract,utterance features,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
CNN+cLSTM,has,c LSTM,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
c LSTM,applied to learn,context representations,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
Baselines,has,c LSTM,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
c LSTM,has,utterance - level bidirectional LSTM,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
utterance - level bidirectional LSTM,to encode,each utterance,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
c LSTM,has,context - level unidirectional LSTM,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
context - level unidirectional LSTM,to encode,context,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
Baselines,has,KET SingleSelfAttn,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
KET SingleSelfAttn,replace,hierarchical self - attention,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
hierarchical self - attention,by,single self - attention layer,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
single self - attention layer,to learn,context representations,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
KET SingleSelfAttn,has,Contextual utterances,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
Contextual utterances,are,concatenated together,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
concatenated together,prior to,single self - attention layer,baselines,/content/training-data/sentiment_analysis/3/triples/baselines.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/sentiment_analysis/3/triples/ablation-analysis.txt
Ablation analysis,has,both context and knowledge,ablation-analysis,/content/training-data/sentiment_analysis/3/triples/ablation-analysis.txt
both context and knowledge,essential to,strong performance,ablation-analysis,/content/training-data/sentiment_analysis/3/triples/ablation-analysis.txt
strong performance,of,KET,ablation-analysis,/content/training-data/sentiment_analysis/3/triples/ablation-analysis.txt
KET,on,all datasets,ablation-analysis,/content/training-data/sentiment_analysis/3/triples/ablation-analysis.txt
Ablation analysis,removing,context,ablation-analysis,/content/training-data/sentiment_analysis/3/triples/ablation-analysis.txt
context,has,greater impact,ablation-analysis,/content/training-data/sentiment_analysis/3/triples/ablation-analysis.txt
greater impact,on,long conversations,ablation-analysis,/content/training-data/sentiment_analysis/3/triples/ablation-analysis.txt
long conversations,than,short conversations,ablation-analysis,/content/training-data/sentiment_analysis/3/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/3/triples/model.txt
Model,separates,context and response,model,/content/training-data/sentiment_analysis/3/triples/model.txt
context and response,into,encoder and decoder,model,/content/training-data/sentiment_analysis/3/triples/model.txt
Model,has,referring process,model,/content/training-data/sentiment_analysis/3/triples/model.txt
referring process,balances between,relatedness and affectiveness,model,/content/training-data/sentiment_analysis/3/triples/model.txt
relatedness and affectiveness,of,retrieved knowledge entities,model,/content/training-data/sentiment_analysis/3/triples/model.txt
retrieved knowledge entities,using,context - aware affective graph attention mechanism,model,/content/training-data/sentiment_analysis/3/triples/model.txt
referring process,is,dynamic,model,/content/training-data/sentiment_analysis/3/triples/model.txt
Model,exploit,commonsense knowledge,model,/content/training-data/sentiment_analysis/3/triples/model.txt
commonsense knowledge,leverage,external knowledge bases,model,/content/training-data/sentiment_analysis/3/triples/model.txt
external knowledge bases,to facilitate,understanding of each word,model,/content/training-data/sentiment_analysis/3/triples/model.txt
understanding of each word,in,utterances,model,/content/training-data/sentiment_analysis/3/triples/model.txt
understanding of each word,by referring to,related knowledge entities,model,/content/training-data/sentiment_analysis/3/triples/model.txt
Model,propose,Knowledge - Enriched Transformer ( KET ,model,/content/training-data/sentiment_analysis/3/triples/model.txt
Knowledge - Enriched Transformer ( KET ),incorporate,contextual information and external knowledge bases,model,/content/training-data/sentiment_analysis/3/triples/model.txt
Model,propose,hierarchical self - attention mechanism,model,/content/training-data/sentiment_analysis/3/triples/model.txt
hierarchical self - attention mechanism,allowing,KET,model,/content/training-data/sentiment_analysis/3/triples/model.txt
KET,to model,hierarchical structure of conversations,model,/content/training-data/sentiment_analysis/3/triples/model.txt
Contribution,has research problem,Emotion Detection in Textual Conversations,research-problem,/content/training-data/sentiment_analysis/3/triples/research-problem.txt
Contribution,has research problem,detecting emotions in textual conversations,research-problem,/content/training-data/sentiment_analysis/3/triples/research-problem.txt
Contribution,has research problem,"detecting emotions ( e.g. , happy , sad , angry , etc. ) in textual conversations",research-problem,/content/training-data/sentiment_analysis/3/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/sentiment_analysis/3/triples/experimental-setup.txt
Experimental setup,preprocessed,all datasets,experimental-setup,/content/training-data/sentiment_analysis/3/triples/experimental-setup.txt
all datasets,by,lower - casing,experimental-setup,/content/training-data/sentiment_analysis/3/triples/experimental-setup.txt
all datasets,by,tokenization,experimental-setup,/content/training-data/sentiment_analysis/3/triples/experimental-setup.txt
tokenization,using,Spacy,experimental-setup,/content/training-data/sentiment_analysis/3/triples/experimental-setup.txt
Experimental setup,use,released code,experimental-setup,/content/training-data/sentiment_analysis/3/triples/experimental-setup.txt
released code,for,BERT BASE and DialogueRNN,experimental-setup,/content/training-data/sentiment_analysis/3/triples/experimental-setup.txt
Experimental setup,use,Glo Ve embedding,experimental-setup,/content/training-data/sentiment_analysis/3/triples/experimental-setup.txt
Glo Ve embedding,for,initialization,experimental-setup,/content/training-data/sentiment_analysis/3/triples/experimental-setup.txt
initialization,in,word and concept embedding layers,experimental-setup,/content/training-data/sentiment_analysis/3/triples/experimental-setup.txt
Experimental setup,For,each dataset,experimental-setup,/content/training-data/sentiment_analysis/3/triples/experimental-setup.txt
each dataset,has,all models,experimental-setup,/content/training-data/sentiment_analysis/3/triples/experimental-setup.txt
all models,are,fine - tuned,experimental-setup,/content/training-data/sentiment_analysis/3/triples/experimental-setup.txt
fine - tuned,based on,performance,experimental-setup,/content/training-data/sentiment_analysis/3/triples/experimental-setup.txt
performance,on,validation set,experimental-setup,/content/training-data/sentiment_analysis/3/triples/experimental-setup.txt
Experimental setup,For,our model,experimental-setup,/content/training-data/sentiment_analysis/3/triples/experimental-setup.txt
our model,in,all datasets,experimental-setup,/content/training-data/sentiment_analysis/3/triples/experimental-setup.txt
all datasets,use,"Adam optimization ( Kingma and Ba , 2014 ",experimental-setup,/content/training-data/sentiment_analysis/3/triples/experimental-setup.txt
"Adam optimization ( Kingma and Ba , 2014 )",with,learning rate,experimental-setup,/content/training-data/sentiment_analysis/3/triples/experimental-setup.txt
learning rate,of,0.0001,experimental-setup,/content/training-data/sentiment_analysis/3/triples/experimental-setup.txt
"Adam optimization ( Kingma and Ba , 2014 )",with,batch size,experimental-setup,/content/training-data/sentiment_analysis/3/triples/experimental-setup.txt
batch size,of,64,experimental-setup,/content/training-data/sentiment_analysis/3/triples/experimental-setup.txt
Experimental setup,For,class weights,experimental-setup,/content/training-data/sentiment_analysis/3/triples/experimental-setup.txt
class weights,set them as,ratio,experimental-setup,/content/training-data/sentiment_analysis/3/triples/experimental-setup.txt
ratio,of,class distribution,experimental-setup,/content/training-data/sentiment_analysis/3/triples/experimental-setup.txt
class distribution,in,validation set,experimental-setup,/content/training-data/sentiment_analysis/3/triples/experimental-setup.txt
class distribution,to,class distribution,experimental-setup,/content/training-data/sentiment_analysis/3/triples/experimental-setup.txt
class distribution,in,training set,experimental-setup,/content/training-data/sentiment_analysis/3/triples/experimental-setup.txt
class weights,in,cross - entropy loss,experimental-setup,/content/training-data/sentiment_analysis/3/triples/experimental-setup.txt
cross - entropy loss,for,each dataset,experimental-setup,/content/training-data/sentiment_analysis/3/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/3/triples/results.txt
Results,indicates that,our model,results,/content/training-data/sentiment_analysis/3/triples/results.txt
our model,is,robust,results,/content/training-data/sentiment_analysis/3/triples/results.txt
robust,across,datasets,results,/content/training-data/sentiment_analysis/3/triples/results.txt
datasets,with,"varying training sizes , context lengths and domains",results,/content/training-data/sentiment_analysis/3/triples/results.txt
Results,has,BERT BASE,results,/content/training-data/sentiment_analysis/3/triples/results.txt
BERT BASE,achieves,very competitive performance,results,/content/training-data/sentiment_analysis/3/triples/results.txt
very competitive performance,on,all datasets,results,/content/training-data/sentiment_analysis/3/triples/results.txt
all datasets,except,EC,results,/content/training-data/sentiment_analysis/3/triples/results.txt
EC,due to,strong representational power,results,/content/training-data/sentiment_analysis/3/triples/results.txt
strong representational power,via,bi-directional context modelling,results,/content/training-data/sentiment_analysis/3/triples/results.txt
bi-directional context modelling,using,Transformer,results,/content/training-data/sentiment_analysis/3/triples/results.txt
Results,has,DialogueRNN,results,/content/training-data/sentiment_analysis/3/triples/results.txt
DialogueRNN,performs better than,our model,results,/content/training-data/sentiment_analysis/3/triples/results.txt
our model,on,IEMOCAP,results,/content/training-data/sentiment_analysis/3/triples/results.txt
Results,has,c LSTM,results,/content/training-data/sentiment_analysis/3/triples/results.txt
c LSTM,performs,worst,results,/content/training-data/sentiment_analysis/3/triples/results.txt
worst,on,"long conversations ( i.e. , MELD , EmoryNLP and IEMOCAP ",results,/content/training-data/sentiment_analysis/3/triples/results.txt
c LSTM,performs,reasonably well,results,/content/training-data/sentiment_analysis/3/triples/results.txt
reasonably well,on,"short conversations ( i.e. , EC and DailyDialog ",results,/content/training-data/sentiment_analysis/3/triples/results.txt
Results,has,KET variants,results,/content/training-data/sentiment_analysis/3/triples/results.txt
KET variants,name,KET SingleSelfAttn and KET StdAttn,results,/content/training-data/sentiment_analysis/3/triples/results.txt
KET variants,perform,on a par,results,/content/training-data/sentiment_analysis/3/triples/results.txt
on a par,with,KET model,results,/content/training-data/sentiment_analysis/3/triples/results.txt
KET model,on,EC,results,/content/training-data/sentiment_analysis/3/triples/results.txt
KET variants,perform,comparably,results,/content/training-data/sentiment_analysis/3/triples/results.txt
comparably,with,best baselines,results,/content/training-data/sentiment_analysis/3/triples/results.txt
best baselines,on,all datasets,results,/content/training-data/sentiment_analysis/3/triples/results.txt
all datasets,except,IEMOCAP,results,/content/training-data/sentiment_analysis/3/triples/results.txt
KET variants,perform,noticeably worse,results,/content/training-data/sentiment_analysis/3/triples/results.txt
noticeably worse,than,KET,results,/content/training-data/sentiment_analysis/3/triples/results.txt
KET,on,all datasets,results,/content/training-data/sentiment_analysis/3/triples/results.txt
all datasets,except,EC,results,/content/training-data/sentiment_analysis/3/triples/results.txt
Results,when,utterance - level LSTM,results,/content/training-data/sentiment_analysis/3/triples/results.txt
utterance - level LSTM,in,c LSTM,results,/content/training-data/sentiment_analysis/3/triples/results.txt
c LSTM,replaced by,features,results,/content/training-data/sentiment_analysis/3/triples/results.txt
features,extracted by,CNN,results,/content/training-data/sentiment_analysis/3/triples/results.txt
features,has,model,results,/content/training-data/sentiment_analysis/3/triples/results.txt
model,performs,significantly better,results,/content/training-data/sentiment_analysis/3/triples/results.txt
significantly better,than,c LSTM,results,/content/training-data/sentiment_analysis/3/triples/results.txt
c LSTM,on,long conversations,results,/content/training-data/sentiment_analysis/3/triples/results.txt
Contribution,has,Dataset,datase,/content/training-data/sentiment_analysis/39/triples/dataset.txt
Dataset,name,SentiHood,datase,/content/training-data/sentiment_analysis/39/triples/dataset.txt
SentiHood,contains,5215 sentences,datase,/content/training-data/sentiment_analysis/39/triples/dataset.txt
5215 sentences,with,3862 sentences,datase,/content/training-data/sentiment_analysis/39/triples/dataset.txt
3862 sentences,containing,single location,datase,/content/training-data/sentiment_analysis/39/triples/dataset.txt
5215 sentences,with,1353 sentences,datase,/content/training-data/sentiment_analysis/39/triples/dataset.txt
1353 sentences,containing,multiple ( two ) locations,datase,/content/training-data/sentiment_analysis/39/triples/dataset.txt
SentiHood,contains,annotated sentences,datase,/content/training-data/sentiment_analysis/39/triples/dataset.txt
annotated sentences,containing,one or two location entity mentions,datase,/content/training-data/sentiment_analysis/39/triples/dataset.txt
Dataset,has,aspect touristy,datase,/content/training-data/sentiment_analysis/39/triples/dataset.txt
aspect touristy,occurred in,less than 100 sentences,datase,/content/training-data/sentiment_analysis/39/triples/dataset.txt
Dataset,has,general aspect,datase,/content/training-data/sentiment_analysis/39/triples/dataset.txt
general aspect,is,most frequent aspect,datase,/content/training-data/sentiment_analysis/39/triples/dataset.txt
most frequent aspect,with,over 2000 sentences,datase,/content/training-data/sentiment_analysis/39/triples/dataset.txt
Dataset,has,""" Positive "" sentiment",datase,/content/training-data/sentiment_analysis/39/triples/dataset.txt
""" Positive "" sentiment",dominant for,aspects,datase,/content/training-data/sentiment_analysis/39/triples/dataset.txt
aspects,such as,dining and shopping,datase,/content/training-data/sentiment_analysis/39/triples/dataset.txt
Dataset,has,Location entity names,datase,/content/training-data/sentiment_analysis/39/triples/dataset.txt
Location entity names,masked by,location1 and location 2,datase,/content/training-data/sentiment_analysis/39/triples/dataset.txt
location1 and location 2,in,whole dataset,datase,/content/training-data/sentiment_analysis/39/triples/dataset.txt
Dataset,has,total number of opinions ( 5920 ,datase,/content/training-data/sentiment_analysis/39/triples/dataset.txt
total number of opinions ( 5920 ),higher than,number of sentences,datase,/content/training-data/sentiment_analysis/39/triples/dataset.txt
number of sentences,since,each sentence,datase,/content/training-data/sentiment_analysis/39/triples/dataset.txt
each sentence,can contain,one or more opinions,datase,/content/training-data/sentiment_analysis/39/triples/dataset.txt
total number of opinions ( 5920 ),in,dataset,datase,/content/training-data/sentiment_analysis/39/triples/dataset.txt
Contribution,has,Baselines,baselines,/content/training-data/sentiment_analysis/39/triples/baselines.txt
Baselines,has,Logistic Regression,baselines,/content/training-data/sentiment_analysis/39/triples/baselines.txt
Logistic Regression,based on,linguistic features,baselines,/content/training-data/sentiment_analysis/39/triples/baselines.txt
linguistic features,such as,n-grams,baselines,/content/training-data/sentiment_analysis/39/triples/baselines.txt
linguistic features,such as,POS information,baselines,/content/training-data/sentiment_analysis/39/triples/baselines.txt
linguistic features,such as,more hand - engineered features,baselines,/content/training-data/sentiment_analysis/39/triples/baselines.txt
Logistic Regression,define,sparse representations of locations,baselines,/content/training-data/sentiment_analysis/39/triples/baselines.txt
sparse representations of locations,name,Mask target entity n-grams,baselines,/content/training-data/sentiment_analysis/39/triples/baselines.txt
sparse representations of locations,name,Left - right n- grams,baselines,/content/training-data/sentiment_analysis/39/triples/baselines.txt
sparse representations of locations,name,Left right pooling,baselines,/content/training-data/sentiment_analysis/39/triples/baselines.txt
Baselines,has,Long Short - Term Memory ( LSTM ,baselines,/content/training-data/sentiment_analysis/39/triples/baselines.txt
Long Short - Term Memory ( LSTM ),use,bidirectional LSTM,baselines,/content/training-data/sentiment_analysis/39/triples/baselines.txt
bidirectional LSTM,to learn,classifier,baselines,/content/training-data/sentiment_analysis/39/triples/baselines.txt
classifier,for,each of the aspects,baselines,/content/training-data/sentiment_analysis/39/triples/baselines.txt
Long Short - Term Memory ( LSTM ),has,Representations,baselines,/content/training-data/sentiment_analysis/39/triples/baselines.txt
Representations,for,location ( e l ,baselines,/content/training-data/sentiment_analysis/39/triples/baselines.txt
location ( e l ),using,Final output state ( LSTM - Final ,baselines,/content/training-data/sentiment_analysis/39/triples/baselines.txt
location ( e l ),using,Location output state ( LSTM - Location ,baselines,/content/training-data/sentiment_analysis/39/triples/baselines.txt
Contribution,has research problem,Targeted Aspect Based Sentiment Analysis,research-problem,/content/training-data/sentiment_analysis/39/triples/research-problem.txt
Contribution,has research problem,targeted aspect - based sentiment analysis,research-problem,/content/training-data/sentiment_analysis/39/triples/research-problem.txt
Contribution,has research problem,Sentiment analysis,research-problem,/content/training-data/sentiment_analysis/39/triples/research-problem.txt
Contribution,has research problem,Aspect - based sentiment analysis ( ABSA ,research-problem,/content/training-data/sentiment_analysis/39/triples/research-problem.txt
Contribution,has research problem,Targeted sentiment analysis,research-problem,/content/training-data/sentiment_analysis/39/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/39/triples/results.txt
Results,see,n-gram representation,results,/content/training-data/sentiment_analysis/39/triples/results.txt
n-gram representation,with,location masking,results,/content/training-data/sentiment_analysis/39/triples/results.txt
n-gram representation,achieves,slightly better results,results,/content/training-data/sentiment_analysis/39/triples/results.txt
slightly better results,over,left - right context,results,/content/training-data/sentiment_analysis/39/triples/results.txt
Results,by adding,POS information,results,/content/training-data/sentiment_analysis/39/triples/results.txt
POS information,gain,increase,results,/content/training-data/sentiment_analysis/39/triples/results.txt
increase,in,performance,results,/content/training-data/sentiment_analysis/39/triples/results.txt
Results,Separating,left and the right context ( LR - Left - Right ,results,/content/training-data/sentiment_analysis/39/triples/results.txt
left and the right context ( LR - Left - Right ),does not improve,performance,results,/content/training-data/sentiment_analysis/39/triples/results.txt
left and the right context ( LR - Left - Right ),for,BoW representation,results,/content/training-data/sentiment_analysis/39/triples/results.txt
Results,Amongst,two variations of LSTM,results,/content/training-data/sentiment_analysis/39/triples/results.txt
two variations of LSTM,has,model,results,/content/training-data/sentiment_analysis/39/triples/results.txt
model,with,final state embeddings,results,/content/training-data/sentiment_analysis/39/triples/results.txt
final state embeddings,does,slightly better,results,/content/training-data/sentiment_analysis/39/triples/results.txt
slightly better,than,model,results,/content/training-data/sentiment_analysis/39/triples/results.txt
model,use,embeddings at the location index,results,/content/training-data/sentiment_analysis/39/triples/results.txt
Results,for,logistic regression model,results,/content/training-data/sentiment_analysis/39/triples/results.txt
logistic regression model,with,n-grams and POS information,results,/content/training-data/sentiment_analysis/39/triples/results.txt
logistic regression model,interesting observation is that,performance,results,/content/training-data/sentiment_analysis/39/triples/results.txt
performance,is,superior,results,/content/training-data/sentiment_analysis/39/triples/results.txt
superior,to,other models,results,/content/training-data/sentiment_analysis/39/triples/results.txt
other models,in terms of,AUC,results,/content/training-data/sentiment_analysis/39/triples/results.txt
logistic regression model,interesting observation is that,F 1 measure,results,/content/training-data/sentiment_analysis/39/triples/results.txt
F 1 measure,is,very low,results,/content/training-data/sentiment_analysis/39/triples/results.txt
Results,interesting to note,best LSTM model,results,/content/training-data/sentiment_analysis/39/triples/results.txt
best LSTM model,not superior to,logistic regression model,results,/content/training-data/sentiment_analysis/39/triples/results.txt
best LSTM model,in terms of,AUC,results,/content/training-data/sentiment_analysis/39/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/sentiment_analysis/11/triples/baselines.txt
Baselines,has,Memn2n,baselines,/content/training-data/sentiment_analysis/11/triples/baselines.txt
Memn2n,generates,memory representations,baselines,/content/training-data/sentiment_analysis/11/triples/baselines.txt
memory representations,for,each historical utterance,baselines,/content/training-data/sentiment_analysis/11/triples/baselines.txt
each historical utterance,using,embedding matrix B,baselines,/content/training-data/sentiment_analysis/11/triples/baselines.txt
embedding matrix B,without,sequential modeling,baselines,/content/training-data/sentiment_analysis/11/triples/baselines.txt
Baselines,has,CMN N A,baselines,/content/training-data/sentiment_analysis/11/triples/baselines.txt
CMN N A,has,Single layer variant,baselines,/content/training-data/sentiment_analysis/11/triples/baselines.txt
Single layer variant,of,CMN,baselines,/content/training-data/sentiment_analysis/11/triples/baselines.txt
CMN,with,no attention module,baselines,/content/training-data/sentiment_analysis/11/triples/baselines.txt
Baselines,has,CMN Self,baselines,/content/training-data/sentiment_analysis/11/triples/baselines.txt
CMN Self,use,only self history,baselines,/content/training-data/sentiment_analysis/11/triples/baselines.txt
only self history,for classifying,emotion,baselines,/content/training-data/sentiment_analysis/11/triples/baselines.txt
emotion,of,utterance u i,baselines,/content/training-data/sentiment_analysis/11/triples/baselines.txt
Baselines,has,bc - LSTM,baselines,/content/training-data/sentiment_analysis/11/triples/baselines.txt
bc - LSTM,has,bi-directional LSTM,baselines,/content/training-data/sentiment_analysis/11/triples/baselines.txt
bi-directional LSTM,equipped with,hierarchical fusion,baselines,/content/training-data/sentiment_analysis/11/triples/baselines.txt
Baselines,has,SVM - ensemble,baselines,/content/training-data/sentiment_analysis/11/triples/baselines.txt
SVM - ensemble,has,strong context - free benchmark model,baselines,/content/training-data/sentiment_analysis/11/triples/baselines.txt
strong context - free benchmark model,which uses,similar multimodal approach,baselines,/content/training-data/sentiment_analysis/11/triples/baselines.txt
similar multimodal approach,on,ensemble of trees,baselines,/content/training-data/sentiment_analysis/11/triples/baselines.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/11/triples/hyperparameters.txt
Hyperparameters,use,10 %,hyperparameters,/content/training-data/sentiment_analysis/11/triples/hyperparameters.txt
10 %,of,training set,hyperparameters,/content/training-data/sentiment_analysis/11/triples/hyperparameters.txt
training set,as,held - out validation set,hyperparameters,/content/training-data/sentiment_analysis/11/triples/hyperparameters.txt
held - out validation set,for,hyperparameter tuning,hyperparameters,/content/training-data/sentiment_analysis/11/triples/hyperparameters.txt
Hyperparameters,use,Stochastic Gradient Descent ( SGD ) optimizer,hyperparameters,/content/training-data/sentiment_analysis/11/triples/hyperparameters.txt
Stochastic Gradient Descent ( SGD ) optimizer,starting with,initial learning Utterances,hyperparameters,/content/training-data/sentiment_analysis/11/triples/hyperparameters.txt
initial learning Utterances,whose history has,atleast 3 similar emotion labels,hyperparameters,/content/training-data/sentiment_analysis/11/triples/hyperparameters.txt
Hyperparameters,decided using,Random Search,hyperparameters,/content/training-data/sentiment_analysis/11/triples/hyperparameters.txt
Hyperparameters,Based on,validation performance,hyperparameters,/content/training-data/sentiment_analysis/11/triples/hyperparameters.txt
validation performance,has,number of hops R,hyperparameters,/content/training-data/sentiment_analysis/11/triples/hyperparameters.txt
number of hops R,fixed at,3 hops,hyperparameters,/content/training-data/sentiment_analysis/11/triples/hyperparameters.txt
validation performance,has,context window length K,hyperparameters,/content/training-data/sentiment_analysis/11/triples/hyperparameters.txt
context window length K,set to,40,hyperparameters,/content/training-data/sentiment_analysis/11/triples/hyperparameters.txt
Hyperparameters,has,Gradient clipping,hyperparameters,/content/training-data/sentiment_analysis/11/triples/hyperparameters.txt
Gradient clipping,used for,regularization,hyperparameters,/content/training-data/sentiment_analysis/11/triples/hyperparameters.txt
regularization,with,norm,hyperparameters,/content/training-data/sentiment_analysis/11/triples/hyperparameters.txt
norm,set to,40,hyperparameters,/content/training-data/sentiment_analysis/11/triples/hyperparameters.txt
Hyperparameters,has,annealing approach,hyperparameters,/content/training-data/sentiment_analysis/11/triples/hyperparameters.txt
annealing approach,halves,lr,hyperparameters,/content/training-data/sentiment_analysis/11/triples/hyperparameters.txt
lr,every,20 epochs,hyperparameters,/content/training-data/sentiment_analysis/11/triples/hyperparameters.txt
annealing approach,has,termination,hyperparameters,/content/training-data/sentiment_analysis/11/triples/hyperparameters.txt
termination,decided using,early - stop measure,hyperparameters,/content/training-data/sentiment_analysis/11/triples/hyperparameters.txt
early - stop measure,with,patience,hyperparameters,/content/training-data/sentiment_analysis/11/triples/hyperparameters.txt
patience,of,12,hyperparameters,/content/training-data/sentiment_analysis/11/triples/hyperparameters.txt
12,by monitoring,validation loss,hyperparameters,/content/training-data/sentiment_analysis/11/triples/hyperparameters.txt
Hyperparameters,has,dimension size,hyperparameters,/content/training-data/sentiment_analysis/11/triples/hyperparameters.txt
dimension size,of,memory cells d,hyperparameters,/content/training-data/sentiment_analysis/11/triples/hyperparameters.txt
memory cells d,set as,50,hyperparameters,/content/training-data/sentiment_analysis/11/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/11/triples/model.txt
Model,has,CMN,model,/content/training-data/sentiment_analysis/11/triples/model.txt
CMN,models,interplay,model,/content/training-data/sentiment_analysis/11/triples/model.txt
interplay,of,memories,model,/content/training-data/sentiment_analysis/11/triples/model.txt
memories,to capture,interspeaker dependencies,model,/content/training-data/sentiment_analysis/11/triples/model.txt
CMN,extracts,"multimodal features ( audio , visual , and text ",model,/content/training-data/sentiment_analysis/11/triples/model.txt
"multimodal features ( audio , visual , and text )",for,all utterances,model,/content/training-data/sentiment_analysis/11/triples/model.txt
all utterances,in,video,model,/content/training-data/sentiment_analysis/11/triples/model.txt
Model,has,memory cells,model,/content/training-data/sentiment_analysis/11/triples/model.txt
memory cells,of,CMN,model,/content/training-data/sentiment_analysis/11/triples/model.txt
CMN,are,continuous vectors,model,/content/training-data/sentiment_analysis/11/triples/model.txt
continuous vectors,store,context information,model,/content/training-data/sentiment_analysis/11/triples/model.txt
context information,found in,utterance histories,model,/content/training-data/sentiment_analysis/11/triples/model.txt
Model,has,proposed CMN,model,/content/training-data/sentiment_analysis/11/triples/model.txt
proposed CMN,improves,speakerbased emotion modeling,model,/content/training-data/sentiment_analysis/11/triples/model.txt
speakerbased emotion modeling,by using,memory networks,model,/content/training-data/sentiment_analysis/11/triples/model.txt
memory networks,efficient in capturing,long - term dependencies,model,/content/training-data/sentiment_analysis/11/triples/model.txt
memory networks,summarizing,task - specific details ,model,/content/training-data/sentiment_analysis/11/triples/model.txt
proposed CMN,by using,emotional context information,model,/content/training-data/sentiment_analysis/11/triples/model.txt
emotional context information,present in,conversation history,model,/content/training-data/sentiment_analysis/11/triples/model.txt
Model,propose,conversational memory network ( CMN ,model,/content/training-data/sentiment_analysis/11/triples/model.txt
conversational memory network ( CMN ),uses,multimodal approach,model,/content/training-data/sentiment_analysis/11/triples/model.txt
multimodal approach,for,emotion detection,model,/content/training-data/sentiment_analysis/11/triples/model.txt
emotion detection,in,utterances ( a unit of speech bound by breathes or pauses ,model,/content/training-data/sentiment_analysis/11/triples/model.txt
utterances ( a unit of speech bound by breathes or pauses ),of,conversational videos,model,/content/training-data/sentiment_analysis/11/triples/model.txt
Contribution,has research problem,Emotion Recognition in Dyadic Dialogue Videos,research-problem,/content/training-data/sentiment_analysis/11/triples/research-problem.txt
Contribution,has research problem,Emotion recognition in conversations,research-problem,/content/training-data/sentiment_analysis/11/triples/research-problem.txt
Contribution,has research problem,emotion detection in videos of dyadic conversations,research-problem,/content/training-data/sentiment_analysis/11/triples/research-problem.txt
Contribution,has research problem,Emotion detection,research-problem,/content/training-data/sentiment_analysis/11/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/11/triples/results.txt
Results,suggests,gathering contexts temporally,results,/content/training-data/sentiment_analysis/11/triples/results.txt
gathering contexts temporally,through,sequential processing,results,/content/training-data/sentiment_analysis/11/triples/results.txt
sequential processing,is,superior method,results,/content/training-data/sentiment_analysis/11/triples/results.txt
superior method,over,non-temporal memory representations,results,/content/training-data/sentiment_analysis/11/triples/results.txt
Results,has,CMN self,results,/content/training-data/sentiment_analysis/11/triples/results.txt
CMN self,provides,lesser performance,results,/content/training-data/sentiment_analysis/11/triples/results.txt
lesser performance,compared to,CMN,results,/content/training-data/sentiment_analysis/11/triples/results.txt
CMN self,uses only,single history channel,results,/content/training-data/sentiment_analysis/11/triples/results.txt
Results,has,predictions,results,/content/training-data/sentiment_analysis/11/triples/results.txt
predictions,on,valence and arousal levels,results,/content/training-data/sentiment_analysis/11/triples/results.txt
valence and arousal levels,show,similar results,results,/content/training-data/sentiment_analysis/11/triples/results.txt
Contribution,Code,https://github.com/tanthongtan/dv-cosine,code,/content/training-data/sentiment_analysis/21/triples/code.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/21/triples/hyperparameters.txt
Hyperparameters,using,L2 regularized dot product,hyperparameters,/content/training-data/sentiment_analysis/21/triples/hyperparameters.txt
L2 regularized dot product,chosen from,"[ 1 , 0.1 , 0.01 ]",hyperparameters,/content/training-data/sentiment_analysis/21/triples/hyperparameters.txt
Hyperparameters,For,distribution,hyperparameters,/content/training-data/sentiment_analysis/21/triples/hyperparameters.txt
distribution,for,sampling negative words,hyperparameters,/content/training-data/sentiment_analysis/21/triples/hyperparameters.txt
sampling negative words,used,n-gram distribution,hyperparameters,/content/training-data/sentiment_analysis/21/triples/hyperparameters.txt
n-gram distribution,raised to,3 / 4 th power,hyperparameters,/content/training-data/sentiment_analysis/21/triples/hyperparameters.txt
Hyperparameters,has,optimal learning rate,hyperparameters,/content/training-data/sentiment_analysis/21/triples/hyperparameters.txt
optimal learning rate,in the case of,cosine similarity,hyperparameters,/content/training-data/sentiment_analysis/21/triples/hyperparameters.txt
cosine similarity,is,extremely small,hyperparameters,/content/training-data/sentiment_analysis/21/triples/hyperparameters.txt
extremely small,suggesting,chaotic error surface,hyperparameters,/content/training-data/sentiment_analysis/21/triples/hyperparameters.txt
Hyperparameters,has,learning rate,hyperparameters,/content/training-data/sentiment_analysis/21/triples/hyperparameters.txt
learning rate,from,"[ 0.25 , 0.025 , 0.0025 , 0.001 ]",hyperparameters,/content/training-data/sentiment_analysis/21/triples/hyperparameters.txt
Hyperparameters,has,Grid search,hyperparameters,/content/training-data/sentiment_analysis/21/triples/hyperparameters.txt
Grid search,performed using,20 % of the training data,hyperparameters,/content/training-data/sentiment_analysis/21/triples/hyperparameters.txt
20 % of the training data,as,validation set,hyperparameters,/content/training-data/sentiment_analysis/21/triples/hyperparameters.txt
validation set,to determine,optimal hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/21/triples/hyperparameters.txt
validation set,to use,constant learning rate or learning rate annealing,hyperparameters,/content/training-data/sentiment_analysis/21/triples/hyperparameters.txt
Hyperparameters,has,weights,hyperparameters,/content/training-data/sentiment_analysis/21/triples/hyperparameters.txt
weights,of,networks,hyperparameters,/content/training-data/sentiment_analysis/21/triples/hyperparameters.txt
networks,were,initialized,hyperparameters,/content/training-data/sentiment_analysis/21/triples/hyperparameters.txt
initialized,from,uniform distribution,hyperparameters,/content/training-data/sentiment_analysis/21/triples/hyperparameters.txt
uniform distribution,in,range,hyperparameters,/content/training-data/sentiment_analysis/21/triples/hyperparameters.txt
range,of,"[ - 0.001 , 0.001 ]",hyperparameters,/content/training-data/sentiment_analysis/21/triples/hyperparameters.txt
Hyperparameters,tune,number of iterations,hyperparameters,/content/training-data/sentiment_analysis/21/triples/hyperparameters.txt
Hyperparameters,requires,larger number of epochs,hyperparameters,/content/training-data/sentiment_analysis/21/triples/hyperparameters.txt
larger number of epochs,for,convergence,hyperparameters,/content/training-data/sentiment_analysis/21/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/21/triples/model.txt
Model,trying to,maximize,model,/content/training-data/sentiment_analysis/21/triples/model.txt
maximize,has,cosine similarity,model,/content/training-data/sentiment_analysis/21/triples/model.txt
Model,trying to,predict,model,/content/training-data/sentiment_analysis/21/triples/model.txt
predict,given,document,model,/content/training-data/sentiment_analysis/21/triples/model.txt
predict,has,words / n - grams,model,/content/training-data/sentiment_analysis/21/triples/model.txt
words / n - grams,in,document,model,/content/training-data/sentiment_analysis/21/triples/model.txt
Model,aims to improve,document embedding models,model,/content/training-data/sentiment_analysis/21/triples/model.txt
document embedding models,by training,document embeddings,model,/content/training-data/sentiment_analysis/21/triples/model.txt
document embeddings,using,cosine similarity,model,/content/training-data/sentiment_analysis/21/triples/model.txt
Contribution,has research problem,Sentiment Classification,research-problem,/content/training-data/sentiment_analysis/21/triples/research-problem.txt
Contribution,has research problem,document - level sentiment classification,research-problem,/content/training-data/sentiment_analysis/21/triples/research-problem.txt
Contribution,has research problem,sentiment classification,research-problem,/content/training-data/sentiment_analysis/21/triples/research-problem.txt
Contribution,has research problem,binary sentiment classification of long movie reviews,research-problem,/content/training-data/sentiment_analysis/21/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/21/triples/results.txt
Results,see that using,cosine similarity,results,/content/training-data/sentiment_analysis/21/triples/results.txt
cosine similarity,instead of,dot product,results,/content/training-data/sentiment_analysis/21/triples/results.txt
dot product,improves,accuracy,results,/content/training-data/sentiment_analysis/21/triples/results.txt
Results,suggest that,switching,results,/content/training-data/sentiment_analysis/21/triples/results.txt
switching,improves,accuracy,results,/content/training-data/sentiment_analysis/21/triples/results.txt
switching,from,dot product,results,/content/training-data/sentiment_analysis/21/triples/results.txt
dot product,to,cosine similarity,results,/content/training-data/sentiment_analysis/21/triples/results.txt
Results,Introducing,L2 regularization,results,/content/training-data/sentiment_analysis/21/triples/results.txt
L2 regularization,improves,accuracy,results,/content/training-data/sentiment_analysis/21/triples/results.txt
accuracy,for,all cases,results,/content/training-data/sentiment_analysis/21/triples/results.txt
all cases,except,depreciation,results,/content/training-data/sentiment_analysis/21/triples/results.txt
depreciation,in the case of using,unigrams,results,/content/training-data/sentiment_analysis/21/triples/results.txt
L2 regularization,to,dot product,results,/content/training-data/sentiment_analysis/21/triples/results.txt
Results,during,grid search,results,/content/training-data/sentiment_analysis/21/triples/results.txt
grid search,has,initial learning rate,results,/content/training-data/sentiment_analysis/21/triples/results.txt
initial learning rate,was,0.25,results,/content/training-data/sentiment_analysis/21/triples/results.txt
Contribution,has,Approach,approach,/content/training-data/sentiment_analysis/45/triples/approach.txt
Approach,transform,( T ) ABSA,approach,/content/training-data/sentiment_analysis/45/triples/approach.txt
 T ) ABSA,into,sentence - pair classification task,approach,/content/training-data/sentiment_analysis/45/triples/approach.txt
Approach,fine - tune,pre-trained model,approach,/content/training-data/sentiment_analysis/45/triples/approach.txt
pre-trained model,from,BERT,approach,/content/training-data/sentiment_analysis/45/triples/approach.txt
Approach,investigate,several methods,approach,/content/training-data/sentiment_analysis/45/triples/approach.txt
several methods,of,constructing,approach,/content/training-data/sentiment_analysis/45/triples/approach.txt
constructing,an,auxiliary sentence,approach,/content/training-data/sentiment_analysis/45/triples/approach.txt
Contribution,has,Baselines,baselines,/content/training-data/sentiment_analysis/45/triples/baselines.txt
Baselines,has,Dmu - Entnet,baselines,/content/training-data/sentiment_analysis/45/triples/baselines.txt
Dmu - Entnet,has,bidirectional EntNet,baselines,/content/training-data/sentiment_analysis/45/triples/baselines.txt
bidirectional EntNet,with,"external "" memory chains """,baselines,/content/training-data/sentiment_analysis/45/triples/baselines.txt
"external "" memory chains """,with,delayed memory update mechanism,baselines,/content/training-data/sentiment_analysis/45/triples/baselines.txt
delayed memory update mechanism,to track,entities,baselines,/content/training-data/sentiment_analysis/45/triples/baselines.txt
Baselines,has,LSTM - Final,baselines,/content/training-data/sentiment_analysis/45/triples/baselines.txt
LSTM - Final,has,biLSTM model,baselines,/content/training-data/sentiment_analysis/45/triples/baselines.txt
biLSTM model,with,final state as a representation,baselines,/content/training-data/sentiment_analysis/45/triples/baselines.txt
Baselines,has,SenticLSTM,baselines,/content/training-data/sentiment_analysis/45/triples/baselines.txt
SenticLSTM,has,upgraded version,baselines,/content/training-data/sentiment_analysis/45/triples/baselines.txt
upgraded version,of,LSTM + TA + SA model,baselines,/content/training-data/sentiment_analysis/45/triples/baselines.txt
LSTM + TA + SA model,introduces,external information from Sentic - Net,baselines,/content/training-data/sentiment_analysis/45/triples/baselines.txt
Baselines,has,LR,baselines,/content/training-data/sentiment_analysis/45/triples/baselines.txt
LR,has,logistic regression classifier,baselines,/content/training-data/sentiment_analysis/45/triples/baselines.txt
logistic regression classifier,with,n-gram and pos-tag features,baselines,/content/training-data/sentiment_analysis/45/triples/baselines.txt
Baselines,has,LSTM + TA + SA,baselines,/content/training-data/sentiment_analysis/45/triples/baselines.txt
LSTM + TA + SA,has,biLSTM model,baselines,/content/training-data/sentiment_analysis/45/triples/baselines.txt
biLSTM model,introduces,complex target - level and sentence - level attention mechanisms,baselines,/content/training-data/sentiment_analysis/45/triples/baselines.txt
Baselines,has,LSTM - Loc,baselines,/content/training-data/sentiment_analysis/45/triples/baselines.txt
LSTM - Loc,has,biLSTM model,baselines,/content/training-data/sentiment_analysis/45/triples/baselines.txt
biLSTM model,with,state,baselines,/content/training-data/sentiment_analysis/45/triples/baselines.txt
state,associated with,target position as a representation,baselines,/content/training-data/sentiment_analysis/45/triples/baselines.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/45/triples/hyperparameters.txt
Hyperparameters,set,number of epochs,hyperparameters,/content/training-data/sentiment_analysis/45/triples/hyperparameters.txt
number of epochs,to,4,hyperparameters,/content/training-data/sentiment_analysis/45/triples/hyperparameters.txt
Hyperparameters,use,pre-trained uncased BERT - base model,hyperparameters,/content/training-data/sentiment_analysis/45/triples/hyperparameters.txt
pre-trained uncased BERT - base model,for,fine - tuning,hyperparameters,/content/training-data/sentiment_analysis/45/triples/hyperparameters.txt
Hyperparameters,has,dropout probability,hyperparameters,/content/training-data/sentiment_analysis/45/triples/hyperparameters.txt
dropout probability,at,0.1,hyperparameters,/content/training-data/sentiment_analysis/45/triples/hyperparameters.txt
Hyperparameters,has,initial learning rate,hyperparameters,/content/training-data/sentiment_analysis/45/triples/hyperparameters.txt
initial learning rate,is,2 e - 5,hyperparameters,/content/training-data/sentiment_analysis/45/triples/hyperparameters.txt
Hyperparameters,has,number of self - attention heads,hyperparameters,/content/training-data/sentiment_analysis/45/triples/hyperparameters.txt
number of self - attention heads,is,12,hyperparameters,/content/training-data/sentiment_analysis/45/triples/hyperparameters.txt
Hyperparameters,has,total number of parameters,hyperparameters,/content/training-data/sentiment_analysis/45/triples/hyperparameters.txt
total number of parameters,for,pretrained model,hyperparameters,/content/training-data/sentiment_analysis/45/triples/hyperparameters.txt
total number of parameters,is,110M,hyperparameters,/content/training-data/sentiment_analysis/45/triples/hyperparameters.txt
Hyperparameters,has,batch size,hyperparameters,/content/training-data/sentiment_analysis/45/triples/hyperparameters.txt
batch size,is,24,hyperparameters,/content/training-data/sentiment_analysis/45/triples/hyperparameters.txt
Hyperparameters,has,hidden layer size,hyperparameters,/content/training-data/sentiment_analysis/45/triples/hyperparameters.txt
hidden layer size,is,768,hyperparameters,/content/training-data/sentiment_analysis/45/triples/hyperparameters.txt
Hyperparameters,has,number of Transformer blocks,hyperparameters,/content/training-data/sentiment_analysis/45/triples/hyperparameters.txt
number of Transformer blocks,is,12,hyperparameters,/content/training-data/sentiment_analysis/45/triples/hyperparameters.txt
Contribution,has research problem,Aspect - Based Sentiment Analysis,research-problem,/content/training-data/sentiment_analysis/45/triples/research-problem.txt
Contribution,has research problem,Aspect - based sentiment analysis ( ABSA ,research-problem,/content/training-data/sentiment_analysis/45/triples/research-problem.txt
Contribution,has research problem,sentiment analysis ( SA ,research-problem,/content/training-data/sentiment_analysis/45/triples/research-problem.txt
Contribution,has research problem,SA,research-problem,/content/training-data/sentiment_analysis/45/triples/research-problem.txt
Contribution,has research problem,ABSA,research-problem,/content/training-data/sentiment_analysis/45/triples/research-problem.txt
Contribution,has research problem,targeted aspect - based sentiment analysis ( TABSA ,research-problem,/content/training-data/sentiment_analysis/45/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/45/triples/results.txt
Results,For,aspect category polarity,results,/content/training-data/sentiment_analysis/45/triples/results.txt
aspect category polarity,has,BERTpair - QA - B,results,/content/training-data/sentiment_analysis/45/triples/results.txt
BERTpair - QA - B,performs,best,results,/content/training-data/sentiment_analysis/45/triples/results.txt
best,on,"all 4 - way , 3 - way , and binary settings",results,/content/training-data/sentiment_analysis/45/triples/results.txt
Results,has,BERT - single,results,/content/training-data/sentiment_analysis/45/triples/results.txt
BERT - single,achieved,better results,results,/content/training-data/sentiment_analysis/45/triples/results.txt
Results,has,BERT - pair,results,/content/training-data/sentiment_analysis/45/triples/results.txt
BERT - pair,achieved,further improvements,results,/content/training-data/sentiment_analysis/45/triples/results.txt
further improvements,over,BERT - single,results,/content/training-data/sentiment_analysis/45/triples/results.txt
Results,has,BERT - pair - NLI - B model,results,/content/training-data/sentiment_analysis/45/triples/results.txt
BERT - pair - NLI - B model,achieves,best performance,results,/content/training-data/sentiment_analysis/45/triples/results.txt
best performance,for,aspect category detection,results,/content/training-data/sentiment_analysis/45/triples/results.txt
Contribution,Code,https :// github.com/stanfordnlp/treelstm,code,/content/training-data/sentiment_analysis/17/triples/code.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/17/triples/hyperparameters.txt
Hyperparameters,tuned on,development set,hyperparameters,/content/training-data/sentiment_analysis/17/triples/hyperparameters.txt
Hyperparameters,For,semantic relatedness task,hyperparameters,/content/training-data/sentiment_analysis/17/triples/hyperparameters.txt
semantic relatedness task,has,word representations,hyperparameters,/content/training-data/sentiment_analysis/17/triples/hyperparameters.txt
word representations,held,fixed,hyperparameters,/content/training-data/sentiment_analysis/17/triples/hyperparameters.txt
Hyperparameters,For,sentiment classification task,hyperparameters,/content/training-data/sentiment_analysis/17/triples/hyperparameters.txt
sentiment classification task,has,word representations,hyperparameters,/content/training-data/sentiment_analysis/17/triples/hyperparameters.txt
word representations,updated,during training,hyperparameters,/content/training-data/sentiment_analysis/17/triples/hyperparameters.txt
during training,with,learning rate,hyperparameters,/content/training-data/sentiment_analysis/17/triples/hyperparameters.txt
learning rate,of,0.1,hyperparameters,/content/training-data/sentiment_analysis/17/triples/hyperparameters.txt
Hyperparameters,initialized,our word representations,hyperparameters,/content/training-data/sentiment_analysis/17/triples/hyperparameters.txt
our word representations,using,publicly available 300 - dimensional Glove vectors,hyperparameters,/content/training-data/sentiment_analysis/17/triples/hyperparameters.txt
Hyperparameters,has,model parameters,hyperparameters,/content/training-data/sentiment_analysis/17/triples/hyperparameters.txt
model parameters,regularized with,per-minibatch L2 regularization strength,hyperparameters,/content/training-data/sentiment_analysis/17/triples/hyperparameters.txt
per-minibatch L2 regularization strength,of,10 ?4,hyperparameters,/content/training-data/sentiment_analysis/17/triples/hyperparameters.txt
Hyperparameters,has,Our models,hyperparameters,/content/training-data/sentiment_analysis/17/triples/hyperparameters.txt
Our models,trained using,AdaGrad,hyperparameters,/content/training-data/sentiment_analysis/17/triples/hyperparameters.txt
AdaGrad,with,minibatch size,hyperparameters,/content/training-data/sentiment_analysis/17/triples/hyperparameters.txt
minibatch size,of,25,hyperparameters,/content/training-data/sentiment_analysis/17/triples/hyperparameters.txt
AdaGrad,with,learning rate,hyperparameters,/content/training-data/sentiment_analysis/17/triples/hyperparameters.txt
learning rate,of,0.05,hyperparameters,/content/training-data/sentiment_analysis/17/triples/hyperparameters.txt
Hyperparameters,has,sentiment classifier,hyperparameters,/content/training-data/sentiment_analysis/17/triples/hyperparameters.txt
sentiment classifier,additionally regularized using,dropout,hyperparameters,/content/training-data/sentiment_analysis/17/triples/hyperparameters.txt
dropout,with,dropout rate,hyperparameters,/content/training-data/sentiment_analysis/17/triples/hyperparameters.txt
dropout rate,of,0.5,hyperparameters,/content/training-data/sentiment_analysis/17/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/17/triples/model.txt
Model,introduce,generalization of the standard LSTM architecture,model,/content/training-data/sentiment_analysis/17/triples/model.txt
generalization of the standard LSTM architecture,to,tree - structured network topologies,model,/content/training-data/sentiment_analysis/17/triples/model.txt
tree - structured network topologies,show,superiority,model,/content/training-data/sentiment_analysis/17/triples/model.txt
superiority,for representing,sentence meaning,model,/content/training-data/sentiment_analysis/17/triples/model.txt
sentence meaning,over,sequential LSTM,model,/content/training-data/sentiment_analysis/17/triples/model.txt
Model,has,"tree - structured LSTM , or Tree - LSTM",model,/content/training-data/sentiment_analysis/17/triples/model.txt
"tree - structured LSTM , or Tree - LSTM",composes,state,model,/content/training-data/sentiment_analysis/17/triples/model.txt
state,from,input vector,model,/content/training-data/sentiment_analysis/17/triples/model.txt
state,from,hidden states,model,/content/training-data/sentiment_analysis/17/triples/model.txt
hidden states,of,arbitrarily many child units,model,/content/training-data/sentiment_analysis/17/triples/model.txt
Contribution,has research problem,Improved Semantic Representations,research-problem,/content/training-data/sentiment_analysis/17/triples/research-problem.txt
Contribution,has research problem,predicting the semantic relatedness of two sentences,research-problem,/content/training-data/sentiment_analysis/17/triples/research-problem.txt
Contribution,has research problem,sentiment classification,research-problem,/content/training-data/sentiment_analysis/17/triples/research-problem.txt
Contribution,has,Baselines,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
Baselines,has,TD- LSTM,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
TD- LSTM,uses,forward LSTM and a backward LSTM,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
forward LSTM and a backward LSTM,to abstract,information,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
information,before and after,target,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
Baselines,has,Average Context,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
Average Context,has,first one,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
first one,named,AC - S,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
first one,averages,word vectors,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
word vectors,before,target,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
word vectors,after,target,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
Average Context,has,second one,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
second one,named,AC,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
second one,averages,word vectors,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
word vectors,of,full context,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
Baselines,has,SVM,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
SVM,on,"surface features , lexicon features and parsing features",baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
Baselines,has,TD - LSTM - A,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
TD - LSTM - A,developed,TD - LSTM,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
TD - LSTM,have,one attention,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
one attention,on,outputs,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
Baselines,has,Rec - NN,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
Rec - NN,performs,semantic composition,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
semantic composition,with,Recursive NNs,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
Recursive NNs,for,sentiment prediction,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
Rec - NN,firstly uses,rules,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
rules,to transform,dependency tree,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
Rec - NN,put,opinion target,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
opinion target,at,root,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
Baselines,has,MemNet,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
MemNet,applies,attention,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
attention,has,multiple times,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
attention,on,word embeddings,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
MemNet,has,last attention 's output,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
last attention 's output,fed to,softmax,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
softmax,for,prediction,baselines,/content/training-data/sentiment_analysis/40/triples/baselines.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/40/triples/model.txt
Model,nonlinearly combine,attention results,model,/content/training-data/sentiment_analysis/40/triples/model.txt
attention results,with,"recurrent network , i.e. GRUs",model,/content/training-data/sentiment_analysis/40/triples/model.txt
Model,apply,softmax,model,/content/training-data/sentiment_analysis/40/triples/model.txt
softmax,to predict,sentiment,model,/content/training-data/sentiment_analysis/40/triples/model.txt
sentiment,on,target,model,/content/training-data/sentiment_analysis/40/triples/model.txt
softmax,on,output,model,/content/training-data/sentiment_analysis/40/triples/model.txt
output,of,GRU network,model,/content/training-data/sentiment_analysis/40/triples/model.txt
Model,pay,multiple attentions,model,/content/training-data/sentiment_analysis/40/triples/model.txt
multiple attentions,on,position - weighted memory,model,/content/training-data/sentiment_analysis/40/triples/model.txt
Model,has,framework,model,/content/training-data/sentiment_analysis/40/triples/model.txt
framework,first adopts,bidirectional LSTM ( BLSTM ,model,/content/training-data/sentiment_analysis/40/triples/model.txt
bidirectional LSTM ( BLSTM ),to produce,memory ( i.e. the states of time steps generated by LSTM ,model,/content/training-data/sentiment_analysis/40/triples/model.txt
memory ( i.e. the states of time steps generated by LSTM ),from,input,model,/content/training-data/sentiment_analysis/40/triples/model.txt
Model,has,memory slices,model,/content/training-data/sentiment_analysis/40/triples/model.txt
memory slices,weighted according to,relative positions,model,/content/training-data/sentiment_analysis/40/triples/model.txt
relative positions,to,target,model,/content/training-data/sentiment_analysis/40/triples/model.txt
Model,has,Our framework,model,/content/training-data/sentiment_analysis/40/triples/model.txt
Our framework,introduces,novel way,model,/content/training-data/sentiment_analysis/40/triples/model.txt
novel way,of applying,multiple - attention mechanism,model,/content/training-data/sentiment_analysis/40/triples/model.txt
multiple - attention mechanism,to synthesize,important features,model,/content/training-data/sentiment_analysis/40/triples/model.txt
important features,in,difficult sentence structures,model,/content/training-data/sentiment_analysis/40/triples/model.txt
Model,propose,novel framework,model,/content/training-data/sentiment_analysis/40/triples/model.txt
novel framework,to solve,problems,model,/content/training-data/sentiment_analysis/40/triples/model.txt
problems,in,target sentiment analysis,model,/content/training-data/sentiment_analysis/40/triples/model.txt
Contribution,has research problem,Aspect Sentiment Analysis,research-problem,/content/training-data/sentiment_analysis/40/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/40/triples/results.txt
Results,has,AC and AC - S,results,/content/training-data/sentiment_analysis/40/triples/results.txt
AC and AC - S,perform,poorly,results,/content/training-data/sentiment_analysis/40/triples/results.txt
Results,has,our RAM,results,/content/training-data/sentiment_analysis/40/triples/results.txt
our RAM,consistently outperforms,compared methods,results,/content/training-data/sentiment_analysis/40/triples/results.txt
Results,has,TD - LSTM - A,results,/content/training-data/sentiment_analysis/40/triples/results.txt
TD - LSTM - A,performs,worse,results,/content/training-data/sentiment_analysis/40/triples/results.txt
worse,than,our method,results,/content/training-data/sentiment_analysis/40/triples/results.txt
Results,has,Rec - NN,results,/content/training-data/sentiment_analysis/40/triples/results.txt
Rec - NN,better than,TD - LSTM,results,/content/training-data/sentiment_analysis/40/triples/results.txt
Rec - NN,not as good as,our method,results,/content/training-data/sentiment_analysis/40/triples/results.txt
Results,has,TD - LSTM,results,/content/training-data/sentiment_analysis/40/triples/results.txt
TD - LSTM,performs,less competitive,results,/content/training-data/sentiment_analysis/40/triples/results.txt
less competitive,than,our method,results,/content/training-data/sentiment_analysis/40/triples/results.txt
Results,has,MemNet,results,/content/training-data/sentiment_analysis/40/triples/results.txt
MemNet,adopts,multiple attentions,results,/content/training-data/sentiment_analysis/40/triples/results.txt
multiple attentions,in order to improve,attention results,results,/content/training-data/sentiment_analysis/40/triples/results.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
Hyperparameters,has,Multi - task training,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
Multi - task training,save,best model,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
best model,take,embedding layer,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
embedding layer,as,Emo2Vec vectors,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
Multi - task training,has,best model,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
best model,uses,L2 regularization,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
L2 regularization,of,1.0,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
best model,uses,learning rate,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
learning rate,of,0.001,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
best model,uses,batch size,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
batch size,of,32,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
Multi - task training,early stop,our model,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
our model,when,averaged dev accuracy,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
averaged dev accuracy,stop,increasing,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
Multi - task training,tune,our parameters,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
our parameters,of,learning rate,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
our parameters,of,L2 regularization,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
Hyperparameters,has,Pre-training Emo2Vec,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
Pre-training Emo2Vec,For,best model,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
best model,use,batch size,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
batch size,of,16,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
best model,use,embedding size,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
embedding size,of,100,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
best model,has,1024 filters,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
1024 filters,has,filter sizes,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
filter sizes,are,"1 , 3 ,5 and 7",hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
Pre-training Emo2Vec,has,Adam,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
Adam,used for,optimization,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
Pre-training Emo2Vec,has,Emo2 Vec embedding matrix and the CNN model,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
Emo2 Vec embedding matrix and the CNN model,pre-trained using,hashtag corpus,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
Pre-training Emo2Vec,has,Parameters of T and CNN,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
Parameters of T and CNN,are,randomly initialized,hyperparameters,/content/training-data/sentiment_analysis/14/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/14/triples/model.txt
Model,demonstrates,effectiveness,model,/content/training-data/sentiment_analysis/14/triples/model.txt
effectiveness,of incorporating,sentiment labels,model,/content/training-data/sentiment_analysis/14/triples/model.txt
sentiment labels,in,wordlevel information,model,/content/training-data/sentiment_analysis/14/triples/model.txt
wordlevel information,for,sentiment - related tasks,model,/content/training-data/sentiment_analysis/14/triples/model.txt
sentiment - related tasks,compared to,other word embeddings,model,/content/training-data/sentiment_analysis/14/triples/model.txt
Model,propose to learn,Emo2Vec,model,/content/training-data/sentiment_analysis/14/triples/model.txt
Emo2Vec,with,multi-task learning framework,model,/content/training-data/sentiment_analysis/14/triples/model.txt
multi-task learning framework,by including,six different emotion - related tasks,model,/content/training-data/sentiment_analysis/14/triples/model.txt
Model,propose,Emo2Vec,model,/content/training-data/sentiment_analysis/14/triples/model.txt
Emo2Vec,are,word - level representations,model,/content/training-data/sentiment_analysis/14/triples/model.txt
word - level representations,that encode,emotional semantics,model,/content/training-data/sentiment_analysis/14/triples/model.txt
emotional semantics,into,"fixed - sized , real - valued vectors",model,/content/training-data/sentiment_analysis/14/triples/model.txt
Contribution,has research problem,Learning Generalized Emotion Representation,research-problem,/content/training-data/sentiment_analysis/14/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/14/triples/results.txt
Results,solely using,simple classifier,results,/content/training-data/sentiment_analysis/14/triples/results.txt
simple classifier,with,good word representation,results,/content/training-data/sentiment_analysis/14/triples/results.txt
good word representation,can achieve,promising results,results,/content/training-data/sentiment_analysis/14/triples/results.txt
Results,shows,multi-task training,results,/content/training-data/sentiment_analysis/14/triples/results.txt
multi-task training,helps to create,better generalized word emotion representations,results,/content/training-data/sentiment_analysis/14/triples/results.txt
better generalized word emotion representations,than just using,single task,results,/content/training-data/sentiment_analysis/14/triples/results.txt
Results,has,GloVe + Emo2 Vec,results,/content/training-data/sentiment_analysis/14/triples/results.txt
GloVe + Emo2 Vec,achieves,comparable result,results,/content/training-data/sentiment_analysis/14/triples/results.txt
comparable result,to,SOTA,results,/content/training-data/sentiment_analysis/14/triples/results.txt
SOTA,on,dataset,results,/content/training-data/sentiment_analysis/14/triples/results.txt
GloVe + Emo2 Vec,achieves,better performances,results,/content/training-data/sentiment_analysis/14/triples/results.txt
better performances,on,SOTA results,results,/content/training-data/sentiment_analysis/14/triples/results.txt
SOTA results,on,"three datasets ( SE0714 , stress and tube tablet ",results,/content/training-data/sentiment_analysis/14/triples/results.txt
Results,has,Emo2 Vec,results,/content/training-data/sentiment_analysis/14/triples/results.txt
Emo2 Vec,works,much better,results,/content/training-data/sentiment_analysis/14/triples/results.txt
much better,on,all datasets,results,/content/training-data/sentiment_analysis/14/triples/results.txt
all datasets,except,SS - T datasets,results,/content/training-data/sentiment_analysis/14/triples/results.txt
all datasets,on,sentiment and other tasks,results,/content/training-data/sentiment_analysis/14/triples/results.txt
sentiment and other tasks,gives,3.3 % accuracy improvement,results,/content/training-data/sentiment_analysis/14/triples/results.txt
sentiment and other tasks,gives,4.7 % f 1 score improvement,results,/content/training-data/sentiment_analysis/14/triples/results.txt
Emo2 Vec,works better than,CNN embedding,results,/content/training-data/sentiment_analysis/14/triples/results.txt
CNN embedding,giving,2.6 % absolute accuracy improvement,results,/content/training-data/sentiment_analysis/14/triples/results.txt
2.6 % absolute accuracy improvement,for,sentiment task,results,/content/training-data/sentiment_analysis/14/triples/results.txt
CNN embedding,giving,1.6 % absolute f1score improvement,results,/content/training-data/sentiment_analysis/14/triples/results.txt
1.6 % absolute f1score improvement,on,other tasks,results,/content/training-data/sentiment_analysis/14/triples/results.txt
CNN embedding,on,14 / 18 datasets,results,/content/training-data/sentiment_analysis/14/triples/results.txt
Emo2 Vec,not trained by,predicting contextual words,results,/content/training-data/sentiment_analysis/14/triples/results.txt
predicting contextual words,is,weak,results,/content/training-data/sentiment_analysis/14/triples/results.txt
weak,on capturing,synthetic and semantic meaning,results,/content/training-data/sentiment_analysis/14/triples/results.txt
Results,Compared with,"GloVe+ DeepMoji , GloVe + Emo2 Vec",results,/content/training-data/sentiment_analysis/14/triples/results.txt
"GloVe+ DeepMoji , GloVe + Emo2 Vec",achieves,same or better results,results,/content/training-data/sentiment_analysis/14/triples/results.txt
same or better results,on average gives,1.0 % improvement,results,/content/training-data/sentiment_analysis/14/triples/results.txt
same or better results,on,11 / 14 datasets,results,/content/training-data/sentiment_analysis/14/triples/results.txt
Results,gives,1.3 % improvement,results,/content/training-data/sentiment_analysis/14/triples/results.txt
1.3 % improvement,in,accuracy,results,/content/training-data/sentiment_analysis/14/triples/results.txt
accuracy,for,sentiment task,results,/content/training-data/sentiment_analysis/14/triples/results.txt
Results,gives,1.1 % improvement,results,/content/training-data/sentiment_analysis/14/triples/results.txt
1.1 % improvement,of,f 1 - score,results,/content/training-data/sentiment_analysis/14/triples/results.txt
f 1 - score,on,other tasks,results,/content/training-data/sentiment_analysis/14/triples/results.txt
Results,to detect,corresponding emotion,results,/content/training-data/sentiment_analysis/14/triples/results.txt
corresponding emotion,more attention needs to be paid to,words,results,/content/training-data/sentiment_analysis/14/triples/results.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
Hyperparameters,has,MSA Model,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
MSA Model,add,Gaussian noise,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
Gaussian noise,with,? = 0.2,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
MSA Model,add,dropout,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
dropout,of,0.3,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
0.3,at,embedding layer,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
dropout,of,0.5,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
0.5,at,LSTM layers,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
dropout,of,0.25,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
0.25,at,recurrent connections,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
recurrent connections,of,LSTM,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
MSA Model,add,L 2 regularization,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
L 2 regularization,of,0.0001,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
0.0001,at,loss function,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
MSA Model,size of,embedding layer,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
embedding layer,is,300,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
MSA Model,has,LSTM layers 150,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
Hyperparameters,has,TSA Model,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
TSA Model,add,L 2 regularization,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
L 2 regularization,of,0.001,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
0.001,at,loss function,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
TSA Model,size of,LSTM layers,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
LSTM layers,has,64,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
TSA Model,size of,embedding layer,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
embedding layer,is,300,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
TSA Model,insert,Gaussian noise,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
Gaussian noise,with,dropout,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
dropout,of,0.2,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
0.2,at,LSTM layer,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
0.2,at,recurrent connection,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
recurrent connection,of,LSTM layer,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
dropout,of,0.3,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
0.3,at,attention layer and the Maxout layer,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
Gaussian noise,with,? = 0.2,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
? = 0.2,at,embedding layer,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
embedding layer,of,both inputs,hyperparameters,/content/training-data/sentiment_analysis/20/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/20/triples/model.txt
Model,For,topic - based sentiment analysis tasks,model,/content/training-data/sentiment_analysis/20/triples/model.txt
topic - based sentiment analysis tasks,propose,Siamese Bidirectional LSTM,model,/content/training-data/sentiment_analysis/20/triples/model.txt
Siamese Bidirectional LSTM,with,contextaware attention mechanism,model,/content/training-data/sentiment_analysis/20/triples/model.txt
Model,has,first model,model,/content/training-data/sentiment_analysis/20/triples/model.txt
first model,employ,2 - layer Bidirectional LSTM,model,/content/training-data/sentiment_analysis/20/triples/model.txt
2 - layer Bidirectional LSTM,equipped with,attention mechanism,model,/content/training-data/sentiment_analysis/20/triples/model.txt
first model,designed for addressing,problem of messagelevel sentiment analysis,model,/content/training-data/sentiment_analysis/20/triples/model.txt
Model,present,two deep - learning systems,model,/content/training-data/sentiment_analysis/20/triples/model.txt
two deep - learning systems,competed at,SemEval - 2017 Task 4,model,/content/training-data/sentiment_analysis/20/triples/model.txt
Contribution,has research problem,Message - level and Topic - based Sentiment Analysis,research-problem,/content/training-data/sentiment_analysis/20/triples/research-problem.txt
Contribution,has research problem,Sentiment Analysis in Twitter,research-problem,/content/training-data/sentiment_analysis/20/triples/research-problem.txt
Contribution,has research problem,Sentiment analysis,research-problem,/content/training-data/sentiment_analysis/20/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/20/triples/results.txt
Results,has,Our official ranking,results,/content/training-data/sentiment_analysis/20/triples/results.txt
Our official ranking,is,1/38 ( tie ,results,/content/training-data/sentiment_analysis/20/triples/results.txt
1/38 ( tie ),in,Subtask A,results,/content/training-data/sentiment_analysis/20/triples/results.txt
Our official ranking,is,2/16,results,/content/training-data/sentiment_analysis/20/triples/results.txt
2/16,in,Subtask C,results,/content/training-data/sentiment_analysis/20/triples/results.txt
2/16,in,Subtask D,results,/content/training-data/sentiment_analysis/20/triples/results.txt
Our official ranking,is,2/24,results,/content/training-data/sentiment_analysis/20/triples/results.txt
2/24,in,Subtask B,results,/content/training-data/sentiment_analysis/20/triples/results.txt
Our official ranking,is,11/12,results,/content/training-data/sentiment_analysis/20/triples/results.txt
11/12,in,Subtask E,results,/content/training-data/sentiment_analysis/20/triples/results.txt
Contribution,has,Approach,approach,/content/training-data/sentiment_analysis/12/triples/approach.txt
Approach,able to automatically and incrementally mine,attention supervision information,approach,/content/training-data/sentiment_analysis/12/triples/approach.txt
attention supervision information,exploited to guide,training,approach,/content/training-data/sentiment_analysis/12/triples/approach.txt
training,of,attention mechanisms,approach,/content/training-data/sentiment_analysis/12/triples/approach.txt
attention mechanisms,in,ASC models,approach,/content/training-data/sentiment_analysis/12/triples/approach.txt
attention supervision information,from,training corpus,approach,/content/training-data/sentiment_analysis/12/triples/approach.txt
Approach,propose,novel progressive self - supervised attention learning approach,approach,/content/training-data/sentiment_analysis/12/triples/approach.txt
novel progressive self - supervised attention learning approach,for,neural ASC models,approach,/content/training-data/sentiment_analysis/12/triples/approach.txt
Approach,roots in,context word,approach,/content/training-data/sentiment_analysis/12/triples/approach.txt
context word,with,maximum attention weight,approach,/content/training-data/sentiment_analysis/12/triples/approach.txt
maximum attention weight,has,greatest impact,approach,/content/training-data/sentiment_analysis/12/triples/approach.txt
greatest impact,on,sentiment prediction,approach,/content/training-data/sentiment_analysis/12/triples/approach.txt
sentiment prediction,of,input sentence,approach,/content/training-data/sentiment_analysis/12/triples/approach.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/12/triples/hyperparameters.txt
Hyperparameters,When implementing,our approach,hyperparameters,/content/training-data/sentiment_analysis/12/triples/hyperparameters.txt
our approach,empirically set,maximum iteration number K,hyperparameters,/content/training-data/sentiment_analysis/12/triples/hyperparameters.txt
maximum iteration number K,as,5,hyperparameters,/content/training-data/sentiment_analysis/12/triples/hyperparameters.txt
maximum iteration number K,as,0.1,hyperparameters,/content/training-data/sentiment_analysis/12/triples/hyperparameters.txt
0.1,on,LAPTOP data set,hyperparameters,/content/training-data/sentiment_analysis/12/triples/hyperparameters.txt
maximum iteration number K,as,0.5,hyperparameters,/content/training-data/sentiment_analysis/12/triples/hyperparameters.txt
0.5,on,REST data set,hyperparameters,/content/training-data/sentiment_analysis/12/triples/hyperparameters.txt
maximum iteration number K,as,0.1,hyperparameters,/content/training-data/sentiment_analysis/12/triples/hyperparameters.txt
0.1,on,TWITTER data set,hyperparameters,/content/training-data/sentiment_analysis/12/triples/hyperparameters.txt
Hyperparameters,For,out - of - vocabulary words,hyperparameters,/content/training-data/sentiment_analysis/12/triples/hyperparameters.txt
out - of - vocabulary words,randomly sampled,embeddings,hyperparameters,/content/training-data/sentiment_analysis/12/triples/hyperparameters.txt
embeddings,from,uniform distribution,hyperparameters,/content/training-data/sentiment_analysis/12/triples/hyperparameters.txt
Hyperparameters,used,pre-trained Glo Ve vectors,hyperparameters,/content/training-data/sentiment_analysis/12/triples/hyperparameters.txt
pre-trained Glo Ve vectors,to initialize,word embeddings,hyperparameters,/content/training-data/sentiment_analysis/12/triples/hyperparameters.txt
word embeddings,with,vector dimension,hyperparameters,/content/training-data/sentiment_analysis/12/triples/hyperparameters.txt
vector dimension,has,300,hyperparameters,/content/training-data/sentiment_analysis/12/triples/hyperparameters.txt
Hyperparameters,has,All hyper - parameters,hyperparameters,/content/training-data/sentiment_analysis/12/triples/hyperparameters.txt
All hyper - parameters,tuned on,20 % randomly held - out training data,hyperparameters,/content/training-data/sentiment_analysis/12/triples/hyperparameters.txt
Hyperparameters,has,"Adam ( Kingma and Ba , 2015 ",hyperparameters,/content/training-data/sentiment_analysis/12/triples/hyperparameters.txt
"Adam ( Kingma and Ba , 2015 )",adopted,optimizer,hyperparameters,/content/training-data/sentiment_analysis/12/triples/hyperparameters.txt
optimizer,with,learning rate,hyperparameters,/content/training-data/sentiment_analysis/12/triples/hyperparameters.txt
learning rate,has,0.001,hyperparameters,/content/training-data/sentiment_analysis/12/triples/hyperparameters.txt
Hyperparameters,To alleviate,overfitting,hyperparameters,/content/training-data/sentiment_analysis/12/triples/hyperparameters.txt
overfitting,employed,"dropout strategy ( Hinton et al. , 2012 ",hyperparameters,/content/training-data/sentiment_analysis/12/triples/hyperparameters.txt
"dropout strategy ( Hinton et al. , 2012 )",on,input word embeddings,hyperparameters,/content/training-data/sentiment_analysis/12/triples/hyperparameters.txt
input word embeddings,of,LSTM,hyperparameters,/content/training-data/sentiment_analysis/12/triples/hyperparameters.txt
Contribution,has research problem,Aspect - Level Sentiment Analysis,research-problem,/content/training-data/sentiment_analysis/12/triples/research-problem.txt
Contribution,has research problem,aspect - level sentiment classification ( ASC ,research-problem,/content/training-data/sentiment_analysis/12/triples/research-problem.txt
Contribution,has research problem,neural ASC,research-problem,/content/training-data/sentiment_analysis/12/triples/research-problem.txt
Contribution,has research problem,Aspect - level sentiment classification ( ASC ,research-problem,/content/training-data/sentiment_analysis/12/triples/research-problem.txt
Contribution,has research problem,sentiment analysis,research-problem,/content/training-data/sentiment_analysis/12/triples/research-problem.txt
Contribution,has research problem,ASC,research-problem,/content/training-data/sentiment_analysis/12/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/12/triples/results.txt
Results,use,both kinds of attention supervision information,results,/content/training-data/sentiment_analysis/12/triples/results.txt
both kinds of attention supervision information,has,MN ( + AS ,results,/content/training-data/sentiment_analysis/12/triples/results.txt
MN ( + AS ),remarkably outperforms,MN,results,/content/training-data/sentiment_analysis/12/triples/results.txt
MN,on,all test sets,results,/content/training-data/sentiment_analysis/12/triples/results.txt
Results,perform,additional K+1 - iteration,results,/content/training-data/sentiment_analysis/12/triples/results.txt
additional K+1 - iteration,of,training,results,/content/training-data/sentiment_analysis/12/triples/results.txt
training,on,neural ASC models,results,/content/training-data/sentiment_analysis/12/triples/results.txt
neural ASC models,performance,not changed significantly,results,/content/training-data/sentiment_analysis/12/triples/results.txt
Results,has,both of our reimplemented MN and TNet,results,/content/training-data/sentiment_analysis/12/triples/results.txt
both of our reimplemented MN and TNet,comparable to,original models,results,/content/training-data/sentiment_analysis/12/triples/results.txt
Results,has,TNet - ATT,results,/content/training-data/sentiment_analysis/12/triples/results.txt
TNet - ATT,slightly inferior to,TNet,results,/content/training-data/sentiment_analysis/12/triples/results.txt
TNet - ATT,replace,CNN,results,/content/training-data/sentiment_analysis/12/triples/results.txt
CNN,of,TNet,results,/content/training-data/sentiment_analysis/12/triples/results.txt
TNet,with,attention mechanism,results,/content/training-data/sentiment_analysis/12/triples/results.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/44/triples/hyperparameters.txt
Hyperparameters,minimize,crossentropy error,hyperparameters,/content/training-data/sentiment_analysis/44/triples/hyperparameters.txt
crossentropy error,using,Adam optimizer,hyperparameters,/content/training-data/sentiment_analysis/44/triples/hyperparameters.txt
crossentropy error,using,L2regularization,hyperparameters,/content/training-data/sentiment_analysis/44/triples/hyperparameters.txt
L2regularization,on,set of weights,hyperparameters,/content/training-data/sentiment_analysis/44/triples/hyperparameters.txt
Hyperparameters,For,individual models ( before joining ,hyperparameters,/content/training-data/sentiment_analysis/44/triples/hyperparameters.txt
individual models ( before joining ),use,200 training epochs,hyperparameters,/content/training-data/sentiment_analysis/44/triples/hyperparameters.txt
individual models ( before joining ),use,batch size,hyperparameters,/content/training-data/sentiment_analysis/44/triples/hyperparameters.txt
batch size,of,100,hyperparameters,/content/training-data/sentiment_analysis/44/triples/hyperparameters.txt
Hyperparameters,implemented using,Tensor Flow python pack,hyperparameters,/content/training-data/sentiment_analysis/44/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/44/triples/model.txt
Model,Given,segmented sentence,model,/content/training-data/sentiment_analysis/44/triples/model.txt
segmented sentence,has,first step,model,/content/training-data/sentiment_analysis/44/triples/model.txt
first step,to create,meaningful vector representations,model,/content/training-data/sentiment_analysis/44/triples/model.txt
meaningful vector representations,for,all the EDUs,model,/content/training-data/sentiment_analysis/44/triples/model.txt
Model,has,framework,model,/content/training-data/sentiment_analysis/44/triples/model.txt
framework,consists of,three main sub parts,model,/content/training-data/sentiment_analysis/44/triples/model.txt
Model,join,Neural Nets,model,/content/training-data/sentiment_analysis/44/triples/model.txt
Neural Nets,in,two different ways,model,/content/training-data/sentiment_analysis/44/triples/model.txt
two different ways,name,Multitasking,model,/content/training-data/sentiment_analysis/44/triples/model.txt
two different ways,name,Pre-training,model,/content/training-data/sentiment_analysis/44/triples/model.txt
Model,devise,three different Recursive Neural Net models,model,/content/training-data/sentiment_analysis/44/triples/model.txt
three different Recursive Neural Net models,designed for,one,model,/content/training-data/sentiment_analysis/44/triples/model.txt
one,of,discourse structure prediction,model,/content/training-data/sentiment_analysis/44/triples/model.txt
one,of,discourse relation prediction,model,/content/training-data/sentiment_analysis/44/triples/model.txt
one,of,sentiment analysis,model,/content/training-data/sentiment_analysis/44/triples/model.txt
Contribution,has research problem,Sentence Level Discourse Parsing and Sentiment Analysis,research-problem,/content/training-data/sentiment_analysis/44/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/44/triples/results.txt
Results,see,some improvement,results,/content/training-data/sentiment_analysis/44/triples/results.txt
some improvement,on,Discourse Structure prediction,results,/content/training-data/sentiment_analysis/44/triples/results.txt
Discourse Structure prediction,when we are using,joint model,results,/content/training-data/sentiment_analysis/44/triples/results.txt
Discourse Structure prediction,has,improvement,results,/content/training-data/sentiment_analysis/44/triples/results.txt
improvement,is,statistically significant,results,/content/training-data/sentiment_analysis/44/triples/results.txt
statistically significant,only for,Nuclearity and Relation predictions,results,/content/training-data/sentiment_analysis/44/triples/results.txt
Results,In,fine grained setting,results,/content/training-data/sentiment_analysis/44/triples/results.txt
fine grained setting,compute,accuracy,results,/content/training-data/sentiment_analysis/44/triples/results.txt
accuracy,across,five classes,results,/content/training-data/sentiment_analysis/44/triples/results.txt
accuracy,of,exact match,results,/content/training-data/sentiment_analysis/44/triples/results.txt
Results,has,improvements,results,/content/training-data/sentiment_analysis/44/triples/results.txt
improvements,on,Relation predictions,results,/content/training-data/sentiment_analysis/44/triples/results.txt
Relation predictions,mainly on,Contrastive set,results,/content/training-data/sentiment_analysis/44/triples/results.txt
Contrastive set,specifically,Contrast,results,/content/training-data/sentiment_analysis/44/triples/results.txt
Contrastive set,specifically,Comparison,results,/content/training-data/sentiment_analysis/44/triples/results.txt
Contrastive set,specifically,Cause,results,/content/training-data/sentiment_analysis/44/triples/results.txt
Contribution,has,Experiments,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
Experiments,has,Tasks,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
Tasks,has,subject - dependent experiment,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
subject - dependent experiment,has,Results,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
Results,see,proposed BiHDM model,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
proposed BiHDM model,outperforms,all the compared methods,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
all the compared methods,verifies,effectiveness of BiHDM,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
all the compared methods,on,all the three public EEG emotional datasets,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
Results,shows,t- test statistical analysis results,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
t- test statistical analysis results,see,BiHDM,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
BiHDM,is,significantly better,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
significantly better,than,baseline method,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
Results,see that,compared method BiDANN,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
compared method BiDANN,considers,bi-hemispheric asymmetry,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
compared method BiDANN,achieves,comparable performance,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
Results,on,SEED - IV,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
SEED - IV,has,proposed method,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
proposed method,improves over,state - of - the - art method Emotion - Meter,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
state - of - the - art method Emotion - Meter,by,4 %,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
subject - dependent experiment,has,Baselines,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
Baselines,using,twelve methods,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
twelve methods,including,linear support vector machine ( SVM ,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
twelve methods,including,random forest ( RF ,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
twelve methods,including,canonical correlation analysis ( CCA ,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
twelve methods,including,group sparse canonical correlation analysis ( GSCCA ,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
twelve methods,including,deep believe network ( DBN ,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
twelve methods,including,graph regularization sparse linear regression ( GRSLR ,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
twelve methods,including,graph convolutional neural network ( GCNN ,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
twelve methods,including,dynamical graph convolutional neural network ( DGCNN ,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
twelve methods,including,domain adversarial neural networks ( DANN ,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
twelve methods,including,bi-hemisphere domain adversarial neural network ( BiDANN ,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
twelve methods,including,EmotionMeter,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
twelve methods,including,attention - long short - term memory ( A - LSTM ,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
Tasks,has,subject - independent experiment,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
subject - independent experiment,has,Results,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
Results,seen that,proposed BiHDM method,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
proposed BiHDM method,achieves,best performance,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
best performance,in,three public datasets,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
best performance,verifies,effectiveness of BiHDM,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
effectiveness of BiHDM,in dealing with,subject - independent EEG emotion recognition,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
Results,For,three datasets,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
three datasets,improvements on,accuracy,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
accuracy,are,"2.2 % , 3.5 % and 2.4 %",experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
"2.2 % , 3.5 % and 2.4 %",compared with,existing state - of - the - art methods,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
Results,has,t- test statistical analysis results,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
t- test statistical analysis results,see,BiHDM,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
BiHDM,is,significantly better,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
significantly better,than,baseline method,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
subject - independent experiment,has,Baselines,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
Baselines,use,twelve methods,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
twelve methods,including,Kullback - Leibler importance estimation procedure ( KLIEP ,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
twelve methods,including,unconstrained least - squares importance fitting ( ULSIF ,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
twelve methods,including,selective transfer machine ( STM ,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
twelve methods,including,"linear SVM , transfer component analysis ( TCA ",experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
twelve methods,including,transfer component analysis ( TCA ,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
twelve methods,including,geodesic flow kernel ( GFK ,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
twelve methods,including,DANN,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
twelve methods,including,DGCNN,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
twelve methods,including,deep adaptation network ( DAN ,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
twelve methods,including,BiDANN,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
twelve methods,including,A - LSTM,experiments,/content/training-data/sentiment_analysis/5/triples/experiments.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/5/triples/model.txt
Model,has,BiHDM,model,/content/training-data/sentiment_analysis/5/triples/model.txt
BiHDM,aims to obtain,deep discrepant features,model,/content/training-data/sentiment_analysis/5/triples/model.txt
deep discrepant features,between,left and right hemispheres,model,/content/training-data/sentiment_analysis/5/triples/model.txt
left and right hemispheres,expected to contain,more discriminative information,model,/content/training-data/sentiment_analysis/5/triples/model.txt
more discriminative information,to recognize,EEG emotion signals,model,/content/training-data/sentiment_analysis/5/triples/model.txt
Model,simplify,graph structure learning process,model,/content/training-data/sentiment_analysis/5/triples/model.txt
graph structure learning process,to avoid losing,intrinsic graph structural information,model,/content/training-data/sentiment_analysis/5/triples/model.txt
intrinsic graph structural information,of,EEG data,model,/content/training-data/sentiment_analysis/5/triples/model.txt
graph structure learning process,by using,horizontal and vertical traversing RNNs,model,/content/training-data/sentiment_analysis/5/triples/model.txt
horizontal and vertical traversing RNNs,which will construct,complete relationship graph,model,/content/training-data/sentiment_analysis/5/triples/model.txt
complete relationship graph,generate,discriminative deep features,model,/content/training-data/sentiment_analysis/5/triples/model.txt
discriminative deep features,for,all the EEG electrodes,model,/content/training-data/sentiment_analysis/5/triples/model.txt
Model,After obtaining,deep features,model,/content/training-data/sentiment_analysis/5/triples/model.txt
deep features,extract,asymmetric discrepancy information,model,/content/training-data/sentiment_analysis/5/triples/model.txt
asymmetric discrepancy information,between,two hemispheres,model,/content/training-data/sentiment_analysis/5/triples/model.txt
two hemispheres,by performing,specific pairwise operations,model,/content/training-data/sentiment_analysis/5/triples/model.txt
specific pairwise operations,for,any paired symmetric electrodes,model,/content/training-data/sentiment_analysis/5/triples/model.txt
deep features,of,each electrodes,model,/content/training-data/sentiment_analysis/5/triples/model.txt
Model,propose,novel neural network model BiHDM,model,/content/training-data/sentiment_analysis/5/triples/model.txt
novel neural network model BiHDM,to learn,bi-hemispheric discrepancy,model,/content/training-data/sentiment_analysis/5/triples/model.txt
bi-hemispheric discrepancy,for,EEG emotion recognition,model,/content/training-data/sentiment_analysis/5/triples/model.txt
Contribution,has research problem,EEG Emotion Recognition,research-problem,/content/training-data/sentiment_analysis/5/triples/research-problem.txt
Contribution,has research problem,electroencephalograph ( EEG ) emotion recognition,research-problem,/content/training-data/sentiment_analysis/5/triples/research-problem.txt
Contribution,has research problem,emotion recognition,research-problem,/content/training-data/sentiment_analysis/5/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
Experimental setup,in,experiment,experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
experiment,set,dimension do,experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
dimension do,of,output feature,experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
output feature,to,16,experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
output feature,without,elaborate traversal,experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
experiment,set,parameters d g and K,experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
parameters d g and K,of,global high - level feature,experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
global high - level feature,to,32 and 6,experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
experiment,set,dimension d l,experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
dimension d l,of,each electrode 's deep representation,experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
each electrode 's deep representation,to,32,experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
Experimental setup,use,released handcrafted features,experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
released handcrafted features,to feed,model,experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
released handcrafted features,i.e.,differential entropy ( DE ,experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
differential entropy ( DE ),in,SEED and SEED - IV,experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
released handcrafted features,i.e.,Short - Time Fourier Transform ( STFT ,experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
Short - Time Fourier Transform ( STFT ),in,MPED,experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
Experimental setup,adopt,subtraction,experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
subtraction,as,pairwise operation,experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
pairwise operation,of,BiHDM model,experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
Experimental setup,implemented,BiHDM,experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
BiHDM,using,Tensor Flow,experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
Tensor Flow,on,one Nvidia 1080 Ti GPU,experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
Experimental setup,has,sizes d N,experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
sizes d N,of,input sample X t,experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
input sample X t,are,"5 62 , 5 62 and 1 62",experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
Experimental setup,has,"learning rate , momentum and weight decay rate",experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
"learning rate , momentum and weight decay rate",set as,"0.003 , 0.9 and 0.95",experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
Experimental setup,has,network,experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
network,trained using,SGD,experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
SGD,with,batch size,experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
batch size,of,200,experimental-setup,/content/training-data/sentiment_analysis/5/triples/experimental-setup.txt
Contribution,has,Baselines,baselines,/content/training-data/sentiment_analysis/51/triples/baselines.txt
Baselines,has,Recursive networks,baselines,/content/training-data/sentiment_analysis/51/triples/baselines.txt
Recursive networks,has,Various types of recursive neural networks ( RNN ,baselines,/content/training-data/sentiment_analysis/51/triples/baselines.txt
Various types of recursive neural networks ( RNN ),applied on,SST,baselines,/content/training-data/sentiment_analysis/51/triples/baselines.txt
Baselines,has,Recurrent networks,baselines,/content/training-data/sentiment_analysis/51/triples/baselines.txt
Recurrent networks,has,Sophisticated recurrent networks,baselines,/content/training-data/sentiment_analysis/51/triples/baselines.txt
Sophisticated recurrent networks,such as,left - to - right and bidrectional LSTM networks,baselines,/content/training-data/sentiment_analysis/51/triples/baselines.txt
left - to - right and bidrectional LSTM networks,applied on,SST,baselines,/content/training-data/sentiment_analysis/51/triples/baselines.txt
Baselines,has,Convolutional networks,baselines,/content/training-data/sentiment_analysis/51/triples/baselines.txt
Convolutional networks,has,input sequences,baselines,/content/training-data/sentiment_analysis/51/triples/baselines.txt
input sequences,passed through,1 - dimensional convolutional neural network,baselines,/content/training-data/sentiment_analysis/51/triples/baselines.txt
1 - dimensional convolutional neural network,as,feature extractors,baselines,/content/training-data/sentiment_analysis/51/triples/baselines.txt
Baselines,has,Word embeddings,baselines,/content/training-data/sentiment_analysis/51/triples/baselines.txt
Word embeddings,has,word vectors,baselines,/content/training-data/sentiment_analysis/51/triples/baselines.txt
word vectors,averaged to get,document vector,baselines,/content/training-data/sentiment_analysis/51/triples/baselines.txt
document vector,fed to,sentiment classifier,baselines,/content/training-data/sentiment_analysis/51/triples/baselines.txt
sentiment classifier,to compute,sentiment score,baselines,/content/training-data/sentiment_analysis/51/triples/baselines.txt
word vectors,pretrained on,large text corpus,baselines,/content/training-data/sentiment_analysis/51/triples/baselines.txt
large text corpus,such as,Wikipedia dump,baselines,/content/training-data/sentiment_analysis/51/triples/baselines.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/51/triples/model.txt
Model,use,pretrained BERT model,model,/content/training-data/sentiment_analysis/51/triples/model.txt
pretrained BERT model,finetune it for,fine - grained sentiment classification task,model,/content/training-data/sentiment_analysis/51/triples/model.txt
fine - grained sentiment classification task,on,Stanford Sentiment Treebank ( SST ) dataset,model,/content/training-data/sentiment_analysis/51/triples/model.txt
Contribution,has research problem,Fine - grained Sentiment Classification,research-problem,/content/training-data/sentiment_analysis/51/triples/research-problem.txt
Contribution,has research problem,Sentiment classification,research-problem,/content/training-data/sentiment_analysis/51/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/51/triples/results.txt
Results,see that,our model,results,/content/training-data/sentiment_analysis/51/triples/results.txt
our model,performs,better,results,/content/training-data/sentiment_analysis/51/triples/results.txt
better,in terms of,accuracy,results,/content/training-data/sentiment_analysis/51/triples/results.txt
accuracy,than,many popular and sophisticated NLP models,results,/content/training-data/sentiment_analysis/51/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/sentiment_analysis/0/triples/ablation-analysis.txt
Ablation analysis,has,ARE model,ablation-analysis,/content/training-data/sentiment_analysis/0/triples/ablation-analysis.txt
ARE model,incorrectly classifies,most instances of happy,ablation-analysis,/content/training-data/sentiment_analysis/0/triples/ablation-analysis.txt
most instances of happy,as,neutral ( 43.51 % ,ablation-analysis,/content/training-data/sentiment_analysis/0/triples/ablation-analysis.txt
ARE model,has,emotion classes,ablation-analysis,/content/training-data/sentiment_analysis/0/triples/ablation-analysis.txt
emotion classes,frequently confused with,neutral class,ablation-analysis,/content/training-data/sentiment_analysis/0/triples/ablation-analysis.txt
Ablation analysis,has,TRE model,ablation-analysis,/content/training-data/sentiment_analysis/0/triples/ablation-analysis.txt
TRE model,shows,greater prediction gains,ablation-analysis,/content/training-data/sentiment_analysis/0/triples/ablation-analysis.txt
greater prediction gains,in predicting,happy class,ablation-analysis,/content/training-data/sentiment_analysis/0/triples/ablation-analysis.txt
happy class,compared to,ARE model ( 35.15 % to 75. 73 % ) ,ablation-analysis,/content/training-data/sentiment_analysis/0/triples/ablation-analysis.txt
Ablation analysis,has,MDRE model,ablation-analysis,/content/training-data/sentiment_analysis/0/triples/ablation-analysis.txt
MDRE model,benefits from,strengths,ablation-analysis,/content/training-data/sentiment_analysis/0/triples/ablation-analysis.txt
strengths,to,surprising degree,ablation-analysis,/content/training-data/sentiment_analysis/0/triples/ablation-analysis.txt
MDRE model,compensates for,weaknesses,ablation-analysis,/content/training-data/sentiment_analysis/0/triples/ablation-analysis.txt
weaknesses,of,previous two models ( ARE and TRE ,ablation-analysis,/content/training-data/sentiment_analysis/0/triples/ablation-analysis.txt
MDRE model,has,occurrence,ablation-analysis,/content/training-data/sentiment_analysis/0/triples/ablation-analysis.txt
occurrence,of,"incorrect "" sad - to - happy "" cases",ablation-analysis,/content/training-data/sentiment_analysis/0/triples/ablation-analysis.txt
"incorrect "" sad - to - happy "" cases",in,TRE model,ablation-analysis,/content/training-data/sentiment_analysis/0/triples/ablation-analysis.txt
TRE model,reduced from,16 . 20 % to 9.15 %,ablation-analysis,/content/training-data/sentiment_analysis/0/triples/ablation-analysis.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
Hyperparameters,use,released transcripts,hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
released transcripts,of,IEMOCAP dataset,hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
IEMOCAP dataset,for,simplicity,hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
Hyperparameters,use,GRUs,hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
GRUs,include,smaller number,hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
smaller number,of,weight parameters,hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
GRUs,as they yield,comparable performance,hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
comparable performance,to that of,LSTM,hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
Hyperparameters,use,max encoder step,hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
max encoder step,of,750,hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
750,for,audio input,hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
max encoder step,of,128,hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
128,for,text input,hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
text input,covers,maximum length,hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
maximum length,of,transcripts,hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
Hyperparameters,has,number of hidden units and the number of layers,hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
number of hidden units and the number of layers,in,RNN,hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
RNN,for,"each model ( ARE , TRE , MDRE and MDREA ",hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
"each model ( ARE , TRE , MDRE and MDREA )",selected based on,extensive hyperparameter search experiments,hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
Hyperparameters,has,vocabulary size,hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
vocabulary size,of,dataset,hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
dataset,is,"3,747",hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
"3,747",including,""" PAD "" token",hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
""" PAD "" token",used to indicate,padding information,hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
padding information,added while preparing,mini-batch data,hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
"3,747",including,""" UNK "" token",hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
""" UNK "" token",represents,unknown words,hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
Hyperparameters,has,weights,hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
weights,of,hidden units,hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
hidden units,initialized using,orthogonal,hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
Hyperparameters,has,text embedding layer,hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
text embedding layer,initialized from,pretrained word - embedding vectors,hyperparameters,/content/training-data/sentiment_analysis/0/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/0/triples/model.txt
Model,uses,high - level text transcription,model,/content/training-data/sentiment_analysis/0/triples/model.txt
high - level text transcription,to utilize,information,model,/content/training-data/sentiment_analysis/0/triples/model.txt
information,contained within,low - resource datasets,model,/content/training-data/sentiment_analysis/0/triples/model.txt
high - level text transcription,as well as,low - level audio signals,model,/content/training-data/sentiment_analysis/0/triples/model.txt
Model,propose,novel deep dual recurrent encoder model,model,/content/training-data/sentiment_analysis/0/triples/model.txt
novel deep dual recurrent encoder model,simultaneously utilizes,audio and text data,model,/content/training-data/sentiment_analysis/0/triples/model.txt
audio and text data,in recognizing,emotions,model,/content/training-data/sentiment_analysis/0/triples/model.txt
emotions,from,speech,model,/content/training-data/sentiment_analysis/0/triples/model.txt
Contribution,has research problem,SPEECH EMOTION RECOGNITION,research-problem,/content/training-data/sentiment_analysis/0/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/0/triples/results.txt
Results,note,recurrent encoder model,results,/content/training-data/sentiment_analysis/0/triples/results.txt
recurrent encoder model,is,effective,results,/content/training-data/sentiment_analysis/0/triples/results.txt
effective,in understanding,types of sequential data,results,/content/training-data/sentiment_analysis/0/triples/results.txt
Results,note,textual data,results,/content/training-data/sentiment_analysis/0/triples/results.txt
textual data,are,informative,results,/content/training-data/sentiment_analysis/0/triples/results.txt
informative,in,emotion prediction tasks,results,/content/training-data/sentiment_analysis/0/triples/results.txt
Results,has,ARE model,results,/content/training-data/sentiment_analysis/0/triples/results.txt
ARE model,shows,baseline performance,results,/content/training-data/sentiment_analysis/0/triples/results.txt
baseline performance,use,minimal audio features,results,/content/training-data/sentiment_analysis/0/triples/results.txt
minimal audio features,such as,MFCC and prosodic features,results,/content/training-data/sentiment_analysis/0/triples/results.txt
MFCC and prosodic features,with,simple architectures,results,/content/training-data/sentiment_analysis/0/triples/results.txt
Results,has,"TRE - ASR , MDRE - ASR and MDREA - ASR models",results,/content/training-data/sentiment_analysis/0/triples/results.txt
"TRE - ASR , MDRE - ASR and MDREA - ASR models",reflect,degraded performance,results,/content/training-data/sentiment_analysis/0/triples/results.txt
degraded performance,compared to,"TRE , MDRE and MDREA models",results,/content/training-data/sentiment_analysis/0/triples/results.txt
Results,has,TRE model,results,/content/training-data/sentiment_analysis/0/triples/results.txt
TRE model,shows,higher performance gain,results,/content/training-data/sentiment_analysis/0/triples/results.txt
higher performance gain,compared to,ARE,results,/content/training-data/sentiment_analysis/0/triples/results.txt
Results,has,label accuracy,results,/content/training-data/sentiment_analysis/0/triples/results.txt
label accuracy,of,processed transcripts,results,/content/training-data/sentiment_analysis/0/triples/results.txt
processed transcripts,is,5.53 % WER,results,/content/training-data/sentiment_analysis/0/triples/results.txt
Results,has,MDREA,results,/content/training-data/sentiment_analysis/0/triples/results.txt
MDREA,outperforms,best existing research results ( WAP 0.690 to 0.688 ,results,/content/training-data/sentiment_analysis/0/triples/results.txt
Results,has,MDRE,results,/content/training-data/sentiment_analysis/0/triples/results.txt
MDRE,shows,substantial performance gain,results,/content/training-data/sentiment_analysis/0/triples/results.txt
MDRE,achieves,state - of - the - art performance,results,/content/training-data/sentiment_analysis/0/triples/results.txt
state - of - the - art performance,with,WAP value,results,/content/training-data/sentiment_analysis/0/triples/results.txt
WAP value,of,0.718,results,/content/training-data/sentiment_analysis/0/triples/results.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
Hyperparameters,empirically set,number of memory chains,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
number of memory chains,with,keys of two of them,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
keys of two of them,set to,same embeddings,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
same embeddings,as,target words LOC1 and LOC2,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
number of memory chains,to,6,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
Hyperparameters,pre-process,corpus,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
corpus,with,tokenisation,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
tokenisation,using,NLTK,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
tokenisation,using,case folding,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
Hyperparameters,use,following hyper - parameters,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
following hyper - parameters,for,weight matrices,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
weight matrices,in,both directions,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
following hyper - parameters,for,hidden size,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
hidden size,of,GRU,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
GRU,is,300,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
Hyperparameters,to curb,overfitting,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
overfitting,regularise,last layer,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
last layer,with,L 2 penalty,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
L 2 penalty,on,weights,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
Hyperparameters,has,Training,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
Training,with,FTRL optimiser,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
Training,carried out over,800 epochs,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
Hyperparameters,has,learning rate,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
learning rate,of,0.05,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
Hyperparameters,has,batch size,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
batch size,of,128,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
Hyperparameters,has,Dropout,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
Dropout,with,rate,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
rate,of,0.2,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
Dropout,applied to,output,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
output,in,final classifier,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
Hyperparameters,initialise,our model,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
our model,with,GloVe,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
GloVe,not updated during,training,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
GloVe,has,300 - D,hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
GloVe,trained on,"42B tokens , 1.9 M vocab",hyperparameters,/content/training-data/sentiment_analysis/30/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/30/triples/model.txt
Model,propose,novel model architecture,model,/content/training-data/sentiment_analysis/30/triples/model.txt
novel model architecture,augmented with,"multiple "" memory chains """,model,/content/training-data/sentiment_analysis/30/triples/model.txt
novel model architecture,for,TABSA,model,/content/training-data/sentiment_analysis/30/triples/model.txt
Model,equipped with,delayed memory update mechanism,model,/content/training-data/sentiment_analysis/30/triples/model.txt
delayed memory update mechanism,to keep track of,numerous entities independently,model,/content/training-data/sentiment_analysis/30/triples/model.txt
Contribution,has research problem,Targeted Aspect - based Sentiment Analysis,research-problem,/content/training-data/sentiment_analysis/30/triples/research-problem.txt
Contribution,has research problem,targeted aspect - based sentiment analysis ( TABSA ,research-problem,/content/training-data/sentiment_analysis/30/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/30/triples/results.txt
Results,has,Ent Net vs. our model,results,/content/training-data/sentiment_analysis/30/triples/results.txt
Ent Net vs. our model,see,consistent performance gains,results,/content/training-data/sentiment_analysis/30/triples/results.txt
consistent performance gains,for,our model,results,/content/training-data/sentiment_analysis/30/triples/results.txt
our model,in both,aspect detection and sentiment classification,results,/content/training-data/sentiment_analysis/30/triples/results.txt
our model,compared to,EntNet,results,/content/training-data/sentiment_analysis/30/triples/results.txt
Results,has,Our model,results,/content/training-data/sentiment_analysis/30/triples/results.txt
Our model,achieves,state - of - the - art results,results,/content/training-data/sentiment_analysis/30/triples/results.txt
state - of - the - art results,for both,aspect detection,results,/content/training-data/sentiment_analysis/30/triples/results.txt
state - of - the - art results,for both,sentiment classification,results,/content/training-data/sentiment_analysis/30/triples/results.txt
Results,has,proposed model,results,/content/training-data/sentiment_analysis/30/triples/results.txt
proposed model,equipped only with,domainindependent general - purpose GloVe embeddings,results,/content/training-data/sentiment_analysis/30/triples/results.txt
proposed model,outperforms,Sentic LSTM,results,/content/training-data/sentiment_analysis/30/triples/results.txt
Sentic LSTM,heavily reliant on,external knowledge bases and domainspecific embeddings,results,/content/training-data/sentiment_analysis/30/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
Baselines,has,LSTM,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
LSTM,takes,sentence,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
sentence,as,input,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
input,to get,hidden representation,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
hidden representation,of,each word,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
LSTM,regards,average value,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
average value,of,hidden states,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
hidden states,as,representation of sentence,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
representation of sentence,puts it into,softmax layer,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
softmax layer,to predict,probability,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
probability,of,each sentiment polarity,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
Baselines,has,AE - LSTM,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
AE - LSTM,models,words,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
words,in,sentence,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
sentence,via,LSTM network,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
AE - LSTM,concatenate,aspect embedding,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
aspect embedding,to,hidden contextual representation,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
hidden contextual representation,for calculating,attention weights,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
attention weights,employed to produce,final representation,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
final representation,for,input sentence,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
input sentence,to judge,sentiment polarity,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
Baselines,has,IAN,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
IAN,considers,separate modeling,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
separate modeling,of,aspect terms and sentences,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
Baselines,has,ATAE - LSTM,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
ATAE - LSTM,extended,AE - LSTM,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
AE - LSTM,by appending,aspect embedding,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
aspect embedding,to,each word embedding,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
each word embedding,to represent,input sentence,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
Baselines,has,MemNet,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
MemNet,applies,attention,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
attention,has,multiple times,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
multiple times,so that,more abstractive evidences,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
more abstractive evidences,could be selected from,external memory,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
multiple times,on,word embedding,baselines,/content/training-data/sentiment_analysis/2/triples/baselines.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/2/triples/hyperparameters.txt
Hyperparameters,employ,Momentum,hyperparameters,/content/training-data/sentiment_analysis/2/triples/hyperparameters.txt
Momentum,as,training method,hyperparameters,/content/training-data/sentiment_analysis/2/triples/hyperparameters.txt
Momentum,whose,momentum parameter ?,hyperparameters,/content/training-data/sentiment_analysis/2/triples/hyperparameters.txt
momentum parameter ?,set to,0.9,hyperparameters,/content/training-data/sentiment_analysis/2/triples/hyperparameters.txt
Momentum,whose,initial learning rate,hyperparameters,/content/training-data/sentiment_analysis/2/triples/hyperparameters.txt
initial learning rate,set to,0.01,hyperparameters,/content/training-data/sentiment_analysis/2/triples/hyperparameters.txt
Hyperparameters,use,Tensorflow,hyperparameters,/content/training-data/sentiment_analysis/2/triples/hyperparameters.txt
Tensorflow,to implement,our proposed model,hyperparameters,/content/training-data/sentiment_analysis/2/triples/hyperparameters.txt
Hyperparameters,has,word embedding,hyperparameters,/content/training-data/sentiment_analysis/2/triples/hyperparameters.txt
word embedding,initialized by,pre-trained Glove vector,hyperparameters,/content/training-data/sentiment_analysis/2/triples/hyperparameters.txt
Hyperparameters,has,biases,hyperparameters,/content/training-data/sentiment_analysis/2/triples/hyperparameters.txt
biases,are set to,zero,hyperparameters,/content/training-data/sentiment_analysis/2/triples/hyperparameters.txt
Hyperparameters,has,weight matrices,hyperparameters,/content/training-data/sentiment_analysis/2/triples/hyperparameters.txt
weight matrices,given,initial value,hyperparameters,/content/training-data/sentiment_analysis/2/triples/hyperparameters.txt
initial value,by,sampling,hyperparameters,/content/training-data/sentiment_analysis/2/triples/hyperparameters.txt
sampling,from,"uniform distribution U ( ?0.1 , 0.1 ",hyperparameters,/content/training-data/sentiment_analysis/2/triples/hyperparameters.txt
Hyperparameters,has,dimension,hyperparameters,/content/training-data/sentiment_analysis/2/triples/hyperparameters.txt
dimension,of,position embedding,hyperparameters,/content/training-data/sentiment_analysis/2/triples/hyperparameters.txt
position embedding,set to,100,hyperparameters,/content/training-data/sentiment_analysis/2/triples/hyperparameters.txt
100,which is,randomly initialized and updated,hyperparameters,/content/training-data/sentiment_analysis/2/triples/hyperparameters.txt
randomly initialized and updated,during,training process,hyperparameters,/content/training-data/sentiment_analysis/2/triples/hyperparameters.txt
dimension,of,word embedding and aspect term embedding,hyperparameters,/content/training-data/sentiment_analysis/2/triples/hyperparameters.txt
word embedding and aspect term embedding,set to,300,hyperparameters,/content/training-data/sentiment_analysis/2/triples/hyperparameters.txt
Hyperparameters,has,hidden units,hyperparameters,/content/training-data/sentiment_analysis/2/triples/hyperparameters.txt
hidden units,set to,200,hyperparameters,/content/training-data/sentiment_analysis/2/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/2/triples/model.txt
Model,has,Obtaining position information,model,/content/training-data/sentiment_analysis/2/triples/model.txt
Obtaining position information,of,each word,model,/content/training-data/sentiment_analysis/2/triples/model.txt
each word,in,corresponding sentence,model,/content/training-data/sentiment_analysis/2/triples/model.txt
corresponding sentence,based on,current aspect term,model,/content/training-data/sentiment_analysis/2/triples/model.txt
current aspect term,converting,position information,model,/content/training-data/sentiment_analysis/2/triples/model.txt
position information,into,position embedding,model,/content/training-data/sentiment_analysis/2/triples/model.txt
Model,has,bidirectional attention mechanism,model,/content/training-data/sentiment_analysis/2/triples/model.txt
bidirectional attention mechanism,to model,mutual relation,model,/content/training-data/sentiment_analysis/2/triples/model.txt
mutual relation,between,aspect term and its corresponding sentence,model,/content/training-data/sentiment_analysis/2/triples/model.txt
Model,has,PBAN,model,/content/training-data/sentiment_analysis/2/triples/model.txt
PBAN,composes of,two Bi - GRU networks,model,/content/training-data/sentiment_analysis/2/triples/model.txt
two Bi - GRU networks,focusing on extracting,aspectlevel features,model,/content/training-data/sentiment_analysis/2/triples/model.txt
two Bi - GRU networks,focusing on extracting,sentence - level features,model,/content/training-data/sentiment_analysis/2/triples/model.txt
Model,propose,position - aware bidirectional attention network ( PBAN ,model,/content/training-data/sentiment_analysis/2/triples/model.txt
position - aware bidirectional attention network ( PBAN ),based on,bidirectional Gated Recurrent Units ( Bi - GRU ,model,/content/training-data/sentiment_analysis/2/triples/model.txt
Contribution,has research problem,Aspect - level Sentiment Analysis,research-problem,/content/training-data/sentiment_analysis/2/triples/research-problem.txt
Contribution,has research problem,Sentiment analysis,research-problem,/content/training-data/sentiment_analysis/2/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/2/triples/results.txt
Results,on,datasets Restaurant and Laptop,results,/content/training-data/sentiment_analysis/2/triples/results.txt
datasets Restaurant and Laptop,by integrating,position information and the bidirectional attention mechanism,results,/content/training-data/sentiment_analysis/2/triples/results.txt
position information and the bidirectional attention mechanism,has,PBAN,results,/content/training-data/sentiment_analysis/2/triples/results.txt
PBAN,effectively judge,sentiment polarity,results,/content/training-data/sentiment_analysis/2/triples/results.txt
sentiment polarity,of,different aspect term,results,/content/training-data/sentiment_analysis/2/triples/results.txt
different aspect term,in,corresponding sentence,results,/content/training-data/sentiment_analysis/2/triples/results.txt
corresponding sentence,to improve,classification accuracy,results,/content/training-data/sentiment_analysis/2/triples/results.txt
PBAN,achieves,state - of - the - art performances,results,/content/training-data/sentiment_analysis/2/triples/results.txt
datasets Restaurant and Laptop,observe,our proposed PBAN model,results,/content/training-data/sentiment_analysis/2/triples/results.txt
our proposed PBAN model,achieves,best performance,results,/content/training-data/sentiment_analysis/2/triples/results.txt
best performance,among,methods,results,/content/training-data/sentiment_analysis/2/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
Baselines,has,CNN - ASP,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
CNN - ASP,is,CNN - based model,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
CNN - based model,which directly concatenates,target representation,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
target representation,to,each word embedding,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
Baselines,has,AE - LSTM,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
AE - LSTM,is,simple LSTM model,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
simple LSTM model,incorporating,target embedding,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
target embedding,as,input,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
Baselines,has,IAN,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
IAN,employs,two LSTMs,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
two LSTMs,to learn,representations,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
representations,of,context and the target phrase,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
Baselines,has,ATAE - LSTM,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
ATAE - LSTM,extends,AE - LSTM,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
AE - LSTM,with,attention,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
Baselines,has,SVM,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
SVM,is,traditional support vector machine based model,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
traditional support vector machine based model,with,extensive feature engineering,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
Baselines,has,AdaRNN,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
AdaRNN,learns,sentence representation,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
sentence representation,toward,target,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
sentence representation,for,sentiment prediction,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
sentiment prediction,via,semantic composition,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
semantic composition,over,dependency tree,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
Baselines,has,BILSTM - ATT -G,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
BILSTM - ATT -G,models,left and right contexts,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
left and right contexts,using,two attention - based LSTMs,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
BILSTM - ATT -G,introduces,gates,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
gates,to measure,"importance of left context , right context , and the entire sentence",baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
"importance of left context , right context , and the entire sentence",for,prediction,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
Baselines,has,TD - LSTM,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
TD - LSTM,performs,predictions,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
predictions,based on,concatenated context representations,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
TD - LSTM,employs,two LSTMs,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
two LSTMs,to model,left and right contexts,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
left and right contexts,of,target,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
Baselines,has,MemNet,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
MemNet,applies,attention mechanism,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
attention mechanism,over,word embeddings multiple times,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
MemNet,predicts,sentiments,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
sentiments,based on,top - most sentence representations,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
Baselines,has,RAM,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
RAM,is,multilayer architecture,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
multilayer architecture,where,each layer,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
each layer,consists of,attention - based aggregation of word features,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
each layer,consists of,GRU cell,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
GRU cell,to learn,sentence representation,baselines,/content/training-data/sentiment_analysis/36/triples/baselines.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/sentiment_analysis/36/triples/ablation-analysis.txt
Ablation analysis,has,TNet w/o context,ablation-analysis,/content/training-data/sentiment_analysis/36/triples/ablation-analysis.txt
TNet w/o context,performs,consistently better,ablation-analysis,/content/training-data/sentiment_analysis/36/triples/ablation-analysis.txt
consistently better,than,TNet w/o transformation,ablation-analysis,/content/training-data/sentiment_analysis/36/triples/ablation-analysis.txt
Ablation analysis,has,produced p-values,ablation-analysis,/content/training-data/sentiment_analysis/36/triples/ablation-analysis.txt
produced p-values,are less than,0.05,ablation-analysis,/content/training-data/sentiment_analysis/36/triples/ablation-analysis.txt
0.05,suggesting that,improvements,ablation-analysis,/content/training-data/sentiment_analysis/36/triples/ablation-analysis.txt
improvements,are,significant,ablation-analysis,/content/training-data/sentiment_analysis/36/triples/ablation-analysis.txt
improvements,brought in by,position information,ablation-analysis,/content/training-data/sentiment_analysis/36/triples/ablation-analysis.txt
Ablation analysis,removing,deep transformation,ablation-analysis,/content/training-data/sentiment_analysis/36/triples/ablation-analysis.txt
deep transformation,shows that,integration,ablation-analysis,/content/training-data/sentiment_analysis/36/triples/ablation-analysis.txt
integration,of,target information,ablation-analysis,/content/training-data/sentiment_analysis/36/triples/ablation-analysis.txt
target information,into,word - level representations,ablation-analysis,/content/training-data/sentiment_analysis/36/triples/ablation-analysis.txt
target information,crucial for,good performance,ablation-analysis,/content/training-data/sentiment_analysis/36/triples/ablation-analysis.txt
deep transformation,has,TNet - LF and TNet - AS,ablation-analysis,/content/training-data/sentiment_analysis/36/triples/ablation-analysis.txt
TNet - LF and TNet - AS,reduced to,TNet w/o transformation,ablation-analysis,/content/training-data/sentiment_analysis/36/triples/ablation-analysis.txt
TNet - LF and TNet - AS,has,results,ablation-analysis,/content/training-data/sentiment_analysis/36/triples/ablation-analysis.txt
results,incomparable with,TNet,ablation-analysis,/content/training-data/sentiment_analysis/36/triples/ablation-analysis.txt
results,in both,accuracy and F 1 measure,ablation-analysis,/content/training-data/sentiment_analysis/36/triples/ablation-analysis.txt
Ablation analysis,Comparing,results,ablation-analysis,/content/training-data/sentiment_analysis/36/triples/ablation-analysis.txt
results,of,TNet and TNet w/o context,ablation-analysis,/content/training-data/sentiment_analysis/36/triples/ablation-analysis.txt
TNet and TNet w/o context,observe,performance,ablation-analysis,/content/training-data/sentiment_analysis/36/triples/ablation-analysis.txt
performance,drops,significantly,ablation-analysis,/content/training-data/sentiment_analysis/36/triples/ablation-analysis.txt
significantly,on,LAPTOP and REST,ablation-analysis,/content/training-data/sentiment_analysis/36/triples/ablation-analysis.txt
performance,of,TNet w/o context,ablation-analysis,/content/training-data/sentiment_analysis/36/triples/ablation-analysis.txt
TNet and TNet w/o context,on,TWITTER,ablation-analysis,/content/training-data/sentiment_analysis/36/triples/ablation-analysis.txt
TWITTER,has,TNet w/o context,ablation-analysis,/content/training-data/sentiment_analysis/36/triples/ablation-analysis.txt
TNet w/o context,performs,very competitive,ablation-analysis,/content/training-data/sentiment_analysis/36/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/36/triples/model.txt
Model,adopt,proximity strategy,model,/content/training-data/sentiment_analysis/36/triples/model.txt
proximity strategy,To help,CNN feature extractor,model,/content/training-data/sentiment_analysis/36/triples/model.txt
CNN feature extractor,locate,sentiment indicators,model,/content/training-data/sentiment_analysis/36/triples/model.txt
sentiment indicators,has,more accurately,model,/content/training-data/sentiment_analysis/36/triples/model.txt
proximity strategy,to scale,input,model,/content/training-data/sentiment_analysis/36/triples/model.txt
input,of,convolutional layer,model,/content/training-data/sentiment_analysis/36/triples/model.txt
convolutional layer,with,positional relevance,model,/content/training-data/sentiment_analysis/36/triples/model.txt
positional relevance,between,a word and the target,model,/content/training-data/sentiment_analysis/36/triples/model.txt
Model,design,contextpreserving mechanism,model,/content/training-data/sentiment_analysis/36/triples/model.txt
contextpreserving mechanism,to contextualize,generated target - specific word representations,model,/content/training-data/sentiment_analysis/36/triples/model.txt
Model,has,TST,model,/content/training-data/sentiment_analysis/36/triples/model.txt
TST,consolidates,each context word,model,/content/training-data/sentiment_analysis/36/triples/model.txt
each context word,with,tailor - made target representation,model,/content/training-data/sentiment_analysis/36/triples/model.txt
tailor - made target representation,to obtain,transformed word representation,model,/content/training-data/sentiment_analysis/36/triples/model.txt
TST,generates,different representations,model,/content/training-data/sentiment_analysis/36/triples/model.txt
different representations,of,target,model,/content/training-data/sentiment_analysis/36/triples/model.txt
target,conditioned on,individual context words,model,/content/training-data/sentiment_analysis/36/triples/model.txt
Model,has,TNet,model,/content/training-data/sentiment_analysis/36/triples/model.txt
TNet,introduces,novel Target - Specific Transformation ( TST ) component,model,/content/training-data/sentiment_analysis/36/triples/model.txt
novel Target - Specific Transformation ( TST ) component,for generating,target - specific word representations,model,/content/training-data/sentiment_analysis/36/triples/model.txt
novel Target - Specific Transformation ( TST ) component,To integrate,target information,model,/content/training-data/sentiment_analysis/36/triples/model.txt
target information,into,word representations,model,/content/training-data/sentiment_analysis/36/triples/model.txt
TNet,generates,contextualized word representations,model,/content/training-data/sentiment_analysis/36/triples/model.txt
contextualized word representations,with,LSTMs,model,/content/training-data/sentiment_analysis/36/triples/model.txt
TNet,firstly encodes,context information,model,/content/training-data/sentiment_analysis/36/triples/model.txt
context information,into,word embeddings,model,/content/training-data/sentiment_analysis/36/triples/model.txt
Model,propose,new architecture,model,/content/training-data/sentiment_analysis/36/triples/model.txt
new architecture,named,Target - Specific Transformation Networks ( TNet ,model,/content/training-data/sentiment_analysis/36/triples/model.txt
Contribution,has research problem,Target - Oriented Sentiment Classification,research-problem,/content/training-data/sentiment_analysis/36/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/36/triples/results.txt
Results,For,ungrammatical text,results,/content/training-data/sentiment_analysis/36/triples/results.txt
ungrammatical text,has,CNN - based models,results,/content/training-data/sentiment_analysis/36/triples/results.txt
CNN - based models,may have,some advantages,results,/content/training-data/sentiment_analysis/36/triples/results.txt
Results,has,LSTM - based models,results,/content/training-data/sentiment_analysis/36/triples/results.txt
LSTM - based models,relying on,sequential information,results,/content/training-data/sentiment_analysis/36/triples/results.txt
LSTM - based models,can perform,well,results,/content/training-data/sentiment_analysis/36/triples/results.txt
well,for,formal sentences,results,/content/training-data/sentiment_analysis/36/triples/results.txt
well,by capturing,more useful context features,results,/content/training-data/sentiment_analysis/36/triples/results.txt
Contribution,has,Dataset,datase,/content/training-data/sentiment_analysis/13/triples/dataset.txt
Dataset,first builds,RRC dataset,datase,/content/training-data/sentiment_analysis/13/triples/dataset.txt
RRC dataset,called,ReviewRC,datase,/content/training-data/sentiment_analysis/13/triples/dataset.txt
ReviewRC,using,reviews,datase,/content/training-data/sentiment_analysis/13/triples/dataset.txt
reviews,from,SemEval 2016 Task 5,datase,/content/training-data/sentiment_analysis/13/triples/dataset.txt
SemEval 2016 Task 5,is a,popular dataset,datase,/content/training-data/sentiment_analysis/13/triples/dataset.txt
popular dataset,for,aspect - based sentiment analysis ( ABSA ,datase,/content/training-data/sentiment_analysis/13/triples/dataset.txt
aspect - based sentiment analysis ( ABSA ),in,domains of laptop and restaurant,datase,/content/training-data/sentiment_analysis/13/triples/dataset.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/13/triples/hyperparameters.txt
Hyperparameters,leverage,FP16 computation,hyperparameters,/content/training-data/sentiment_analysis/13/triples/hyperparameters.txt
FP16 computation,to reduce,size,hyperparameters,/content/training-data/sentiment_analysis/13/triples/hyperparameters.txt
size,of both,model,hyperparameters,/content/training-data/sentiment_analysis/13/triples/hyperparameters.txt
size,of both,hidden representations,hyperparameters,/content/training-data/sentiment_analysis/13/triples/hyperparameters.txt
hidden representations,of,data,hyperparameters,/content/training-data/sentiment_analysis/13/triples/hyperparameters.txt
Hyperparameters,set,static loss scale,hyperparameters,/content/training-data/sentiment_analysis/13/triples/hyperparameters.txt
static loss scale,in,FP,hyperparameters,/content/training-data/sentiment_analysis/13/triples/hyperparameters.txt
static loss scale,of,2,hyperparameters,/content/training-data/sentiment_analysis/13/triples/hyperparameters.txt
Hyperparameters,adopt,BERT BASE ( uncased ,hyperparameters,/content/training-data/sentiment_analysis/13/triples/hyperparameters.txt
BERT BASE ( uncased ),as,basis,hyperparameters,/content/training-data/sentiment_analysis/13/triples/hyperparameters.txt
basis,for,all experiments,hyperparameters,/content/training-data/sentiment_analysis/13/triples/hyperparameters.txt
Hyperparameters,use,Adam optimizer,hyperparameters,/content/training-data/sentiment_analysis/13/triples/hyperparameters.txt
Adam optimizer,set,learning rate,hyperparameters,/content/training-data/sentiment_analysis/13/triples/hyperparameters.txt
learning rate,to be,3e - 5,hyperparameters,/content/training-data/sentiment_analysis/13/triples/hyperparameters.txt
Hyperparameters,has,number of subbatch u,hyperparameters,/content/training-data/sentiment_analysis/13/triples/hyperparameters.txt
number of subbatch u,set to,2,hyperparameters,/content/training-data/sentiment_analysis/13/triples/hyperparameters.txt
2,good enough to store,sub - batch iteration,hyperparameters,/content/training-data/sentiment_analysis/13/triples/hyperparameters.txt
sub - batch iteration,into,GPU memory,hyperparameters,/content/training-data/sentiment_analysis/13/triples/hyperparameters.txt
GPU memory,of,11G,hyperparameters,/content/training-data/sentiment_analysis/13/triples/hyperparameters.txt
Hyperparameters,has,maximum length,hyperparameters,/content/training-data/sentiment_analysis/13/triples/hyperparameters.txt
maximum length,of,post -training,hyperparameters,/content/training-data/sentiment_analysis/13/triples/hyperparameters.txt
post -training,set to,320,hyperparameters,/content/training-data/sentiment_analysis/13/triples/hyperparameters.txt
320,with,batch size,hyperparameters,/content/training-data/sentiment_analysis/13/triples/hyperparameters.txt
batch size,of,16,hyperparameters,/content/training-data/sentiment_analysis/13/triples/hyperparameters.txt
16,for each type of,knowledge,hyperparameters,/content/training-data/sentiment_analysis/13/triples/hyperparameters.txt
Hyperparameters,train,"140,000 steps",hyperparameters,/content/training-data/sentiment_analysis/13/triples/hyperparameters.txt
"140,000 steps",for,restaurant domain,hyperparameters,/content/training-data/sentiment_analysis/13/triples/hyperparameters.txt
Hyperparameters,train,"70,000 steps",hyperparameters,/content/training-data/sentiment_analysis/13/triples/hyperparameters.txt
"70,000 steps",for,laptop domain,hyperparameters,/content/training-data/sentiment_analysis/13/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/13/triples/model.txt
Model,adapt,BERT,model,/content/training-data/sentiment_analysis/13/triples/model.txt
BERT,with,both domain knowledge and task ( MRC ) knowledge,model,/content/training-data/sentiment_analysis/13/triples/model.txt
both domain knowledge and task ( MRC ) knowledge,before,fine - tuning,model,/content/training-data/sentiment_analysis/13/triples/model.txt
fine - tuning,using,domain end task annotated data,model,/content/training-data/sentiment_analysis/13/triples/model.txt
domain end task annotated data,for,domain RRC,model,/content/training-data/sentiment_analysis/13/triples/model.txt
Model,adopts,BERT,model,/content/training-data/sentiment_analysis/13/triples/model.txt
BERT,as,base model,model,/content/training-data/sentiment_analysis/13/triples/model.txt
Model,leverages,knowledge,model,/content/training-data/sentiment_analysis/13/triples/model.txt
knowledge,from,two sources,model,/content/training-data/sentiment_analysis/13/triples/model.txt
two sources,name,unsupervised domain reviews,model,/content/training-data/sentiment_analysis/13/triples/model.txt
two sources,name,supervised ( yet out - of - domain ) MRC data,model,/content/training-data/sentiment_analysis/13/triples/model.txt
Model,propose,novel joint post - training technique,model,/content/training-data/sentiment_analysis/13/triples/model.txt
novel joint post - training technique,takes,BERT 's pre-trained weights,model,/content/training-data/sentiment_analysis/13/triples/model.txt
BERT 's pre-trained weights,as,initialization,model,/content/training-data/sentiment_analysis/13/triples/model.txt
initialization,for,basic language understanding,model,/content/training-data/sentiment_analysis/13/triples/model.txt
Contribution,has research problem,Aspect - based Sentiment Analysis,research-problem,/content/training-data/sentiment_analysis/13/triples/research-problem.txt
Contribution,has research problem,aspect extraction,research-problem,/content/training-data/sentiment_analysis/13/triples/research-problem.txt
Contribution,has research problem,aspect sentiment classification,research-problem,/content/training-data/sentiment_analysis/13/triples/research-problem.txt
Contribution,has research problem,aspect - based sentiment analysis,research-problem,/content/training-data/sentiment_analysis/13/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/13/triples/results.txt
Results,found that,vanilla pre-trained weights,results,/content/training-data/sentiment_analysis/13/triples/results.txt
vanilla pre-trained weights,of,BERT,results,/content/training-data/sentiment_analysis/13/triples/results.txt
BERT,do not work,well,results,/content/training-data/sentiment_analysis/13/triples/results.txt
well,for,review - based tasks,results,/content/training-data/sentiment_analysis/13/triples/results.txt
Results,noticed that,roles,results,/content/training-data/sentiment_analysis/13/triples/results.txt
roles,of,domain knowledge and task knowledge,results,/content/training-data/sentiment_analysis/13/triples/results.txt
domain knowledge and task knowledge,vary for,different tasks and domains,results,/content/training-data/sentiment_analysis/13/triples/results.txt
Results,For,ASC,results,/content/training-data/sentiment_analysis/13/triples/results.txt
ASC,observed that,large - scale annotated MRC data,results,/content/training-data/sentiment_analysis/13/triples/results.txt
large - scale annotated MRC data,is,very useful,results,/content/training-data/sentiment_analysis/13/triples/results.txt
Results,For,RRC,results,/content/training-data/sentiment_analysis/13/triples/results.txt
RRC,found that,performance gain,results,/content/training-data/sentiment_analysis/13/triples/results.txt
performance gain,of,BERT - PT,results,/content/training-data/sentiment_analysis/13/triples/results.txt
BERT - PT,mostly comes from,task - awareness ( MRC ) post -training,results,/content/training-data/sentiment_analysis/13/triples/results.txt
Results,For,AE,results,/content/training-data/sentiment_analysis/13/triples/results.txt
AE,found that,great performance boost,results,/content/training-data/sentiment_analysis/13/triples/results.txt
great performance boost,comes mostly from,domain knowledge posttraining,results,/content/training-data/sentiment_analysis/13/triples/results.txt
AE,has,errors,results,/content/training-data/sentiment_analysis/13/triples/results.txt
errors,mostly come from,annotation inconsistency,results,/content/training-data/sentiment_analysis/13/triples/results.txt
errors,mostly come from,boundaries,results,/content/training-data/sentiment_analysis/13/triples/results.txt
boundaries,of,aspects,results,/content/training-data/sentiment_analysis/13/triples/results.txt
Results,has,BERT - MRC,results,/content/training-data/sentiment_analysis/13/triples/results.txt
BERT - MRC,has almost no,improvement,results,/content/training-data/sentiment_analysis/13/triples/results.txt
improvement,on,restaurant,results,/content/training-data/sentiment_analysis/13/triples/results.txt
Results,has,ASC,results,/content/training-data/sentiment_analysis/13/triples/results.txt
ASC,tends to have,errors,results,/content/training-data/sentiment_analysis/13/triples/results.txt
errors,as,decision boundary,results,/content/training-data/sentiment_analysis/13/triples/results.txt
decision boundary,between,negative and neutral examples,results,/content/training-data/sentiment_analysis/13/triples/results.txt
negative and neutral examples,is,unclear,results,/content/training-data/sentiment_analysis/13/triples/results.txt
Results,has,errors,results,/content/training-data/sentiment_analysis/13/triples/results.txt
errors,on,RRC,results,/content/training-data/sentiment_analysis/13/triples/results.txt
RRC,come from,incorrect location,results,/content/training-data/sentiment_analysis/13/triples/results.txt
incorrect location,of,spans,results,/content/training-data/sentiment_analysis/13/triples/results.txt
spans,that may have,certain nearby words,results,/content/training-data/sentiment_analysis/13/triples/results.txt
certain nearby words,related to,question,results,/content/training-data/sentiment_analysis/13/triples/results.txt
RRC,come from,boundaries,results,/content/training-data/sentiment_analysis/13/triples/results.txt
boundaries,of,spans,results,/content/training-data/sentiment_analysis/13/triples/results.txt
spans,that are not,concise enough,results,/content/training-data/sentiment_analysis/13/triples/results.txt
Results,further investigated,BERT - MRC,results,/content/training-data/sentiment_analysis/13/triples/results.txt
BERT - MRC,improved,examples,results,/content/training-data/sentiment_analysis/13/triples/results.txt
examples,found that,boundaries of spans ( especially short spans ,results,/content/training-data/sentiment_analysis/13/triples/results.txt
boundaries of spans ( especially short spans ),were,greatly improved,results,/content/training-data/sentiment_analysis/13/triples/results.txt
Results,observed,proposed joint post - training ( BERT - PT ,results,/content/training-data/sentiment_analysis/13/triples/results.txt
proposed joint post - training ( BERT - PT ),has,best performance,results,/content/training-data/sentiment_analysis/13/triples/results.txt
best performance,over,all tasks,results,/content/training-data/sentiment_analysis/13/triples/results.txt
all tasks,in,all domains,results,/content/training-data/sentiment_analysis/13/triples/results.txt
best performance,show,benefits,results,/content/training-data/sentiment_analysis/13/triples/results.txt
benefits,of having,two types of knowledge,results,/content/training-data/sentiment_analysis/13/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/sentiment_analysis/33/triples/baselines.txt
Baselines,compare against,RNN variants,baselines,/content/training-data/sentiment_analysis/33/triples/baselines.txt
RNN variants,name,GRU,baselines,/content/training-data/sentiment_analysis/33/triples/baselines.txt
RNN variants,name,LSTM,baselines,/content/training-data/sentiment_analysis/33/triples/baselines.txt
RNN variants,name,GRU,baselines,/content/training-data/sentiment_analysis/33/triples/baselines.txt
GRU,with,max pooling,baselines,/content/training-data/sentiment_analysis/33/triples/baselines.txt
RNN variants,name,LSTM,baselines,/content/training-data/sentiment_analysis/33/triples/baselines.txt
LSTM,with,max pooling,baselines,/content/training-data/sentiment_analysis/33/triples/baselines.txt
Baselines,For,RNFs,baselines,/content/training-data/sentiment_analysis/33/triples/baselines.txt
RNFs,adopt,two implementations,baselines,/content/training-data/sentiment_analysis/33/triples/baselines.txt
two implementations,based on,GRUs and LSTMs,baselines,/content/training-data/sentiment_analysis/33/triples/baselines.txt
Baselines,consider,CNN variants,baselines,/content/training-data/sentiment_analysis/33/triples/baselines.txt
CNN variants,with,linear filters and RNFs,baselines,/content/training-data/sentiment_analysis/33/triples/baselines.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/33/triples/model.txt
Model,propose to employ,recurrent neural networks ( RNNs ,model,/content/training-data/sentiment_analysis/33/triples/model.txt
recurrent neural networks ( RNNs ),as,convolution filters,model,/content/training-data/sentiment_analysis/33/triples/model.txt
convolution filters,of,CNN systems,model,/content/training-data/sentiment_analysis/33/triples/model.txt
Model,computation of,convolution operation,model,/content/training-data/sentiment_analysis/33/triples/model.txt
convolution operation,with,RNFs,model,/content/training-data/sentiment_analysis/33/triples/model.txt
RNFs,can be,parallelized,model,/content/training-data/sentiment_analysis/33/triples/model.txt
parallelized,As in,conventional CNNs,model,/content/training-data/sentiment_analysis/33/triples/model.txt
Model,has,recurrent neural filters ( RNFs ,model,/content/training-data/sentiment_analysis/33/triples/model.txt
recurrent neural filters ( RNFs ),deal with,language compositionality,model,/content/training-data/sentiment_analysis/33/triples/model.txt
language compositionality,with,recurrent function,model,/content/training-data/sentiment_analysis/33/triples/model.txt
recurrent function,that models,word relations,model,/content/training-data/sentiment_analysis/33/triples/model.txt
recurrent neural filters ( RNFs ),to implicitly model,long - term dependencies,model,/content/training-data/sentiment_analysis/33/triples/model.txt
Model,has,RNF - based CNN models,model,/content/training-data/sentiment_analysis/33/triples/model.txt
RNF - based CNN models,can be,3 - 8 x faster,model,/content/training-data/sentiment_analysis/33/triples/model.txt
3 - 8 x faster,than,RNN counterparts,model,/content/training-data/sentiment_analysis/33/triples/model.txt
Model,has,RNFs,model,/content/training-data/sentiment_analysis/33/triples/model.txt
RNFs,alleviates,some well - known drawbacks,model,/content/training-data/sentiment_analysis/33/triples/model.txt
some well - known drawbacks,including,vulnerability,model,/content/training-data/sentiment_analysis/33/triples/model.txt
vulnerability,to,gradient vanishing and exploding problems,model,/content/training-data/sentiment_analysis/33/triples/model.txt
some well - known drawbacks,of,RNNs,model,/content/training-data/sentiment_analysis/33/triples/model.txt
RNFs,applied to,word sequences,model,/content/training-data/sentiment_analysis/33/triples/model.txt
word sequences,of,moderate lengths,model,/content/training-data/sentiment_analysis/33/triples/model.txt
Model,present,two RNF - based CNN architectures,model,/content/training-data/sentiment_analysis/33/triples/model.txt
two RNF - based CNN architectures,for,sentence classification and answer sentence selection problems,model,/content/training-data/sentiment_analysis/33/triples/model.txt
Contribution,has research problem,Convolutional Neural Networks with Recurrent Neural Filters,research-problem,/content/training-data/sentiment_analysis/33/triples/research-problem.txt
Contribution,has research problem,model convolution filters with RNNs,research-problem,/content/training-data/sentiment_analysis/33/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/33/triples/results.txt
Results,has,Conventional RNN models,results,/content/training-data/sentiment_analysis/33/triples/results.txt
Conventional RNN models,benefit from,max pooling,results,/content/training-data/sentiment_analysis/33/triples/results.txt
max pooling,especially on,answer sentence selection,results,/content/training-data/sentiment_analysis/33/triples/results.txt
Results,has,RNF - based CNN models,results,/content/training-data/sentiment_analysis/33/triples/results.txt
RNF - based CNN models,perform,consistently better,results,/content/training-data/sentiment_analysis/33/triples/results.txt
consistently better,than,max - pooled RNN models,results,/content/training-data/sentiment_analysis/33/triples/results.txt
Results,has,CNN - RNF - LSTM,results,/content/training-data/sentiment_analysis/33/triples/results.txt
CNN - RNF - LSTM,obtains,competitive results,results,/content/training-data/sentiment_analysis/33/triples/results.txt
competitive results,despite,simple model architecture,results,/content/training-data/sentiment_analysis/33/triples/results.txt
simple model architecture,compared to,state - of - the - art systems,results,/content/training-data/sentiment_analysis/33/triples/results.txt
competitive results,on,answer sentence selection datasets,results,/content/training-data/sentiment_analysis/33/triples/results.txt
CNN - RNF - LSTM,achieves,53.4 % and 90.0 % accuracies,results,/content/training-data/sentiment_analysis/33/triples/results.txt
53.4 % and 90.0 % accuracies,match,state - of the - art results,results,/content/training-data/sentiment_analysis/33/triples/results.txt
state - of the - art results,on,Stanford Sentiment Treebank,results,/content/training-data/sentiment_analysis/33/triples/results.txt
53.4 % and 90.0 % accuracies,on,fine - grained and binary sentiment classification tasks,results,/content/training-data/sentiment_analysis/33/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/sentiment_analysis/34/triples/baselines.txt
Baselines,For,ASTD,baselines,/content/training-data/sentiment_analysis/34/triples/baselines.txt
ASTD,has,best reported results,baselines,/content/training-data/sentiment_analysis/34/triples/baselines.txt
best reported results,by,ensemble system,baselines,/content/training-data/sentiment_analysis/34/triples/baselines.txt
ensemble system,combining output of,CNN and Bi - LSTM architectures,baselines,/content/training-data/sentiment_analysis/34/triples/baselines.txt
Baselines,has,best performing system,baselines,/content/training-data/sentiment_analysis/34/triples/baselines.txt
best performing system,in,SemEval 2017 task,baselines,/content/training-data/sentiment_analysis/34/triples/baselines.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/34/triples/model.txt
Model,present,Mazajak,model,/content/training-data/sentiment_analysis/34/triples/model.txt
Mazajak,utilises,deep learning,model,/content/training-data/sentiment_analysis/34/triples/model.txt
Mazajak,utilises,massive Arabic word embeddings,model,/content/training-data/sentiment_analysis/34/triples/model.txt
Mazajak,has,Online Arabic sentiment analysis system,model,/content/training-data/sentiment_analysis/34/triples/model.txt
Model,available as,online API,model,/content/training-data/sentiment_analysis/34/triples/model.txt
online API,used by,other researchers,model,/content/training-data/sentiment_analysis/34/triples/model.txt
Contribution,has research problem,Arabic Sentiment Analyser,research-problem,/content/training-data/sentiment_analysis/34/triples/research-problem.txt
Contribution,has research problem,Sentiment analysis ( SA ,research-problem,/content/training-data/sentiment_analysis/34/triples/research-problem.txt
Contribution,has research problem,Sentiment analysis,research-problem,/content/training-data/sentiment_analysis/34/triples/research-problem.txt
Contribution,has research problem,SA,research-problem,/content/training-data/sentiment_analysis/34/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/34/triples/results.txt
Results,for,our tool,results,/content/training-data/sentiment_analysis/34/triples/results.txt
our tool,represents,current state - of - the - art,results,/content/training-data/sentiment_analysis/34/triples/results.txt
current state - of - the - art,for,Arabic SA,results,/content/training-data/sentiment_analysis/34/triples/results.txt
Results,has,reported scores,results,/content/training-data/sentiment_analysis/34/triples/results.txt
reported scores,higher than,current top systems,results,/content/training-data/sentiment_analysis/34/triples/results.txt
reported scores,for,all the evaluation scores,results,/content/training-data/sentiment_analysis/34/triples/results.txt
all the evaluation scores,including,"average recall , F P N , and accuracy",results,/content/training-data/sentiment_analysis/34/triples/results.txt
Results,has,Mazajak model,results,/content/training-data/sentiment_analysis/34/triples/results.txt
Mazajak model,outperformed,current state - of - the - art models,results,/content/training-data/sentiment_analysis/34/triples/results.txt
current state - of - the - art models,on,SemEval and ASTD datasets,results,/content/training-data/sentiment_analysis/34/triples/results.txt
Mazajak model,achieved,high performance,results,/content/training-data/sentiment_analysis/34/triples/results.txt
high performance,on,ArSAS dataset,results,/content/training-data/sentiment_analysis/34/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
Baselines,has,Feature - enhanced SVM,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
Feature - enhanced SVM,is,SVM classifier,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
SVM classifier,with,state - of - the - art feature template,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
state - of - the - art feature template,which contains,n-gram features,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
state - of - the - art feature template,which contains,parse features,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
state - of - the - art feature template,which contains,lexicon features,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
Baselines,has,AE - LSTM,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
AE - LSTM,is,upgraded version,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
upgraded version,of,LSTM,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
Baselines,has,GRNN - G3,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
GRNN - G3,adopts,Gated - RNN,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
Gated - RNN,use,three - way structure,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
three - way structure,to leverage,contexts,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
Gated - RNN,to represent,sentence,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
Baselines,has,IAN,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
IAN,generate,representations,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
representations,for,targets and contexts,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
IAN,interactively learns,attentions,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
attentions,in,contexts and targets,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
Baselines,has,ATAE - LSTM,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
ATAE - LSTM,developed based on,AE - LSTM,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
Baselines,has,Simple SVM,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
Simple SVM,is,SVM classifier,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
SVM classifier,with,simple features,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
simple features,such as,unigrams and bigrams,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
Baselines,has,Majority,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
Majority,assigns,sentiment polarity,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
sentiment polarity,has,largest probability,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
largest probability,in,training set,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
Baselines,has,TD - LSTM,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
TD - LSTM,adopts,two LSTMs,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
TD - LSTM,to model,right context,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
right context,with,target,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
TD - LSTM,to model,left context,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
left context,with,target,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
Baselines,has,MemNet,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
MemNet,is,deep memory network,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
deep memory network,considers,content and position,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
content and position,of,target,baselines,/content/training-data/sentiment_analysis/47/triples/baselines.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/47/triples/hyperparameters.txt
Hyperparameters,In,model training,hyperparameters,/content/training-data/sentiment_analysis/47/triples/hyperparameters.txt
model training,has,learning rate,hyperparameters,/content/training-data/sentiment_analysis/47/triples/hyperparameters.txt
learning rate,set to,0.1,hyperparameters,/content/training-data/sentiment_analysis/47/triples/hyperparameters.txt
model training,has,weight,hyperparameters,/content/training-data/sentiment_analysis/47/triples/hyperparameters.txt
weight,for,L 2 - norm regularization,hyperparameters,/content/training-data/sentiment_analysis/47/triples/hyperparameters.txt
L 2 - norm regularization,set to,1 e - 5,hyperparameters,/content/training-data/sentiment_analysis/47/triples/hyperparameters.txt
model training,has,dropout rate,hyperparameters,/content/training-data/sentiment_analysis/47/triples/hyperparameters.txt
dropout rate,set to,0.5,hyperparameters,/content/training-data/sentiment_analysis/47/triples/hyperparameters.txt
Hyperparameters,use,GloVe 2 vectors,hyperparameters,/content/training-data/sentiment_analysis/47/triples/hyperparameters.txt
GloVe 2 vectors,with,300 dimensions,hyperparameters,/content/training-data/sentiment_analysis/47/triples/hyperparameters.txt
300 dimensions,to initialize,word embeddings,hyperparameters,/content/training-data/sentiment_analysis/47/triples/hyperparameters.txt
Hyperparameters,has,paired t- test,hyperparameters,/content/training-data/sentiment_analysis/47/triples/hyperparameters.txt
paired t- test,used for,significance testing,hyperparameters,/content/training-data/sentiment_analysis/47/triples/hyperparameters.txt
Hyperparameters,has,All out - ofvocabulary words and weight matrices,hyperparameters,/content/training-data/sentiment_analysis/47/triples/hyperparameters.txt
All out - ofvocabulary words and weight matrices,randomly initialized by,"uniform distribution U ( - 0.1 , 0.1 ",hyperparameters,/content/training-data/sentiment_analysis/47/triples/hyperparameters.txt
Hyperparameters,has,Tensor Flow,hyperparameters,/content/training-data/sentiment_analysis/47/triples/hyperparameters.txt
Tensor Flow,for implementing,neural network model,hyperparameters,/content/training-data/sentiment_analysis/47/triples/hyperparameters.txt
Hyperparameters,has,all bias,hyperparameters,/content/training-data/sentiment_analysis/47/triples/hyperparameters.txt
all bias,set to,zero,hyperparameters,/content/training-data/sentiment_analysis/47/triples/hyperparameters.txt
Hyperparameters,has,dimension,hyperparameters,/content/training-data/sentiment_analysis/47/triples/hyperparameters.txt
dimension,of,word embedding vectors,hyperparameters,/content/training-data/sentiment_analysis/47/triples/hyperparameters.txt
dimension,of,hidden state vectors,hyperparameters,/content/training-data/sentiment_analysis/47/triples/hyperparameters.txt
dimension,is,300,hyperparameters,/content/training-data/sentiment_analysis/47/triples/hyperparameters.txt
Hyperparameters,train,model,hyperparameters,/content/training-data/sentiment_analysis/47/triples/hyperparameters.txt
model,use,stochastic gradient descent optimizer,hyperparameters,/content/training-data/sentiment_analysis/47/triples/hyperparameters.txt
stochastic gradient descent optimizer,with,momentum,hyperparameters,/content/training-data/sentiment_analysis/47/triples/hyperparameters.txt
momentum,of,0.9,hyperparameters,/content/training-data/sentiment_analysis/47/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/47/triples/model.txt
Model,concatenate,component representations,model,/content/training-data/sentiment_analysis/47/triples/model.txt
component representations,as,final representation,model,/content/training-data/sentiment_analysis/47/triples/model.txt
final representation,of,sentence,model,/content/training-data/sentiment_analysis/47/triples/model.txt
component representations,feed it into,softmax layer,model,/content/training-data/sentiment_analysis/47/triples/model.txt
softmax layer,to predict,sentiment polarity,model,/content/training-data/sentiment_analysis/47/triples/model.txt
Model,leads to,two - side representation,model,/content/training-data/sentiment_analysis/47/triples/model.txt
two - side representation,of,target,model,/content/training-data/sentiment_analysis/47/triples/model.txt
two - side representation,name,left - aware target,model,/content/training-data/sentiment_analysis/47/triples/model.txt
two - side representation,name,right - aware target,model,/content/training-data/sentiment_analysis/47/triples/model.txt
Model,design,left - center - right separated LSTMs,model,/content/training-data/sentiment_analysis/47/triples/model.txt
left - center - right separated LSTMs,contains,three LSTMs,model,/content/training-data/sentiment_analysis/47/triples/model.txt
three LSTMs,i.e.,"left - , center - and right - LSTM",model,/content/training-data/sentiment_analysis/47/triples/model.txt
left - center - right separated LSTMs,modeling,three parts,model,/content/training-data/sentiment_analysis/47/triples/model.txt
three parts,of,review,model,/content/training-data/sentiment_analysis/47/triples/model.txt
review,name,left context,model,/content/training-data/sentiment_analysis/47/triples/model.txt
review,name,target phrase,model,/content/training-data/sentiment_analysis/47/triples/model.txt
review,name,right context,model,/content/training-data/sentiment_analysis/47/triples/model.txt
Model,has,target2context attention,model,/content/training-data/sentiment_analysis/47/triples/model.txt
target2context attention,to capture,most indicative sentiment words,model,/content/training-data/sentiment_analysis/47/triples/model.txt
most indicative sentiment words,in,left / right contexts,model,/content/training-data/sentiment_analysis/47/triples/model.txt
Model,has,context2target attention,model,/content/training-data/sentiment_analysis/47/triples/model.txt
context2target attention,to capture,most important word,model,/content/training-data/sentiment_analysis/47/triples/model.txt
most important word,in,target,model,/content/training-data/sentiment_analysis/47/triples/model.txt
Model,propose,left - center - right separated neural network,model,/content/training-data/sentiment_analysis/47/triples/model.txt
left - center - right separated neural network,with,rotatory attention mechanism ( LCR - Rot ,model,/content/training-data/sentiment_analysis/47/triples/model.txt
Model,propose,rotatory attention mechanism,model,/content/training-data/sentiment_analysis/47/triples/model.txt
rotatory attention mechanism,take into account,interaction,model,/content/training-data/sentiment_analysis/47/triples/model.txt
interaction,to better represent,targets and contexts,model,/content/training-data/sentiment_analysis/47/triples/model.txt
interaction,between,targets and contexts,model,/content/training-data/sentiment_analysis/47/triples/model.txt
Contribution,has research problem,Aspect - based Sentiment Analysis,research-problem,/content/training-data/sentiment_analysis/47/triples/research-problem.txt
Contribution,has research problem,sentiment analysis,research-problem,/content/training-data/sentiment_analysis/47/triples/research-problem.txt
Contribution,has research problem,sentiment classification,research-problem,/content/training-data/sentiment_analysis/47/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/47/triples/results.txt
Results,find,Majority method,results,/content/training-data/sentiment_analysis/47/triples/results.txt
Majority method,is,worst,results,/content/training-data/sentiment_analysis/47/triples/results.txt
Majority method,has,majority sentiment polarity,results,/content/training-data/sentiment_analysis/47/triples/results.txt
majority sentiment polarity,occupies,"53.50 % , 65.00 % and 50 %",results,/content/training-data/sentiment_analysis/47/triples/results.txt
"53.50 % , 65.00 % and 50 %",on,"Restaurant , Laptop and Twitter testing datasets",results,/content/training-data/sentiment_analysis/47/triples/results.txt
Results,has,LCR - Rot model,results,/content/training-data/sentiment_analysis/47/triples/results.txt
LCR - Rot model,achieves,best results,results,/content/training-data/sentiment_analysis/47/triples/results.txt
best results,among,all models,results,/content/training-data/sentiment_analysis/47/triples/results.txt
best results,on,all datasets,results,/content/training-data/sentiment_analysis/47/triples/results.txt
Results,has,IAN,results,/content/training-data/sentiment_analysis/47/triples/results.txt
IAN,considers,separate representations,results,/content/training-data/sentiment_analysis/47/triples/results.txt
separate representations,of,targets,results,/content/training-data/sentiment_analysis/47/triples/results.txt
IAN,obtains,better result,results,/content/training-data/sentiment_analysis/47/triples/results.txt
better result,on,Laptop dataset,results,/content/training-data/sentiment_analysis/47/triples/results.txt
Results,has,GRNN - G3,results,/content/training-data/sentiment_analysis/47/triples/results.txt
GRNN - G3,achieves,competitive results,results,/content/training-data/sentiment_analysis/47/triples/results.txt
competitive results,on,all datasets,results,/content/training-data/sentiment_analysis/47/triples/results.txt
Results,has,Our model,results,/content/training-data/sentiment_analysis/47/triples/results.txt
Our model,achieves,significantly better results,results,/content/training-data/sentiment_analysis/47/triples/results.txt
significantly better results,than,feature - enhanced SVM,results,/content/training-data/sentiment_analysis/47/triples/results.txt
Results,has,Simple SVM model,results,/content/training-data/sentiment_analysis/47/triples/results.txt
Simple SVM model,performs,better,results,/content/training-data/sentiment_analysis/47/triples/results.txt
better,than,Majority,results,/content/training-data/sentiment_analysis/47/triples/results.txt
Results,has,basic LSTM approach,results,/content/training-data/sentiment_analysis/47/triples/results.txt
basic LSTM approach,performs,worst,results,/content/training-data/sentiment_analysis/47/triples/results.txt
Results,has,TD - LSTM,results,/content/training-data/sentiment_analysis/47/triples/results.txt
TD - LSTM,obtains,improvement,results,/content/training-data/sentiment_analysis/47/triples/results.txt
improvement,of,1 - 2 %,results,/content/training-data/sentiment_analysis/47/triples/results.txt
1 - 2 %,over,LSTM,results,/content/training-data/sentiment_analysis/47/triples/results.txt
improvement,when,target signals,results,/content/training-data/sentiment_analysis/47/triples/results.txt
target signals,taken into,consideration,results,/content/training-data/sentiment_analysis/47/triples/results.txt
Results,has,MemNet,results,/content/training-data/sentiment_analysis/47/triples/results.txt
MemNet,achieves,better results,results,/content/training-data/sentiment_analysis/47/triples/results.txt
better results,than,other models,results,/content/training-data/sentiment_analysis/47/triples/results.txt
better results,on,Restaurant dataset,results,/content/training-data/sentiment_analysis/47/triples/results.txt
Results,With the help of,feature engineering,results,/content/training-data/sentiment_analysis/47/triples/results.txt
feature engineering,has,Feature - enhanced SVM,results,/content/training-data/sentiment_analysis/47/triples/results.txt
Feature - enhanced SVM,achieves,much better results,results,/content/training-data/sentiment_analysis/47/triples/results.txt
Contribution,has,Approach,approach,/content/training-data/sentiment_analysis/29/triples/approach.txt
Approach,models,aspects and texts simultaneously,approach,/content/training-data/sentiment_analysis/29/triples/approach.txt
aspects and texts simultaneously,using,LSTMs,approach,/content/training-data/sentiment_analysis/29/triples/approach.txt
Approach,based on,long short - term memory ( LSTM ) neural networks,approach,/content/training-data/sentiment_analysis/29/triples/approach.txt
Approach,choose,AOA,approach,/content/training-data/sentiment_analysis/29/triples/approach.txt
AOA,to attend,most important parts,approach,/content/training-data/sentiment_analysis/29/triples/approach.txt
most important parts,in both,aspect and sentence,approach,/content/training-data/sentiment_analysis/29/triples/approach.txt
Approach,has,AOA,approach,/content/training-data/sentiment_analysis/29/triples/approach.txt
AOA,automatically generates,mutual attentions,approach,/content/training-data/sentiment_analysis/29/triples/approach.txt
mutual attentions,but also,text - to - aspect,approach,/content/training-data/sentiment_analysis/29/triples/approach.txt
mutual attentions,not only from,aspect - to - text,approach,/content/training-data/sentiment_analysis/29/triples/approach.txt
Approach,has,target representation and text representation,approach,/content/training-data/sentiment_analysis/29/triples/approach.txt
target representation and text representation,generated from,LSTMs,approach,/content/training-data/sentiment_analysis/29/triples/approach.txt
target representation and text representation,interact with,each other,approach,/content/training-data/sentiment_analysis/29/triples/approach.txt
each other,by,attention - over - attention ( AOA ) module,approach,/content/training-data/sentiment_analysis/29/triples/approach.txt
Contribution,has,Baselines,baselines,/content/training-data/sentiment_analysis/29/triples/baselines.txt
Baselines,has,LSTM,baselines,/content/training-data/sentiment_analysis/29/triples/baselines.txt
LSTM,uses,one LSTM network,baselines,/content/training-data/sentiment_analysis/29/triples/baselines.txt
one LSTM network,to model,sentence,baselines,/content/training-data/sentiment_analysis/29/triples/baselines.txt
LSTM,has,last hidden state,baselines,/content/training-data/sentiment_analysis/29/triples/baselines.txt
last hidden state,used as,sentence representation,baselines,/content/training-data/sentiment_analysis/29/triples/baselines.txt
sentence representation,for,final classification,baselines,/content/training-data/sentiment_analysis/29/triples/baselines.txt
Baselines,has,IAN,baselines,/content/training-data/sentiment_analysis/29/triples/baselines.txt
IAN,uses,two LSTM networks,baselines,/content/training-data/sentiment_analysis/29/triples/baselines.txt
two LSTM networks,to model,sentence and aspect term,baselines,/content/training-data/sentiment_analysis/29/triples/baselines.txt
Baselines,has,ATAE - LSTM,baselines,/content/training-data/sentiment_analysis/29/triples/baselines.txt
ATAE - LSTM,further extends,AT - LSTM,baselines,/content/training-data/sentiment_analysis/29/triples/baselines.txt
AT - LSTM,by appending,aspect embedding,baselines,/content/training-data/sentiment_analysis/29/triples/baselines.txt
aspect embedding,into,each word vector,baselines,/content/training-data/sentiment_analysis/29/triples/baselines.txt
Baselines,has,AT - LSTM,baselines,/content/training-data/sentiment_analysis/29/triples/baselines.txt
AT - LSTM,first models,sentence,baselines,/content/training-data/sentiment_analysis/29/triples/baselines.txt
sentence,via,LSTM model,baselines,/content/training-data/sentiment_analysis/29/triples/baselines.txt
Baselines,has,Majority,baselines,/content/training-data/sentiment_analysis/29/triples/baselines.txt
Majority,is,basic baseline method,baselines,/content/training-data/sentiment_analysis/29/triples/baselines.txt
Majority,assigns,largest sentiment polarity,baselines,/content/training-data/sentiment_analysis/29/triples/baselines.txt
largest sentiment polarity,in,training set,baselines,/content/training-data/sentiment_analysis/29/triples/baselines.txt
training set,to,each sample,baselines,/content/training-data/sentiment_analysis/29/triples/baselines.txt
each sample,in,test set,baselines,/content/training-data/sentiment_analysis/29/triples/baselines.txt
Baselines,has,TD - LSTM,baselines,/content/training-data/sentiment_analysis/29/triples/baselines.txt
TD - LSTM,uses,two LSTM networks,baselines,/content/training-data/sentiment_analysis/29/triples/baselines.txt
two LSTM networks,to model,preceding and following contexts,baselines,/content/training-data/sentiment_analysis/29/triples/baselines.txt
preceding and following contexts,surrounding,aspect term,baselines,/content/training-data/sentiment_analysis/29/triples/baselines.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
Hyperparameters,For,out of vocabulary words,hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
out of vocabulary words,initialize them,randomly,hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
randomly,from,"uniform distribution U ( ? 0.01 , 0.01 ",hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
Hyperparameters,has,training loss,hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
training loss,does not,drop,hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
drop,after,every three epochs,hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
drop,decrease,learning rate,hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
learning rate,by,half,hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
Hyperparameters,has,L 2 regularization coefficient,hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
L 2 regularization coefficient,set to,10 ? 4,hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
Hyperparameters,has,initial learning rate,hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
initial learning rate,is,0.01,hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
0.01,for,Adam optimizer,hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
Hyperparameters,has,weight matrices,hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
weight matrices,randomly initialized from,"uniform distribution U ( ?10 ?4 , 10 ?4 ",hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
Hyperparameters,has,bias terms,hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
bias terms,set to,zero,hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
Hyperparameters,has,batch size,hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
batch size,set as,25,hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
Hyperparameters,has,word embeddings,hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
word embeddings,fixed during,training,hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
word embeddings,initialized with,300 - dimensional Glove vectors,hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
Hyperparameters,has,dropout keep rate,hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
dropout keep rate,set to,0.2,hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
Hyperparameters,has,dimension,hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
dimension,of,LSTM hidden states,hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
LSTM hidden states,set to,150,hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
Hyperparameters,randomly select,20 %,hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
20 %,of,training data,hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
training data,as,validation set,hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
training data,to tune,hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/29/triples/hyperparameters.txt
Contribution,has research problem,Aspect Level Sentiment Classification,research-problem,/content/training-data/sentiment_analysis/29/triples/research-problem.txt
Contribution,has research problem,Aspect - level sentiment classification,research-problem,/content/training-data/sentiment_analysis/29/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/29/triples/results.txt
Results,found that,performance fluctuates,results,/content/training-data/sentiment_analysis/29/triples/results.txt
performance fluctuates,with,different random initialization,results,/content/training-data/sentiment_analysis/29/triples/results.txt
Results,On average,our best trained model,results,/content/training-data/sentiment_analysis/29/triples/results.txt
our best trained model,outperforms them in,large margin,results,/content/training-data/sentiment_analysis/29/triples/results.txt
Results,On average,our algorithm,results,/content/training-data/sentiment_analysis/29/triples/results.txt
our algorithm,better than,baseline methods,results,/content/training-data/sentiment_analysis/29/triples/results.txt
Contribution,has,Approach,approach,/content/training-data/sentiment_analysis/7/triples/approach.txt
Approach,explore,various deep learning based architectures,approach,/content/training-data/sentiment_analysis/7/triples/approach.txt
various deep learning based architectures,combine them in,ensemble based architecture,approach,/content/training-data/sentiment_analysis/7/triples/approach.txt
ensemble based architecture,to allow,training,approach,/content/training-data/sentiment_analysis/7/triples/approach.txt
training,across,different modalities,approach,/content/training-data/sentiment_analysis/7/triples/approach.txt
different modalities,using,variations of the better individual models,approach,/content/training-data/sentiment_analysis/7/triples/approach.txt
Approach,has,Our ensemble,approach,/content/training-data/sentiment_analysis/7/triples/approach.txt
Our ensemble,consists of,Long Short Term Memory networks,approach,/content/training-data/sentiment_analysis/7/triples/approach.txt
Our ensemble,consists of,Convolution Neural Networks,approach,/content/training-data/sentiment_analysis/7/triples/approach.txt
Our ensemble,consists of,fully connected Multi - Layer Perceptrons,approach,/content/training-data/sentiment_analysis/7/triples/approach.txt
Our ensemble,using techniques,Dropout,approach,/content/training-data/sentiment_analysis/7/triples/approach.txt
Our ensemble,using techniques,adaptive optimizers,approach,/content/training-data/sentiment_analysis/7/triples/approach.txt
adaptive optimizers,such as,Adam,approach,/content/training-data/sentiment_analysis/7/triples/approach.txt
Our ensemble,using techniques,pretrained word - embedding models and Attention based RNN decoders,approach,/content/training-data/sentiment_analysis/7/triples/approach.txt
Approach,allows us to,perform feature fusion,approach,/content/training-data/sentiment_analysis/7/triples/approach.txt
perform feature fusion,at,final stage,approach,/content/training-data/sentiment_analysis/7/triples/approach.txt
Approach,allows us to,individually target,approach,/content/training-data/sentiment_analysis/7/triples/approach.txt
individually target,each,modality,approach,/content/training-data/sentiment_analysis/7/triples/approach.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/7/triples/hyperparameters.txt
Hyperparameters,For,Mocap data,hyperparameters,/content/training-data/sentiment_analysis/7/triples/hyperparameters.txt
Mocap data,average each of,200 arrays,hyperparameters,/content/training-data/sentiment_analysis/7/triples/hyperparameters.txt
200 arrays,concatenate all of them to obtain,"( 200,189 ) dimension vector",hyperparameters,/content/training-data/sentiment_analysis/7/triples/hyperparameters.txt
" 200,189 ) dimension vector",for,utterance,hyperparameters,/content/training-data/sentiment_analysis/7/triples/hyperparameters.txt
200 arrays,along,"columns ( 165 for faces , 18 for hands , and 6 for rotation ",hyperparameters,/content/training-data/sentiment_analysis/7/triples/hyperparameters.txt
Mocap data,for,each different mode,hyperparameters,/content/training-data/sentiment_analysis/7/triples/hyperparameters.txt
each different mode,such as,"face , hand , head rotation",hyperparameters,/content/training-data/sentiment_analysis/7/triples/hyperparameters.txt
each different mode,sample,feature values,hyperparameters,/content/training-data/sentiment_analysis/7/triples/hyperparameters.txt
feature values,split them into,200 partitioned arrays,hyperparameters,/content/training-data/sentiment_analysis/7/triples/hyperparameters.txt
feature values,between,start and finish time values,hyperparameters,/content/training-data/sentiment_analysis/7/triples/hyperparameters.txt
Hyperparameters,has,text transcript,hyperparameters,/content/training-data/sentiment_analysis/7/triples/hyperparameters.txt
text transcript,use,pretrained Glove embeddings,hyperparameters,/content/training-data/sentiment_analysis/7/triples/hyperparameters.txt
pretrained Glove embeddings,of,dimension 300,hyperparameters,/content/training-data/sentiment_analysis/7/triples/hyperparameters.txt
pretrained Glove embeddings,along with,maximum sequence length,hyperparameters,/content/training-data/sentiment_analysis/7/triples/hyperparameters.txt
maximum sequence length,of,500,hyperparameters,/content/training-data/sentiment_analysis/7/triples/hyperparameters.txt
pretrained Glove embeddings,to obtain,"( 500,300 ) vector",hyperparameters,/content/training-data/sentiment_analysis/7/triples/hyperparameters.txt
" 500,300 ) vector",for,each utterance,hyperparameters,/content/training-data/sentiment_analysis/7/triples/hyperparameters.txt
text transcript,of,each of the utterance,hyperparameters,/content/training-data/sentiment_analysis/7/triples/hyperparameters.txt
Contribution,has research problem,MULTI - MODAL EMOTION RECOGNITION,research-problem,/content/training-data/sentiment_analysis/7/triples/research-problem.txt
Contribution,has research problem,Emotion recognition,research-problem,/content/training-data/sentiment_analysis/7/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/7/triples/results.txt
Results,has,Our performance,results,/content/training-data/sentiment_analysis/7/triples/results.txt
Our performance,matches,prior state of the art,results,/content/training-data/sentiment_analysis/7/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/sentiment_analysis/1/triples/ablation-analysis.txt
Ablation analysis,has,NodeDAT regularizer,ablation-analysis,/content/training-data/sentiment_analysis/1/triples/ablation-analysis.txt
NodeDAT regularizer,has,noticeable positive impact,ablation-analysis,/content/training-data/sentiment_analysis/1/triples/ablation-analysis.txt
noticeable positive impact,on,performance,ablation-analysis,/content/training-data/sentiment_analysis/1/triples/ablation-analysis.txt
performance,of,our model,ablation-analysis,/content/training-data/sentiment_analysis/1/triples/ablation-analysis.txt
Ablation analysis,has,DL regularizer,ablation-analysis,/content/training-data/sentiment_analysis/1/triples/ablation-analysis.txt
DL regularizer,improves,performance,ablation-analysis,/content/training-data/sentiment_analysis/1/triples/ablation-analysis.txt
performance,of,our model,ablation-analysis,/content/training-data/sentiment_analysis/1/triples/ablation-analysis.txt
our model,by,around 3 %,ablation-analysis,/content/training-data/sentiment_analysis/1/triples/ablation-analysis.txt
around 3 %,in,accuracy,ablation-analysis,/content/training-data/sentiment_analysis/1/triples/ablation-analysis.txt
accuracy,on,both datasets,ablation-analysis,/content/training-data/sentiment_analysis/1/triples/ablation-analysis.txt
Ablation analysis,has,global connection,ablation-analysis,/content/training-data/sentiment_analysis/1/triples/ablation-analysis.txt
global connection,models,asymmetric difference,ablation-analysis,/content/training-data/sentiment_analysis/1/triples/ablation-analysis.txt
asymmetric difference,between,neuronal activities,ablation-analysis,/content/training-data/sentiment_analysis/1/triples/ablation-analysis.txt
neuronal activities,in,left and right hemispheres,ablation-analysis,/content/training-data/sentiment_analysis/1/triples/ablation-analysis.txt
global connection,shown to reveal,certain emotions,ablation-analysis,/content/training-data/sentiment_analysis/1/triples/ablation-analysis.txt
Ablation analysis,has,two major designs,ablation-analysis,/content/training-data/sentiment_analysis/1/triples/ablation-analysis.txt
two major designs,in,adjacency matrix A,ablation-analysis,/content/training-data/sentiment_analysis/1/triples/ablation-analysis.txt
adjacency matrix A,helpful in,recognizing emotions,ablation-analysis,/content/training-data/sentiment_analysis/1/triples/ablation-analysis.txt
adjacency matrix A,i.e.,global connection and symmetric adjacency matrix designs,ablation-analysis,/content/training-data/sentiment_analysis/1/triples/ablation-analysis.txt
Ablation analysis,if,NodeDAT,ablation-analysis,/content/training-data/sentiment_analysis/1/triples/ablation-analysis.txt
NodeDAT,is,removed,ablation-analysis,/content/training-data/sentiment_analysis/1/triples/ablation-analysis.txt
removed,has,performance,ablation-analysis,/content/training-data/sentiment_analysis/1/triples/ablation-analysis.txt
performance,of,our model,ablation-analysis,/content/training-data/sentiment_analysis/1/triples/ablation-analysis.txt
our model,has,greater variance,ablation-analysis,/content/training-data/sentiment_analysis/1/triples/ablation-analysis.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/1/triples/hyperparameters.txt
Hyperparameters,use,Adam optimization,hyperparameters,/content/training-data/sentiment_analysis/1/triples/hyperparameters.txt
Adam optimization,with,default values,hyperparameters,/content/training-data/sentiment_analysis/1/triples/hyperparameters.txt
default values,i.e.,? 1 = 0.9 and ? 2 = 0.999,hyperparameters,/content/training-data/sentiment_analysis/1/triples/hyperparameters.txt
Hyperparameters,has,RGNN,hyperparameters,/content/training-data/sentiment_analysis/1/triples/hyperparameters.txt
RGNN,empirically,set,hyperparameters,/content/training-data/sentiment_analysis/1/triples/hyperparameters.txt
set,has,number,hyperparameters,/content/training-data/sentiment_analysis/1/triples/hyperparameters.txt
number,of,convolutional layers L = 2,hyperparameters,/content/training-data/sentiment_analysis/1/triples/hyperparameters.txt
set,has,batch size,hyperparameters,/content/training-data/sentiment_analysis/1/triples/hyperparameters.txt
batch size,of,16,hyperparameters,/content/training-data/sentiment_analysis/1/triples/hyperparameters.txt
set,has,dropout rate,hyperparameters,/content/training-data/sentiment_analysis/1/triples/hyperparameters.txt
dropout rate,of,0.7,hyperparameters,/content/training-data/sentiment_analysis/1/triples/hyperparameters.txt
0.7,at,output fully - connected layer,hyperparameters,/content/training-data/sentiment_analysis/1/triples/hyperparameters.txt
Hyperparameters,tune,output feature dimension d,hyperparameters,/content/training-data/sentiment_analysis/1/triples/hyperparameters.txt
Hyperparameters,tune,label noise level,hyperparameters,/content/training-data/sentiment_analysis/1/triples/hyperparameters.txt
Hyperparameters,tune,learning rate ?,hyperparameters,/content/training-data/sentiment_analysis/1/triples/hyperparameters.txt
Hyperparameters,tune,L1 regularization factor ?,hyperparameters,/content/training-data/sentiment_analysis/1/triples/hyperparameters.txt
Hyperparameters,tune,L2 regularization,hyperparameters,/content/training-data/sentiment_analysis/1/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/1/triples/model.txt
Model,consider,each channel,model,/content/training-data/sentiment_analysis/1/triples/model.txt
each channel,in,EEG signals,model,/content/training-data/sentiment_analysis/1/triples/model.txt
EEG signals,as,node,model,/content/training-data/sentiment_analysis/1/triples/model.txt
node,in,our graph,model,/content/training-data/sentiment_analysis/1/triples/model.txt
Model,has,Local interchannel relations,model,/content/training-data/sentiment_analysis/1/triples/model.txt
Local interchannel relations,reveal,anatomical connectivity,model,/content/training-data/sentiment_analysis/1/triples/model.txt
anatomical connectivity,at,macroscale,model,/content/training-data/sentiment_analysis/1/triples/model.txt
Local interchannel relations,connect,nearby groups of neurons,model,/content/training-data/sentiment_analysis/1/triples/model.txt
Model,has,RGNN model,model,/content/training-data/sentiment_analysis/1/triples/model.txt
RGNN model,extends,simple graph convolution network ( SGC ,model,/content/training-data/sentiment_analysis/1/triples/model.txt
RGNN model,leverages,topological structure,model,/content/training-data/sentiment_analysis/1/triples/model.txt
topological structure,of,EEG signals,model,/content/training-data/sentiment_analysis/1/triples/model.txt
EEG signals,according to,economy of brain network organization,model,/content/training-data/sentiment_analysis/1/triples/model.txt
economy of brain network organization,propose,biologically supported sparse adjacency matrix,model,/content/training-data/sentiment_analysis/1/triples/model.txt
biologically supported sparse adjacency matrix,to capture,local and global inter-channel relations,model,/content/training-data/sentiment_analysis/1/triples/model.txt
Model,has,Global inter-channel relations,model,/content/training-data/sentiment_analysis/1/triples/model.txt
Global inter-channel relations,connect,distant groups of neurons,model,/content/training-data/sentiment_analysis/1/triples/model.txt
distant groups of neurons,between,left and right hemispheres,model,/content/training-data/sentiment_analysis/1/triples/model.txt
Global inter-channel relations,may reveal,emotion - related functional connectivity,model,/content/training-data/sentiment_analysis/1/triples/model.txt
Model,propose,regularized graph neural network ( RGNN ,model,/content/training-data/sentiment_analysis/1/triples/model.txt
Contribution,has research problem,EEG - Based Emotion Recognition,research-problem,/content/training-data/sentiment_analysis/1/triples/research-problem.txt
Contribution,has research problem,E MOTION recognition,research-problem,/content/training-data/sentiment_analysis/1/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/1/triples/results.txt
Results,In,subject - independent experiments,results,/content/training-data/sentiment_analysis/1/triples/results.txt
subject - independent experiments,on,SEED,results,/content/training-data/sentiment_analysis/1/triples/results.txt
SEED,has,BiDANN - S,results,/content/training-data/sentiment_analysis/1/triples/results.txt
BiDANN - S,achieves,highest accuracy,results,/content/training-data/sentiment_analysis/1/triples/results.txt
highest accuracy,in,theta and alpha bands,results,/content/training-data/sentiment_analysis/1/triples/results.txt
SEED,has,our model,results,/content/training-data/sentiment_analysis/1/triples/results.txt
our model,performs,best,results,/content/training-data/sentiment_analysis/1/triples/results.txt
best,in,"delta , beta and gamma bands",results,/content/training-data/sentiment_analysis/1/triples/results.txt
Results,In,subject - dependent experiments,results,/content/training-data/sentiment_analysis/1/triples/results.txt
subject - dependent experiments,on,SEED,results,/content/training-data/sentiment_analysis/1/triples/results.txt
SEED,has,BiDANN,results,/content/training-data/sentiment_analysis/1/triples/results.txt
BiDANN,performs,best,results,/content/training-data/sentiment_analysis/1/triples/results.txt
best,in,beta band,results,/content/training-data/sentiment_analysis/1/triples/results.txt
SEED,has,STRNN,results,/content/training-data/sentiment_analysis/1/triples/results.txt
STRNN,achieves,highest accuracy,results,/content/training-data/sentiment_analysis/1/triples/results.txt
highest accuracy,in,"delta , theta and alpha bands",results,/content/training-data/sentiment_analysis/1/triples/results.txt
SEED,has,our model,results,/content/training-data/sentiment_analysis/1/triples/results.txt
our model,performs,best,results,/content/training-data/sentiment_analysis/1/triples/results.txt
best,in,gamma band,results,/content/training-data/sentiment_analysis/1/triples/results.txt
Results,For,subject - dependent and subjectindependent settings,results,/content/training-data/sentiment_analysis/1/triples/results.txt
subject - dependent and subjectindependent settings,on,SEED,results,/content/training-data/sentiment_analysis/1/triples/results.txt
SEED,has,our model,results,/content/training-data/sentiment_analysis/1/triples/results.txt
our model,achieve,better performance,results,/content/training-data/sentiment_analysis/1/triples/results.txt
better performance,with one exception of,STRNN,results,/content/training-data/sentiment_analysis/1/triples/results.txt
STRNN,performs,worst,results,/content/training-data/sentiment_analysis/1/triples/results.txt
worst,on,gamma band,results,/content/training-data/sentiment_analysis/1/triples/results.txt
better performance,on,beta and gamma bands,results,/content/training-data/sentiment_analysis/1/triples/results.txt
beta and gamma bands,than,"delta , theta and alpha bands",results,/content/training-data/sentiment_analysis/1/triples/results.txt
Results,has,our model,results,/content/training-data/sentiment_analysis/1/triples/results.txt
our model,performs,better,results,/content/training-data/sentiment_analysis/1/triples/results.txt
better,than,DGCNN,results,/content/training-data/sentiment_analysis/1/triples/results.txt
DGCNN,another,GNN - based model,results,/content/training-data/sentiment_analysis/1/triples/results.txt
GNN - based model,leverages,topological structure,results,/content/training-data/sentiment_analysis/1/triples/results.txt
topological structure,in,EEG signals,results,/content/training-data/sentiment_analysis/1/triples/results.txt
our model,performs,consistently better,results,/content/training-data/sentiment_analysis/1/triples/results.txt
consistently better,in,gamma band,results,/content/training-data/sentiment_analysis/1/triples/results.txt
gamma band,than,beta band,results,/content/training-data/sentiment_analysis/1/triples/results.txt
our model,improves,accuracy,results,/content/training-data/sentiment_analysis/1/triples/results.txt
accuracy,of,state - of - the - art model,results,/content/training-data/sentiment_analysis/1/triples/results.txt
state - of - the - art model,on,SEED - IV,results,/content/training-data/sentiment_analysis/1/triples/results.txt
SEED - IV,by,around 5 %,results,/content/training-data/sentiment_analysis/1/triples/results.txt
our model,achieves,superior performance,results,/content/training-data/sentiment_analysis/1/triples/results.txt
superior performance,on,both datasets,results,/content/training-data/sentiment_analysis/1/triples/results.txt
both datasets,compared to,all baselines,results,/content/training-data/sentiment_analysis/1/triples/results.txt
all baselines,including,stateof - the - art BiHDM when DE features from all frequency bands are used,results,/content/training-data/sentiment_analysis/1/triples/results.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/23/triples/hyperparameters.txt
Hyperparameters,In,our d-TBCNN model,hyperparameters,/content/training-data/sentiment_analysis/23/triples/hyperparameters.txt
our d-TBCNN model,has,number of units,hyperparameters,/content/training-data/sentiment_analysis/23/triples/hyperparameters.txt
number of units,is,200,hyperparameters,/content/training-data/sentiment_analysis/23/triples/hyperparameters.txt
200,for,last hidden layer,hyperparameters,/content/training-data/sentiment_analysis/23/triples/hyperparameters.txt
number of units,is,300,hyperparameters,/content/training-data/sentiment_analysis/23/triples/hyperparameters.txt
300,for,convolution,hyperparameters,/content/training-data/sentiment_analysis/23/triples/hyperparameters.txt
Hyperparameters,use,ReLU,hyperparameters,/content/training-data/sentiment_analysis/23/triples/hyperparameters.txt
ReLU,as,activation function,hyperparameters,/content/training-data/sentiment_analysis/23/triples/hyperparameters.txt
Hyperparameters,For,regularization,hyperparameters,/content/training-data/sentiment_analysis/23/triples/hyperparameters.txt
regularization,add,2 penalty,hyperparameters,/content/training-data/sentiment_analysis/23/triples/hyperparameters.txt
2 penalty,for,weights,hyperparameters,/content/training-data/sentiment_analysis/23/triples/hyperparameters.txt
weights,with,coefficient,hyperparameters,/content/training-data/sentiment_analysis/23/triples/hyperparameters.txt
coefficient,of,10 ?5,hyperparameters,/content/training-data/sentiment_analysis/23/triples/hyperparameters.txt
Hyperparameters,To train,our model,hyperparameters,/content/training-data/sentiment_analysis/23/triples/hyperparameters.txt
our model,compute,gradient,hyperparameters,/content/training-data/sentiment_analysis/23/triples/hyperparameters.txt
gradient,by,back - propagation,hyperparameters,/content/training-data/sentiment_analysis/23/triples/hyperparameters.txt
our model,apply,stochastic gradient descent,hyperparameters,/content/training-data/sentiment_analysis/23/triples/hyperparameters.txt
stochastic gradient descent,with,mini-batch 200,hyperparameters,/content/training-data/sentiment_analysis/23/triples/hyperparameters.txt
Hyperparameters,has,embeddings,hyperparameters,/content/training-data/sentiment_analysis/23/triples/hyperparameters.txt
embeddings,has,40 %,hyperparameters,/content/training-data/sentiment_analysis/23/triples/hyperparameters.txt
Hyperparameters,has,All hidden layers,hyperparameters,/content/training-data/sentiment_analysis/23/triples/hyperparameters.txt
All hidden layers,dropped out by,50 %,hyperparameters,/content/training-data/sentiment_analysis/23/triples/hyperparameters.txt
Hyperparameters,has,Word embeddings,hyperparameters,/content/training-data/sentiment_analysis/23/triples/hyperparameters.txt
Word embeddings,are,300 dimensional,hyperparameters,/content/training-data/sentiment_analysis/23/triples/hyperparameters.txt
Word embeddings,pretrained ourselves using,word2vec,hyperparameters,/content/training-data/sentiment_analysis/23/triples/hyperparameters.txt
Hyperparameters,has,Dropout,hyperparameters,/content/training-data/sentiment_analysis/23/triples/hyperparameters.txt
Dropout,applied to,both weights and embeddings,hyperparameters,/content/training-data/sentiment_analysis/23/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/23/triples/model.txt
Model,leverage,different sentence parsing trees,model,/content/training-data/sentiment_analysis/23/triples/model.txt
Model,called,Tree - Based Convolutional Neural Network ( TBCNN ,model,/content/training-data/sentiment_analysis/23/triples/model.txt
Model,has,tree - based convolution,model,/content/training-data/sentiment_analysis/23/triples/model.txt
tree - based convolution,apply,set of subtree feature detectors,model,/content/training-data/sentiment_analysis/23/triples/model.txt
set of subtree feature detectors,sliding over,entire parsing tree,model,/content/training-data/sentiment_analysis/23/triples/model.txt
entire parsing tree,of,sentence,model,/content/training-data/sentiment_analysis/23/triples/model.txt
set of subtree feature detectors,pooling,aggregates these extracted feature vectors,model,/content/training-data/sentiment_analysis/23/triples/model.txt
aggregates these extracted feature vectors,taking,maximum value,model,/content/training-data/sentiment_analysis/23/triples/model.txt
maximum value,in,each dimension,model,/content/training-data/sentiment_analysis/23/triples/model.txt
Model,has,variants,model,/content/training-data/sentiment_analysis/23/triples/model.txt
variants,denoted as,c- TBCNN,model,/content/training-data/sentiment_analysis/23/triples/model.txt
variants,denoted as,d - TBCNN,model,/content/training-data/sentiment_analysis/23/triples/model.txt
Model,propose,novel neural architecture,model,/content/training-data/sentiment_analysis/23/triples/model.txt
novel neural architecture,for,discriminative sentence modeling,model,/content/training-data/sentiment_analysis/23/triples/model.txt
Contribution,has research problem,Discriminative Neural Sentence Modeling,research-problem,/content/training-data/sentiment_analysis/23/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/23/triples/results.txt
Results,In,more controlled comparison,results,/content/training-data/sentiment_analysis/23/triples/results.txt
more controlled comparison,with,shallow architectures,results,/content/training-data/sentiment_analysis/23/triples/results.txt
more controlled comparison,with,basic interaction ( linearly transformed and non-linearly squashed ,results,/content/training-data/sentiment_analysis/23/triples/results.txt
more controlled comparison,has,TBCNNs,results,/content/training-data/sentiment_analysis/23/triples/results.txt
TBCNNs,consistently outperform,RNNs,results,/content/training-data/sentiment_analysis/23/triples/results.txt
RNNs,to,large extent ( 50.4 - 51.4 % versus 43.2 % ,results,/content/training-data/sentiment_analysis/23/triples/results.txt
TBCNNs,consistently outperform,""" flat "" CNNs",results,/content/training-data/sentiment_analysis/23/triples/results.txt
""" flat "" CNNs",by more than,10 %,results,/content/training-data/sentiment_analysis/23/triples/results.txt
Results,has,d-TBCNN model,results,/content/training-data/sentiment_analysis/23/triples/results.txt
d-TBCNN model,achieves,87.9 % accuracy,results,/content/training-data/sentiment_analysis/23/triples/results.txt
Results,observe,d- TBCNN,results,/content/training-data/sentiment_analysis/23/triples/results.txt
d- TBCNN,achieves,higher performance,results,/content/training-data/sentiment_analysis/23/triples/results.txt
higher performance,than,c - TBCNN,results,/content/training-data/sentiment_analysis/23/triples/results.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/26/triples/model.txt
Model,investigate,extrinsic signals,model,/content/training-data/sentiment_analysis/26/triples/model.txt
extrinsic signals,incorporated into,deep neural networks,model,/content/training-data/sentiment_analysis/26/triples/model.txt
deep neural networks,for,sentiment analysis,model,/content/training-data/sentiment_analysis/26/triples/model.txt
Model,consider,word embeddings,model,/content/training-data/sentiment_analysis/26/triples/model.txt
word embeddings,lead to,stronger and more consistent gains,model,/content/training-data/sentiment_analysis/26/triples/model.txt
stronger and more consistent gains,obtained using,out - of - domain data,model,/content/training-data/sentiment_analysis/26/triples/model.txt
word embeddings,specialized for,sentiment analysis,model,/content/training-data/sentiment_analysis/26/triples/model.txt
Model,propose,bespoke convolutional neural network architecture,model,/content/training-data/sentiment_analysis/26/triples/model.txt
bespoke convolutional neural network architecture,with,separate memory module,model,/content/training-data/sentiment_analysis/26/triples/model.txt
separate memory module,dedicated to,sentiment embeddings,model,/content/training-data/sentiment_analysis/26/triples/model.txt
Contribution,has research problem,Deep Sentiment Analysis,research-problem,/content/training-data/sentiment_analysis/26/triples/research-problem.txt
Contribution,has research problem,sentiment analysis,research-problem,/content/training-data/sentiment_analysis/26/triples/research-problem.txt
Contribution,has research problem,supervised sentiment polarity classification,research-problem,/content/training-data/sentiment_analysis/26/triples/research-problem.txt
Contribution,has,Experimental Setup,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
Experimental Setup,For,DM - MCNN models,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
DM - MCNN models,has,configuration,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
configuration,of,convolutional module,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
convolutional module,same as for,CNNs,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
DM - MCNN models,has,remaining hyperparameter values,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
remaining hyperparameter values,tuned on,validation sets,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
Experimental Setup,For,greater efficiency and better convergence properties,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
greater efficiency and better convergence properties,has,training,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
training,relies on,mini-batches,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
Experimental Setup,For,CNNs,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
CNNs,make use of,well - known CNN - non-static architecture and hyperparameters,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
well - known CNN - non-static architecture and hyperparameters,with,learning rate,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
learning rate,of,0.0006,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
0.0006,obtained by,tuning,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
tuning,on,validation data,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
Experimental Setup,has,Embeddings,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
Embeddings,For,cross - lingual projection,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
cross - lingual projection,extract,links,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
links,between,words,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
words,from,2017 dump of the English edition of Wiktionary,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
Embeddings,For,transfer learning approach,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
transfer learning approach,rely on,multi-domain sentiment dataset,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
multi-domain sentiment dataset,collected from,Amazon customers reviews,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
Embeddings,has,vectors,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
vectors,fed to,CNN,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
vectors,fed to,convolutional module,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
convolutional module,of,DM - MCNN,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
DM - MCNN,during,initialization,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
Embeddings,has,standard pre-trained word vectors,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
standard pre-trained word vectors,are,300 - dimensional,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
standard pre-trained word vectors,for,English,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
English,are,GloVe ones,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
GloVe ones,trained on,840 billion tokens,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
840 billion tokens,of,Common Crawl data,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
standard pre-trained word vectors,for,other languages,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
other languages,rely on,Facebook fastText Wikipedia embeddings,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
Facebook fastText Wikipedia embeddings,as,input representations,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
Embeddings,has,All words,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
All words,including,unknown ones,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
All words,are,fine - tuned,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
fine - tuned,during,training process,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
Embeddings,has,unknown words,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
unknown words,initialized with,zeros,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
Embeddings,consider,several alternative forms,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
several alternative forms,of infusing,external cues,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
Embeddings,consider,sentiment lexicon,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
sentiment lexicon,called,VADER,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
VADER,contain,separate domain - specific scores,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
separate domain - specific scores,for,250 different Reddit communities,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
250 different Reddit communities,result in,250 - dimensional embeddings,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
Embeddings,restrict,vocabulary link,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
vocabulary link,set to include,languages,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
vocabulary link,mining,Wiktionary,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
Wiktionary,corresponding,translation,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
Wiktionary,corresponding,synonymy,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
Wiktionary,corresponding,derivation,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
Wiktionary,corresponding,etymological links,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
Embeddings,train,linear SVMs,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
linear SVMs,using,scikit - learn,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
scikit - learn,to extract,word coefficients,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
word coefficients,yielding,26 - dimensional sentiment embedding,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
word coefficients,in,each domain,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
word coefficients,also for,union of all domains,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
Experimental Setup,has,implementation,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
implementation,considers,maximal sentence length,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
maximal sentence length,in,each mini-batch,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
implementation,zero - pads,all other sentences,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
all other sentences,to,this length,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
this length,under,convolutional module,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
convolutional module,enabling,uniform and fast processing,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
uniform and fast processing,of,each mini-batch,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
Experimental Setup,has,neural network architectures,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
neural network architectures,implemented using,PyTorch,experimental-setup,/content/training-data/sentiment_analysis/26/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/26/triples/results.txt
Results,shows,word vectors,results,/content/training-data/sentiment_analysis/26/triples/results.txt
word vectors,tend to convey,pertinent word semantics signals,results,/content/training-data/sentiment_analysis/26/triples/results.txt
pertinent word semantics signals,that enable,models,results,/content/training-data/sentiment_analysis/26/triples/results.txt
models,to generalize,better,results,/content/training-data/sentiment_analysis/26/triples/results.txt
Results,Comparing this to,CNNs,results,/content/training-data/sentiment_analysis/26/triples/results.txt
CNNs,with,GloVe / fastText embeddings,results,/content/training-data/sentiment_analysis/26/triples/results.txt
GloVe / fastText embeddings,where,fastText,results,/content/training-data/sentiment_analysis/26/triples/results.txt
fastText,used for,all other languages,results,/content/training-data/sentiment_analysis/26/triples/results.txt
GloVe / fastText embeddings,where,Glo Ve,results,/content/training-data/sentiment_analysis/26/triples/results.txt
Glo Ve,used for,English,results,/content/training-data/sentiment_analysis/26/triples/results.txt
CNNs,observe,substantial improvements,results,/content/training-data/sentiment_analysis/26/triples/results.txt
substantial improvements,across all,datasets,results,/content/training-data/sentiment_analysis/26/triples/results.txt
Results,Note,accuracy,results,/content/training-data/sentiment_analysis/26/triples/results.txt
accuracy,using,GloVe,results,/content/training-data/sentiment_analysis/26/triples/results.txt
GloVe,on,English movies review dataset,results,/content/training-data/sentiment_analysis/26/triples/results.txt
English movies review dataset,consistent with,numbers,results,/content/training-data/sentiment_analysis/26/triples/results.txt
numbers,reported in,previous work,results,/content/training-data/sentiment_analysis/26/triples/results.txt
Results,consider,our DM - MCNNs,results,/content/training-data/sentiment_analysis/26/triples/results.txt
our DM - MCNNs,with,dual - module mechanism,results,/content/training-data/sentiment_analysis/26/triples/results.txt
Results,has,automatically projected cross - lingual embeddings,results,/content/training-data/sentiment_analysis/26/triples/results.txt
automatically projected cross - lingual embeddings,are,very noisy and limited in their coverage,results,/content/training-data/sentiment_analysis/26/triples/results.txt
very noisy and limited in their coverage,particularly with respect to,inflected forms,results,/content/training-data/sentiment_analysis/26/triples/results.txt
automatically projected cross - lingual embeddings,has,our model,results,/content/training-data/sentiment_analysis/26/triples/results.txt
our model,succeeds in exploiting them to obtain,substantial gains,results,/content/training-data/sentiment_analysis/26/triples/results.txt
substantial gains,in,several different languages and domains,results,/content/training-data/sentiment_analysis/26/triples/results.txt
Results,observe,fairly consistent and sometimes quite substan - tial gains,results,/content/training-data/sentiment_analysis/26/triples/results.txt
fairly consistent and sometimes quite substan - tial gains,over,CNNs,results,/content/training-data/sentiment_analysis/26/triples/results.txt
CNNs,with just,GloVe / fastText vectors,results,/content/training-data/sentiment_analysis/26/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
Baselines,has,BERT - ADA,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
BERT - ADA,is,domain - adapted BERT - based model,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
domain - adapted BERT - based model,finetuned,BERT - BASE model,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
BERT - BASE model,on,task - related corpus,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
domain - adapted BERT - based model,proposed for,APC task,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
Baselines,has,LCF - ATE,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
LCF - ATE,are,variations of the LCF - ATEPC model,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
variations of the LCF - ATEPC model,optimize for,ATE task,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
Baselines,has,ATAE - LSTM,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
ATAE - LSTM,is,classical LSTM - based network,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
classical LSTM - based network,applies,attention mechanism,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
attention mechanism,to focus on,important words,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
important words,in,context,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
classical LSTM - based network,for,APC task,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
Baselines,has,ATSM -S,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
ATSM -S,is,baseline model,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
baseline model,of,ATSM variations,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
ATSM variations,for,Chinese language - oriented ABSA task,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
Baselines,has,LCF - APC,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
LCF - APC,are,variations of LCF - ATEPC,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
variations of LCF - ATEPC,optimize for,APC task,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
APC task,during,training process,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
Baselines,has,LCF - ATEPC,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
LCF - ATEPC,is,multi -task learning model,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
multi -task learning model,for,ATE and APC tasks,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
Baselines,has,AEN,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
AEN,is,attentional encoder network,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
attentional encoder network,aims to solve,aspect polarity classification,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
attentional encoder network,based on,pretrained BERT model,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
Baselines,has,BERT,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
BERT,is,BERT - adapted model,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
BERT - adapted model,for,Review Reading Comprehension ( RRC ) task,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
Baselines,has,GANN,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
GANN,is,novel neural network model,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
novel neural network model,aimed to solve,shortcomings of traditional RNNs and CNNs,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
novel neural network model,for,APC task,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
Baselines,has,BERT - BASE,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
BERT - BASE,adapt it to,ABSA multi-task learning,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
ABSA multi-task learning,equips,same ability,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
same ability,to automatically extract,aspect terms,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
same ability,classify,aspects polarity,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
aspects polarity,as,LCF - ATEPC model,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
BERT - BASE,is,basic pretrained BERT model,baselines,/content/training-data/sentiment_analysis/9/triples/baselines.txt
Contribution,Code,https://github.com/yangheng95/LCF-ATEPC,code,/content/training-data/sentiment_analysis/9/triples/code.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/9/triples/model.txt
Model,training on,small amount of annotated data of aspect and their polarity,model,/content/training-data/sentiment_analysis/9/triples/model.txt
small amount of annotated data of aspect and their polarity,adapted to,large - scale dataset,model,/content/training-data/sentiment_analysis/9/triples/model.txt
large - scale dataset,automatically extracting,aspects,model,/content/training-data/sentiment_analysis/9/triples/model.txt
large - scale dataset,predicting,sentiment polarities,model,/content/training-data/sentiment_analysis/9/triples/model.txt
Model,based on,multi-head self - attention ( MHSA ,model,/content/training-data/sentiment_analysis/9/triples/model.txt
Model,proposes,multi-task learning model,model,/content/training-data/sentiment_analysis/9/triples/model.txt
multi-task learning model,for,aspect - based sentiment analysis,model,/content/training-data/sentiment_analysis/9/triples/model.txt
Model,has,LCF - ATEPC 3 model,model,/content/training-data/sentiment_analysis/9/triples/model.txt
LCF - ATEPC 3 model,is,novel multilingual and multi-task - oriented model,model,/content/training-data/sentiment_analysis/9/triples/model.txt
Model,integrates,pre-trained and the local context focus mechanism,model,/content/training-data/sentiment_analysis/9/triples/model.txt
pre-trained and the local context focus mechanism,namely,LCF - ATEPC,model,/content/training-data/sentiment_analysis/9/triples/model.txt
Contribution,has research problem,Aspect Polarity Classification and Aspect Term Extraction,research-problem,/content/training-data/sentiment_analysis/9/triples/research-problem.txt
Contribution,has research problem,Aspect - based sentiment analysis ( ABSA ,research-problem,/content/training-data/sentiment_analysis/9/triples/research-problem.txt
Contribution,has research problem,aspect term extraction ( ATE ,research-problem,/content/training-data/sentiment_analysis/9/triples/research-problem.txt
Contribution,has research problem,aspect polarity classification ( APC ,research-problem,/content/training-data/sentiment_analysis/9/triples/research-problem.txt
Contribution,has research problem,aspect term polarity inferring,research-problem,/content/training-data/sentiment_analysis/9/triples/research-problem.txt
Contribution,has research problem,aspect term extraction,research-problem,/content/training-data/sentiment_analysis/9/triples/research-problem.txt
Contribution,has research problem,aspect polarity classification,research-problem,/content/training-data/sentiment_analysis/9/triples/research-problem.txt
Contribution,has research problem,Aspect - based sentiment analysis,research-problem,/content/training-data/sentiment_analysis/9/triples/research-problem.txt
Contribution,has research problem,APC,research-problem,/content/training-data/sentiment_analysis/9/triples/research-problem.txt
Contribution,has research problem,ATE,research-problem,/content/training-data/sentiment_analysis/9/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/9/triples/results.txt
Results,for,first time,results,/content/training-data/sentiment_analysis/9/triples/results.txt
first time,has,BERT - SPC,results,/content/training-data/sentiment_analysis/9/triples/results.txt
BERT - SPC,has increased,F 1 score,results,/content/training-data/sentiment_analysis/9/triples/results.txt
F 1 score,up to,99 %,results,/content/training-data/sentiment_analysis/9/triples/results.txt
F 1 score,of,ATE subtask,results,/content/training-data/sentiment_analysis/9/triples/results.txt
ATE subtask,on,three datasets,results,/content/training-data/sentiment_analysis/9/triples/results.txt
Results,has,ATEPC - Fusion,results,/content/training-data/sentiment_analysis/9/triples/results.txt
ATEPC - Fusion,adopts,moderate approach,results,/content/training-data/sentiment_analysis/9/triples/results.txt
moderate approach,to generate,local context features,results,/content/training-data/sentiment_analysis/9/triples/results.txt
ATEPC - Fusion,show,performance,results,/content/training-data/sentiment_analysis/9/triples/results.txt
performance,better than,existing BERT - based models,results,/content/training-data/sentiment_analysis/9/triples/results.txt
ATEPC - Fusion,is,supplementary scheme,results,/content/training-data/sentiment_analysis/9/triples/results.txt
supplementary scheme,of,LCF mechanism,results,/content/training-data/sentiment_analysis/9/triples/results.txt
Results,has,joint model based on BERT - BASE,results,/content/training-data/sentiment_analysis/9/triples/results.txt
joint model based on BERT - BASE,even surpassed,other proposed BERT based improved models,results,/content/training-data/sentiment_analysis/9/triples/results.txt
other proposed BERT based improved models,on,some datasets,results,/content/training-data/sentiment_analysis/9/triples/results.txt
joint model based on BERT - BASE,achieved,hopeful performance,results,/content/training-data/sentiment_analysis/9/triples/results.txt
hopeful performance,on,all three datasets,results,/content/training-data/sentiment_analysis/9/triples/results.txt
Results,has,CDM layer,results,/content/training-data/sentiment_analysis/9/triples/results.txt
CDM layer,works better on,twitter dataset,results,/content/training-data/sentiment_analysis/9/triples/results.txt
Results,Compared with,BERT - BASE model,results,/content/training-data/sentiment_analysis/9/triples/results.txt
BERT - BASE model,has,BERT - SPC,results,/content/training-data/sentiment_analysis/9/triples/results.txt
BERT - SPC,significantly improves,accuracy and F 1 score,results,/content/training-data/sentiment_analysis/9/triples/results.txt
accuracy and F 1 score,of,aspect polarity classification,results,/content/training-data/sentiment_analysis/9/triples/results.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
Hyperparameters,trained,all models,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
all models,with,L 2 - regularization weight,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
L 2 - regularization weight,of,0.001,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
all models,with,initial learning rate,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
initial learning rate,of,0.01,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
0.01,for,AdaGrad,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
all models,with,batch size,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
batch size,of,25 examples,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
all models,with,momentum,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
momentum,of,0.9,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
Hyperparameters,apply,proposed model,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
proposed model,to,aspect - level sentiment classification,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
Hyperparameters,In,our experiments,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
our experiments,has,all word vectors,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
all word vectors,are,initialized,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
initialized,by,Glove,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
Hyperparameters,has,Theano,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
Theano,used for,implementing,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
implementing,has,neural network models,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
Hyperparameters,has,word embedding vectors,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
word embedding vectors,pre-trained on,unlabeled corpus,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
unlabeled corpus,whose,size,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
size,is,about 840 billion,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
Hyperparameters,has,length,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
length,of,attention weights,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
attention weights,same as,length,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
length,of,sentence,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
Hyperparameters,has,other parameters,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
other parameters,initialized by,sampling,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
sampling,from,"uniform distribution U (?? , ? ",hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
Hyperparameters,has,dimension,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
dimension,are,300,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
300,of,word vectors,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
300,of,aspect embeddings,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
300,of,size of hidden layer,hyperparameters,/content/training-data/sentiment_analysis/41/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/41/triples/model.txt
Model,explore,potential correlation,model,/content/training-data/sentiment_analysis/41/triples/model.txt
potential correlation,of,aspect and sentiment polarity,model,/content/training-data/sentiment_analysis/41/triples/model.txt
aspect and sentiment polarity,in,aspect - level sentiment classification,model,/content/training-data/sentiment_analysis/41/triples/model.txt
Model,design,aspect - tosentence attention mechanism,model,/content/training-data/sentiment_analysis/41/triples/model.txt
aspect - tosentence attention mechanism,can concentrate on,key part,model,/content/training-data/sentiment_analysis/41/triples/model.txt
key part,given,aspect,model,/content/training-data/sentiment_analysis/41/triples/model.txt
key part,of,sentence,model,/content/training-data/sentiment_analysis/41/triples/model.txt
Model,to capture,important information,model,/content/training-data/sentiment_analysis/41/triples/model.txt
important information,in response to,given aspect,model,/content/training-data/sentiment_analysis/41/triples/model.txt
Model,to capture,design,model,/content/training-data/sentiment_analysis/41/triples/model.txt
design,has,attentionbased LSTM,model,/content/training-data/sentiment_analysis/41/triples/model.txt
Model,propose,attention mechanism,model,/content/training-data/sentiment_analysis/41/triples/model.txt
attention mechanism,to attend,important part of a sentence,model,/content/training-data/sentiment_analysis/41/triples/model.txt
attention mechanism,to enforce,model,model,/content/training-data/sentiment_analysis/41/triples/model.txt
Contribution,has research problem,Aspect - level Sentiment Classification,research-problem,/content/training-data/sentiment_analysis/41/triples/research-problem.txt
Contribution,has research problem,sentiment analysis,research-problem,/content/training-data/sentiment_analysis/41/triples/research-problem.txt
Contribution,has research problem,aspect - level sentiment classification,research-problem,/content/training-data/sentiment_analysis/41/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/41/triples/results.txt
Results,has,LSTM,results,/content/training-data/sentiment_analysis/41/triples/results.txt
LSTM,can not take advantage of,aspect information,results,/content/training-data/sentiment_analysis/41/triples/results.txt
aspect information,has,worst performance,results,/content/training-data/sentiment_analysis/41/triples/results.txt
LSTM,has,Standard LSTM,results,/content/training-data/sentiment_analysis/41/triples/results.txt
Standard LSTM,can not capture,aspect information,results,/content/training-data/sentiment_analysis/41/triples/results.txt
aspect information,in,sentence,results,/content/training-data/sentiment_analysis/41/triples/results.txt
Results,has,ATAE - LSTM,results,/content/training-data/sentiment_analysis/41/triples/results.txt
ATAE - LSTM,addresses,shortcoming,results,/content/training-data/sentiment_analysis/41/triples/results.txt
shortcoming,of,unconformity,results,/content/training-data/sentiment_analysis/41/triples/results.txt
unconformity,between,word vectors and aspect embeddings,results,/content/training-data/sentiment_analysis/41/triples/results.txt
ATAE - LSTM,capture,most important information,results,/content/training-data/sentiment_analysis/41/triples/results.txt
most important information,in response to,given aspect,results,/content/training-data/sentiment_analysis/41/triples/results.txt
Results,has,TD - LSTM,results,/content/training-data/sentiment_analysis/41/triples/results.txt
TD - LSTM,there is no,attention mechanism,results,/content/training-data/sentiment_analysis/41/triples/results.txt
attention mechanism,in,TD - LSTM,results,/content/training-data/sentiment_analysis/41/triples/results.txt
TD - LSTM,can not,know,results,/content/training-data/sentiment_analysis/41/triples/results.txt
know,which,words,results,/content/training-data/sentiment_analysis/41/triples/results.txt
words,are,important,results,/content/training-data/sentiment_analysis/41/triples/results.txt
important,for,given aspect,results,/content/training-data/sentiment_analysis/41/triples/results.txt
TD - LSTM,can improve,performance,results,/content/training-data/sentiment_analysis/41/triples/results.txt
performance,of,sentiment classifier,results,/content/training-data/sentiment_analysis/41/triples/results.txt
performance,by treating,aspect,results,/content/training-data/sentiment_analysis/41/triples/results.txt
aspect,as,target,results,/content/training-data/sentiment_analysis/41/triples/results.txt
Results,has,TC - LSTM,results,/content/training-data/sentiment_analysis/41/triples/results.txt
TC - LSTM,performs,worse,results,/content/training-data/sentiment_analysis/41/triples/results.txt
worse,than,LSTM and TD - LSTM,results,/content/training-data/sentiment_analysis/41/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/sentiment_analysis/31/triples/baselines.txt
Baselines,has,CNN,baselines,/content/training-data/sentiment_analysis/31/triples/baselines.txt
CNN,uses,architecture,baselines,/content/training-data/sentiment_analysis/31/triples/baselines.txt
architecture,without explicitly considering,aspect,baselines,/content/training-data/sentiment_analysis/31/triples/baselines.txt
Baselines,has,AF - LSTM,baselines,/content/training-data/sentiment_analysis/31/triples/baselines.txt
AF - LSTM,introduces,word - aspect fusion attention,baselines,/content/training-data/sentiment_analysis/31/triples/baselines.txt
word - aspect fusion attention,to learn,associative relationships,baselines,/content/training-data/sentiment_analysis/31/triples/baselines.txt
associative relationships,between,aspect and context words,baselines,/content/training-data/sentiment_analysis/31/triples/baselines.txt
Baselines,has,ATAE - LSTM,baselines,/content/training-data/sentiment_analysis/31/triples/baselines.txt
ATAE - LSTM,extends,AT - LSTM,baselines,/content/training-data/sentiment_analysis/31/triples/baselines.txt
AT - LSTM,by appending,aspect embedding,baselines,/content/training-data/sentiment_analysis/31/triples/baselines.txt
aspect embedding,into,each word vector,baselines,/content/training-data/sentiment_analysis/31/triples/baselines.txt
Baselines,has,AT - LSTM,baselines,/content/training-data/sentiment_analysis/31/triples/baselines.txt
AT - LSTM,combines,sentence hidden states,baselines,/content/training-data/sentiment_analysis/31/triples/baselines.txt
sentence hidden states,with,aspect term embedding,baselines,/content/training-data/sentiment_analysis/31/triples/baselines.txt
aspect term embedding,to generate,attention vector,baselines,/content/training-data/sentiment_analysis/31/triples/baselines.txt
sentence hidden states,from,LSTM,baselines,/content/training-data/sentiment_analysis/31/triples/baselines.txt
Baselines,has,TD - LSTM,baselines,/content/training-data/sentiment_analysis/31/triples/baselines.txt
TD - LSTM,uses,two LSTM networks,baselines,/content/training-data/sentiment_analysis/31/triples/baselines.txt
two LSTM networks,to model,preceding and following contexts,baselines,/content/training-data/sentiment_analysis/31/triples/baselines.txt
preceding and following contexts,surrounding,aspect term,baselines,/content/training-data/sentiment_analysis/31/triples/baselines.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
Hyperparameters,apply,dropout,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
dropout,on,final classification features,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
final classification features,of,PG - CNN,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
Hyperparameters,use,minibatch size,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
minibatch size,of,25,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
Hyperparameters,use,rectifier,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
rectifier,as,non-linear function f,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
non-linear function f,in,"CNN g , CNN t",hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
rectifier,as,sigmoid,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
sigmoid,in,CNN s,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
Hyperparameters,use,l 2 regularization term,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
l 2 regularization term,of,0.001,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
Hyperparameters,use,filter window sizes,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
filter window sizes,of,"1 , 2 , 3 , 4 with 100 feature maps each",hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
Hyperparameters,adopt,early stopping,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
early stopping,based on,validation loss,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
validation loss,on,development sets,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
Hyperparameters,For,out of vocabulary words,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
out of vocabulary words,initialize them,randomly,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
out of vocabulary words,from,"uniform distribution U ( ? 0.01 , 0.01 ",hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
Hyperparameters,has,Training,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
Training,done through,mini-batch stochastic gradient descent,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
mini-batch stochastic gradient descent,with,Adam update rule,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
Hyperparameters,has,training loss,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
training loss,does not,drop,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
drop,after,every three epochs,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
every three epochs,decrease,learning rate,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
learning rate,by,half,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
Hyperparameters,has,initial learning rate,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
initial learning rate,is,0.001,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
Hyperparameters,has,Parameterized filters and gates,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
Parameterized filters and gates,generated uniformly by,CNN,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
CNN,with,window sizes,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
window sizes,of,"1 , 2 , 3 , 4",hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
Parameterized filters and gates,have,same size and number,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
same size and number,as,normal filters,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
Hyperparameters,has,word embeddings,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
word embeddings,initialized with,300 - dimensional Glove vectors,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
300 - dimensional Glove vectors,fixed during,training,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
Hyperparameters,has,dropout rate,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
dropout rate,chosen as,0.3,hyperparameters,/content/training-data/sentiment_analysis/31/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/31/triples/model.txt
Model,design,two novel neural units,model,/content/training-data/sentiment_analysis/31/triples/model.txt
two novel neural units,take,target aspects,model,/content/training-data/sentiment_analysis/31/triples/model.txt
target aspects,into,account,model,/content/training-data/sentiment_analysis/31/triples/model.txt
two novel neural units,other is,parameterized gate,model,/content/training-data/sentiment_analysis/31/triples/model.txt
two novel neural units,One is,parameterized filter,model,/content/training-data/sentiment_analysis/31/triples/model.txt
Model,has,units,model,/content/training-data/sentiment_analysis/31/triples/model.txt
units,generated from,aspect - specific features,model,/content/training-data/sentiment_analysis/31/triples/model.txt
Model,propose,two simple yet effective convolutional neural networks,model,/content/training-data/sentiment_analysis/31/triples/model.txt
two simple yet effective convolutional neural networks,with,aspect information,model,/content/training-data/sentiment_analysis/31/triples/model.txt
Contribution,has research problem,Aspect Level Sentiment Classification,research-problem,/content/training-data/sentiment_analysis/31/triples/research-problem.txt
Contribution,has research problem,sentiment classification,research-problem,/content/training-data/sentiment_analysis/31/triples/research-problem.txt
Contribution,has research problem,general sentiment classification,research-problem,/content/training-data/sentiment_analysis/31/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/31/triples/results.txt
Results,Compared to,recently proposed model AF - LSTM,results,/content/training-data/sentiment_analysis/31/triples/results.txt
recently proposed model AF - LSTM,has,our method,results,/content/training-data/sentiment_analysis/31/triples/results.txt
our method,achieve,2 % - 5 % improvements,results,/content/training-data/sentiment_analysis/31/triples/results.txt
Results,has,Our two models,results,/content/training-data/sentiment_analysis/31/triples/results.txt
Our two models,achieve,best performance,results,/content/training-data/sentiment_analysis/31/triples/results.txt
best performance,compared to,baselines,results,/content/training-data/sentiment_analysis/31/triples/results.txt
Results,has,vanilla CNN,results,/content/training-data/sentiment_analysis/31/triples/results.txt
vanilla CNN,works,quite well,results,/content/training-data/sentiment_analysis/31/triples/results.txt
vanilla CNN,beats,welldesigned LSTM models,results,/content/training-data/sentiment_analysis/31/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/sentiment_analysis/50/triples/ablation-analysis.txt
Ablation analysis,has,Transfer of the embedding layer,ablation-analysis,/content/training-data/sentiment_analysis/50/triples/ablation-analysis.txt
Transfer of the embedding layer,is,more helpful,ablation-analysis,/content/training-data/sentiment_analysis/50/triples/ablation-analysis.txt
more helpful,on,D3 and D4,ablation-analysis,/content/training-data/sentiment_analysis/50/triples/ablation-analysis.txt
Ablation analysis,has,transfers,ablation-analysis,/content/training-data/sentiment_analysis/50/triples/ablation-analysis.txt
transfers,of,LSTM and embedding layer,ablation-analysis,/content/training-data/sentiment_analysis/50/triples/ablation-analysis.txt
LSTM and embedding layer,are,more useful,ablation-analysis,/content/training-data/sentiment_analysis/50/triples/ablation-analysis.txt
more useful,than,output layer,ablation-analysis,/content/training-data/sentiment_analysis/50/triples/ablation-analysis.txt
Ablation analysis,has,Sentiment information,ablation-analysis,/content/training-data/sentiment_analysis/50/triples/ablation-analysis.txt
Sentiment information,not adequately captured by,Glo Ve word embeddings,ablation-analysis,/content/training-data/sentiment_analysis/50/triples/ablation-analysis.txt
Contribution,Code,https://github.com/ruidan/Aspect-level-sentiment,code,/content/training-data/sentiment_analysis/50/triples/code.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentiment_analysis/50/triples/hyperparameters.txt
Hyperparameters,randomly sample,20 %,hyperparameters,/content/training-data/sentiment_analysis/50/triples/hyperparameters.txt
20 %,as,development set,hyperparameters,/content/training-data/sentiment_analysis/50/triples/hyperparameters.txt
20 %,of,original training data,hyperparameters,/content/training-data/sentiment_analysis/50/triples/hyperparameters.txt
original training data,from,aspectlevel dataset,hyperparameters,/content/training-data/sentiment_analysis/50/triples/hyperparameters.txt
Hyperparameters,use,RMSProp,hyperparameters,/content/training-data/sentiment_analysis/50/triples/hyperparameters.txt
RMSProp,as,optimizer,hyperparameters,/content/training-data/sentiment_analysis/50/triples/hyperparameters.txt
optimizer,with,decay rate,hyperparameters,/content/training-data/sentiment_analysis/50/triples/hyperparameters.txt
decay rate,set to,0.9,hyperparameters,/content/training-data/sentiment_analysis/50/triples/hyperparameters.txt
optimizer,with,base learning rate,hyperparameters,/content/training-data/sentiment_analysis/50/triples/hyperparameters.txt
base learning rate,set to,0.001,hyperparameters,/content/training-data/sentiment_analysis/50/triples/hyperparameters.txt
Hyperparameters,use,mini - batch size,hyperparameters,/content/training-data/sentiment_analysis/50/triples/hyperparameters.txt
mini - batch size,set to,32,hyperparameters,/content/training-data/sentiment_analysis/50/triples/hyperparameters.txt
Hyperparameters,use,dropout,hyperparameters,/content/training-data/sentiment_analysis/50/triples/hyperparameters.txt
dropout,with,probability 0.5,hyperparameters,/content/training-data/sentiment_analysis/50/triples/hyperparameters.txt
probability 0.5,on,sentence / document representations,hyperparameters,/content/training-data/sentiment_analysis/50/triples/hyperparameters.txt
sentence / document representations,before,output layer,hyperparameters,/content/training-data/sentiment_analysis/50/triples/hyperparameters.txt
Hyperparameters,For,all experiments,hyperparameters,/content/training-data/sentiment_analysis/50/triples/hyperparameters.txt
all experiments,has,dimension,hyperparameters,/content/training-data/sentiment_analysis/50/triples/hyperparameters.txt
dimension,of,LSTM hidden vectors,hyperparameters,/content/training-data/sentiment_analysis/50/triples/hyperparameters.txt
LSTM hidden vectors,set to,300,hyperparameters,/content/training-data/sentiment_analysis/50/triples/hyperparameters.txt
Hyperparameters,has,vectors,hyperparameters,/content/training-data/sentiment_analysis/50/triples/hyperparameters.txt
vectors,used for,initializing E,hyperparameters,/content/training-data/sentiment_analysis/50/triples/hyperparameters.txt
initializing E,in,pretraining phase,hyperparameters,/content/training-data/sentiment_analysis/50/triples/hyperparameters.txt
Hyperparameters,has,300 - dimension Glo Ve vectors,hyperparameters,/content/training-data/sentiment_analysis/50/triples/hyperparameters.txt
300 - dimension Glo Ve vectors,to initialize,E and E,hyperparameters,/content/training-data/sentiment_analysis/50/triples/hyperparameters.txt
E and E,when,pretraining,hyperparameters,/content/training-data/sentiment_analysis/50/triples/hyperparameters.txt
pretraining,not conducted for,weight initialization,hyperparameters,/content/training-data/sentiment_analysis/50/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/50/triples/model.txt
Model,explore,two transfer methods,model,/content/training-data/sentiment_analysis/50/triples/model.txt
two transfer methods,to incorporate,knowledge,model,/content/training-data/sentiment_analysis/50/triples/model.txt
knowledge,name,pretraining,model,/content/training-data/sentiment_analysis/50/triples/model.txt
knowledge,name,multi-task learning,model,/content/training-data/sentiment_analysis/50/triples/model.txt
Contribution,has research problem,Aspect - level Sentiment Classification,research-problem,/content/training-data/sentiment_analysis/50/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/50/triples/results.txt
Results,observe that,PRET,results,/content/training-data/sentiment_analysis/50/triples/results.txt
PRET,consistently gives,1 - 3 % increase,results,/content/training-data/sentiment_analysis/50/triples/results.txt
1 - 3 % increase,in,accuracy,results,/content/training-data/sentiment_analysis/50/triples/results.txt
accuracy,over,LSTM + ATT,results,/content/training-data/sentiment_analysis/50/triples/results.txt
PRET,is,very helpful,results,/content/training-data/sentiment_analysis/50/triples/results.txt
Results,has,MULT,results,/content/training-data/sentiment_analysis/50/triples/results.txt
MULT,gives,similar performance,results,/content/training-data/sentiment_analysis/50/triples/results.txt
similar performance,as,LSTM + ATT,results,/content/training-data/sentiment_analysis/50/triples/results.txt
LSTM + ATT,on,D1 and D2,results,/content/training-data/sentiment_analysis/50/triples/results.txt
Results,has,numbers of neutral examples,results,/content/training-data/sentiment_analysis/50/triples/results.txt
numbers of neutral examples,in,test sets,results,/content/training-data/sentiment_analysis/50/triples/results.txt
test sets,are,very small,results,/content/training-data/sentiment_analysis/50/triples/results.txt
test sets,of,D3 and D4,results,/content/training-data/sentiment_analysis/50/triples/results.txt
Results,has,combination ( PRET + MULT ,results,/content/training-data/sentiment_analysis/50/triples/results.txt
combination ( PRET + MULT ),yields,better results,results,/content/training-data/sentiment_analysis/50/triples/results.txt
Contribution,has,Approach,approach,/content/training-data/sentiment_analysis/38/triples/approach.txt
Approach,attempt to learn,unsupervised representation,approach,/content/training-data/sentiment_analysis/38/triples/approach.txt
unsupervised representation,accurately contains,sentiment analysis,approach,/content/training-data/sentiment_analysis/38/triples/approach.txt
Approach,train on,very large corpus,approach,/content/training-data/sentiment_analysis/38/triples/approach.txt
very large corpus,picked to have,similar distribution,approach,/content/training-data/sentiment_analysis/38/triples/approach.txt
similar distribution,as,our task of interest,approach,/content/training-data/sentiment_analysis/38/triples/approach.txt
Approach,consider,research benchmark of byte ( character ) level language modelling,approach,/content/training-data/sentiment_analysis/38/triples/approach.txt
research benchmark of byte ( character ) level language modelling,due to,its further simplicity and generality,approach,/content/training-data/sentiment_analysis/38/triples/approach.txt
Contribution,has research problem,Representation learning,research-problem,/content/training-data/sentiment_analysis/38/triples/research-problem.txt
Contribution,has research problem,sentiment analysis,research-problem,/content/training-data/sentiment_analysis/38/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/38/triples/results.txt
Results,has,Review Sentiment Analysis,results,/content/training-data/sentiment_analysis/38/triples/results.txt
Review Sentiment Analysis,Using,full 4096 unit representation,results,/content/training-data/sentiment_analysis/38/triples/results.txt
full 4096 unit representation,achieves,92.88 %,results,/content/training-data/sentiment_analysis/38/triples/results.txt
Review Sentiment Analysis,has,The representation learned by our model,results,/content/training-data/sentiment_analysis/38/triples/results.txt
The representation learned by our model,achieves,91.8 %,results,/content/training-data/sentiment_analysis/38/triples/results.txt
91.8 %,significantly outperforming,state of the art,results,/content/training-data/sentiment_analysis/38/triples/results.txt
state of the art,of,90.2 %,results,/content/training-data/sentiment_analysis/38/triples/results.txt
state of the art,by,30 model ensemble,results,/content/training-data/sentiment_analysis/38/triples/results.txt
The representation learned by our model,matches,performance,results,/content/training-data/sentiment_analysis/38/triples/results.txt
performance,using,dozen labeled examples,results,/content/training-data/sentiment_analysis/38/triples/results.txt
performance,of,baselines,results,/content/training-data/sentiment_analysis/38/triples/results.txt
The representation learned by our model,does not reach,state of the art,results,/content/training-data/sentiment_analysis/38/triples/results.txt
state of the art,achieving,52.9 %,results,/content/training-data/sentiment_analysis/38/triples/results.txt
state of the art,of,53.6 %,results,/content/training-data/sentiment_analysis/38/triples/results.txt
state of the art,on,fine - grained subtask,results,/content/training-data/sentiment_analysis/38/triples/results.txt
The representation learned by our model,outperforms,all previous results,results,/content/training-data/sentiment_analysis/38/triples/results.txt
all previous results,with,few hundred labeled examples,results,/content/training-data/sentiment_analysis/38/triples/results.txt
Review Sentiment Analysis,has,L1 regularization,results,/content/training-data/sentiment_analysis/38/triples/results.txt
L1 regularization,Fitting,threshold,results,/content/training-data/sentiment_analysis/38/triples/results.txt
threshold,achieves,test accuracy,results,/content/training-data/sentiment_analysis/38/triples/results.txt
test accuracy,of,92.30 %,results,/content/training-data/sentiment_analysis/38/triples/results.txt
test accuracy,outperforms,strong supervised results,results,/content/training-data/sentiment_analysis/38/triples/results.txt
threshold,has,91.87 %,results,/content/training-data/sentiment_analysis/38/triples/results.txt
91.87 %,of,NB - SVM trigram,results,/content/training-data/sentiment_analysis/38/triples/results.txt
91.87 %,still below,semi-supervised state of the art,results,/content/training-data/sentiment_analysis/38/triples/results.txt
semi-supervised state of the art,of,94.09 %,results,/content/training-data/sentiment_analysis/38/triples/results.txt
Results,has,Capacity Ceiling,results,/content/training-data/sentiment_analysis/38/triples/results.txt
Capacity Ceiling,Using,full dataset,results,/content/training-data/sentiment_analysis/38/triples/results.txt
full dataset,achieve,95 . 22 % test accuracy,results,/content/training-data/sentiment_analysis/38/triples/results.txt
Capacity Ceiling,there is,notable drop,results,/content/training-data/sentiment_analysis/38/triples/results.txt
notable drop,in,relative performance,results,/content/training-data/sentiment_analysis/38/triples/results.txt
relative performance,of,our approach,results,/content/training-data/sentiment_analysis/38/triples/results.txt
our approach,transitioning from,sentence to document datasets,results,/content/training-data/sentiment_analysis/38/triples/results.txt
Capacity Ceiling,try,approach,results,/content/training-data/sentiment_analysis/38/triples/results.txt
approach,on,binary version,results,/content/training-data/sentiment_analysis/38/triples/results.txt
binary version,of,Yelp Dataset Challenge in 2015,results,/content/training-data/sentiment_analysis/38/triples/results.txt
Capacity Ceiling,has,labeled data,results,/content/training-data/sentiment_analysis/38/triples/results.txt
labeled data,has,increases,results,/content/training-data/sentiment_analysis/38/triples/results.txt
increases,has,performance,results,/content/training-data/sentiment_analysis/38/triples/results.txt
performance,of,simple linear model,results,/content/training-data/sentiment_analysis/38/triples/results.txt
simple linear model,train on top of,static representation,results,/content/training-data/sentiment_analysis/38/triples/results.txt
performance,will eventually,saturate,results,/content/training-data/sentiment_analysis/38/triples/results.txt
Capacity Ceiling,observed,capacity ceiling,results,/content/training-data/sentiment_analysis/38/triples/results.txt
capacity ceiling,is,interesting phenomena and stumbling point,results,/content/training-data/sentiment_analysis/38/triples/results.txt
interesting phenomena and stumbling point,for,scaling our unsupervised representations,results,/content/training-data/sentiment_analysis/38/triples/results.txt
Contribution,has,Model,model,/content/training-data/sentiment_analysis/6/triples/model.txt
Model,preserves,sequential order,model,/content/training-data/sentiment_analysis/6/triples/model.txt
sequential order,of,utterances,model,/content/training-data/sentiment_analysis/6/triples/model.txt
Model,develop,framework,model,/content/training-data/sentiment_analysis/6/triples/model.txt
framework,based on,long shortterm memory ( LSTM ,model,/content/training-data/sentiment_analysis/6/triples/model.txt
long shortterm memory ( LSTM ),extracts,contextual utterancelevel features,model,/content/training-data/sentiment_analysis/6/triples/model.txt
long shortterm memory ( LSTM ),takes,sequence of utterances,model,/content/training-data/sentiment_analysis/6/triples/model.txt
sequence of utterances,as,input,model,/content/training-data/sentiment_analysis/6/triples/model.txt
Model,enables,consecutive utterances,model,/content/training-data/sentiment_analysis/6/triples/model.txt
consecutive utterances,to share,information,model,/content/training-data/sentiment_analysis/6/triples/model.txt
information,providing,contextual information,model,/content/training-data/sentiment_analysis/6/triples/model.txt
contextual information,to,utterance - level sentiment classification process,model,/content/training-data/sentiment_analysis/6/triples/model.txt
Contribution,has research problem,Context - Dependent Sentiment Analysis,research-problem,/content/training-data/sentiment_analysis/6/triples/research-problem.txt
Contribution,has research problem,identification of sentiments in videos,research-problem,/content/training-data/sentiment_analysis/6/triples/research-problem.txt
Contribution,has research problem,Sentiment analysis,research-problem,/content/training-data/sentiment_analysis/6/triples/research-problem.txt
Contribution,has research problem,Emotion recognition,research-problem,/content/training-data/sentiment_analysis/6/triples/research-problem.txt
Contribution,has research problem,multimodal sentiment analysis,research-problem,/content/training-data/sentiment_analysis/6/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentiment_analysis/6/triples/results.txt
Results,noted,both sc - LSTM and bc - LSTM,results,/content/training-data/sentiment_analysis/6/triples/results.txt
both sc - LSTM and bc - LSTM,perform,quite well,results,/content/training-data/sentiment_analysis/6/triples/results.txt
quite well,on,multimodal emotion recognition and sentiment analysis datasets,results,/content/training-data/sentiment_analysis/6/triples/results.txt
Results,show,proposed method,results,/content/training-data/sentiment_analysis/6/triples/results.txt
proposed method,outperformes by,significant margin,results,/content/training-data/sentiment_analysis/6/triples/results.txt
Results,has,Every LSTM network variant,results,/content/training-data/sentiment_analysis/6/triples/results.txt
Every LSTM network variant,outperformed,baseline uni - SVM,results,/content/training-data/sentiment_analysis/6/triples/results.txt
baseline uni - SVM,on,all the datasets,results,/content/training-data/sentiment_analysis/6/triples/results.txt
all the datasets,by the margin of,2 % to 5 %,results,/content/training-data/sentiment_analysis/6/triples/results.txt
Results,has,trained contextual unimodal features,results,/content/training-data/sentiment_analysis/6/triples/results.txt
trained contextual unimodal features,help,hierarchical fusion framework,results,/content/training-data/sentiment_analysis/6/triples/results.txt
hierarchical fusion framework,to outperform,non-hierarchical framework,results,/content/training-data/sentiment_analysis/6/triples/results.txt
Results,has,bc - LSTM,results,/content/training-data/sentiment_analysis/6/triples/results.txt
bc - LSTM,has access to,preceding and following information,results,/content/training-data/sentiment_analysis/6/triples/results.txt
preceding and following information,of,utterance sequence,results,/content/training-data/sentiment_analysis/6/triples/results.txt
utterance sequence,performs,consistently better,results,/content/training-data/sentiment_analysis/6/triples/results.txt
consistently better,on,all the datasets over sc - LSTM,results,/content/training-data/sentiment_analysis/6/triples/results.txt
Results,has,performance improvement,results,/content/training-data/sentiment_analysis/6/triples/results.txt
performance improvement,is,in the range,results,/content/training-data/sentiment_analysis/6/triples/results.txt
in the range,of,0.3 % to 1.5 %,results,/content/training-data/sentiment_analysis/6/triples/results.txt
0.3 % to 1.5 %,on,MOSI and MOUD datasets,results,/content/training-data/sentiment_analysis/6/triples/results.txt
Results,has,non-hierarchical model,results,/content/training-data/sentiment_analysis/6/triples/results.txt
non-hierarchical model,outperforms,baseline uni - SVM,results,/content/training-data/sentiment_analysis/6/triples/results.txt
Results,On,IEMOCAP dataset,results,/content/training-data/sentiment_analysis/6/triples/results.txt
IEMOCAP dataset,has,performance improvement,results,/content/training-data/sentiment_analysis/6/triples/results.txt
performance improvement,of,bc - LSTM and sc - LSTM,results,/content/training-data/sentiment_analysis/6/triples/results.txt
bc - LSTM and sc - LSTM,over,h- LSTM,results,/content/training-data/sentiment_analysis/6/triples/results.txt
performance improvement,in the range of,1 % to 5 %,results,/content/training-data/sentiment_analysis/6/triples/results.txt
Contribution,has,Model,model,/content/training-data/question_similarity/0/triples/model.txt
Model,build,neural network model,model,/content/training-data/question_similarity/0/triples/model.txt
neural network model,with,four components,model,/content/training-data/question_similarity/0/triples/model.txt
Model,builds,sequence representation vectors,model,/content/training-data/question_similarity/0/triples/model.txt
sequence representation vectors,to predict,relation between the question pairs,model,/content/training-data/question_similarity/0/triples/model.txt
Model,uses,ELMo ( which stands for Embeddings from Language Models ) pre-trained contextual embeddings,model,/content/training-data/question_similarity/0/triples/model.txt
ELMo ( which stands for Embeddings from Language Models ) pre-trained contextual embeddings,as,input,model,/content/training-data/question_similarity/0/triples/model.txt
Contribution,has research problem,Semantic Question Similarity in Arabic,research-problem,/content/training-data/question_similarity/0/triples/research-problem.txt
Contribution,has research problem,semantic text question similarity,research-problem,/content/training-data/question_similarity/0/triples/research-problem.txt
Contribution,has research problem,Semantic Text Similarity ( STS ,research-problem,/content/training-data/question_similarity/0/triples/research-problem.txt
Contribution,has research problem,STS,research-problem,/content/training-data/question_similarity/0/triples/research-problem.txt
Contribution,has research problem,Semantic Question Similarity ( SQS ) for the Arabic language,research-problem,/content/training-data/question_similarity/0/triples/research-problem.txt
Contribution,has research problem,SQS,research-problem,/content/training-data/question_similarity/0/triples/research-problem.txt
Contribution,has,Experimental Setup,experimental-setup,/content/training-data/question_similarity/0/triples/experimental-setup.txt
Experimental Setup,done on,Google Colab 7 environment,experimental-setup,/content/training-data/question_similarity/0/triples/experimental-setup.txt
Google Colab 7 environment,using,Tesla T4 GPU accelerator,experimental-setup,/content/training-data/question_similarity/0/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/question_similarity/0/triples/results.txt
Results,using,pre-trained FastText embeddings,results,/content/training-data/question_similarity/0/triples/results.txt
pre-trained FastText embeddings,as,input,results,/content/training-data/question_similarity/0/triples/results.txt
input,to,our model,results,/content/training-data/question_similarity/0/triples/results.txt
our model,yields,worse F1score,results,/content/training-data/question_similarity/0/triples/results.txt
our model,compared with,ELMo contextual embeddings model,results,/content/training-data/question_similarity/0/triples/results.txt
our model,on,public and private leaderboards,results,/content/training-data/question_similarity/0/triples/results.txt
public and private leaderboards,with,94.254 and 93.118,results,/content/training-data/question_similarity/0/triples/results.txt
Results,overcome,weakness,results,/content/training-data/question_similarity/0/triples/results.txt
weakness,of,Arabic ELMo model,results,/content/training-data/question_similarity/0/triples/results.txt
Arabic ELMo model,by translating,data,results,/content/training-data/question_similarity/0/triples/results.txt
data,to,English,results,/content/training-data/question_similarity/0/triples/results.txt
English,using,Google Translate,results,/content/training-data/question_similarity/0/triples/results.txt
Arabic ELMo model,results are,much worse,results,/content/training-data/question_similarity/0/triples/results.txt
much worse,with,88.868 and 87.504 F1 - scores,results,/content/training-data/question_similarity/0/triples/results.txt
88.868 and 87.504 F1 - scores,on,public and private leaderboards,results,/content/training-data/question_similarity/0/triples/results.txt
Arabic ELMo model,treating,problem,results,/content/training-data/question_similarity/0/triples/results.txt
problem,as,English SQS problem,results,/content/training-data/question_similarity/0/triples/results.txt
Results,has,GRU cells,results,/content/training-data/question_similarity/0/triples/results.txt
GRU cells,are,most efficient,results,/content/training-data/question_similarity/0/triples/results.txt
Results,has,Effect of Data Augmentation,results,/content/training-data/question_similarity/0/triples/results.txt
Effect of Data Augmentation,not,each increment step,results,/content/training-data/question_similarity/0/triples/results.txt
each increment step,has,positive effect,results,/content/training-data/question_similarity/0/triples/results.txt
positive effect,on,model 's effectiveness,results,/content/training-data/question_similarity/0/triples/results.txt
Effect of Data Augmentation,show,each augmentation step,results,/content/training-data/question_similarity/0/triples/results.txt
each augmentation step,affects,model 's efficiency,results,/content/training-data/question_similarity/0/triples/results.txt
model 's efficiency,has,negatively,results,/content/training-data/question_similarity/0/triples/results.txt
Results,has,sequence weighted attention,results,/content/training-data/question_similarity/0/triples/results.txt
sequence weighted attention,gives,better results,results,/content/training-data/question_similarity/0/triples/results.txt
better results,by,about 1 point,results,/content/training-data/question_similarity/0/triples/results.txt
about 1 point,of,F1-score,results,/content/training-data/question_similarity/0/triples/results.txt
Results,has,ON - LSTM cells,results,/content/training-data/question_similarity/0/triples/results.txt
ON - LSTM cells,with,chunk size 8,results,/content/training-data/question_similarity/0/triples/results.txt
ON - LSTM cells,are,most effective,results,/content/training-data/question_similarity/0/triples/results.txt
most effective,in terms of,all considered measures,results,/content/training-data/question_similarity/0/triples/results.txt
Contribution,has,Dataset,datase,/content/training-data/query_wellformedness/0/triples/dataset.txt
Dataset,construct and publicly release,"dataset of 25,100 queries",datase,/content/training-data/query_wellformedness/0/triples/dataset.txt
"dataset of 25,100 queries",annotated with,probability,datase,/content/training-data/query_wellformedness/0/triples/dataset.txt
probability,of being,well - formed natural language question,datase,/content/training-data/query_wellformedness/0/triples/dataset.txt
Dataset,available for download at,http://goo.gl/language/ query-wellformedness,datase,/content/training-data/query_wellformedness/0/triples/dataset.txt
Contribution,has,Baselines,baselines,/content/training-data/query_wellformedness/0/triples/baselines.txt
Baselines,has,majority class baseline,baselines,/content/training-data/query_wellformedness/0/triples/baselines.txt
majority class baseline,corresponds to,all queries being classified non-wellformed,baselines,/content/training-data/query_wellformedness/0/triples/baselines.txt
majority class baseline,is,61.5 %,baselines,/content/training-data/query_wellformedness/0/triples/baselines.txt
Baselines,has,question word baseline,baselines,/content/training-data/query_wellformedness/0/triples/baselines.txt
question word baseline,classifies,any query,baselines,/content/training-data/query_wellformedness/0/triples/baselines.txt
any query,starting with,question word,baselines,/content/training-data/query_wellformedness/0/triples/baselines.txt
Contribution,has,Model,model,/content/training-data/query_wellformedness/0/triples/model.txt
Model,to predict,given query,model,/content/training-data/query_wellformedness/0/triples/model.txt
given query,is,well - formed natural language question,model,/content/training-data/query_wellformedness/0/triples/model.txt
Model,train,feed - forward neural network classifier,model,/content/training-data/query_wellformedness/0/triples/model.txt
feed - forward neural network classifier,uses,lexical and syntactic features,model,/content/training-data/query_wellformedness/0/triples/model.txt
lexical and syntactic features,extracted from,query,model,/content/training-data/query_wellformedness/0/triples/model.txt
query,on,data,model,/content/training-data/query_wellformedness/0/triples/model.txt
Contribution,has research problem,Identifying Well - formed Natural Language Questions,research-problem,/content/training-data/query_wellformedness/0/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/query_wellformedness/0/triples/results.txt
Results,Using,POS n-grams,results,/content/training-data/query_wellformedness/0/triples/results.txt
POS n-grams,gave,strong boost,results,/content/training-data/query_wellformedness/0/triples/results.txt
strong boost,of,5.2 points,results,/content/training-data/query_wellformedness/0/triples/results.txt
5.2 points,over,word unigrams and bigrams,results,/content/training-data/query_wellformedness/0/triples/results.txt
Results,has,best performance,results,/content/training-data/query_wellformedness/0/triples/results.txt
best performance,obtained,70.7 %,results,/content/training-data/query_wellformedness/0/triples/results.txt
70.7 %,while using,"word - 1 , 2 - grams",results,/content/training-data/query_wellformedness/0/triples/results.txt
70.7 %,while using,"POS - 1 , 2 , 3 - grams",results,/content/training-data/query_wellformedness/0/triples/results.txt
Results,has,"character - 3 , 4 grams",results,/content/training-data/query_wellformedness/0/triples/results.txt
"character - 3 , 4 grams",gave improvement over,word unigrams and bigrams,results,/content/training-data/query_wellformedness/0/triples/results.txt
"character - 3 , 4 grams",when,combined,results,/content/training-data/query_wellformedness/0/triples/results.txt
combined,with,POS tags,results,/content/training-data/query_wellformedness/0/triples/results.txt
combined,has,performance,results,/content/training-data/query_wellformedness/0/triples/results.txt
performance,not,sustain,results,/content/training-data/query_wellformedness/0/triples/results.txt
Contribution,has,Dataset,datase,/content/training-data/sentence_classification/0/triples/dataset.txt
Dataset,introduce,Sci - Cite,datase,/content/training-data/sentence_classification/0/triples/dataset.txt
Sci - Cite,has,new dataset,datase,/content/training-data/sentence_classification/0/triples/dataset.txt
new dataset,that is,"significantly larger , more coarse - grained and generaldomain",datase,/content/training-data/sentence_classification/0/triples/dataset.txt
"significantly larger , more coarse - grained and generaldomain",compared with,existing datasets,datase,/content/training-data/sentence_classification/0/triples/dataset.txt
new dataset,of,citation intents,datase,/content/training-data/sentence_classification/0/triples/dataset.txt
Dataset,consider,three intent categories,datase,/content/training-data/sentence_classification/0/triples/dataset.txt
three intent categories,name,BACK - GROUND,datase,/content/training-data/sentence_classification/0/triples/dataset.txt
three intent categories,name,METHOD,datase,/content/training-data/sentence_classification/0/triples/dataset.txt
three intent categories,name,RESULTCOMPARISON,datase,/content/training-data/sentence_classification/0/triples/dataset.txt
Dataset,has,Citation contexts,datase,/content/training-data/sentence_classification/0/triples/dataset.txt
Citation contexts,resulted in,"total 9,159",datase,/content/training-data/sentence_classification/0/triples/dataset.txt
"total 9,159",has,crowdsourced instances,datase,/content/training-data/sentence_classification/0/triples/dataset.txt
crowdsourced instances,divided to,training and validation sets,datase,/content/training-data/sentence_classification/0/triples/dataset.txt
training and validation sets,with,90 %,datase,/content/training-data/sentence_classification/0/triples/dataset.txt
90 %,of,data,datase,/content/training-data/sentence_classification/0/triples/dataset.txt
data,used for,training set,datase,/content/training-data/sentence_classification/0/triples/dataset.txt
Citation contexts,has,sentence,datase,/content/training-data/sentence_classification/0/triples/dataset.txt
sentence,was,annotated,datase,/content/training-data/sentence_classification/0/triples/dataset.txt
annotated,on average,3.74 times,datase,/content/training-data/sentence_classification/0/triples/dataset.txt
Citation contexts,annotated by,850 crowdsource workers,datase,/content/training-data/sentence_classification/0/triples/dataset.txt
850 crowdsource workers,individually made,4 and 240 annotations,datase,/content/training-data/sentence_classification/0/triples/dataset.txt
850 crowdsource workers,made,"total of 29,926 annotations",datase,/content/training-data/sentence_classification/0/triples/dataset.txt
Dataset,has,Citation intent,datase,/content/training-data/sentence_classification/0/triples/dataset.txt
Citation intent,of,sentence extractions,datase,/content/training-data/sentence_classification/0/triples/dataset.txt
sentence extractions,labeled through,crowdsourcing platform,datase,/content/training-data/sentence_classification/0/triples/dataset.txt
Contribution,has,Baselines,baselines,/content/training-data/sentence_classification/0/triples/baselines.txt
Baselines,has,BiLSTM Attention ( with and without ELMo ,baselines,/content/training-data/sentence_classification/0/triples/baselines.txt
BiLSTM Attention ( with and without ELMo ),optimizes,network,baselines,/content/training-data/sentence_classification/0/triples/baselines.txt
network,for,main loss,baselines,/content/training-data/sentence_classification/0/triples/baselines.txt
main loss,regarding,citation intent classification ( L 1 ,baselines,/content/training-data/sentence_classification/0/triples/baselines.txt
BiLSTM Attention ( with and without ELMo ),uses,similar architecture,baselines,/content/training-data/sentence_classification/0/triples/baselines.txt
similar architecture,to,proposed neural multitask learning framework,baselines,/content/training-data/sentence_classification/0/triples/baselines.txt
Contribution,Code,https://github.com/ allenai/scicite,code,/content/training-data/sentence_classification/0/triples/code.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentence_classification/0/triples/hyperparameters.txt
Hyperparameters,use,Beaker,hyperparameters,/content/training-data/sentence_classification/0/triples/hyperparameters.txt
Beaker,for,running the experiments,hyperparameters,/content/training-data/sentence_classification/0/triples/hyperparameters.txt
Hyperparameters,use,single - layer BiLSTM,hyperparameters,/content/training-data/sentence_classification/0/triples/hyperparameters.txt
single - layer BiLSTM,with,hidden dimension size,hyperparameters,/content/training-data/sentence_classification/0/triples/hyperparameters.txt
hidden dimension size,of,50,hyperparameters,/content/training-data/sentence_classification/0/triples/hyperparameters.txt
Hyperparameters,For,word representations,hyperparameters,/content/training-data/sentence_classification/0/triples/hyperparameters.txt
word representations,use,100 - dimensional GloVe vectors,hyperparameters,/content/training-data/sentence_classification/0/triples/hyperparameters.txt
100 - dimensional GloVe vectors,trained on,corpus,hyperparameters,/content/training-data/sentence_classification/0/triples/hyperparameters.txt
corpus,of,6B tokens,hyperparameters,/content/training-data/sentence_classification/0/triples/hyperparameters.txt
corpus,from,Wikipedia and Gigaword,hyperparameters,/content/training-data/sentence_classification/0/triples/hyperparameters.txt
Hyperparameters,For,contextual representations,hyperparameters,/content/training-data/sentence_classification/0/triples/hyperparameters.txt
contextual representations,use,ELMo vectors,hyperparameters,/content/training-data/sentence_classification/0/triples/hyperparameters.txt
ELMo vectors,with,output dimension size,hyperparameters,/content/training-data/sentence_classification/0/triples/hyperparameters.txt
output dimension size,of,"1,024",hyperparameters,/content/training-data/sentence_classification/0/triples/hyperparameters.txt
ELMo vectors,trained on,dataset,hyperparameters,/content/training-data/sentence_classification/0/triples/hyperparameters.txt
dataset,of,5.5 B tokens,hyperparameters,/content/training-data/sentence_classification/0/triples/hyperparameters.txt
Hyperparameters,For,each of scaffold tasks,hyperparameters,/content/training-data/sentence_classification/0/triples/hyperparameters.txt
each of scaffold tasks,use,single - layer MLP,hyperparameters,/content/training-data/sentence_classification/0/triples/hyperparameters.txt
single - layer MLP,between,hidden and input layers,hyperparameters,/content/training-data/sentence_classification/0/triples/hyperparameters.txt
hidden and input layers,with,20 hidden nodes,hyperparameters,/content/training-data/sentence_classification/0/triples/hyperparameters.txt
hidden and input layers,with,ReLU activation,hyperparameters,/content/training-data/sentence_classification/0/triples/hyperparameters.txt
hidden and input layers,with,Dropout rate,hyperparameters,/content/training-data/sentence_classification/0/triples/hyperparameters.txt
Dropout rate,of,0.2,hyperparameters,/content/training-data/sentence_classification/0/triples/hyperparameters.txt
Hyperparameters,implement,proposed scaffold framework,hyperparameters,/content/training-data/sentence_classification/0/triples/hyperparameters.txt
proposed scaffold framework,using,AllenNLP library,hyperparameters,/content/training-data/sentence_classification/0/triples/hyperparameters.txt
Hyperparameters,has,Batch size,hyperparameters,/content/training-data/sentence_classification/0/triples/hyperparameters.txt
Batch size,is,8,hyperparameters,/content/training-data/sentence_classification/0/triples/hyperparameters.txt
8,for,ACL - ARC dataset,hyperparameters,/content/training-data/sentence_classification/0/triples/hyperparameters.txt
Batch size,is,32,hyperparameters,/content/training-data/sentence_classification/0/triples/hyperparameters.txt
32,for,SciCite dataset,hyperparameters,/content/training-data/sentence_classification/0/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentence_classification/0/triples/model.txt
Model,propose,two auxiliary tasks,model,/content/training-data/sentence_classification/0/triples/model.txt
two auxiliary tasks,as,structural scaffolds,model,/content/training-data/sentence_classification/0/triples/model.txt
structural scaffolds,to improve,citation intent prediction,model,/content/training-data/sentence_classification/0/triples/model.txt
Model,propose,neural multitask learning framework,model,/content/training-data/sentence_classification/0/triples/model.txt
neural multitask learning framework,to incorporate,knowledge,model,/content/training-data/sentence_classification/0/triples/model.txt
knowledge,into,citations,model,/content/training-data/sentence_classification/0/triples/model.txt
citations,from,structure of scientific papers,model,/content/training-data/sentence_classification/0/triples/model.txt
Model,propose,neural scaffold framework,model,/content/training-data/sentence_classification/0/triples/model.txt
neural scaffold framework,for,citation intent classification,model,/content/training-data/sentence_classification/0/triples/model.txt
citation intent classification,to incorporate into,citations,model,/content/training-data/sentence_classification/0/triples/model.txt
citations,has,knowledge,model,/content/training-data/sentence_classification/0/triples/model.txt
knowledge,from,structure of scientific papers,model,/content/training-data/sentence_classification/0/triples/model.txt
Model,On,two datasets,model,/content/training-data/sentence_classification/0/triples/model.txt
two datasets,show that,proposed neural scaffold model,model,/content/training-data/sentence_classification/0/triples/model.txt
proposed neural scaffold model,by,large margins,model,/content/training-data/sentence_classification/0/triples/model.txt
proposed neural scaffold model,outperforms,existing methods,model,/content/training-data/sentence_classification/0/triples/model.txt
Contribution,has research problem,Citation Intent Classification in Scientific Publications,research-problem,/content/training-data/sentence_classification/0/triples/research-problem.txt
Contribution,has research problem,Identifying the intent of a citation in scientific papers,research-problem,/content/training-data/sentence_classification/0/triples/research-problem.txt
Contribution,has research problem,citation intent classification,research-problem,/content/training-data/sentence_classification/0/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentence_classification/0/triples/results.txt
Results,note,scaffold tasks,results,/content/training-data/sentence_classification/0/triples/results.txt
scaffold tasks,provide,major contributions,results,/content/training-data/sentence_classification/0/triples/results.txt
major contributions,on top of,ELMo - enabled baseline ( ?= 13.6 ,results,/content/training-data/sentence_classification/0/triples/results.txt
Results,Starting with,BiLSTM - Attn,results,/content/training-data/sentence_classification/0/triples/results.txt
BiLSTM - Attn,with,macro F1 score,results,/content/training-data/sentence_classification/0/triples/results.txt
macro F1 score,of,51.8,results,/content/training-data/sentence_classification/0/triples/results.txt
BiLSTM - Attn,adding,first scaffold task in ' BiLSTM - Attn + section title scaffold ',results,/content/training-data/sentence_classification/0/triples/results.txt
first scaffold task in ' BiLSTM - Attn + section title scaffold ',improves,F1 score,results,/content/training-data/sentence_classification/0/triples/results.txt
F1 score,to,56.9 (?= 5.1 ,results,/content/training-data/sentence_classification/0/triples/results.txt
Results,Adding,both scaffolds,results,/content/training-data/sentence_classification/0/triples/results.txt
both scaffolds,results in,further improvements,results,/content/training-data/sentence_classification/0/triples/results.txt
Results,Adding,second scaffold,results,/content/training-data/sentence_classification/0/triples/results.txt
second scaffold,in,BiLSTM - Attn + citation worthiness scaffold,results,/content/training-data/sentence_classification/0/triples/results.txt
BiLSTM - Attn + citation worthiness scaffold,results in,similar improvements,results,/content/training-data/sentence_classification/0/triples/results.txt
similar improvements,has,56.3 (?= 4.5 ,results,/content/training-data/sentence_classification/0/triples/results.txt
Results,has,Each scaffold task,results,/content/training-data/sentence_classification/0/triples/results.txt
Each scaffold task,improves,model performance,results,/content/training-data/sentence_classification/0/triples/results.txt
Results,has,both scaffolds,results,/content/training-data/sentence_classification/0/triples/results.txt
both scaffolds,used simultaneously in,BiLSTM - Attn + both scaffolds,results,/content/training-data/sentence_classification/0/triples/results.txt
both scaffolds,has,F1 score,results,/content/training-data/sentence_classification/0/triples/results.txt
F1 score,improves to,63.1 ( ?= 11.3 ,results,/content/training-data/sentence_classification/0/triples/results.txt
Results,has,best results,results,/content/training-data/sentence_classification/0/triples/results.txt
best results,add,ELMo vectors,results,/content/training-data/sentence_classification/0/triples/results.txt
ELMo vectors,achieving,F1,results,/content/training-data/sentence_classification/0/triples/results.txt
F1,of,67.9,results,/content/training-data/sentence_classification/0/triples/results.txt
67.9,has,major improvement,results,/content/training-data/sentence_classification/0/triples/results.txt
major improvement,from,previous state - of - the - art results,results,/content/training-data/sentence_classification/0/triples/results.txt
previous state - of - the - art results,of,54.6 ( ?= 13.3 ,results,/content/training-data/sentence_classification/0/triples/results.txt
ELMo vectors,to,input representations,results,/content/training-data/sentence_classification/0/triples/results.txt
input representations,in,BiLSTM - Attn w / ELMo + both scaffolds,results,/content/training-data/sentence_classification/0/triples/results.txt
best results,by using,ELMo representation,results,/content/training-data/sentence_classification/0/triples/results.txt
ELMo representation,in addition to,both scaffolds,results,/content/training-data/sentence_classification/0/triples/results.txt
Results,experimented with,adding features,results,/content/training-data/sentence_classification/0/triples/results.txt
adding features,used in,our best model,results,/content/training-data/sentence_classification/0/triples/results.txt
adding features,observed,at least 1.7 % decline,results,/content/training-data/sentence_classification/0/triples/results.txt
at least 1.7 % decline,in,performance,results,/content/training-data/sentence_classification/0/triples/results.txt
Results,observe,scaffold - enhanced models,results,/content/training-data/sentence_classification/0/triples/results.txt
scaffold - enhanced models,achieve,clear improvements,results,/content/training-data/sentence_classification/0/triples/results.txt
clear improvements,over,state - of - the - art approach,results,/content/training-data/sentence_classification/0/triples/results.txt
Results,observe,results,results,/content/training-data/sentence_classification/0/triples/results.txt
results,on,categories,results,/content/training-data/sentence_classification/0/triples/results.txt
categories,with,more number of instances,results,/content/training-data/sentence_classification/0/triples/results.txt
more number of instances,are,higher,results,/content/training-data/sentence_classification/0/triples/results.txt
Results,on,ACL - ARC,results,/content/training-data/sentence_classification/0/triples/results.txt
ACL - ARC,has,results,results,/content/training-data/sentence_classification/0/triples/results.txt
results,on,BACKGROUND category,results,/content/training-data/sentence_classification/0/triples/results.txt
BACKGROUND category,are,highest,results,/content/training-data/sentence_classification/0/triples/results.txt
highest,as,category,results,/content/training-data/sentence_classification/0/triples/results.txt
category,is,most common,results,/content/training-data/sentence_classification/0/triples/results.txt
results,on,FUTUREWORK category,results,/content/training-data/sentence_classification/0/triples/results.txt
FUTUREWORK category,are,lowest,results,/content/training-data/sentence_classification/0/triples/results.txt
FUTUREWORK category,has,fewest data points,results,/content/training-data/sentence_classification/0/triples/results.txt
fewest data points,thus,harder,results,/content/training-data/sentence_classification/0/triples/results.txt
harder,for,model,results,/content/training-data/sentence_classification/0/triples/results.txt
model,to learn,optimal parameters,results,/content/training-data/sentence_classification/0/triples/results.txt
optimal parameters,for,correct classification,results,/content/training-data/sentence_classification/0/triples/results.txt
Contribution,has,Approach,approach,/content/training-data/sentence_classification/2/triples/approach.txt
Approach,has,MCFA,approach,/content/training-data/sentence_classification/2/triples/approach.txt
MCFA,computes,two sentence usability metrics,approach,/content/training-data/sentence_classification/2/triples/approach.txt
two sentence usability metrics,to control,noise when fixing vectors,approach,/content/training-data/sentence_classification/2/triples/approach.txt
two sentence usability metrics,has,relative usability,approach,/content/training-data/sentence_classification/2/triples/approach.txt
relative usability,weighs,confidence of,approach,/content/training-data/sentence_classification/2/triples/approach.txt
confidence of,using,sentence a,approach,/content/training-data/sentence_classification/2/triples/approach.txt
sentence a,fixing,sentence b,approach,/content/training-data/sentence_classification/2/triples/approach.txt
two sentence usability metrics,has,self usability,approach,/content/training-data/sentence_classification/2/triples/approach.txt
self usability,weighs,confidence,approach,/content/training-data/sentence_classification/2/triples/approach.txt
confidence,of using,sentence a,approach,/content/training-data/sentence_classification/2/triples/approach.txt
sentence a,in solving,task,approach,/content/training-data/sentence_classification/2/triples/approach.txt
MCFA,moves,vectors,approach,/content/training-data/sentence_classification/2/triples/approach.txt
vectors,preserves,meaning,approach,/content/training-data/sentence_classification/2/triples/approach.txt
meaning,of,vector dimensions,approach,/content/training-data/sentence_classification/2/triples/approach.txt
vectors,inside,same space,approach,/content/training-data/sentence_classification/2/triples/approach.txt
MCFA,is,series of modules,approach,/content/training-data/sentence_classification/2/triples/approach.txt
series of modules,uses,all the sentence vectors,approach,/content/training-data/sentence_classification/2/triples/approach.txt
all the sentence vectors,as,context,approach,/content/training-data/sentence_classification/2/triples/approach.txt
all the sentence vectors,to fix,a sentence vector,approach,/content/training-data/sentence_classification/2/triples/approach.txt
MCFA,attached after,encoding the sentence,approach,/content/training-data/sentence_classification/2/triples/approach.txt
encoding the sentence,makes it widely adaptable,to other models,approach,/content/training-data/sentence_classification/2/triples/approach.txt
Approach,present,neural attentionbased multiple context fixing attachment ( MCFA ,approach,/content/training-data/sentence_classification/2/triples/approach.txt
Approach,propose,method,approach,/content/training-data/sentence_classification/2/triples/approach.txt
method,to mitigate,possible problems,approach,/content/training-data/sentence_classification/2/triples/approach.txt
possible problems,when using,translated sentences,approach,/content/training-data/sentence_classification/2/triples/approach.txt
translated sentences,as,context,approach,/content/training-data/sentence_classification/2/triples/approach.txt
Approach,propose,usage of translations,approach,/content/training-data/sentence_classification/2/triples/approach.txt
usage of translations,as,compelling and effective domain - free contexts,approach,/content/training-data/sentence_classification/2/triples/approach.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
Hyperparameters,experiment on using,only one additional context ( N = 1 ,hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
Hyperparameters,experiment on using,all ten languages at once ( N = 10 ,hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
Hyperparameters,use,dropout,hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
dropout,with,dropout rate,hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
dropout rate,of,0.5,hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
dropout,on,all nonlinear connections,hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
Hyperparameters,use,FastText pre-trained vectors,hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
FastText pre-trained vectors,for,all our data sets,hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
Hyperparameters,use,l 2 constraint,hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
l 2 constraint,of,3,hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
Hyperparameters,For,our CNN,hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
our CNN,use,rectified linear units,hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
our CNN,use,three filters,hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
three filters,with,"different window sizes h = 3 , 4 , 5",hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
"different window sizes h = 3 , 4 , 5",with,100 feature maps each,hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
Hyperparameters,For,final sentence vector,hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
final sentence vector,concatenate,feature maps,hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
feature maps,to get,300 - dimension vector,hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
Hyperparameters,perform,early stopping,hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
early stopping,using,random 10 %,hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
random 10 %,as,development set,hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
random 10 %,of,training set,hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
Hyperparameters,has,Training,hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
Training,done via,stochastic gradient descent,hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
stochastic gradient descent,over,shuffled mini-batches,hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
shuffled mini-batches,with,Adadelta update rule,hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
Hyperparameters,has,Tokenization,hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
Tokenization,done using,polyglot library,hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
Hyperparameters,During,training,hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
training,use,mini-batch size,hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
mini-batch size,of,50,hyperparameters,/content/training-data/sentence_classification/2/triples/hyperparameters.txt
Contribution,has research problem,Sentence Classification,research-problem,/content/training-data/sentence_classification/2/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/sentence_classification/2/triples/results.txt
Results,using,single context,results,/content/training-data/sentence_classification/2/triples/results.txt
single context,has,translations,results,/content/training-data/sentence_classification/2/triples/results.txt
translations,always outperform,topics,results,/content/training-data/sentence_classification/2/triples/results.txt
Results,When,N = 1,results,/content/training-data/sentence_classification/2/triples/results.txt
N = 1,has,MCFA,results,/content/training-data/sentence_classification/2/triples/results.txt
MCFA,increases,performance,results,/content/training-data/sentence_classification/2/triples/results.txt
performance,of,normal CNN,results,/content/training-data/sentence_classification/2/triples/results.txt
normal CNN,beating,current state of the art,results,/content/training-data/sentence_classification/2/triples/results.txt
current state of the art,on,CR data set,results,/content/training-data/sentence_classification/2/triples/results.txt
normal CNN,from,85.0,results,/content/training-data/sentence_classification/2/triples/results.txt
85.0,to,87.6,results,/content/training-data/sentence_classification/2/triples/results.txt
Results,When,N = 10,results,/content/training-data/sentence_classification/2/triples/results.txt
N = 10,has,MCFA,results,/content/training-data/sentence_classification/2/triples/results.txt
MCFA,additionally beats,state of the art,results,/content/training-data/sentence_classification/2/triples/results.txt
state of the art,on,TREC data set,results,/content/training-data/sentence_classification/2/triples/results.txt
Results,Using,topics,results,/content/training-data/sentence_classification/2/triples/results.txt
topics,as,additional context,results,/content/training-data/sentence_classification/2/triples/results.txt
topics,decreases,performance,results,/content/training-data/sentence_classification/2/triples/results.txt
performance,of,CNN classifier,results,/content/training-data/sentence_classification/2/triples/results.txt
CNN classifier,on,most data sets,results,/content/training-data/sentence_classification/2/triples/results.txt
Results,show,CNN + MCFA,results,/content/training-data/sentence_classification/2/triples/results.txt
CNN + MCFA,performs,competitively,results,/content/training-data/sentence_classification/2/triples/results.txt
competitively,on,one data set,results,/content/training-data/sentence_classification/2/triples/results.txt
CNN + MCFA,achieves,state of the art performance,results,/content/training-data/sentence_classification/2/triples/results.txt
state of the art performance,on,three of the four data sets,results,/content/training-data/sentence_classification/2/triples/results.txt
Results,has,our ensemble classifier,results,/content/training-data/sentence_classification/2/triples/results.txt
our ensemble classifier,additionally outperforms,all competing models,results,/content/training-data/sentence_classification/2/triples/results.txt
all competing models,on,MR data set,results,/content/training-data/sentence_classification/2/triples/results.txt
Results,conclude that,translations,results,/content/training-data/sentence_classification/2/triples/results.txt
translations,are,better additional contexts,results,/content/training-data/sentence_classification/2/triples/results.txt
better additional contexts,than,topics,results,/content/training-data/sentence_classification/2/triples/results.txt
Results,On,all data sets,results,/content/training-data/sentence_classification/2/triples/results.txt
all data sets,except,SUBJ,results,/content/training-data/sentence_classification/2/triples/results.txt
all data sets,has,accuracy,results,/content/training-data/sentence_classification/2/triples/results.txt
accuracy,of,CNN + B1,results,/content/training-data/sentence_classification/2/triples/results.txt
CNN + B1,decreases from,base CNN accuracy,results,/content/training-data/sentence_classification/2/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/sentence_classification/1/triples/ablation-analysis.txt
Ablation analysis,When,context enriching layer,ablation-analysis,/content/training-data/sentence_classification/1/triples/ablation-analysis.txt
context enriching layer,is,removed,ablation-analysis,/content/training-data/sentence_classification/1/triples/ablation-analysis.txt
removed,has,both models,ablation-analysis,/content/training-data/sentence_classification/1/triples/ablation-analysis.txt
both models,experience,most significant performance drop,ablation-analysis,/content/training-data/sentence_classification/1/triples/ablation-analysis.txt
both models,on par with,previous stateof - the - art results,ablation-analysis,/content/training-data/sentence_classification/1/triples/ablation-analysis.txt
Ablation analysis,has,HSLN - CNN model,ablation-analysis,/content/training-data/sentence_classification/1/triples/ablation-analysis.txt
HSLN - CNN model,suffers a little more from,component removal,ablation-analysis,/content/training-data/sentence_classification/1/triples/ablation-analysis.txt
HSLN - CNN model,than,HSLN - RNN model,ablation-analysis,/content/training-data/sentence_classification/1/triples/ablation-analysis.txt
Ablation analysis,has,dropout regularization and attention - based pooling components,ablation-analysis,/content/training-data/sentence_classification/1/triples/ablation-analysis.txt
dropout regularization and attention - based pooling components,further improve,model,ablation-analysis,/content/training-data/sentence_classification/1/triples/ablation-analysis.txt
model,in,limited extent,ablation-analysis,/content/training-data/sentence_classification/1/triples/ablation-analysis.txt
dropout regularization and attention - based pooling components,add to,our system,ablation-analysis,/content/training-data/sentence_classification/1/triples/ablation-analysis.txt
Ablation analysis,even without,label sequence optimization layer,ablation-analysis,/content/training-data/sentence_classification/1/triples/ablation-analysis.txt
label sequence optimization layer,has,our model,ablation-analysis,/content/training-data/sentence_classification/1/triples/ablation-analysis.txt
our model,significantly outperforms,best published methods,ablation-analysis,/content/training-data/sentence_classification/1/triples/ablation-analysis.txt
best published methods,empowered by,this layer,ablation-analysis,/content/training-data/sentence_classification/1/triples/ablation-analysis.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/sentence_classification/1/triples/hyperparameters.txt
Hyperparameters,fixed during,training phase,hyperparameters,/content/training-data/sentence_classification/1/triples/hyperparameters.txt
training phase,to avoid,over-fitting,hyperparameters,/content/training-data/sentence_classification/1/triples/hyperparameters.txt
Hyperparameters,For,regularization,hyperparameters,/content/training-data/sentence_classification/1/triples/hyperparameters.txt
regularization,has,dropout,hyperparameters,/content/training-data/sentence_classification/1/triples/hyperparameters.txt
dropout,applied to,each layer,hyperparameters,/content/training-data/sentence_classification/1/triples/hyperparameters.txt
Hyperparameters,has,window sizes,hyperparameters,/content/training-data/sentence_classification/1/triples/hyperparameters.txt
window sizes,of,CNN encoder,hyperparameters,/content/training-data/sentence_classification/1/triples/hyperparameters.txt
CNN encoder,in,sentence encoding layer,hyperparameters,/content/training-data/sentence_classification/1/triples/hyperparameters.txt
sentence encoding layer,are,"2 , 3 , 4 and 5",hyperparameters,/content/training-data/sentence_classification/1/triples/hyperparameters.txt
Hyperparameters,has,learning rate,hyperparameters,/content/training-data/sentence_classification/1/triples/hyperparameters.txt
learning rate,initially set,0.003,hyperparameters,/content/training-data/sentence_classification/1/triples/hyperparameters.txt
learning rate,decayed by,0.9,hyperparameters,/content/training-data/sentence_classification/1/triples/hyperparameters.txt
0.9,after,each epoch,hyperparameters,/content/training-data/sentence_classification/1/triples/hyperparameters.txt
Hyperparameters,has,token embeddings,hyperparameters,/content/training-data/sentence_classification/1/triples/hyperparameters.txt
token embeddings,using,word2vec tool,hyperparameters,/content/training-data/sentence_classification/1/triples/hyperparameters.txt
token embeddings,pre-trained on,large corpus,hyperparameters,/content/training-data/sentence_classification/1/triples/hyperparameters.txt
large corpus,combining,"Wikipedia , PubMed , and PMC texts",hyperparameters,/content/training-data/sentence_classification/1/triples/hyperparameters.txt
Hyperparameters,trained using,Adam optimization method,hyperparameters,/content/training-data/sentence_classification/1/triples/hyperparameters.txt
Hyperparameters,adopted,dropout,hyperparameters,/content/training-data/sentence_classification/1/triples/hyperparameters.txt
dropout,with,expectation - linear regularization,hyperparameters,/content/training-data/sentence_classification/1/triples/hyperparameters.txt
expectation - linear regularization,improve,generaliza - tion performance,hyperparameters,/content/training-data/sentence_classification/1/triples/hyperparameters.txt
expectation - linear regularization,to explicitly control,inference gap,hyperparameters,/content/training-data/sentence_classification/1/triples/hyperparameters.txt
Hyperparameters,optimized via,grid search,hyperparameters,/content/training-data/sentence_classification/1/triples/hyperparameters.txt
grid search,based on,validation set,hyperparameters,/content/training-data/sentence_classification/1/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/sentence_classification/1/triples/model.txt
Model,subsequently uses,single - hidden - layer feed - forward network,model,/content/training-data/sentence_classification/1/triples/model.txt
single - hidden - layer feed - forward network,to transform,sentence representation,model,/content/training-data/sentence_classification/1/triples/model.txt
sentence representation,to,probability vector,model,/content/training-data/sentence_classification/1/triples/model.txt
Model,optimizes,predicted label sequence jointly,model,/content/training-data/sentence_classification/1/triples/model.txt
predicted label sequence jointly,via,CRF layer,model,/content/training-data/sentence_classification/1/triples/model.txt
Model,uses,another bi - LSTM layer,model,/content/training-data/sentence_classification/1/triples/model.txt
another bi - LSTM layer,output,contextualized sentence representation,model,/content/training-data/sentence_classification/1/triples/model.txt
another bi - LSTM layer,take as input,individual sentence representation,model,/content/training-data/sentence_classification/1/triples/model.txt
Model,present,hierarchical neural network model,model,/content/training-data/sentence_classification/1/triples/model.txt
hierarchical neural network model,call,hierarchical sequential labeling network ( HSLN ,model,/content/training-data/sentence_classification/1/triples/model.txt
hierarchical neural network model,for,sequential sentence classification task,model,/content/training-data/sentence_classification/1/triples/model.txt
Model,first uses,RNN or CNN layer,model,/content/training-data/sentence_classification/1/triples/model.txt
RNN or CNN layer,to individually encode,sentence representation,model,/content/training-data/sentence_classification/1/triples/model.txt
sentence representation,from,sequence of word embeddings,model,/content/training-data/sentence_classification/1/triples/model.txt
Contribution,has research problem,Sequential Sentence Classification in Medical Scientific Abstracts,research-problem,/content/training-data/sentence_classification/1/triples/research-problem.txt
Contribution,has research problem,sequential sentence classification,research-problem,/content/training-data/sentence_classification/1/triples/research-problem.txt
Contribution,has,Baselines,baselines,/content/training-data/semantic_role_labeling/4/triples/baselines.txt
Baselines,use,BiLSTM - CRF model,baselines,/content/training-data/semantic_role_labeling/4/triples/baselines.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/semantic_role_labeling/4/triples/hyperparameters.txt
Hyperparameters,As,base function f base,hyperparameters,/content/training-data/semantic_role_labeling/4/triples/hyperparameters.txt
base function f base,use,4 BiLSTM layers,hyperparameters,/content/training-data/semantic_role_labeling/4/triples/hyperparameters.txt
4 BiLSTM layers,with,300 dimensional hidden units,hyperparameters,/content/training-data/semantic_role_labeling/4/triples/hyperparameters.txt
Hyperparameters,As,objective function,hyperparameters,/content/training-data/semantic_role_labeling/4/triples/hyperparameters.txt
objective function,use,crossentropy L,hyperparameters,/content/training-data/semantic_role_labeling/4/triples/hyperparameters.txt
crossentropy L,with,L2 weight decay,hyperparameters,/content/training-data/semantic_role_labeling/4/triples/hyperparameters.txt
Hyperparameters,To validate,model performance,hyperparameters,/content/training-data/semantic_role_labeling/4/triples/hyperparameters.txt
model performance,use,two types of word embeddings,hyperparameters,/content/training-data/semantic_role_labeling/4/triples/hyperparameters.txt
Hyperparameters,optimize,model parameters,hyperparameters,/content/training-data/semantic_role_labeling/4/triples/hyperparameters.txt
model parameters,use,Adam,hyperparameters,/content/training-data/semantic_role_labeling/4/triples/hyperparameters.txt
Hyperparameters,has,embeddings,hyperparameters,/content/training-data/semantic_role_labeling/4/triples/hyperparameters.txt
embeddings,fixed during,training,hyperparameters,/content/training-data/semantic_role_labeling/4/triples/hyperparameters.txt
Hyperparameters,has,Typical word embeddings,hyperparameters,/content/training-data/semantic_role_labeling/4/triples/hyperparameters.txt
Typical word embeddings,has,SENNA,hyperparameters,/content/training-data/semantic_role_labeling/4/triples/hyperparameters.txt
Typical word embeddings,has,ELMo,hyperparameters,/content/training-data/semantic_role_labeling/4/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/semantic_role_labeling/4/triples/model.txt
Model,At,decoding time,model,/content/training-data/semantic_role_labeling/4/triples/model.txt
decoding time,greedily select,higher scoring labeled spans,model,/content/training-data/semantic_role_labeling/4/triples/model.txt
Model,directly scores,all possible labeled spans,model,/content/training-data/semantic_role_labeling/4/triples/model.txt
all possible labeled spans,based on,span representations,model,/content/training-data/semantic_role_labeling/4/triples/model.txt
span representations,induced from,neural networks,model,/content/training-data/semantic_role_labeling/4/triples/model.txt
Model,has,model parameters,model,/content/training-data/semantic_role_labeling/4/triples/model.txt
model parameters,learned by,optimizing loglikelihood,model,/content/training-data/semantic_role_labeling/4/triples/model.txt
optimizing loglikelihood,of,correct labeled spans,model,/content/training-data/semantic_role_labeling/4/triples/model.txt
Model,presents,simple and accurate span - based model,model,/content/training-data/semantic_role_labeling/4/triples/model.txt
Contribution,has research problem,Semantic Role Labeling,research-problem,/content/training-data/semantic_role_labeling/4/triples/research-problem.txt
Contribution,has research problem,semantic role labeling ( SRL ,research-problem,/content/training-data/semantic_role_labeling/4/triples/research-problem.txt
Contribution,has research problem,SRL,research-problem,/content/training-data/semantic_role_labeling/4/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/semantic_role_labeling/4/triples/results.txt
Results,In comparison with,CRF - based single model,results,/content/training-data/semantic_role_labeling/4/triples/results.txt
CRF - based single model,has,our span - based single model,results,/content/training-data/semantic_role_labeling/4/triples/results.txt
our span - based single model,consistently yielded,better F 1 scores,results,/content/training-data/semantic_role_labeling/4/triples/results.txt
better F 1 scores,regardless of,"word embeddings , SENNA and ELMO",results,/content/training-data/semantic_role_labeling/4/triples/results.txt
Results,report,averaged scores,results,/content/training-data/semantic_role_labeling/4/triples/results.txt
averaged scores,across,five different runs,results,/content/training-data/semantic_role_labeling/4/triples/results.txt
five different runs,of,model,results,/content/training-data/semantic_role_labeling/4/triples/results.txt
Results,has,Our single and ensemble models,results,/content/training-data/semantic_role_labeling/4/triples/results.txt
Our single and ensemble models,achieved,best F 1 scores,results,/content/training-data/semantic_role_labeling/4/triples/results.txt
best F 1 scores,on,all the test sets,results,/content/training-data/semantic_role_labeling/4/triples/results.txt
all the test sets,except,Brown test set,results,/content/training-data/semantic_role_labeling/4/triples/results.txt
Results,has,span - based ensemble model,results,/content/training-data/semantic_role_labeling/4/triples/results.txt
span - based ensemble model,using,ELMo,results,/content/training-data/semantic_role_labeling/4/triples/results.txt
ELMo,achieved,best F1 scores,results,/content/training-data/semantic_role_labeling/4/triples/results.txt
best F1 scores,has,87.4 F1 and 87.0 F1,results,/content/training-data/semantic_role_labeling/4/triples/results.txt
best F1 scores,on,CoNLL - 2005 and CoNLL - 2012 datasets,results,/content/training-data/semantic_role_labeling/4/triples/results.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
Hyperparameters,employ,label smoothing technique,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
label smoothing technique,with,smoothing value,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
smoothing value,of,0.1,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
0.1,during,training,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
Hyperparameters,set,number of hidden units d,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
number of hidden units d,to,200,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
Hyperparameters,adopt,Adadelta,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
Adadelta,as,optimizer,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
Hyperparameters,For,DEEP - ATT,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
DEEP - ATT,with,FFN sub - layers,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
DEEP - ATT,has,whole training stage,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
whole training stage,takes,two days,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
two days,to,finish,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
finish,on,single Titan X GPU,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
two days,which is,2.5 times faster,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
Hyperparameters,initialize,other parameters,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
other parameters,by sampling,each element,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
each element,from,Gaussian distribution,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
Gaussian distribution,with,mean 0 and variance 1 ? d,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
Hyperparameters,initialize,weights,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
weights,of,all sub-layers,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
all sub-layers,as,random orthogonal matrices,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
Hyperparameters,has,number of heads h,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
number of heads h,set to,8,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
Hyperparameters,has,SGD,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
SGD,contains,mini-batch,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
mini-batch,of approximately,4096 tokens,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
4096 tokens,for,CoNLL - 2005 dataset,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
mini-batch,of approximately,8192 tokens,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
8192 tokens,for,CoNLL - 2012 dataset,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
Hyperparameters,has,Dropout layers,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
Dropout layers,added before,residual connections,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
residual connections,with,keep probability,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
keep probability,of,0.8,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
Hyperparameters,has,number of hidden layers,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
number of hidden layers,set to,10,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
Hyperparameters,has,learning rate,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
learning rate,initialized to,1.0,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
Hyperparameters,has,embedding layer,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
embedding layer,can be,initialized randomly,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
embedding layer,can be,pre-trained word embeddings,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
Hyperparameters,has,dimension,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
dimension,of,word embeddings and predicate mask embeddings,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
word embeddings and predicate mask embeddings,set to,100,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
Hyperparameters,has,Learning Parameter optimization,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
Learning Parameter optimization,performed using,stochastic gradient descent,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
Hyperparameters,has,Dropout,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
Dropout,applied before,attention softmax layer,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
Dropout,applied before,feed - froward ReLU hidden layer,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
Dropout,applied before,keep probabilities,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
keep probabilities,set to,0.9,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
Hyperparameters,clip,norm,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
norm,of,gradients,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
gradients,with,predefined threshold 1.0,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
norm,To avoid,exploding gradients problem,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
Hyperparameters,After training,400 k steps,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
400 k steps,halve,learning rate,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
learning rate,every,100 K steps,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
Hyperparameters,train,all models,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
all models,for,600 K steps,hyperparameters,/content/training-data/semantic_role_labeling/3/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/semantic_role_labeling/3/triples/model.txt
Model,rely on,self - attention mechanism,model,/content/training-data/semantic_role_labeling/3/triples/model.txt
self - attention mechanism,directly draws,global dependencies,model,/content/training-data/semantic_role_labeling/3/triples/model.txt
global dependencies,of,inputs,model,/content/training-data/semantic_role_labeling/3/triples/model.txt
Model,major advantage of,self - attention,model,/content/training-data/semantic_role_labeling/3/triples/model.txt
self - attention,provides,more flexible way,model,/content/training-data/semantic_role_labeling/3/triples/model.txt
more flexible way,"to select , represent and synthesize",information,model,/content/training-data/semantic_role_labeling/3/triples/model.txt
information,of,inputs,model,/content/training-data/semantic_role_labeling/3/triples/model.txt
information,complementary to,RNN based models,model,/content/training-data/semantic_role_labeling/3/triples/model.txt
self - attention,conducts,direct connections,model,/content/training-data/semantic_role_labeling/3/triples/model.txt
direct connections,between,two arbitrary tokens,model,/content/training-data/semantic_role_labeling/3/triples/model.txt
two arbitrary tokens,in,sentence,model,/content/training-data/semantic_role_labeling/3/triples/model.txt
self - attention,has,distant elements,model,/content/training-data/semantic_role_labeling/3/triples/model.txt
distant elements,can interact with,each other,model,/content/training-data/semantic_role_labeling/3/triples/model.txt
each other,allows,unimpeded information flow,model,/content/training-data/semantic_role_labeling/3/triples/model.txt
unimpeded information flow,through,network,model,/content/training-data/semantic_role_labeling/3/triples/model.txt
each other,by,shorter paths,model,/content/training-data/semantic_role_labeling/3/triples/model.txt
Model,Along with,self - attention,model,/content/training-data/semantic_role_labeling/3/triples/model.txt
self - attention,has,DEEP - ATT,model,/content/training-data/semantic_role_labeling/3/triples/model.txt
DEEP - ATT,comes with,three variants,model,/content/training-data/semantic_role_labeling/3/triples/model.txt
three variants,to further enhance,representations,model,/content/training-data/semantic_role_labeling/3/triples/model.txt
representations,uses,recurrent ( RNN ,model,/content/training-data/semantic_role_labeling/3/triples/model.txt
representations,uses,convolutional ( CNN ,model,/content/training-data/semantic_role_labeling/3/triples/model.txt
representations,uses,feed - forward ( FFN ) neural network,model,/content/training-data/semantic_role_labeling/3/triples/model.txt
Model,present,deep attentional neural network ( DEEPATT ,model,/content/training-data/semantic_role_labeling/3/triples/model.txt
deep attentional neural network ( DEEPATT ),for,SRL,model,/content/training-data/semantic_role_labeling/3/triples/model.txt
Contribution,has research problem,Deep Semantic Role Labeling,research-problem,/content/training-data/semantic_role_labeling/3/triples/research-problem.txt
Contribution,has research problem,Semantic Role Labeling ( SRL ,research-problem,/content/training-data/semantic_role_labeling/3/triples/research-problem.txt
Contribution,has research problem,SRL,research-problem,/content/training-data/semantic_role_labeling/3/triples/research-problem.txt
Contribution,has research problem,Semantic Role Labeling,research-problem,/content/training-data/semantic_role_labeling/3/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/semantic_role_labeling/3/triples/results.txt
Results,When ensembling,5 models,results,/content/training-data/semantic_role_labeling/3/triples/results.txt
5 models,with,FFN nonlinear sub - layers,results,/content/training-data/semantic_role_labeling/3/triples/results.txt
FFN nonlinear sub - layers,has,our approach,results,/content/training-data/semantic_role_labeling/3/triples/results.txt
our approach,achieves,F 1 score,results,/content/training-data/semantic_role_labeling/3/triples/results.txt
F 1 score,of,84.6 and 83.9,results,/content/training-data/semantic_role_labeling/3/triples/results.txt
our approach,has,absolute improvement,results,/content/training-data/semantic_role_labeling/3/triples/results.txt
absolute improvement,over,previous state - of - the - art,results,/content/training-data/semantic_role_labeling/3/triples/results.txt
absolute improvement,of,1.4 and 0.5,results,/content/training-data/semantic_role_labeling/3/triples/results.txt
Results,get,74.1 F 1 score,results,/content/training-data/semantic_role_labeling/3/triples/results.txt
74.1 F 1 score,on,out - of - domain dataset,results,/content/training-data/semantic_role_labeling/3/triples/results.txt
Results,has,results,results,/content/training-data/semantic_role_labeling/3/triples/results.txt
results,that,self - attention layers,results,/content/training-data/semantic_role_labeling/3/triples/results.txt
self - attention layers,helpful to capture,structural information,results,/content/training-data/semantic_role_labeling/3/triples/results.txt
self - attention layers,helpful to capture,long distance dependencies,results,/content/training-data/semantic_role_labeling/3/triples/results.txt
Contribution,Code,https://github.com/luheng/lsgn,code,/content/training-data/semantic_role_labeling/0/triples/code.txt
Contribution,has,Model,model,/content/training-data/semantic_role_labeling/0/triples/model.txt
Model,builds on,recent coreference resolution model,model,/content/training-data/semantic_role_labeling/0/triples/model.txt
recent coreference resolution model,by making,central,model,/content/training-data/semantic_role_labeling/0/triples/model.txt
central,use of,"learned , contextualized span representations",model,/content/training-data/semantic_role_labeling/0/triples/model.txt
Model,use,representations,model,/content/training-data/semantic_role_labeling/0/triples/model.txt
representations,to predict,SRL graphs,model,/content/training-data/semantic_role_labeling/0/triples/model.txt
SRL graphs,directly over,text spans,model,/content/training-data/semantic_role_labeling/0/triples/model.txt
Model,is,first span - based SRL model,model,/content/training-data/semantic_role_labeling/0/triples/model.txt
first span - based SRL model,that does not assume,predicates are given,model,/content/training-data/semantic_role_labeling/0/triples/model.txt
Model,has,final graph,model,/content/training-data/semantic_role_labeling/0/triples/model.txt
final graph,union of,predicted SRL roles ( edges ,model,/content/training-data/semantic_role_labeling/0/triples/model.txt
final graph,union of,associated text spans ( nodes ,model,/content/training-data/semantic_role_labeling/0/triples/model.txt
Model,has,span representations,model,/content/training-data/semantic_role_labeling/0/triples/model.txt
span representations,generalize,token - level representations,model,/content/training-data/semantic_role_labeling/0/triples/model.txt
token - level representations,in,BIObased models,model,/content/training-data/semantic_role_labeling/0/triples/model.txt
token - level representations,letting,model,model,/content/training-data/semantic_role_labeling/0/triples/model.txt
model,dynamically decide,which spans and roles to include,model,/content/training-data/semantic_role_labeling/0/triples/model.txt
which spans and roles to include,without using,previously standard syntactic features,model,/content/training-data/semantic_role_labeling/0/triples/model.txt
Model,has,Each edge,model,/content/training-data/semantic_role_labeling/0/triples/model.txt
Each edge,identified by,independently predicting,model,/content/training-data/semantic_role_labeling/0/triples/model.txt
independently predicting,which,role,model,/content/training-data/semantic_role_labeling/0/triples/model.txt
role,holds between,every possible pair of text spans,model,/content/training-data/semantic_role_labeling/0/triples/model.txt
Model,propose,end - to - end approach,model,/content/training-data/semantic_role_labeling/0/triples/model.txt
end - to - end approach,in,one forward pass,model,/content/training-data/semantic_role_labeling/0/triples/model.txt
one forward pass,for predicting,all the predicates,model,/content/training-data/semantic_role_labeling/0/triples/model.txt
one forward pass,for predicting,argument spans,model,/content/training-data/semantic_role_labeling/0/triples/model.txt
Contribution,has research problem,Neural Semantic Role Labeling,research-problem,/content/training-data/semantic_role_labeling/0/triples/research-problem.txt
Contribution,has research problem,Semantic role labeling ( SRL ,research-problem,/content/training-data/semantic_role_labeling/0/triples/research-problem.txt
Contribution,has research problem,SRL,research-problem,/content/training-data/semantic_role_labeling/0/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/semantic_role_labeling/0/triples/results.txt
Results,has,joint model,results,/content/training-data/semantic_role_labeling/0/triples/results.txt
joint model,by,F1 difference,results,/content/training-data/semantic_role_labeling/0/triples/results.txt
F1 difference,of,anywhere between 1.3 and 6.0,results,/content/training-data/semantic_role_labeling/0/triples/results.txt
joint model,outperforms,previous best pipeline system,results,/content/training-data/semantic_role_labeling/0/triples/results.txt
Results,On,all datasets,results,/content/training-data/semantic_role_labeling/0/triples/results.txt
all datasets,has,our model,results,/content/training-data/semantic_role_labeling/0/triples/results.txt
our model,able to,predict,results,/content/training-data/semantic_role_labeling/0/triples/results.txt
predict,over,40 %,results,/content/training-data/semantic_role_labeling/0/triples/results.txt
40 %,of,sentences,results,/content/training-data/semantic_role_labeling/0/triples/results.txt
40 %,has,completely correctly,results,/content/training-data/semantic_role_labeling/0/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/semantic_role_labeling/2/triples/ablation-analysis.txt
Ablation analysis,has,Orthonormal parameter initialization,ablation-analysis,/content/training-data/semantic_role_labeling/2/triples/ablation-analysis.txt
Orthonormal parameter initialization,is,surprisingly important,ablation-analysis,/content/training-data/semantic_role_labeling/2/triples/ablation-analysis.txt
Orthonormal parameter initialization,without this,model,ablation-analysis,/content/training-data/semantic_role_labeling/2/triples/ablation-analysis.txt
model,achieves,only 65 F1,ablation-analysis,/content/training-data/semantic_role_labeling/2/triples/ablation-analysis.txt
only 65 F1,within,first 50 epochs,ablation-analysis,/content/training-data/semantic_role_labeling/2/triples/ablation-analysis.txt
Ablation analysis,has,8 layer ablations,ablation-analysis,/content/training-data/semantic_role_labeling/2/triples/ablation-analysis.txt
8 layer ablations,suffer,loss,ablation-analysis,/content/training-data/semantic_role_labeling/2/triples/ablation-analysis.txt
loss,more than,1.7,ablation-analysis,/content/training-data/semantic_role_labeling/2/triples/ablation-analysis.txt
loss,in,absolute F 1,ablation-analysis,/content/training-data/semantic_role_labeling/2/triples/ablation-analysis.txt
loss,compared to,full model,ablation-analysis,/content/training-data/semantic_role_labeling/2/triples/ablation-analysis.txt
Ablation analysis,has,Without dropout,ablation-analysis,/content/training-data/semantic_role_labeling/2/triples/ablation-analysis.txt
Without dropout,has,model,ablation-analysis,/content/training-data/semantic_role_labeling/2/triples/ablation-analysis.txt
model,overfits,around 300 epochs,ablation-analysis,/content/training-data/semantic_role_labeling/2/triples/ablation-analysis.txt
around 300 epochs,at,78 F1,ablation-analysis,/content/training-data/semantic_role_labeling/2/triples/ablation-analysis.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/semantic_role_labeling/2/triples/hyperparameters.txt
Hyperparameters,set,RNN - dropout probability,hyperparameters,/content/training-data/semantic_role_labeling/2/triples/hyperparameters.txt
RNN - dropout probability,to,0.1,hyperparameters,/content/training-data/semantic_role_labeling/2/triples/hyperparameters.txt
Hyperparameters,use,Adadelta,hyperparameters,/content/training-data/semantic_role_labeling/2/triples/hyperparameters.txt
Adadelta,with,1e ?6 and ? = 0.95,hyperparameters,/content/training-data/semantic_role_labeling/2/triples/hyperparameters.txt
Hyperparameters,use,mini-batches,hyperparameters,/content/training-data/semantic_role_labeling/2/triples/hyperparameters.txt
mini-batches,size,80,hyperparameters,/content/training-data/semantic_role_labeling/2/triples/hyperparameters.txt
Hyperparameters,has,models,hyperparameters,/content/training-data/semantic_role_labeling/2/triples/hyperparameters.txt
models,trained for,500 epochs,hyperparameters,/content/training-data/semantic_role_labeling/2/triples/hyperparameters.txt
500 epochs,with,early stopping,hyperparameters,/content/training-data/semantic_role_labeling/2/triples/hyperparameters.txt
early stopping,based on,development results,hyperparameters,/content/training-data/semantic_role_labeling/2/triples/hyperparameters.txt
Hyperparameters,has,weight matrices,hyperparameters,/content/training-data/semantic_role_labeling/2/triples/hyperparameters.txt
weight matrices,in,BiL - STMs,hyperparameters,/content/training-data/semantic_role_labeling/2/triples/hyperparameters.txt
BiL - STMs,initialized with,random orthonormal matrices,hyperparameters,/content/training-data/semantic_role_labeling/2/triples/hyperparameters.txt
Hyperparameters,has,tokens,hyperparameters,/content/training-data/semantic_role_labeling/2/triples/hyperparameters.txt
tokens,are,lower - cased,hyperparameters,/content/training-data/semantic_role_labeling/2/triples/hyperparameters.txt
tokens,initialized with,100 - dimensional GloVe embeddings,hyperparameters,/content/training-data/semantic_role_labeling/2/triples/hyperparameters.txt
100 - dimensional GloVe embeddings,pre-trained on,6B tokens,hyperparameters,/content/training-data/semantic_role_labeling/2/triples/hyperparameters.txt
100 - dimensional GloVe embeddings,updated during,training,hyperparameters,/content/training-data/semantic_role_labeling/2/triples/hyperparameters.txt
Hyperparameters,has,network,hyperparameters,/content/training-data/semantic_role_labeling/2/triples/hyperparameters.txt
network,consists of,8 BiLSTM layers ( 4 forward LSTMs and 4 reversed LSTMs ,hyperparameters,/content/training-data/semantic_role_labeling/2/triples/hyperparameters.txt
8 BiLSTM layers ( 4 forward LSTMs and 4 reversed LSTMs ),with,300 dimensional hidden units,hyperparameters,/content/training-data/semantic_role_labeling/2/triples/hyperparameters.txt
network,consists of,softmax layer,hyperparameters,/content/training-data/semantic_role_labeling/2/triples/hyperparameters.txt
softmax layer,for predicting,output distribution,hyperparameters,/content/training-data/semantic_role_labeling/2/triples/hyperparameters.txt
Hyperparameters,has,Tokens,hyperparameters,/content/training-data/semantic_role_labeling/2/triples/hyperparameters.txt
Tokens,not covered by,GloVe,hyperparameters,/content/training-data/semantic_role_labeling/2/triples/hyperparameters.txt
GloVe,replaced with,randomly initialized UNK embedding,hyperparameters,/content/training-data/semantic_role_labeling/2/triples/hyperparameters.txt
Hyperparameters,clip,gradients,hyperparameters,/content/training-data/semantic_role_labeling/2/triples/hyperparameters.txt
gradients,with,norm,hyperparameters,/content/training-data/semantic_role_labeling/2/triples/hyperparameters.txt
norm,larger than,1,hyperparameters,/content/training-data/semantic_role_labeling/2/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/semantic_role_labeling/2/triples/model.txt
Model,using,deep highway bidirectional LSTMs,model,/content/training-data/semantic_role_labeling/2/triples/model.txt
deep highway bidirectional LSTMs,with,constrained decoding,model,/content/training-data/semantic_role_labeling/2/triples/model.txt
Model,treat,SRL,model,/content/training-data/semantic_role_labeling/2/triples/model.txt
SRL,as,BIO tagging problem,model,/content/training-data/semantic_role_labeling/2/triples/model.txt
Model,use,deep bidirectional LSTMs,model,/content/training-data/semantic_role_labeling/2/triples/model.txt
deep bidirectional LSTMs,differ by,using,model,/content/training-data/semantic_role_labeling/2/triples/model.txt
using,has,recurrent dropout,model,/content/training-data/semantic_role_labeling/2/triples/model.txt
deep bidirectional LSTMs,differ by,ensembling,model,/content/training-data/semantic_role_labeling/2/triples/model.txt
ensembling,with,product of experts,model,/content/training-data/semantic_role_labeling/2/triples/model.txt
deep bidirectional LSTMs,differ by,introducing,model,/content/training-data/semantic_role_labeling/2/triples/model.txt
introducing,has,highway connections,model,/content/training-data/semantic_role_labeling/2/triples/model.txt
deep bidirectional LSTMs,differ by,decoding,model,/content/training-data/semantic_role_labeling/2/triples/model.txt
decoding,with,BIOconstraints,model,/content/training-data/semantic_role_labeling/2/triples/model.txt
deep bidirectional LSTMs,differ by,simplifying,model,/content/training-data/semantic_role_labeling/2/triples/model.txt
simplifying,has,input and output layers,model,/content/training-data/semantic_role_labeling/2/triples/model.txt
Contribution,has research problem,Deep Semantic Role Labeling,research-problem,/content/training-data/semantic_role_labeling/2/triples/research-problem.txt
Contribution,has research problem,semantic role labeling ( SRL ,research-problem,/content/training-data/semantic_role_labeling/2/triples/research-problem.txt
Contribution,has research problem,SRL,research-problem,/content/training-data/semantic_role_labeling/2/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/semantic_role_labeling/2/triples/results.txt
Results,In comparison with,best reported results,results,/content/training-data/semantic_role_labeling/2/triples/results.txt
best reported results,has,our percentage of completely correct predicates,results,/content/training-data/semantic_role_labeling/2/triples/results.txt
our percentage of completely correct predicates,improves by,5.9 points,results,/content/training-data/semantic_role_labeling/2/triples/results.txt
Results,show,accurate syntactic information,results,/content/training-data/semantic_role_labeling/2/triples/results.txt
accurate syntactic information,improve,deep models,results,/content/training-data/semantic_role_labeling/2/triples/results.txt
Results,has,Our single model,results,/content/training-data/semantic_role_labeling/2/triples/results.txt
Our single model,achieves,more than a 0.4 improvement,results,/content/training-data/semantic_role_labeling/2/triples/results.txt
more than a 0.4 improvement,on,both datasets,results,/content/training-data/semantic_role_labeling/2/triples/results.txt
Results,has,Our ensemble ( PoE ,results,/content/training-data/semantic_role_labeling/2/triples/results.txt
Our ensemble ( PoE ),has,an absolute improvement,results,/content/training-data/semantic_role_labeling/2/triples/results.txt
an absolute improvement,over,previous state of the art,results,/content/training-data/semantic_role_labeling/2/triples/results.txt
previous state of the art,of,2.1 F1,results,/content/training-data/semantic_role_labeling/2/triples/results.txt
previous state of the art,on,CoNLL 2005,results,/content/training-data/semantic_role_labeling/2/triples/results.txt
previous state of the art,on,CoNLL 2012,results,/content/training-data/semantic_role_labeling/2/triples/results.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/semantic_role_labeling/1/triples/hyperparameters.txt
Hyperparameters,regularize,our model,hyperparameters,/content/training-data/semantic_role_labeling/1/triples/hyperparameters.txt
our model,using,dropout,hyperparameters,/content/training-data/semantic_role_labeling/1/triples/hyperparameters.txt
Hyperparameters,use,gradient clipping,hyperparameters,/content/training-data/semantic_role_labeling/1/triples/hyperparameters.txt
gradient clipping,to avoid,exploding gradients,hyperparameters,/content/training-data/semantic_role_labeling/1/triples/hyperparameters.txt
Hyperparameters,train,model,hyperparameters,/content/training-data/semantic_role_labeling/1/triples/hyperparameters.txt
model,using,"Nadam ( Dozat , 2016 ) SGD",hyperparameters,/content/training-data/semantic_role_labeling/1/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/semantic_role_labeling/1/triples/model.txt
Model,encode,each sentence,model,/content/training-data/semantic_role_labeling/1/triples/model.txt
each sentence,predict,predicates,model,/content/training-data/semantic_role_labeling/1/triples/model.txt
predicates,then predict,semantic roles,model,/content/training-data/semantic_role_labeling/1/triples/model.txt
each sentence,predict,part - of - speech tags,model,/content/training-data/semantic_role_labeling/1/triples/model.txt
each sentence,predict,labeled syntactic parse,model,/content/training-data/semantic_role_labeling/1/triples/model.txt
each sentence,has,only once,model,/content/training-data/semantic_role_labeling/1/triples/model.txt
Model,trained to,jointly predict,model,/content/training-data/semantic_role_labeling/1/triples/model.txt
jointly predict,has,parts of speech and predicates,model,/content/training-data/semantic_role_labeling/1/triples/model.txt
Model,trained to,perform parsing,model,/content/training-data/semantic_role_labeling/1/triples/model.txt
Model,trained to,attend,model,/content/training-data/semantic_role_labeling/1/triples/model.txt
attend,to,syntactic parse parents,model,/content/training-data/semantic_role_labeling/1/triples/model.txt
Model,trained to,assigning,model,/content/training-data/semantic_role_labeling/1/triples/model.txt
assigning,has,semantic role labels,model,/content/training-data/semantic_role_labeling/1/triples/model.txt
Model,has,end - to - end,model,/content/training-data/semantic_role_labeling/1/triples/model.txt
end - to - end,has,earlier layers,model,/content/training-data/semantic_role_labeling/1/triples/model.txt
earlier layers,trained to predict,prerequisite parts - of - speech and predicates,model,/content/training-data/semantic_role_labeling/1/triples/model.txt
end - to - end,has,latter,model,/content/training-data/semantic_role_labeling/1/triples/model.txt
latter,supplied to,later layers,model,/content/training-data/semantic_role_labeling/1/triples/model.txt
later layers,for,scoring,model,/content/training-data/semantic_role_labeling/1/triples/model.txt
Model,propose,linguistically - informed self - attention ( LISA ,model,/content/training-data/semantic_role_labeling/1/triples/model.txt
Contribution,has research problem,Semantic Role Labeling,research-problem,/content/training-data/semantic_role_labeling/1/triples/research-problem.txt
Contribution,has research problem,semantic role labeling ( SRL ,research-problem,/content/training-data/semantic_role_labeling/1/triples/research-problem.txt
Contribution,has research problem,SRL,research-problem,/content/training-data/semantic_role_labeling/1/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
Results,demonstrate that,our models,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
our models,benefit from,injecting,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
injecting,at,test time,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
injecting,has,state - of - the - art predicted parses,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
Results,For,models,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
models,using,GloVe embeddings,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
models,has,our syntax - free SA model,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
our syntax - free SA model,achieves,new state - of - the - art,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
new state - of - the - art,by jointly predicting,predicates,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
new state - of - the - art,by jointly predicting,POS,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
new state - of - the - art,by jointly predicting,SRL,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
Results,has,LISA,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
LISA,with,own parses,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
own parses,performs,comparably,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
comparably,to,SA,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
LISA,when supplied with,D&M parses,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
D&M parses,out - performs,previous state - of - the - art,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
previous state - of - the - art,by,2.5 F1 points,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
Results,Incorporating,ELMo em-beddings,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
ELMo em-beddings,improves,all scores,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
Results,on,CoNLL - 2005 shared task and the CoNLL - 2012 English subset of OntoNotes 5.0,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
CoNLL - 2005 shared task and the CoNLL - 2012 English subset of OntoNotes 5.0,achieving,state - of - the - art results,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
state - of - the - art results,for,single model,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
single model,with,predicted predicates,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
single model,on,both corpora,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
Results,On,out - ofdomain Brown test set,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
out - ofdomain Brown test set,has,LISA,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
LISA,performs,comparably,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
comparably,with,its own parses,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
comparably,to,syntax - free counterpart,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
LISA,with,D&M parses,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
D&M parses,performs,exceptionally well,results,/content/training-data/semantic_role_labeling/1/triples/results.txt
Contribution,has,Approach,approach,/content/training-data/text-to-speech_synthesis/0/triples/approach.txt
Approach,transfer,knowledge of the ensemble models,approach,/content/training-data/text-to-speech_synthesis/0/triples/approach.txt
knowledge of the ensemble models,to,light - weight model,approach,/content/training-data/text-to-speech_synthesis/0/triples/approach.txt
light - weight model,suitable for,online deployment,approach,/content/training-data/text-to-speech_synthesis/0/triples/approach.txt
Approach,use,knowledge distillation,approach,/content/training-data/text-to-speech_synthesis/0/triples/approach.txt
knowledge distillation,to leverage,large amount of unlabeled words,approach,/content/training-data/text-to-speech_synthesis/0/triples/approach.txt
knowledge distillation,train,teacher model,approach,/content/training-data/text-to-speech_synthesis/0/triples/approach.txt
teacher model,to generate,phoneme sequence,approach,/content/training-data/text-to-speech_synthesis/0/triples/approach.txt
phoneme sequence,as well as,probability distribution,approach,/content/training-data/text-to-speech_synthesis/0/triples/approach.txt
probability distribution,given,unlabeled grapheme sequence,approach,/content/training-data/text-to-speech_synthesis/0/triples/approach.txt
Approach,adopt,Transformer,approach,/content/training-data/text-to-speech_synthesis/0/triples/approach.txt
Transformer,as,basic encoder - decoder model structure,approach,/content/training-data/text-to-speech_synthesis/0/triples/approach.txt
Transformer,instead of,RNN or CNN,approach,/content/training-data/text-to-speech_synthesis/0/triples/approach.txt
Approach,propose,token - level ensemble distillation,approach,/content/training-data/text-to-speech_synthesis/0/triples/approach.txt
token - level ensemble distillation,for,G2P conversion,approach,/content/training-data/text-to-speech_synthesis/0/triples/approach.txt
Approach,train,"variety of models ( CNN , RNN and Transformer ",approach,/content/training-data/text-to-speech_synthesis/0/triples/approach.txt
"variety of models ( CNN , RNN and Transformer )",for,ensemble,approach,/content/training-data/text-to-speech_synthesis/0/triples/approach.txt
ensemble,to get,higher accuracy,approach,/content/training-data/text-to-speech_synthesis/0/triples/approach.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/text-to-speech_synthesis/0/triples/ablation-analysis.txt
Ablation analysis,study,effect of ensemble teacher model in knowledge distillation,ablation-analysis,/content/training-data/text-to-speech_synthesis/0/triples/ablation-analysis.txt
effect of ensemble teacher model in knowledge distillation,boost,accuracy,ablation-analysis,/content/training-data/text-to-speech_synthesis/0/triples/ablation-analysis.txt
accuracy,by,more than 1 % WER,ablation-analysis,/content/training-data/text-to-speech_synthesis/0/triples/ablation-analysis.txt
accuracy,compared with,single teacher model ( a Transformer model with 6 - layer encoder and 6 - layer decoder ,ablation-analysis,/content/training-data/text-to-speech_synthesis/0/triples/ablation-analysis.txt
Ablation analysis,study,effect of distilling from unlabeled source words,ablation-analysis,/content/training-data/text-to-speech_synthesis/0/triples/ablation-analysis.txt
effect of distilling from unlabeled source words,boost,accuracy,ablation-analysis,/content/training-data/text-to-speech_synthesis/0/triples/ablation-analysis.txt
accuracy,by,nearly 1 % WER,ablation-analysis,/content/training-data/text-to-speech_synthesis/0/triples/ablation-analysis.txt
accuracy,demonstrating the effectiveness by,introducing abundant unlabeled data,ablation-analysis,/content/training-data/text-to-speech_synthesis/0/triples/ablation-analysis.txt
introducing abundant unlabeled data,into,knowledge distillation,ablation-analysis,/content/training-data/text-to-speech_synthesis/0/triples/ablation-analysis.txt
Ablation analysis,compare,Transformer,ablation-analysis,/content/training-data/text-to-speech_synthesis/0/triples/ablation-analysis.txt
Transformer,with,RNN and CNN based models,ablation-analysis,/content/training-data/text-to-speech_synthesis/0/triples/ablation-analysis.txt
Transformer,without using,knowledge distillation and unlabeled data,ablation-analysis,/content/training-data/text-to-speech_synthesis/0/triples/ablation-analysis.txt
Transformer,outperforms,RNN and CNN based models,ablation-analysis,/content/training-data/text-to-speech_synthesis/0/triples/ablation-analysis.txt
Contribution,has,Experiments,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
Experiments,has,Experimental setup,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
Experimental setup,use,WER ( word error rate ) and PER ( phoneme error rate ,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
WER ( word error rate ) and PER ( phoneme error rate ),to measure,accuracy of G2P conversion,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
Experimental setup,use,Adam optimizer,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
Adam optimizer,for,all models,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
Adam optimizer,follow,learning rate schedule,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
Experimental setup,use,beam search,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
beam search,set,beam size to 10,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
beam search,during,inference,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
Experimental setup,has,"residual dropout , attention dropout and ReLU dropout",experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
"residual dropout , attention dropout and ReLU dropout",for,Transformer models,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
Transformer models,is,"0.2 , 0.4 , 0.4",experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
Experimental setup,has,dropout,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
dropout,is,0.3,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
0.3,for,Bi - LSTM and CNN models,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
Experimental setup,implement experiments with,fairseq - py 4 library in Py-Torch,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
Experimental setup,train,each model,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
each model,on,8 NVIDIA M40 GPUs,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
8 NVIDIA M40 GPUs,contains,roughly 4000 tokens,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
roughly 4000 tokens,in,one mini-batch,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
Experiments,has,Model,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
Model,has,Ensemble Model,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
Ensemble Model,use,4 Transformer models,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
Ensemble Model,use,3 CNN models,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
Ensemble Model,use,3 Bi - LSTM models,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
Ensemble Model,has,4 Transformer models,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
4 Transformer models,vary in,number of the encoder - decoder layers,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
4 Transformer models,share,same hidden size ( 256 ,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
Ensemble Model,has,3 Bi - LSTM models,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
3 Bi - LSTM models,share,same number of encoder - decoder layers ( 1 - 1 ,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
3 Bi - LSTM models,with different,"hidden sizes ( 256 , 384 and 512 ",experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
Ensemble Model,has,3 CNN models,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
3 CNN models,vary in,"number of encoder - decoder layers ( 10 - 10 , 10 - 10 , 8 - 8 ) and convolutional kernel widths ( 3 , 2 , 2 ",experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
3 CNN models,share,same hidden size ( 256 ,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
Model,has,Student Model,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
Student Model,use,default configurations ( 256 hidden size and 6 - 6 layers of encoder - decoder ,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
Student Model,choose,Transformer,experiments,/content/training-data/text-to-speech_synthesis/0/triples/experiments.txt
Contribution,has research problem,Grapheme - to - Phoneme Conversion,research-problem,/content/training-data/text-to-speech_synthesis/0/triples/research-problem.txt
Contribution,has research problem,Grapheme - to - phoneme ( G2P ) conversion,research-problem,/content/training-data/text-to-speech_synthesis/0/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/text-to-speech_synthesis/0/triples/results.txt
Results,on,CMUDict,results,/content/training-data/text-to-speech_synthesis/0/triples/results.txt
CMUDict,can be seen,our method on 6 - layer encoder and 6 - layer decoder Transformer,results,/content/training-data/text-to-speech_synthesis/0/triples/results.txt
our method on 6 - layer encoder and 6 - layer decoder Transformer,achieves,new state - of - the - art result,results,/content/training-data/text-to-speech_synthesis/0/triples/results.txt
new state - of - the - art result,of,19.88 % WER,results,/content/training-data/text-to-speech_synthesis/0/triples/results.txt
new state - of - the - art result,outperforming,NSGD,results,/content/training-data/text-to-speech_synthesis/0/triples/results.txt
NSGD,by,4.22 % WER,results,/content/training-data/text-to-speech_synthesis/0/triples/results.txt
Results,on,internal dataset,results,/content/training-data/text-to-speech_synthesis/0/triples/results.txt
internal dataset,outperforms,CNN with NSGD,results,/content/training-data/text-to-speech_synthesis/0/triples/results.txt
CNN with NSGD,demonstrates,effectiveness of our method,results,/content/training-data/text-to-speech_synthesis/0/triples/results.txt
effectiveness of our method,for,G2P conversion,results,/content/training-data/text-to-speech_synthesis/0/triples/results.txt
CNN with NSGD,by,3.52 % WER,results,/content/training-data/text-to-speech_synthesis/0/triples/results.txt
Contribution,has,Approach,approach,/content/training-data/text-to-speech_synthesis/2/triples/approach.txt
Approach,to decouple,speaker modeling,approach,/content/training-data/text-to-speech_synthesis/2/triples/approach.txt
speaker modeling,from,speech synthesis,approach,/content/training-data/text-to-speech_synthesis/2/triples/approach.txt
speaker modeling,by independently training,speaker - discriminative embedding network,approach,/content/training-data/text-to-speech_synthesis/2/triples/approach.txt
speaker - discriminative embedding network,captures,space of speaker characteristics,approach,/content/training-data/text-to-speech_synthesis/2/triples/approach.txt
speaker modeling,training,high quality TTS model,approach,/content/training-data/text-to-speech_synthesis/2/triples/approach.txt
high quality TTS model,conditioned on,representation learned by the first network,approach,/content/training-data/text-to-speech_synthesis/2/triples/approach.txt
high quality TTS model,on,smaller dataset,approach,/content/training-data/text-to-speech_synthesis/2/triples/approach.txt
Approach,trained on,untranscribed speech,approach,/content/training-data/text-to-speech_synthesis/2/triples/approach.txt
untranscribed speech,containing,reverberation and background noise,approach,/content/training-data/text-to-speech_synthesis/2/triples/approach.txt
reverberation and background noise,from,large number of speakers,approach,/content/training-data/text-to-speech_synthesis/2/triples/approach.txt
Approach,train,speaker embedding network,approach,/content/training-data/text-to-speech_synthesis/2/triples/approach.txt
speaker embedding network,on,speaker verification task,approach,/content/training-data/text-to-speech_synthesis/2/triples/approach.txt
speaker verification task,to determine,two different utterances were spoken by the same speaker,approach,/content/training-data/text-to-speech_synthesis/2/triples/approach.txt
Contribution,has research problem,Text - To - Speech Synthesis,research-problem,/content/training-data/text-to-speech_synthesis/2/triples/research-problem.txt
Contribution,has research problem,text - to - speech ( TTS ) synthesis,research-problem,/content/training-data/text-to-speech_synthesis/2/triples/research-problem.txt
Contribution,has research problem,build a TTS system,research-problem,/content/training-data/text-to-speech_synthesis/2/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
Results,has,Number of speaker encoder training speakers,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
Number of speaker encoder training speakers,improve significantly,both naturalness and similarity,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
both naturalness and similarity,As,increases,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
increases,has,number of training speakers,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
Results,has,Speech naturalness,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
Speech naturalness,has,MOS on unseen speakers,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
MOS on unseen speakers,is higher than,seen speakers,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
seen speakers,by as much as,0.2 points,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
0.2 points,on,LibriSpeech,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
Speech naturalness,has,audio generated,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
audio generated,for,unseen speakers,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
unseen speakers,is deemed to be at least as natural as,generated for seen speakers,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
Speech naturalness,has,proposed model,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
proposed model,achieved,about 4.0 MOS,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
about 4.0 MOS,in,all datasets,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
Results,has,Speaker embedding space,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
Speaker embedding space,has,PCA visualization,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
PCA visualization,shows,synthesized utterances,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
synthesized utterances,tend to lie very close to,real speech from the same speaker,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
real speech from the same speaker,in,embedding space,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
Speaker embedding space,has,t - SNE visualization,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
t - SNE visualization,where,utterances,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
utterances,from,each synthetic speaker,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
each synthetic speaker,from,distinct cluster,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
distinct cluster,adjacent to,cluster of real utterances,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
cluster of real utterances,from,corresponding speaker,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
t - SNE visualization,demonstrated,synthetic utterances,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
synthetic utterances,easily distinguishable from,real human speech,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
Results,has,Fictitious speakers,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
Fictitious speakers,conditioning,synthesizer,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
synthesizer,on,random points,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
random points,in,speaker embedding space,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
Fictitious speakers,Bypassing,speaker encoder network,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
Results,has,Speaker similarity,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
Speaker similarity,scores for,VCTK model,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
VCTK model,tend to be higher than those for,LibriSpeech,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
LibriSpeech,reflecting,cleaner nature of the dataset,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
Speaker similarity,For,seen speakers on VCTK,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
seen speakers on VCTK,has,proposed model,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
proposed model,performs about as well as,baseline,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
baseline,uses,embedding lookup table for speaker conditioning,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
Results,has,Speaker verification,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
Speaker verification,trained on,LibriSpeech,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
LibriSpeech,has,synthesized speech,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
synthesized speech,most similar to,ground truth voices,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
Speaker verification,On,20 voice discrimination task,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
20 voice discrimination task,obtain,EER of 2.86 %,results,/content/training-data/text-to-speech_synthesis/2/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/text-to-speech_synthesis/1/triples/ablation-analysis.txt
Ablation analysis,has,1D Convolution in FFT Block,ablation-analysis,/content/training-data/text-to-speech_synthesis/1/triples/ablation-analysis.txt
1D Convolution in FFT Block,replace,original fully connected layer,ablation-analysis,/content/training-data/text-to-speech_synthesis/1/triples/ablation-analysis.txt
1D Convolution in FFT Block,results in,- 0.113 CMOS,ablation-analysis,/content/training-data/text-to-speech_synthesis/1/triples/ablation-analysis.txt
Ablation analysis,has,Sequence - Level Knowledge Distillation,ablation-analysis,/content/training-data/text-to-speech_synthesis/1/triples/ablation-analysis.txt
Sequence - Level Knowledge Distillation,removing,sequence - level knowledge distillation,ablation-analysis,/content/training-data/text-to-speech_synthesis/1/triples/ablation-analysis.txt
sequence - level knowledge distillation,results in,- 0.325 CMOS,ablation-analysis,/content/training-data/text-to-speech_synthesis/1/triples/ablation-analysis.txt
Contribution,has,Model,model,/content/training-data/text-to-speech_synthesis/1/triples/model.txt
Model,adopts,feed - forward network,model,/content/training-data/text-to-speech_synthesis/1/triples/model.txt
feed - forward network,based on,self - attention in Transformer and 1D convolution,model,/content/training-data/text-to-speech_synthesis/1/triples/model.txt
Model,adopts,length regulator,model,/content/training-data/text-to-speech_synthesis/1/triples/model.txt
length regulator,up - samples,phoneme sequence according to the phoneme duration,model,/content/training-data/text-to-speech_synthesis/1/triples/model.txt
phoneme sequence according to the phoneme duration,to match,length of the mel-spectrogram sequence,model,/content/training-data/text-to-speech_synthesis/1/triples/model.txt
length regulator,built on,phoneme duration predictor,model,/content/training-data/text-to-speech_synthesis/1/triples/model.txt
phoneme duration predictor,predicts,duration of each phoneme,model,/content/training-data/text-to-speech_synthesis/1/triples/model.txt
Model,propose,FastSpeech,model,/content/training-data/text-to-speech_synthesis/1/triples/model.txt
FastSpeech,generates,mel-spectrograms non-autoregressively,model,/content/training-data/text-to-speech_synthesis/1/triples/model.txt
FastSpeech,takes,text ( phoneme ) sequence as input,model,/content/training-data/text-to-speech_synthesis/1/triples/model.txt
Contribution,has research problem,Text to Speech,research-problem,/content/training-data/text-to-speech_synthesis/1/triples/research-problem.txt
Contribution,has research problem,Neural network based end - to - end text to speech ( TTS ,research-problem,/content/training-data/text-to-speech_synthesis/1/triples/research-problem.txt
Contribution,has research problem,Text to speech ( TTS ,research-problem,/content/training-data/text-to-speech_synthesis/1/triples/research-problem.txt
Contribution,has research problem,Neural network based TTS,research-problem,/content/training-data/text-to-speech_synthesis/1/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/text-to-speech_synthesis/1/triples/experimental-setup.txt
Experimental setup,leverage,sequence - level knowledge distillation,experimental-setup,/content/training-data/text-to-speech_synthesis/1/triples/experimental-setup.txt
Experimental setup,In the inference process,output mel-spectrograms,experimental-setup,/content/training-data/text-to-speech_synthesis/1/triples/experimental-setup.txt
output mel-spectrograms,transformed into,audio samples using the pretrained WaveGlow,experimental-setup,/content/training-data/text-to-speech_synthesis/1/triples/experimental-setup.txt
Experimental setup,use,Adam optimizer,experimental-setup,/content/training-data/text-to-speech_synthesis/1/triples/experimental-setup.txt
Adam optimizer,with,"? 1 = 0.9 , ? 2 = 0.98 , ? = 10 ?9",experimental-setup,/content/training-data/text-to-speech_synthesis/1/triples/experimental-setup.txt
Experimental setup,train,autoregressive Transformer TTS model,experimental-setup,/content/training-data/text-to-speech_synthesis/1/triples/experimental-setup.txt
autoregressive Transformer TTS model,on,4 NVIDIA V100 GPUs,experimental-setup,/content/training-data/text-to-speech_synthesis/1/triples/experimental-setup.txt
4 NVIDIA V100 GPUs,with,batchsize of 16 sentences,experimental-setup,/content/training-data/text-to-speech_synthesis/1/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/text-to-speech_synthesis/1/triples/results.txt
Results,has,Audio Quality,results,/content/training-data/text-to-speech_synthesis/1/triples/results.txt
Audio Quality,conduct,MOS ( mean opinion score ) evaluation,results,/content/training-data/text-to-speech_synthesis/1/triples/results.txt
MOS ( mean opinion score ) evaluation,on,test set,results,/content/training-data/text-to-speech_synthesis/1/triples/results.txt
test set,to measure,audio quality,results,/content/training-data/text-to-speech_synthesis/1/triples/results.txt
Results,has,Voice Speed,results,/content/training-data/text-to-speech_synthesis/1/triples/results.txt
Voice Speed,demonstrated by,samples,results,/content/training-data/text-to-speech_synthesis/1/triples/results.txt
samples,has,FastSpeech,results,/content/training-data/text-to-speech_synthesis/1/triples/results.txt
FastSpeech,can adjust the voice speed,from 0.5x to 1.5 x smoothly,results,/content/training-data/text-to-speech_synthesis/1/triples/results.txt
from 0.5x to 1.5 x smoothly,with,stable and almost unchanged pitch,results,/content/training-data/text-to-speech_synthesis/1/triples/results.txt
Results,has,Robustness,results,/content/training-data/text-to-speech_synthesis/1/triples/results.txt
Robustness,can be seen that,FastSpeech,results,/content/training-data/text-to-speech_synthesis/1/triples/results.txt
FastSpeech,effectively eliminate,word repeating and skipping,results,/content/training-data/text-to-speech_synthesis/1/triples/results.txt
word repeating and skipping,to improve,intelligibility,results,/content/training-data/text-to-speech_synthesis/1/triples/results.txt
Robustness,can be seen that,Transformer TTS,results,/content/training-data/text-to-speech_synthesis/1/triples/results.txt
Transformer TTS,not robust to,hard cases,results,/content/training-data/text-to-speech_synthesis/1/triples/results.txt
hard cases,gets,34 % error rate,results,/content/training-data/text-to-speech_synthesis/1/triples/results.txt
Contribution,has,Model,model,/content/training-data/part-of-speech_tagging/4/triples/model.txt
Model,aim to encode,sequences,model,/content/training-data/part-of-speech_tagging/4/triples/model.txt
sequences,of,label distributions,model,/content/training-data/part-of-speech_tagging/4/triples/model.txt
label distributions,using,recurrent neural network,model,/content/training-data/part-of-speech_tagging/4/triples/model.txt
Model,investigate,neural network model,model,/content/training-data/part-of-speech_tagging/4/triples/model.txt
neural network model,for,output label sequences,model,/content/training-data/part-of-speech_tagging/4/triples/model.txt
Model,represent,each possible label,model,/content/training-data/part-of-speech_tagging/4/triples/model.txt
each possible label,using,embedding vector,model,/content/training-data/part-of-speech_tagging/4/triples/model.txt
Model,represent,full - exponential search space,model,/content/training-data/part-of-speech_tagging/4/triples/model.txt
full - exponential search space,without making,Markov assumptions,model,/content/training-data/part-of-speech_tagging/4/triples/model.txt
Contribution,has research problem,Sequence Labeling,research-problem,/content/training-data/part-of-speech_tagging/4/triples/research-problem.txt
Contribution,has research problem,statistical sequence labeling,research-problem,/content/training-data/part-of-speech_tagging/4/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/part-of-speech_tagging/4/triples/results.txt
Results,has,OntoNotes 5.0,results,/content/training-data/part-of-speech_tagging/4/triples/results.txt
OntoNotes 5.0,has,BiLSTM - LAN,results,/content/training-data/part-of-speech_tagging/4/triples/results.txt
BiLSTM - LAN,obtains,new state - of - theart results,results,/content/training-data/part-of-speech_tagging/4/triples/results.txt
new state - of - theart results,on,CCGBank,results,/content/training-data/part-of-speech_tagging/4/triples/results.txt
BiLSTM - LAN,has,significantly outperforms,results,/content/training-data/part-of-speech_tagging/4/triples/results.txt
significantly outperforms,by,1.17 F1-score,results,/content/training-data/part-of-speech_tagging/4/triples/results.txt
significantly outperforms,has,BiLSTM - CRF,results,/content/training-data/part-of-speech_tagging/4/triples/results.txt
significantly outperforms,both,BiLSTMsoftmax and BiLSTM - CRF ( p < 0.01 ,results,/content/training-data/part-of-speech_tagging/4/triples/results.txt
BiLSTMsoftmax and BiLSTM - CRF ( p < 0.01 ),showing,advantage,results,/content/training-data/part-of-speech_tagging/4/triples/results.txt
advantage,of,LAN,results,/content/training-data/part-of-speech_tagging/4/triples/results.txt
Results,has,Universal Dependencies ( UD ) v 2.2,results,/content/training-data/part-of-speech_tagging/4/triples/results.txt
Universal Dependencies ( UD ) v 2.2,has,Our model,results,/content/training-data/part-of-speech_tagging/4/triples/results.txt
Our model,outperforms,all the baselines,results,/content/training-data/part-of-speech_tagging/4/triples/results.txt
all the baselines,on,all the languages,results,/content/training-data/part-of-speech_tagging/4/triples/results.txt
Universal Dependencies ( UD ) v 2.2,has,improvements,results,/content/training-data/part-of-speech_tagging/4/triples/results.txt
improvements,are,statistically significant,results,/content/training-data/part-of-speech_tagging/4/triples/results.txt
statistically significant,for,all the languages ( p < 0.01 ,results,/content/training-data/part-of-speech_tagging/4/triples/results.txt
all the languages ( p < 0.01 ),suggesting that,BiLSTM - LAN,results,/content/training-data/part-of-speech_tagging/4/triples/results.txt
BiLSTM - LAN,is,generally effective,results,/content/training-data/part-of-speech_tagging/4/triples/results.txt
generally effective,across,languages,results,/content/training-data/part-of-speech_tagging/4/triples/results.txt
Results,has,WSJ,results,/content/training-data/part-of-speech_tagging/4/triples/results.txt
WSJ,has,BiLSTM - LAN,results,/content/training-data/part-of-speech_tagging/4/triples/results.txt
BiLSTM - LAN,gives,significant accuracy improvements,results,/content/training-data/part-of-speech_tagging/4/triples/results.txt
significant accuracy improvements,over,BiLSTM - CRF,results,/content/training-data/part-of-speech_tagging/4/triples/results.txt
significant accuracy improvements,over,BiLSTM- softmax,results,/content/training-data/part-of-speech_tagging/4/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/part-of-speech_tagging/3/triples/baselines.txt
Baselines,has,three baseline systems,baselines,/content/training-data/part-of-speech_tagging/3/triples/baselines.txt
three baseline systems,name,"BRNN , the bi-direction RNN",baselines,/content/training-data/part-of-speech_tagging/3/triples/baselines.txt
three baseline systems,name,"BLSTM , the bidirection LSTM",baselines,/content/training-data/part-of-speech_tagging/3/triples/baselines.txt
three baseline systems,name,"BLSTM - CNNs , the combination of BLSTM with CNN",baselines,/content/training-data/part-of-speech_tagging/3/triples/baselines.txt
"BLSTM - CNNs , the combination of BLSTM with CNN",to model,characterlevel information,baselines,/content/training-data/part-of-speech_tagging/3/triples/baselines.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
Hyperparameters,reduce,effects,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
effects,of,gradient exploding,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
gradient exploding,use,gradient clipping,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
gradient clipping,of,5.0,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
Hyperparameters,For each of,embeddings,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
embeddings,fine - tune,initial embeddings,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
embeddings,modifying them during,gradient updates,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
gradient updates,of,neural network model,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
neural network model,by,back - propagating gradients,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
Hyperparameters,fix,dropout rate,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
dropout rate,at,0.5,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
0.5,for,all dropout layers,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
Hyperparameters,apply,dropout,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
dropout,on,character embeddings,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
character embeddings,before inputting to,CNN,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
dropout,on,input and output vectors,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
input and output vectors,of,BLSTM,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
Hyperparameters,use,early stopping,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
early stopping,based on,performance,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
performance,on,validation sets,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
Hyperparameters,To mitigate,overfitting,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
overfitting,apply,dropout method,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
dropout method,to regularize,model,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
Hyperparameters,choose,initial learning rate,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
initial learning rate,updated on,each epoch,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
each epoch,of,training,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
initial learning rate,of,0.01,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
0.01,for,POS tagging,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
initial learning rate,of,0.015,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
0.015,for,NER,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
Hyperparameters,has,""" best "" parameters",hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
""" best "" parameters",appear at,around 50 epochs,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
Hyperparameters,has,Parameter optimization,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
Parameter optimization,performed with,minibatch stochastic gradient descent ( SGD ,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
minibatch stochastic gradient descent ( SGD ),with,batch size,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
batch size,has,10,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
minibatch stochastic gradient descent ( SGD ),with,momentum,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
momentum,has,0.9,hyperparameters,/content/training-data/part-of-speech_tagging/3/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/part-of-speech_tagging/3/triples/model.txt
Model,first use,convolutional neural networks ( CNNs ,model,/content/training-data/part-of-speech_tagging/3/triples/model.txt
convolutional neural networks ( CNNs ),to encode,character - level information,model,/content/training-data/part-of-speech_tagging/3/triples/model.txt
character - level information,of,a word,model,/content/training-data/part-of-speech_tagging/3/triples/model.txt
a word,into,its character - level representation,model,/content/training-data/part-of-speech_tagging/3/triples/model.txt
Model,easily applied to,wide range,model,/content/training-data/part-of-speech_tagging/3/triples/model.txt
wide range,of,sequence labeling tasks,model,/content/training-data/part-of-speech_tagging/3/triples/model.txt
sequence labeling tasks,on,different languages and domains,model,/content/training-data/part-of-speech_tagging/3/triples/model.txt
Model,is,endto - end model,model,/content/training-data/part-of-speech_tagging/3/triples/model.txt
endto - end model,requiring no,task - specific resources,model,/content/training-data/part-of-speech_tagging/3/triples/model.txt
endto - end model,requiring no,feature engineering,model,/content/training-data/part-of-speech_tagging/3/triples/model.txt
endto - end model,requiring no,data pre-processing,model,/content/training-data/part-of-speech_tagging/3/triples/model.txt
endto - end model,beyond,pre-trained word embeddings,model,/content/training-data/part-of-speech_tagging/3/triples/model.txt
pre-trained word embeddings,on,unlabeled corpora,model,/content/training-data/part-of-speech_tagging/3/triples/model.txt
Model,On top of,BLSTM,model,/content/training-data/part-of-speech_tagging/3/triples/model.txt
BLSTM,use,sequential CRF,model,/content/training-data/part-of-speech_tagging/3/triples/model.txt
sequential CRF,to jointly decode,labels,model,/content/training-data/part-of-speech_tagging/3/triples/model.txt
labels,for,whole sentence,model,/content/training-data/part-of-speech_tagging/3/triples/model.txt
Model,propose,neural network architecture,model,/content/training-data/part-of-speech_tagging/3/triples/model.txt
neural network architecture,for,sequence labeling,model,/content/training-data/part-of-speech_tagging/3/triples/model.txt
Model,combine,character - and word - level representations,model,/content/training-data/part-of-speech_tagging/3/triples/model.txt
character - and word - level representations,feed them into,bi-directional LSTM ( BLSTM ,model,/content/training-data/part-of-speech_tagging/3/triples/model.txt
bi-directional LSTM ( BLSTM ),to model,context information,model,/content/training-data/part-of-speech_tagging/3/triples/model.txt
context information,of,each word,model,/content/training-data/part-of-speech_tagging/3/triples/model.txt
Contribution,has research problem,End - to - end Sequence Labeling,research-problem,/content/training-data/part-of-speech_tagging/3/triples/research-problem.txt
Contribution,has research problem,sequence labeling,research-problem,/content/training-data/part-of-speech_tagging/3/triples/research-problem.txt
Contribution,has research problem,Linguistic sequence labeling,research-problem,/content/training-data/part-of-speech_tagging/3/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/part-of-speech_tagging/3/triples/results.txt
Results,Comparing with,traditional statistical models,results,/content/training-data/part-of-speech_tagging/3/triples/results.txt
traditional statistical models,has,our system,results,/content/training-data/part-of-speech_tagging/3/triples/results.txt
our system,achieves,state - of - the - art accuracy,results,/content/training-data/part-of-speech_tagging/3/triples/results.txt
our system,obtaining,0.05 % improvement,results,/content/training-data/part-of-speech_tagging/3/triples/results.txt
0.05 % improvement,over,previously best reported results,results,/content/training-data/part-of-speech_tagging/3/triples/results.txt
Results,adding,CRF layer,results,/content/training-data/part-of-speech_tagging/3/triples/results.txt
CRF layer,achieve,significant improvements,results,/content/training-data/part-of-speech_tagging/3/triples/results.txt
significant improvements,over,BLSTM - CNN models,results,/content/training-data/part-of-speech_tagging/3/triples/results.txt
significant improvements,for,POS tagging and NER,results,/content/training-data/part-of-speech_tagging/3/triples/results.txt
CRF layer,for,joint decoding,results,/content/training-data/part-of-speech_tagging/3/triples/results.txt
Results,has,our model,results,/content/training-data/part-of-speech_tagging/3/triples/results.txt
our model,achieves,significant improvements,results,/content/training-data/part-of-speech_tagging/3/triples/results.txt
significant improvements,over,Senna,results,/content/training-data/part-of-speech_tagging/3/triples/results.txt
significant improvements,over,other three neural models,results,/content/training-data/part-of-speech_tagging/3/triples/results.txt
Contribution,has,Ablation analysis,ablation-analysis,/content/training-data/part-of-speech_tagging/5/triples/ablation-analysis.txt
Ablation analysis,shows that,separately optimized models,ablation-analysis,/content/training-data/part-of-speech_tagging/5/triples/ablation-analysis.txt
separately optimized models,are,significantly more accurate,ablation-analysis,/content/training-data/part-of-speech_tagging/5/triples/ablation-analysis.txt
significantly more accurate,than,jointly optimized models,ablation-analysis,/content/training-data/part-of-speech_tagging/5/triples/ablation-analysis.txt
significantly more accurate,on,average,ablation-analysis,/content/training-data/part-of-speech_tagging/5/triples/ablation-analysis.txt
Ablation analysis,for,Separate optimization,ablation-analysis,/content/training-data/part-of-speech_tagging/5/triples/ablation-analysis.txt
Separate optimization,leads to,better accuracy,ablation-analysis,/content/training-data/part-of-speech_tagging/5/triples/ablation-analysis.txt
better accuracy,for,30 out of 39 treebanks,ablation-analysis,/content/training-data/part-of-speech_tagging/5/triples/ablation-analysis.txt
30 out of 39 treebanks,for,xpos tagging,ablation-analysis,/content/training-data/part-of-speech_tagging/5/triples/ablation-analysis.txt
better accuracy,for,34 out of 40 treebanks,ablation-analysis,/content/training-data/part-of-speech_tagging/5/triples/ablation-analysis.txt
34 out of 40 treebanks,for,morphological features task,ablation-analysis,/content/training-data/part-of-speech_tagging/5/triples/ablation-analysis.txt
Separate optimization,has,outperformed,ablation-analysis,/content/training-data/part-of-speech_tagging/5/triples/ablation-analysis.txt
outperformed,has,joint optimization,ablation-analysis,/content/training-data/part-of-speech_tagging/5/triples/ablation-analysis.txt
joint optimization,by,up to 2.1 percent absolute,ablation-analysis,/content/training-data/part-of-speech_tagging/5/triples/ablation-analysis.txt
Ablation analysis,For,all of the network sizes,ablation-analysis,/content/training-data/part-of-speech_tagging/5/triples/ablation-analysis.txt
all of the network sizes,in,grid search,ablation-analysis,/content/training-data/part-of-speech_tagging/5/triples/ablation-analysis.txt
all of the network sizes,observed during,training,ablation-analysis,/content/training-data/part-of-speech_tagging/5/triples/ablation-analysis.txt
training,that,accuracy,ablation-analysis,/content/training-data/part-of-speech_tagging/5/triples/ablation-analysis.txt
accuracy,degrades with,more iterations,ablation-analysis,/content/training-data/part-of-speech_tagging/5/triples/ablation-analysis.txt
more iterations,for,character and word model,ablation-analysis,/content/training-data/part-of-speech_tagging/5/triples/ablation-analysis.txt
accuracy,reach,high value,ablation-analysis,/content/training-data/part-of-speech_tagging/5/triples/ablation-analysis.txt
Ablation analysis,has,joint,ablation-analysis,/content/training-data/part-of-speech_tagging/5/triples/ablation-analysis.txt
joint,never,out - performed,ablation-analysis,/content/training-data/part-of-speech_tagging/5/triples/ablation-analysis.txt
out - performed,by,more than 0.5 % absolute,ablation-analysis,/content/training-data/part-of-speech_tagging/5/triples/ablation-analysis.txt
out - performed,has,separate,ablation-analysis,/content/training-data/part-of-speech_tagging/5/triples/ablation-analysis.txt
Ablation analysis,has,combined model,ablation-analysis,/content/training-data/part-of-speech_tagging/5/triples/ablation-analysis.txt
combined model,has,significantly higher accuracy,ablation-analysis,/content/training-data/part-of-speech_tagging/5/triples/ablation-analysis.txt
significantly higher accuracy,compared with,character and word models,ablation-analysis,/content/training-data/part-of-speech_tagging/5/triples/ablation-analysis.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/part-of-speech_tagging/5/triples/hyperparameters.txt
Hyperparameters,has,dropout,hyperparameters,/content/training-data/part-of-speech_tagging/5/triples/hyperparameters.txt
dropout,used on,embeddings,hyperparameters,/content/training-data/part-of-speech_tagging/5/triples/hyperparameters.txt
embeddings,achieved by,RRIE,hyperparameters,/content/training-data/part-of-speech_tagging/5/triples/hyperparameters.txt
RRIE,is,relative reduction in error,hyperparameters,/content/training-data/part-of-speech_tagging/5/triples/hyperparameters.txt
Hyperparameters,has,word embeddings,hyperparameters,/content/training-data/part-of-speech_tagging/5/triples/hyperparameters.txt
word embeddings,are,initialized,hyperparameters,/content/training-data/part-of-speech_tagging/5/triples/hyperparameters.txt
initialized,with,zero values,hyperparameters,/content/training-data/part-of-speech_tagging/5/triples/hyperparameters.txt
Hyperparameters,has,pre-trained embeddings,hyperparameters,/content/training-data/part-of-speech_tagging/5/triples/hyperparameters.txt
pre-trained embeddings,not,updated,hyperparameters,/content/training-data/part-of-speech_tagging/5/triples/hyperparameters.txt
updated,during,training,hyperparameters,/content/training-data/part-of-speech_tagging/5/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/part-of-speech_tagging/5/triples/model.txt
Model,learn,context sensitive initial character and word representations,model,/content/training-data/part-of-speech_tagging/5/triples/model.txt
context sensitive initial character and word representations,through,two separate sentence - level recurrent models,model,/content/training-data/part-of-speech_tagging/5/triples/model.txt
context sensitive initial character and word representations,combined via,meta-BiLSTM model,model,/content/training-data/part-of-speech_tagging/5/triples/model.txt
meta-BiLSTM model,builds,unified representation,model,/content/training-data/part-of-speech_tagging/5/triples/model.txt
unified representation,used for,syntactic tagging,model,/content/training-data/part-of-speech_tagging/5/triples/model.txt
unified representation,of,each word,model,/content/training-data/part-of-speech_tagging/5/triples/model.txt
Contribution,has research problem,Morphosyntactic Tagging,research-problem,/content/training-data/part-of-speech_tagging/5/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/part-of-speech_tagging/5/triples/results.txt
Results,has,Morphological Tagging Results,results,/content/training-data/part-of-speech_tagging/5/triples/results.txt
Morphological Tagging Results,has,Our models,results,/content/training-data/part-of-speech_tagging/5/triples/results.txt
Our models,produce,significantly better results,results,/content/training-data/part-of-speech_tagging/5/triples/results.txt
significantly better results,than,winners of the CoNLL 2017 Shared Task,results,/content/training-data/part-of-speech_tagging/5/triples/results.txt
Results,has,Part - of - Speech Tagging Results,results,/content/training-data/part-of-speech_tagging/5/triples/results.txt
Part - of - Speech Tagging Results,has,Our model,results,/content/training-data/part-of-speech_tagging/5/triples/results.txt
Our model,has,outperforms,results,/content/training-data/part-of-speech_tagging/5/triples/results.txt
outperforms,with,13 ties,results,/content/training-data/part-of-speech_tagging/5/triples/results.txt
outperforms,in,32,results,/content/training-data/part-of-speech_tagging/5/triples/results.txt
32,of,54 treebanks,results,/content/training-data/part-of-speech_tagging/5/triples/results.txt
Our model,produce,better results,results,/content/training-data/part-of-speech_tagging/5/triples/results.txt
better results,especially for,morphologically rich languages,results,/content/training-data/part-of-speech_tagging/5/triples/results.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/part-of-speech_tagging/0/triples/hyperparameters.txt
Hyperparameters,use,gradient clipping,hyperparameters,/content/training-data/part-of-speech_tagging/0/triples/hyperparameters.txt
gradient clipping,of,5.0,hyperparameters,/content/training-data/part-of-speech_tagging/0/triples/hyperparameters.txt
Hyperparameters,trained with,early stopping,hyperparameters,/content/training-data/part-of-speech_tagging/0/triples/hyperparameters.txt
early stopping,based on,development performance,hyperparameters,/content/training-data/part-of-speech_tagging/0/triples/hyperparameters.txt
Hyperparameters,train,model parameters and word / character embeddings,hyperparameters,/content/training-data/part-of-speech_tagging/0/triples/hyperparameters.txt
model parameters and word / character embeddings,by,mini-batch stochastic gradient descent ( SGD ,hyperparameters,/content/training-data/part-of-speech_tagging/0/triples/hyperparameters.txt
mini-batch stochastic gradient descent ( SGD ),with,initial learning rate,hyperparameters,/content/training-data/part-of-speech_tagging/0/triples/hyperparameters.txt
initial learning rate,has,0.01,hyperparameters,/content/training-data/part-of-speech_tagging/0/triples/hyperparameters.txt
mini-batch stochastic gradient descent ( SGD ),with,decay rate,hyperparameters,/content/training-data/part-of-speech_tagging/0/triples/hyperparameters.txt
decay rate,has,0.05,hyperparameters,/content/training-data/part-of-speech_tagging/0/triples/hyperparameters.txt
mini-batch stochastic gradient descent ( SGD ),with,batch size,hyperparameters,/content/training-data/part-of-speech_tagging/0/triples/hyperparameters.txt
batch size,has,10,hyperparameters,/content/training-data/part-of-speech_tagging/0/triples/hyperparameters.txt
mini-batch stochastic gradient descent ( SGD ),with,momentum,hyperparameters,/content/training-data/part-of-speech_tagging/0/triples/hyperparameters.txt
momentum,has,0.9,hyperparameters,/content/training-data/part-of-speech_tagging/0/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/part-of-speech_tagging/0/triples/model.txt
Model,propose and carefully analyze,neural part - of - speech ( POS ) tagging model,model,/content/training-data/part-of-speech_tagging/0/triples/model.txt
neural part - of - speech ( POS ) tagging model,that exploits,adversarial training,model,/content/training-data/part-of-speech_tagging/0/triples/model.txt
Model,With,BiLSTM - CRF model,model,/content/training-data/part-of-speech_tagging/0/triples/model.txt
BiLSTM - CRF model,as,baseline POS tagger,model,/content/training-data/part-of-speech_tagging/0/triples/model.txt
BiLSTM - CRF model,apply,adversarial training,model,/content/training-data/part-of-speech_tagging/0/triples/model.txt
adversarial training,considering,perturbations,model,/content/training-data/part-of-speech_tagging/0/triples/model.txt
perturbations,to,input word / character embeddings,model,/content/training-data/part-of-speech_tagging/0/triples/model.txt
Contribution,has research problem,Part - of - Speech Tagging,research-problem,/content/training-data/part-of-speech_tagging/0/triples/research-problem.txt
Contribution,has research problem,neural POS tagging,research-problem,/content/training-data/part-of-speech_tagging/0/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/part-of-speech_tagging/0/triples/results.txt
Results,has,PTB - WSJ dataset,results,/content/training-data/part-of-speech_tagging/0/triples/results.txt
PTB - WSJ dataset,has,baseline ( BiLSTM - CRF ) model,results,/content/training-data/part-of-speech_tagging/0/triples/results.txt
baseline ( BiLSTM - CRF ) model,performs,on par,results,/content/training-data/part-of-speech_tagging/0/triples/results.txt
on par,with,other state - of - the - art systems,results,/content/training-data/part-of-speech_tagging/0/triples/results.txt
baseline ( BiLSTM - CRF ) model,Built upon,adversarial training ( AT ) model,results,/content/training-data/part-of-speech_tagging/0/triples/results.txt
adversarial training ( AT ) model,reaches,accuracy,results,/content/training-data/part-of-speech_tagging/0/triples/results.txt
accuracy,has,97.58 %,results,/content/training-data/part-of-speech_tagging/0/triples/results.txt
adversarial training ( AT ) model,outperforming,recent POS taggers,results,/content/training-data/part-of-speech_tagging/0/triples/results.txt
baseline ( BiLSTM - CRF ) model,has,accuracy,results,/content/training-data/part-of-speech_tagging/0/triples/results.txt
accuracy,has,97.54 %,results,/content/training-data/part-of-speech_tagging/0/triples/results.txt
Contribution,has,Approach,approach,/content/training-data/part-of-speech_tagging/2/triples/approach.txt
Approach,uses,gradient - based methods,approach,/content/training-data/part-of-speech_tagging/2/triples/approach.txt
gradient - based methods,for,efficient training,approach,/content/training-data/part-of-speech_tagging/2/triples/approach.txt
Approach,present,transfer learning approach,approach,/content/training-data/part-of-speech_tagging/2/triples/approach.txt
transfer learning approach,based on,deep hierarchical recurrent neural network,approach,/content/training-data/part-of-speech_tagging/2/triples/approach.txt
transfer learning approach,between,source task and the target task,approach,/content/training-data/part-of-speech_tagging/2/triples/approach.txt
source task and the target task,shares,hidden feature repre-sentation,approach,/content/training-data/part-of-speech_tagging/2/triples/approach.txt
source task and the target task,shares,part of the model parameters,approach,/content/training-data/part-of-speech_tagging/2/triples/approach.txt
Contribution,has,Experiments,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
Experiments,has,TRANSFER LEARNING PERFORMANCE,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
TRANSFER LEARNING PERFORMANCE,has,Hyperparameters,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
Hyperparameters,set,word embedding dimension,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
word embedding dimension,at,50,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
50,for,English,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
word embedding dimension,at,64,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
64,for,Spanish,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
Hyperparameters,set,initial learning rate,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
initial learning rate,at,0.01,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
Hyperparameters,set,character embedding dimension,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
character embedding dimension,at,25,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
Hyperparameters,set,dimension,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
dimension,of,hidden states,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
hidden states,of,character - level GRUs,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
character - level GRUs,at,80,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
hidden states,of,word - level GRUs,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
word - level GRUs,at,300,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
TRANSFER LEARNING PERFORMANCE,has,Results,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
Results,observe that,improvement,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
improvement,by,transfer learning,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
transfer learning,is,more substantial,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
more substantial,when,labeling rate,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
labeling rate,is,lower,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
Results,has,Cross - application transfer,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
Cross - application transfer,leads to,substantial improvement,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
substantial improvement,under,low - resource conditions,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
Results,has,our transfer learning approach,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
our transfer learning approach,has,improve the performance,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
improve the performance,on,Twitter POS tagging and NER,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
Twitter POS tagging and NER,for,all labeling rates,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
our transfer learning approach,has,improvements,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
improvements,with,0.1 labels,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
0.1 labels,are,more than 8 %,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
more than 8 %,for,both datasets,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
Results,see that,transfer learning approach,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
transfer learning approach,has,consistently improved,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
consistently improved,over,non-transfer results,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
Experiments,has,COMPARISON WITH STATE - OF - THE - ART RESULTS,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
COMPARISON WITH STATE - OF - THE - ART RESULTS,has,Hyperparameters,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
Hyperparameters,set,hidden state dimensions,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
hidden state dimensions,to be,300,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
300,for,word - level GRU,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
Hyperparameters,use,publicly available pretrained word embeddings,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
publicly available pretrained word embeddings,as,initialization,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
Hyperparameters,For,Spanish and Dutch,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
Spanish and Dutch,use,64 - dimensional Polyglot embeddings,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
Hyperparameters,has,initial learning rate,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
initial learning rate,for,AdaGrad,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
AdaGrad,fixed at,0.01,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
Hyperparameters,On,English datasets,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
English datasets,experiment with,50 - dimensional SENNA embeddings,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
English datasets,experiment with,100 - dimensional GloVe embeddings,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
English datasets,use,development set,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
development set,to choose,embeddings,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
embeddings,for,different tasks and settings,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
COMPARISON WITH STATE - OF - THE - ART RESULTS,has,Results,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
Results,has,our base model ( w/o transfer ,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
our base model ( w/o transfer ),performs,competitively,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
competitively,compared to,state - of - the - art systems,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
Results,has,transfer learning approach,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
transfer learning approach,achieves,new state - of - the - art results,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
new state - of - the - art results,on,all the considered benchmark datasets,experiments,/content/training-data/part-of-speech_tagging/2/triples/experiments.txt
Contribution,Code,https://github.com/kimiyoung/transfer,code,/content/training-data/part-of-speech_tagging/2/triples/code.txt
Contribution,has research problem,SEQUENCE TAGGING,research-problem,/content/training-data/part-of-speech_tagging/2/triples/research-problem.txt
Contribution,Code,https : //github.com/bplank/bilstm-aux,code,/content/training-data/part-of-speech_tagging/7/triples/code.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/part-of-speech_tagging/7/triples/hyperparameters.txt
Hyperparameters,use,offthe - shelf polyglot embeddings,hyperparameters,/content/training-data/part-of-speech_tagging/7/triples/hyperparameters.txt
Hyperparameters,has,100,hyperparameters,/content/training-data/part-of-speech_tagging/7/triples/hyperparameters.txt
100,for,character and byte embeddings,hyperparameters,/content/training-data/part-of-speech_tagging/7/triples/hyperparameters.txt
100,has,hidden states,hyperparameters,/content/training-data/part-of-speech_tagging/7/triples/hyperparameters.txt
Hyperparameters,has,Gaussian noise,hyperparameters,/content/training-data/part-of-speech_tagging/7/triples/hyperparameters.txt
Gaussian noise,with,0.2,hyperparameters,/content/training-data/part-of-speech_tagging/7/triples/hyperparameters.txt
Hyperparameters,has,128 dimensions,hyperparameters,/content/training-data/part-of-speech_tagging/7/triples/hyperparameters.txt
128 dimensions,for,word embeddings,hyperparameters,/content/training-data/part-of-speech_tagging/7/triples/hyperparameters.txt
Hyperparameters,has,default learning rate,hyperparameters,/content/training-data/part-of-speech_tagging/7/triples/hyperparameters.txt
default learning rate,has,0.1,hyperparameters,/content/training-data/part-of-speech_tagging/7/triples/hyperparameters.txt
Hyperparameters,has,training,hyperparameters,/content/training-data/part-of-speech_tagging/7/triples/hyperparameters.txt
training,is,stochastic,hyperparameters,/content/training-data/part-of-speech_tagging/7/triples/hyperparameters.txt
stochastic,use,fixed seed,hyperparameters,/content/training-data/part-of-speech_tagging/7/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/part-of-speech_tagging/7/triples/model.txt
Model,jointly predicts,POS and the log frequency of the word,model,/content/training-data/part-of-speech_tagging/7/triples/model.txt
Model,introduce,novel model,model,/content/training-data/part-of-speech_tagging/7/triples/model.txt
novel model,has,bi - LSTM trained with auxiliary loss,model,/content/training-data/part-of-speech_tagging/7/triples/model.txt
Contribution,has research problem,Multilingual Part - of - Speech Tagging,research-problem,/content/training-data/part-of-speech_tagging/7/triples/research-problem.txt
Contribution,has research problem,POS tagging,research-problem,/content/training-data/part-of-speech_tagging/7/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/part-of-speech_tagging/7/triples/results.txt
Results,has,over all best system,results,/content/training-data/part-of-speech_tagging/7/triples/results.txt
over all best system,is,multi-task bi - LSTM FREQBIN,results,/content/training-data/part-of-speech_tagging/7/triples/results.txt
Results,has,combined word + character representation model,results,/content/training-data/part-of-speech_tagging/7/triples/results.txt
combined word + character representation model,reaches,biggest improvement,results,/content/training-data/part-of-speech_tagging/7/triples/results.txt
biggest improvement,has,more than + 2 % accuracy,results,/content/training-data/part-of-speech_tagging/7/triples/results.txt
biggest improvement,on,Hebrew and Slovene,results,/content/training-data/part-of-speech_tagging/7/triples/results.txt
combined word + character representation model,is,best representation,results,/content/training-data/part-of-speech_tagging/7/triples/results.txt
best representation,has,outperforming,results,/content/training-data/part-of-speech_tagging/7/triples/results.txt
outperforming,has,baseline,results,/content/training-data/part-of-speech_tagging/7/triples/results.txt
baseline,on,all except one language ( Indonesian ,results,/content/training-data/part-of-speech_tagging/7/triples/results.txt
Results,compared,"Tnt , HunPos and TreeTagger",results,/content/training-data/part-of-speech_tagging/7/triples/results.txt
"Tnt , HunPos and TreeTagger",found,Tnt,results,/content/training-data/part-of-speech_tagging/7/triples/results.txt
Tnt,to be,consistently better,results,/content/training-data/part-of-speech_tagging/7/triples/results.txt
consistently better,than,Treetagger,results,/content/training-data/part-of-speech_tagging/7/triples/results.txt
Results,Initializing,word embeddings ( + POLYGLOT ,results,/content/training-data/part-of-speech_tagging/7/triples/results.txt
word embeddings ( + POLYGLOT ),with,off - the - shelf languagespecific embeddings,results,/content/training-data/part-of-speech_tagging/7/triples/results.txt
off - the - shelf languagespecific embeddings,improves,accuracy,results,/content/training-data/part-of-speech_tagging/7/triples/results.txt
Contribution,has,Baselines,baselines,/content/training-data/part-of-speech_tagging/1/triples/baselines.txt
Baselines,add,more layers,baselines,/content/training-data/part-of-speech_tagging/1/triples/baselines.txt
more layers,to,char - CNN model,baselines,/content/training-data/part-of-speech_tagging/1/triples/baselines.txt
char - CNN model,refer to,char - CNN - 5 and char - CNN - 9,baselines,/content/training-data/part-of-speech_tagging/1/triples/baselines.txt
char - CNN - 5 and char - CNN - 9,for,5 and 9 convolutional layers,baselines,/content/training-data/part-of-speech_tagging/1/triples/baselines.txt
Baselines,add,residual connections,baselines,/content/training-data/part-of-speech_tagging/1/triples/baselines.txt
residual connections,to,char - CNN - 9,baselines,/content/training-data/part-of-speech_tagging/1/triples/baselines.txt
char - CNN - 9,refer it as,char - ResNet,baselines,/content/training-data/part-of-speech_tagging/1/triples/baselines.txt
Baselines,apply,3 dense blocks,baselines,/content/training-data/part-of-speech_tagging/1/triples/baselines.txt
3 dense blocks,based on,char - ResNet,baselines,/content/training-data/part-of-speech_tagging/1/triples/baselines.txt
char - ResNet,refer to as,char - DenseNet,baselines,/content/training-data/part-of-speech_tagging/1/triples/baselines.txt
Baselines,use,LSTM - CRF,baselines,/content/training-data/part-of-speech_tagging/1/triples/baselines.txt
LSTM - CRF,with,randomly initialized word embeddings,baselines,/content/training-data/part-of-speech_tagging/1/triples/baselines.txt
Baselines,adopt,two state - of - the - art methods,baselines,/content/training-data/part-of-speech_tagging/1/triples/baselines.txt
two state - of - the - art methods,in,sequence labeling,baselines,/content/training-data/part-of-speech_tagging/1/triples/baselines.txt
two state - of - the - art methods,denoted as,char - LSTM,baselines,/content/training-data/part-of-speech_tagging/1/triples/baselines.txt
two state - of - the - art methods,denoted as,char - CNN,baselines,/content/training-data/part-of-speech_tagging/1/triples/baselines.txt
Contribution,has,Hyperparameters,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
Hyperparameters,employ,mini-batch stochastic gradient descent,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
mini-batch stochastic gradient descent,with,momentum,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
Hyperparameters,adopt,standard BIOES tagging scheme,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
standard BIOES tagging scheme,for,NER and Chunking,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
Hyperparameters,adopt,same initialization method,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
same initialization method,for,randomly initialized word embeddings,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
randomly initialized word embeddings,updated during,training,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
Hyperparameters,use,fastText 300 dimension word embeddings,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
fastText 300 dimension word embeddings,for,"Spanish , Dutch , and German",hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
Hyperparameters,use,GloVe 100 - dimension word embeddings,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
GloVe 100 - dimension word embeddings,for,English,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
Hyperparameters,use,pre-trained word embeddings,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
pre-trained word embeddings,for,initialization,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
Hyperparameters,For,IntNet,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
IntNet,has,filter size,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
filter size,of,initial convolution,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
initial convolution,is,32,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
filter size,of,other convolutions,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
other convolutions,is,16,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
Hyperparameters,has,gradient clipping,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
gradient clipping,is,5,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
Hyperparameters,has,size,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
size,of,dimensions,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
dimensions,of,character embeddings,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
character embeddings,are,randomly initialized,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
randomly initialized,using,uniform distribution,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
character embeddings,is,32,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
Hyperparameters,has,state size,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
state size,of,bi-directional LSTMs,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
bi-directional LSTMs,set to,256,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
Hyperparameters,has,number of convolutional layers,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
number of convolutional layers,are,5 and 9,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
5 and 9,for,IntNet - 5 and IntNet - 9,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
Hyperparameters,has,decay ratio,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
decay ratio,is,0.05,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
Hyperparameters,has,Dropout,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
Dropout,applied on,input,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
input,of,"IntNet , LSTMs , and CRF",hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
Dropout,ratio,0.5,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
Hyperparameters,adopted,same weight initialization,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
same weight initialization,as,ResNet,hyperparameters,/content/training-data/part-of-speech_tagging/1/triples/hyperparameters.txt
Contribution,has,Model,model,/content/training-data/part-of-speech_tagging/1/triples/model.txt
Model,has,funnel - shaped Int - Net,model,/content/training-data/part-of-speech_tagging/1/triples/model.txt
funnel - shaped Int - Net,explores,deeper and wider architecture,model,/content/training-data/part-of-speech_tagging/1/triples/model.txt
deeper and wider architecture,with no,down - sampling,model,/content/training-data/part-of-speech_tagging/1/triples/model.txt
down - sampling,for learning,character - to - word representations,model,/content/training-data/part-of-speech_tagging/1/triples/model.txt
character - to - word representations,from,limited supervised training corpora,model,/content/training-data/part-of-speech_tagging/1/triples/model.txt
Model,propose,IntNet,model,/content/training-data/part-of-speech_tagging/1/triples/model.txt
IntNet,has,funnel - shaped wide convolutional neural network,model,/content/training-data/part-of-speech_tagging/1/triples/model.txt
funnel - shaped wide convolutional neural network,for learning,internal structure of words,model,/content/training-data/part-of-speech_tagging/1/triples/model.txt
internal structure of words,by composing,characters,model,/content/training-data/part-of-speech_tagging/1/triples/model.txt
Model,combine,IntNet model,model,/content/training-data/part-of-speech_tagging/1/triples/model.txt
IntNet model,with,LSTM - CRF,model,/content/training-data/part-of-speech_tagging/1/triples/model.txt
LSTM - CRF,jointly decode,tags,model,/content/training-data/part-of-speech_tagging/1/triples/model.txt
tags,for,sequence labeling,model,/content/training-data/part-of-speech_tagging/1/triples/model.txt
LSTM - CRF,captures,word shape,model,/content/training-data/part-of-speech_tagging/1/triples/model.txt
LSTM - CRF,captures,context information,model,/content/training-data/part-of-speech_tagging/1/triples/model.txt
Contribution,has research problem,Sequence Labeling,research-problem,/content/training-data/part-of-speech_tagging/1/triples/research-problem.txt
Contribution,has,Results,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
Results,shows that,results,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
results,of,LSTM - CRF,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
LSTM - CRF,are,significantly improved,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
significantly improved,after adding,character - to - word models,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
Results,has,Character - to - word Models,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
Character - to - word Models,add,residual connections,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
residual connections,to,char - CNN - 9,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
char - CNN - 9,as,char - ResNet - 9,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
char - CNN - 9,confirms that,residual connections,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
residual connections,help train,deep layers,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
Character - to - word Models,improve,char - ResNet - 9,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
char - ResNet - 9,by changing,residual connections,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
residual connections,into,dense connection blocks,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
dense connection blocks,as,char - DenseNet - 9,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
dense connection blocks,shows that,dense connections,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
dense connections,are,better,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
better,than,residual connections,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
residual connections,for learning,word shape information,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
Character - to - word Models,has,"proposed character - to - word model , char - IntNet - 5 and char - IntNet - 9",results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
"proposed character - to - word model , char - IntNet - 5 and char - IntNet - 9",improves,results,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
results,across,all datasets,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
Character - to - word Models,has,IntNet,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
IntNet,significantly outperforms,other character embedding models,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
Character - to - word Models,has,F1 score,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
F1 score,does not,improve much,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
improve much,when,directly add,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
directly add,has,more layers,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
Character - to - word Models,observe,char - IntNet - 5,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
char - IntNet - 5,is,more effective,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
more effective,for learning,character - to - word representations,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
character - to - word representations,in,most of the cases,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
character - to - word representations,than,char - IntNet - 9,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
Character - to - word Models,observe,some accuracy drop,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
some accuracy drop,when,continuously increase,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
continuously increase,has,depth,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
Results,has,IntNet,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
IntNet,improves,F - 1 score,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
F - 1 score,over,stateof - the - art results,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
F - 1 score,by more than,2 %,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
2 %,for,Dutch and Spanish,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
Results,in comparison with,state - of - the - art results,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
state - of - the - art results,show that,our char - IntNet,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
our char - IntNet,improves,results,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
results,across,different models and datasets,results,/content/training-data/part-of-speech_tagging/1/triples/results.txt
Contribution,Code,https://github.com/ datquocnguyen/jPTDP,code,/content/training-data/part-of-speech_tagging/6/triples/code.txt
Contribution,has,Model,model,/content/training-data/part-of-speech_tagging/6/triples/model.txt
Model,learns,latent feature representations,model,/content/training-data/part-of-speech_tagging/6/triples/model.txt
latent feature representations,shared for,POS tagging and dependency parsing tasks,model,/content/training-data/part-of-speech_tagging/6/triples/model.txt
POS tagging and dependency parsing tasks,using,BiLSTMthe bidirectional LSTM,model,/content/training-data/part-of-speech_tagging/6/triples/model.txt
Model,propose,novel neural architecture,model,/content/training-data/part-of-speech_tagging/6/triples/model.txt
novel neural architecture,for,joint POS tagging and graph - based dependency parsing,model,/content/training-data/part-of-speech_tagging/6/triples/model.txt
Contribution,has research problem,Joint POS Tagging and Graph - based Dependency Parsing,research-problem,/content/training-data/part-of-speech_tagging/6/triples/research-problem.txt
Contribution,has,Experimental setup,experimental-setup,/content/training-data/part-of-speech_tagging/6/triples/experimental-setup.txt
Experimental setup,optimize,objective function,experimental-setup,/content/training-data/part-of-speech_tagging/6/triples/experimental-setup.txt
objective function,using,"Adam ( Kingma and Ba , 2014 ",experimental-setup,/content/training-data/part-of-speech_tagging/6/triples/experimental-setup.txt
"Adam ( Kingma and Ba , 2014 )",with,default DYNET parameter settings,experimental-setup,/content/training-data/part-of-speech_tagging/6/triples/experimental-setup.txt
Experimental setup,apply,Gaussian noise,experimental-setup,/content/training-data/part-of-speech_tagging/6/triples/experimental-setup.txt
Gaussian noise,with,? = 0.2,experimental-setup,/content/training-data/part-of-speech_tagging/6/triples/experimental-setup.txt
Experimental setup,apply,word dropout rate,experimental-setup,/content/training-data/part-of-speech_tagging/6/triples/experimental-setup.txt
word dropout rate,of,0.25,experimental-setup,/content/training-data/part-of-speech_tagging/6/triples/experimental-setup.txt
Experimental setup,perform,minimal grid search,experimental-setup,/content/training-data/part-of-speech_tagging/6/triples/experimental-setup.txt
minimal grid search,of,hyper - parameters,experimental-setup,/content/training-data/part-of-speech_tagging/6/triples/experimental-setup.txt
hyper - parameters,on,English,experimental-setup,/content/training-data/part-of-speech_tagging/6/triples/experimental-setup.txt
Experimental setup,has,jPTDP,experimental-setup,/content/training-data/part-of-speech_tagging/6/triples/experimental-setup.txt
jPTDP,implemented using,DYNET v 2.0,experimental-setup,/content/training-data/part-of-speech_tagging/6/triples/experimental-setup.txt
Experimental setup,compares,POS tagging and dependency parsing results,experimental-setup,/content/training-data/part-of-speech_tagging/6/triples/experimental-setup.txt
POS tagging and dependency parsing results,of,our model jPTDP,experimental-setup,/content/training-data/part-of-speech_tagging/6/triples/experimental-setup.txt
Experimental setup,run for,30 epochs,experimental-setup,/content/training-data/part-of-speech_tagging/6/triples/experimental-setup.txt
Contribution,has,Results,results,/content/training-data/part-of-speech_tagging/6/triples/results.txt
Results,Without taking,nl,results,/content/training-data/part-of-speech_tagging/6/triples/results.txt
nl,into,account,results,/content/training-data/part-of-speech_tagging/6/triples/results.txt
nl,has,averaged LAS score,results,/content/training-data/part-of-speech_tagging/6/triples/results.txt
averaged LAS score,over,all remaining languages,results,/content/training-data/part-of-speech_tagging/6/triples/results.txt
all remaining languages,is,1.1 % absolute higher,results,/content/training-data/part-of-speech_tagging/6/triples/results.txt
1.1 % absolute higher,than,Stack - propagation 's,results,/content/training-data/part-of-speech_tagging/6/triples/results.txt
Results,shows,absolute LAS improvement,results,/content/training-data/part-of-speech_tagging/6/triples/results.txt
absolute LAS improvement,of,4.4 %,results,/content/training-data/part-of-speech_tagging/6/triples/results.txt
4.4 %,on,average,results,/content/training-data/part-of-speech_tagging/6/triples/results.txt
Results,In terms of,dependency parsing,results,/content/training-data/part-of-speech_tagging/6/triples/results.txt
dependency parsing,has,our model jPTDP,results,/content/training-data/part-of-speech_tagging/6/triples/results.txt
our model jPTDP,outperforms,Stack - propagation,results,/content/training-data/part-of-speech_tagging/6/triples/results.txt
Results,produces,about 7 % absolute lower LAS score,results,/content/training-data/part-of-speech_tagging/6/triples/results.txt
about 7 % absolute lower LAS score,than,Stack - propagation,results,/content/training-data/part-of-speech_tagging/6/triples/results.txt
Stack - propagation,on,Dutch ( nl ,results,/content/training-data/part-of-speech_tagging/6/triples/results.txt
Results,has,morphologically rich languages,results,/content/training-data/part-of-speech_tagging/6/triples/results.txt
morphologically rich languages,get,averaged improvement,results,/content/training-data/part-of-speech_tagging/6/triples/results.txt
averaged improvement,of,9.3 %,results,/content/training-data/part-of-speech_tagging/6/triples/results.txt
Results,has,jPDTP,results,/content/training-data/part-of-speech_tagging/6/triples/results.txt
jPDTP,good for,morphologically rich languages,results,/content/training-data/part-of-speech_tagging/6/triples/results.txt
morphologically rich languages,with,1.7 % higher averaged LAS,results,/content/training-data/part-of-speech_tagging/6/triples/results.txt
1.7 % higher averaged LAS,than,Stack - propagation,results,/content/training-data/part-of-speech_tagging/6/triples/results.txt
